{"title": "PDF", "author": "PDF", "url": "https://rua.ua.es/dspace/bitstream/10045/127446/1/PLN_69.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "id": "PDF", "license": "PDF", "body": "PDF", "comments": "PDF", "commentsbody": "PDF", "raw_text": "PDF", "text": "ISSN: 1135 -5948  \nArt\n\u00edculos  \nA computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the \ninterface of dependency resolution and training time  \nIria de -Dios -Flores, Marcos Garcia  ..........................................................................................................  15 \nInformation fusion for mental disorders detection: multimodal BERT against fusioning multiple BERTs Mario Ezra Arag \u00f3n, A. Pastor L \u00f3pez-Monroy, Luis C . Gonz \u00e1lez-Gurrola , Manuel Montes -y-G\u00f3 mez ..... 27 \nUn redactor asistido para adaptar textos administrativos a lenguaje claro \nIria da Cunha .............................................................................................................................................  39 \nExploiting user -frequency information for mining regionalisms in Argentinian Spanish from Twitter  \nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano  ................................ 51 \nReflexive pronouns in Spanish Universal Dependencies: from annotation to automatic morphosyntactic \nanalysis  \nJasper Degraeuwe , Patrick Goethals  .........................................................................................................  63 \nMulti- label Text Classification for Public Procurement in Spanish  \nMaria Navas -Loro, Daniel Garijo, Oscar Corcho .....................................................................................  73 \nSelecci\u00f3n de colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un filtro de interdisciplinariedad  \nEleonora Guzzi, Margarita Alonso Ramos  ................................................................................................. 83 \nCompilaci\u00f3n del corpus acad\u00e9mico de noveles en euskera HARTAeus y su explotaci\u00f3n para el estudio de \nla fraseolog\u00eda acad \u00e9mica  \nMar\u00eda Jes\u00fas Aranzabe, Antton Gurrutxaga, Igone Zabala .........................................................................  95 \nExtraction and Semantic Representation of Domain -Specific Relations in Spanish Labour Law  \nArtem Revenko, Patricia Mart\u00edn-C hozas...... .................... ................. .................... ...................... ............. 105 \nA\n Semantic -Proximity Term -Weighting Scheme for Aspect Category Detection  \nMonserrat V\u00e1zquez -Hern\u00e1ndez, Luis Villase\u00f1or -Pineda, Manuel Montes -y-G\u00f3mez  ...............................  117 \nDetecci\u00f3n de Indicios de Autolesiones No Suicidas en Informes M\u00e9dicos de Psiquiatr\u00eda Mediante el An\u00e1lisis del Lenguaje  \nJuan Martinez -Romo, Lourdes Araujo, Bla nca Reneses, J. Sevilla -Llewellyn -Jones , Ignacio Mart\u00ednez -\nCapella , Germ\u00e1n Seara- Aguilar  ..............................................................................................................  129 \nSemantic Relations Predict the Bracketing of Three -Component Multiword Terms \nJuan Rojas -Garcia  ...................................................................................................................................  141 \nEvaluating Contextualized Vectors from both Large Language Models and Compositional Strategies \nPablo Gamallo, Marcos Garcia, Iria de -Dios -Flores  ..............................................................................  153 \nAn Overview of Drugs, Diseases, Genes and Proteins in the CORD -19 Corpus \nCarlos Badenes -Olmedo, \u00c1lvaro Alo nso, Oscar Corcho  .........................................................................  165 \nTransformers for Lexical Complexity Prediction in Spanish Language  \nJenny Ortiz -Zambrano, C\u00e9sar Espin- Riofrio, Arturo Montejo -R\u00e1ez ........................................................ 177 \nBuilding a comparable corpus and a benchmark for Spanish medical text simplification  \nLeonardo Campillos -Llanos, Ana R. Terroba Reinares, Sof\u00eda Zakhir Puig, Ana Valverde -Mateos, Adri\u00e1n \nCapllonch -Carri\u00f3n  ...................................................................................................................................  189 \nIb\nerLEF 202 2: Res\u00famenes de las tareas  de evaluaci\u00f3n  \nABSAPT 2022 at IberLEF: Overview of the Task on Aspect -Based Sentiment Analysis in Portuguese  \nFelix L. V. da Silva, Guilherme da S. Xavier, Heliks M. Mensenburg, Rodrigo F. Rodrigues, Leonardo P. dos Santos, Ricardo M. Ara\u00fajo, Ulisses B. Corr\u00eaa, Larissa A. de Freitas  ...............................................  199 \nOverview of DA -VINCIS at IberLEF 2022: Detection of Agg ressive and Violent Incidents from Social \nMedia in Spanish  \nLuis Joaqu\u00edn Arellano, Hugo Jair Escalante, Luis Villase\u00f1or -Pineda, Manuel Montes -y-G\u00f3mez, \nFernando Sanchez -Vega ...........................................................................................................................  207 \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022  \nISSN: 1135 -5948  \n \n \nComit\u00e9 Editorial  \n \nConsejo de redacci\u00f3n  \nL. Alfonso Ure\u00f1a L\u00f3pez  Universidad de Ja\u00e9n  laurena@ujaen.es (Director)  \nPatricio Mart\u00ednez Barco  Universidad de Alicante  patricio@dlsi.ua.es  (Secretario)  \nManuel Palomar Sanz  Universidad de Alicante  mpalomar@dlsi.ua.es  \nFelisa Verdejo Ma\u00edllo   UNED  felisa@lsi.uned.es  \n \n ISSN : 1135- 5948\n \nISSN electr\u00f3nico : 1989- 7553  \nDep\u00f3sito Legal : B:3941-91  \nEditado en:  Universidad de Ja\u00e9n  \nA\u00f1o de edici\u00f3n:  2022  \nEditores:  Miguel A. Alonso  Universidad de A Coru\u00f1a   miguel.alonso@udc.es  \n Margarita Alonso -Ramos  Universidad de A Coru\u00f1a   margarita.alonso@udc.es  \n Carlos G\u00f3mez-Rodr\u00edguez  Universidad d e A Coru\u00f1a   carlos.gomez@udc.es  \n David Vilares  Universidad de A Coru\u00f1a   david.vilares@udc.es  \n Jes\u00fas Vilares  Universidad de A Coru\u00f1a   jesus.vilares@udc.es  \n \n \nPublicado por: Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural  \nDepartamento de Inform\u00e1tica. Universidad de Ja\u00e9n  \nCampus Las Lagunillas, EdificioA3. Despacho 127. 23071 Ja\u00e9n  \nsecretaria.sepln@ujaen.es  \n \n Consejo asesor\n \nMargarita Alonso -Ramos  Universidad de A Coru\u00f1a y CITIC (Espa\u00f1a) \nXabier Arregi  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nManuel de Buenaga  Universidad de Alcal\u00e1 (Espa\u00f1a) \nJose Camacho Collados  Cardiff University (Reino Unido)  \nSylviane Cardey -Greenfield  Centre de recherche en linguistique et traitement automatique des langues (Francia)  \nIrene Castell\u00f3n  Universidad de Barcelona (Espa\u00f1a)  \nArantza D\u00edaz de Ilarraza  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nAntonio Ferr\u00e1ndez  Universidad de Alicante (Espa\u00f1a)  \nAlexander Gelbukh  Instituto Polit\u00e9cnico Nacional  (M\u00e9xico)  \nKoldo Gojenola  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nXavier G\u00f3mez Guinovart  Universidad de Vigo (Espa\u00f1a)  \nCarlos G\u00f3mez-Rodr\u00edguez  Universidad de A Coru\u00f1a y CITIC (Espa\u00f1a) \nJos\u00e9 Miguel Go\u00f1i  Universidad Polit\u00e9cnica de Madrid (Espa\u00f1a)  \nInma Hernaez Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nElena Lloret Universidad de Alicante (Espa\u00f1a)  \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022Ram\u00f3n L\u00f3pez -C\u00f3zar Delgado  Universidad de Granada (Espa\u00f1a) \nBernardo Magnini  Fondazione Bruno Kessler (Italia)  \nNuno J. Mamede Instituto de Engenharia de Sistemas e Computadores \n(Portugal)  \nM. Antonia Mart\u00ed Universidad de Barcelona (Espa\u00f1a) \nM. Teresa Mart\u00edn Valdivia Universidad de Ja\u00e9n (Espa\u00f1a) \nPatricio Mart\u00ednez -Barco  Universidad de Alicante (Espa\u00f1a)  \nEugenio Mart\u00ednez C\u00e1mara  Universidad de Granada (Espa\u00f1a) \nPaloma Mart\u00ednez Fern\u00e1ndez Universidad Carlos III (Espa\u00f1a)  \nRaquel Mart\u00ednez Unanue Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) \nRuslan Mitkov  University of Wolverhampton (Reino Unido)  \nManuel Montes y G\u00f3mez  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico)  \nMariana Lara Neves  Bundesinstitut f\u00fcr Risikobewertung (Alemania)  \nLlu\u00eds Padr\u00f3  Universidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a)  \nManuel Palomar  Universidad de Alicante (Espa\u00f1a)  \nFerr\u00e1n Pla  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nGerman Rigau  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nHoracio Rodr\u00edguez  Universidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a)  \nPaolo Rosso  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nLeonel Ruiz Miyares  Centro de Ling\u00fc\u00edstica Aplicada de Santiago de Cuba (Cuba) \nHoracio Saggion  Universidad Pompeu Fabra (Espa\u00f1a)  \nEmilio Sanch\u00eds  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nEncarna Segarra Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nThamar Solorio  University of Houston (Estados Unidos de Am\u00e9rica)  \nMaite Taboada Simon Fraser University (Canad\u00e1)  \nMariona Taul\u00e9 Universidad de Barcelona (Espa\u00f1a) \nJuan-Manuel Torres-Moreno  Laboratoire Informatique d\u2019Avignon / Universit\u00e9 d\u2019Avignon (Francia)  \nJos\u00e9 Antonio Troyano Jim\u00e9nez  Universidad de Sevilla (Espa\u00f1a)  \nL. Alfonso Ure\u00f1a L\u00f3pez Universidad de Ja\u00e9n (Espa\u00f1a) \nRafael Valencia Garc\u00eda  Universidad de Murcia (Espa\u00f1a) \nRen\u00e9 Venegas Vel\u00e1sques  Pontificia Universidad Cat\u00f3lica de Valpara\u00edso (Chile)  \nFelisa Verdejo Ma\u00edllo  Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a) \nKarin Vespoor  University of Melbourne (Australia)  \nManuel Vilares  Universidad de  Vigo  (Espa\u00f1a)  \nLuis Villase\u00f1or -Pineda  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico)  \n \n Revisores adicionales\n \nLaura Alonso Alemany  Universidad Nacional de C\u00f3rdoba  (Argentina)  \nAna-Maria Bucur  University of Bucharest  (Ruman\u00eda) \n\u00d3scar Araque Iborra Universidad Polit\u00e9cnica de Madrid  (Espa\u00f1a) \nMarco Casavantes  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico)  \nRiccardo Cervero  Universidad  Polit\u00e9 cnica de Val encia (Espa\u00f1a) \nElisabet  Comelles  Universidad  de Barcelona (Espa\u00f1a) \nLaritza Coello  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico)  \nV\u00edctor Manuel Darriba Bilbao  Universidad de Vigo  (Espa\u00f1a)  \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalAgust\u00edn Daniel Delgado Mu\u00f1oz  Universidad Nacional de Educaci\u00f3n a Distancia \n(Espa\u00f1a) \nAndr\u00e9s Duqu e Universidad Nacional de Educaci\u00f3n a Distancia \n(Espa\u00f1a) \nMiguel Angel Garc\u00eda Cumbreras  Universidad de Ja\u00e9n  (Espa\u00f1a)  \nJos\u00e9 Antonio Garc\u00eda -D\u00edaz  Universidad de Murcia (Espa\u00f1a) \nJuan Luis Garc\u00eda Mendoza  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica \n(M\u00e9xico)  \nDelia Iraz\u00fa Hern\u00e1ndez-Farias  Universidad de Guanajuato  (M\u00e9 xico)  \nSalud Mar\u00eda Jim\u00e9nez-Zafra Universidad de Ja\u00e9n  (Espa\u00f1a)  \nArturo Montejo-R\u00e1ez  Universidad de Ja\u00e9n  (Espa\u00f1a)  \nArantxa Otegi  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nDavid Owen  Cardiff University  (Reino Unido)  \nJos\u00e9 M. Perea -Ortega  Universidad de Extremadura  (Espa\u00f1a)  \nFlor-Miriam Plaza -del-Arco Universidad de Ja\u00e9n  (Espa\u00f1a)  \nFrancisco J. Ribadas-Pena  Universidad de Vigo  (Espa\u00f1a)  \nGiulia Rizzi  Universit\u00e0 degli studi di Milano-Bicocca  (Italia)  \nJuan Fernando S\u00e1nchez Rada  Universidad Polit\u00e9cnica de Madrid  (Espa\u00f1a) \nDavid Vilares  Universidad de A Coru\u00f1a y CITIC (Espa\u00f1a) \n \n  \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural  ISSN: 1135 -5948          \n \n \n \nPre\u00e1mbulo  \n \nLa revista Procesamiento del Lenguaje Natural  pretende ser un foro de publicaci\u00f3n de art\u00edculos \ncient\u00edfico-t\u00e9cnicos in\u00e9ditos de calidad relevante en el \u00e1mbito del Procesamiento de Lenguaje \nNatural (PLN) tanto para la comunidad cient\u00edfica nacional e internacional, como para las empresas del sector. Adem\u00e1s, se quiere potenciar el desarrollo de las diferentes \u00e1reas relacionadas con el PLN, mejorar la divulgaci\u00f3n de las investigaciones que se llevan a cabo, identificar las futuras directrices de la investigaci\u00f3n b\u00e1sica y mostrar las posibilidades reales de aplicaci\u00f3n en \neste campo. Anualmente la SEPLN (Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural) publica dos n\u00fameros de la revista, que incluyen art\u00edculos originales, presentaciones de proyectos en marcha, rese\u00f1as bibliogr\u00e1ficas y res\u00famenes de tesis doctorales. Esta revista se \ndistribuye gratuitamente a todos los socios, y con el fin de conseguir una mayor expansi\u00f3n y facilitar el acceso a la publicaci\u00f3n, su contenido es libremente accesible por Internet.  \n \nLas \u00e1reas tem\u00e1ticas tratadas son las siguientes:  \n\u2022 Modelos ling\u00fc\u00edsticos, matem\u00e1ticos y psicoling\u00fc\u00edsticos del lenguaje  \n\u2022 Ling\u00fc\u00edstica de corpus  \n\u2022 Desarrollo de recursos y herramientas ling\u00fc\u00edsticas  \n\u2022 Gram\u00e1ticas y formalismos para el an\u00e1lisis morfol\u00f3gico y sint\u00e1ctico  \n\u2022 Sem\u00e1ntica, pragm\u00e1tica y discurso  \n\u2022 Lexicograf\u00eda y terminolog\u00eda computacional  \n\u2022 Resoluci\u00f3n de la ambig\u00fcedad l\u00e9xica  \n\u2022 Aprendizaje autom\u00e1tico en PLN \n\u2022 Generaci\u00f3n textual monoling\u00fce y multiling\u00fce  \n\u2022 Traducci\u00f3n autom\u00e1tica  \n\u2022 Reconocimiento y s\u00edntesis del habla \n\u2022 Extracci\u00f3n y recuperaci\u00f3n de informaci\u00f3n monoling\u00fce, multiling\u00fce y multimodal  \n\u2022 Sistemas de b\u00fasqueda de respuestas  \n\u2022 An\u00e1lisis autom\u00e1tico del contenido textual \n\u2022 Resumen autom\u00e1tico  \n\u2022 PLN para la generaci\u00f3n de recursos educativos \n\u2022 PLN para lenguas con recursos limitados \n\u2022 Aplicaciones industriales del PLN \n\u2022 Sistemas de di\u00e1logo  \n\u2022 An\u00e1lisis de sentimientos y opiniones  \n\u2022 Miner\u00eda de texto  \n\u2022 Evaluaci\u00f3n de sistemas de PLN \n\u2022 Implicaci\u00f3n textual y par\u00e1frasis  \n El ejemplar n\u00famero 6 9 de la revista Procesamiento del Lenguaje Natural  contiene trabajos \ncorrespondientes a dos apartados diferentes: comunicaciones cient\u00edficas y res\u00famenes de las tareas \nde evaluaci\u00f3n competitiva de la edici\u00f3n del a\u00f1o 202 2 del foro de evaluaci\u00f3n Iberian Language \nEvaluation Forum  (IberLEF). Todos ellos han sido aceptados mediante el proceso de revisi\u00f3n \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022tradicional en la revista. Queremos agradecer a los miembros del Comit\u00e9 Asesor y a los revisores \nadicionales la labor que han realizado.  \n \nSe recibieron 40 trabajos para este n\u00famero, de los cuales 30 eran art\u00edculos cient\u00edficos y 10 res\u00fames \nde las tareas de evaluaci\u00f3n competitiva del foro de evaluaci\u00f3n IberLEF 2022 . De entre los 30 \nart\u00edculos recibidos, 1 6 han sido finalmente seleccionados para su publicaci\u00f3n, lo cual fija una tasa \nde aceptaci\u00f3n del 53%.  \n El Comit\u00e9 Asesor de la revista se ha hecho cargo de la revisi\u00f3n de los trabajos. Este proceso de \nrevisi\u00f3n es de doble anonimato: se  mantiene oculta la identidad de los autores que son evaluados \ny de los revisores que realizan las evaluaciones.  En un primer  paso, cada art\u00edculo ha sido \nexaminado de manera ciega o an\u00f3nima por tres revisores. En un segundo paso, para aquellos art\u00edculos que ten\u00edan una divergencia m\u00ednima de tres puntos (sobre siete) en sus puntuaciones, sus tres revisores han reconsiderado su evaluaci\u00f3n en conjunto. Finalmente, la evaluaci\u00f3n de aquellos art\u00edculos que estaban en posici\u00f3n muy cercana a la frontera de aceptaci\u00f3n ha sido supervisada por \nm\u00e1s miembros del comit\u00e9 editorial. El criterio de corte adoptado ha sido la media de las tres \ncalificaciones, siempre y cuando hayan sido iguales o superiores a 5 sobre 7.  \n La elaboraci\u00f3n de este n\u00famero ha contado con la aportaci\u00f3n del Vicerrectorado de Pol\u00edtica Cient\u00edfica, Investigaci\u00f3n y Transferencia de la Universidad de A Coru\u00f1a, con cofinanciaci\u00f3n del \nConvenio de Acciones Estrat\u00e9gicas I+D+i para 2022 entre la Conseller\u00eda de Cultura, Educaci\u00f3n y Universidad de la Xunta de Galicia y la Universidad de A Coru\u00f1a. \n    \n \nSeptiembre de 2022\n \nLos editores.  \n \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural ISSN: 1135 -5948          \n \n \n \nPreamble  \n \nThe Natural Language Processing journal aims to be a forum for the publication of high- quality \nunpublished scientific and technical papers on Natural Language Processing (NLP) for both the \nnational and international scientific community and companies. Furthermore, we want to \nstrengthen the development of different areas related to NLP, widening the dissemination of \nresearch carried out, identifying the future directions of basic research and demonstrating the \npossibilities of its application in this field. Every year, the Spanish Society for Natural Language \nProcessing (SEPLN) publishes two issues of the journal that include original articles, ongoing projects, book reviews and summaries of doctoral theses. All issues published are freely \ndistributed to all members, and contents are freely available online.\n \n \nThe subject areas addressed are the following:  \n \n\u2022 Linguistic, Mathematical and Psychological models to language  \n\u2022 Grammars and Formalisms for Morphological and Syntactic Analysis  \n\u2022 Semantics, Pragmatics and Discourse  \n\u2022 Computational Lexicography and Terminology  \n\u2022 Linguistic resources and tools  \n\u2022 Corpus Linguistics  \n\u2022 Speech Recognition and Synthesis  \n\u2022 Dialogue Systems  \n\u2022 Machine Translation  \n\u2022 Word Sense Disambiguation  \n\u2022 Machine Learning in NLP  \n\u2022 Monolingual and multilingual Text Generation  \n\u2022 Information Extraction and Information Retrieval  \n\u2022 Question Answering  \n\u2022 Automatic Text Analysis  \n\u2022 Automatic Summarization  \n\u2022 NLP Resources for Learning  \n\u2022 NLP for languages with limited resources  \n\u2022 Business Applications of NLP  \n\u2022 Sentiment Analysis  \n\u2022 Opinion Mining  \n\u2022 Text Mining  \n\u2022 Evaluation of NLP systems  \n\u2022 Textual Entailment and Paraphrases  \n \nThe 6 9th issue of the Procesamiento del Lenguaje Natural  journal contains scientific papers and \nsummaries of the shared -tasks of the edition of 202 2 of the evaluation forum Ib erian Languages \nEvaluation Forum (IberLEF). All of these were accepted by a peer review process. We would \nlike to thank the Advisory Committee members and additional reviewers for their work.  \n \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022Forty  papers were submitted for this issue, from which thirty  were scientific papers and ten  were \nsummaries of the evaluation tasks of the evaluation forum IberLEF 2022. From these thirty  \npapers, we selected sixteen  (53%) for publication.  \n \nThe Advisory Committee of the journal has reviewed the papers in a double -blind process. Under \ndouble -blind review the identity of the reviewers and the authors are hidden from each other. In \nthe first step, each paper was reviewed blindly by three reviewers. In the second step, the three \nreviewers have given a second ove rall evaluation of those papers with a difference of three or \nmore points out of seven in their individual reviewer scores. Finally, the evaluation of those papers \nthat were in a position very close to the acceptance limit were supervised by the editorial board. \nThe cut -off criterion adopted was the mean of the three scores given, as long as it is equal or \ngreater than 5 out of 7.  \n The preparation of this issue  has been supported partially by the Vice -Rectorate for Science \nPolicy, Research and Transfer of the University of A Coru\u00f1a, with co-funding from the R&D \nAgreement on Strategic Actions for 2022 between the Department of Culture, Education and \nUniversity of  the Xunta de Galicia and the University of A Coru\u00f1a.  \n   \nSeptember  2022\n \nEditorial board.  \n \n \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalISSN: 1135 -5948  \nA\nrt\u00edculos  \nA computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the \ninterface of dependency resolution and training time  \nIria de -Dios -Flores, Marcos Garcia  ..........................................................................................................  15 \nInformation fusion for mental disorders detection: multimodal BERT against fusioning multiple BERTs Mario Ezra Arag \u00f3n, A. Pastor L \u00f3pez-Monroy, Luis C . Gonz \u00e1lez-Gurrola , Manuel Montes -y-G\u00f3 mez ..... 27 \nUn redactor asistido para adaptar textos administrativos a lenguaje claro \nIria da Cunha .............................................................................................................................................  39 \nExploiting user -frequency information for mining regionalisms in Argentinian Spanish from Twitter  \nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano  ................................ 51 \nReflexive pronouns in Spanish Universal Dependencies: from annotation to automatic morphosyntactic analysis  \nJasper Degraeuwe , Patrick Goethals  .........................................................................................................  63 \nMulti- label Text Classification for Public Procurement in Spanish  \nMaria Navas -Loro, Daniel Garijo, Oscar Corcho .....................................................................................  73 \nSelecci\u00f3n de colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un filtro de interdisciplinariedad  \nEleonora Guzzi, Margarita Alonso Ramos  ................................................................................................. 83 \nCompilaci\u00f3n del corpus acad\u00e9mico de noveles en euskera HARTAeus y su explotaci\u00f3n para el estudio de \nla fraseolog\u00eda acad \u00e9mica  \nMar\u00eda Jes\u00fas Aranzabe, Antton Gurrutxaga, Igone Zabala .........................................................................  95 \nExtraction and Semantic Representation of Domain -Specific Relations in Spanish Labour Law  \nArtem Revenko, Patricia Mart\u00edn-C hozas ......... ................. ................. .................... ...................... ............. 105 \nA\n Semantic -Proximity Term -Weighting Scheme for Aspect Category  Detection  \nMonserrat V\u00e1zquez -Hern\u00e1ndez, Luis Villase\u00f1or -Pineda, Manuel Montes -y-G\u00f3mez  ...............................  117 \nDetecci\u00f3n de Indicios de Autolesiones No Suicidas en Informes M\u00e9dicos de Psiquiatr\u00eda Mediante el \nAn\u00e1lisis del Lenguaje  \nJuan Martinez -Romo, Lourdes Araujo, Blanca Reneses, J . Sevilla -Llewellyn -Jones , Ignacio Mart\u00ednez -\nCapella , Germ\u00e1n Seara- Aguilar  ..............................................................................................................  129 \nSemantic Relations Predict the Bracketing of Three -Component Multiword Terms \nJuan Rojas -Garcia  ...................................................................................................................................  141 \nEvaluating Contextualized Vectors from  both Large Language Models and Compositional Strategies \nPablo Gamallo, Marcos Garcia, Iria de -Dios -Flores  ..............................................................................  153 \nAn Overview of Drugs, Diseases, Genes and Proteins in the CORD -19 Corpus \nCarlos Badenes -Olmedo, \u00c1lvaro Alonso , Oscar Corcho  .........................................................................  165 \nTransformers for Lexical Complexity Prediction in Spanish Language  \nJenny Ortiz -Zambrano, C\u00e9sar Espin- Riofrio, Arturo Montejo -R\u00e1ez ........................................................ 177 \nBuilding a comparable corpus and a benchmark for Spanish medical text simplification  \nLeonardo Campillos -Llanos, Ana R. Terroba Reinares, Sof\u00eda Zakhir Puig, Ana Valverde -Mateos, Adri\u00e1n \nCapllonch -Carri\u00f3n  ...................................................................................................................................  189 \nI\nberLEF 202 2: Res\u00famenes de las tareas  de evaluaci\u00f3n  \nABSAPT 2022 at IberLEF: Overview of the Task on Aspect -Based Sentiment Analysis in Portuguese  \nFelix L. V. da Silva, Guilherme da S. Xavier, Heliks M. Mensenburg, Rodrigo F. Rodrigues , Leonardo P. \ndos Santos, Ricardo M. Ara\u00fajo, Ulisses B. Corr\u00eaa, Larissa A. de Freitas  ...............................................  199 \nOverview of DA -VINCIS at IberLEF 2022: Detection of Aggressive and Violent Incidents from Social \nMedia in Spanish  \nLuis Joaqu\u00edn Arellano, Hugo Jair Escalante, Luis Vil lase\u00f1or -Pineda, Manuel Montes -y-G\u00f3mez, \nFernando Sanchez -Vega ...........................................................................................................................  207 \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022Overview of DETESTS at IberLEF 2022: DETEction and classification of racial STereotypes in Spanish  \nAlejandro Ariza -Casabona, Wolfgang S. Schmeisser -Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique \nAmig\u00f3, Berta Chulvi, Paolo Rosso  ...........................................................................................................  217 \nOverview of EXIST 2022: sEXism Identification in Social neTworks  \nFrancisco Rodr\u00edguez -S\u00e1nchez, Jorge Carrillo- de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta- Arag\u00f3n, \nGuillermo Marco -Rem\u00f3n, Maryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n ..................................................................................................................................................................  229 \nMention detection, normalization & classification of species, pathogens, humans and food in clinical \ndocuments: Overview of the LivingNER shared task and resources  \nAntonio Miranda- Escalada, Eul\u00e0lia Farr\u00e9- Maduell, Salvador Lima -L\u00f3pez, Darryl Estrada, Luis G asc\u00f3, \nMartin Krallinger  .....................................................................................................................................  241 \nOverview of PAR -MEX at Iberlef 2022: Paraphrase Detection in Spanish Shared Task  \nGemma Bel -Enguix, Gerardo Sierra, Helena G\u00f3mez -Adorno, Juan -Manuel Torres -Moreno, Jesus -\nGerman Ortiz -Barajas, Juan V\u00e1squez  ......................................................................................................  255 \nOverview of PoliticE s 2022: Spanish Author Profiling for Political Ideology  \nJos\u00e9 Antonio Garc\u00eda- D\u00edaz, Salud Mar\u00eda Jim\u00e9nez -Zafra, Mar\u00eda -Teresa Mart\u00edn Valdivia, Francisco \nGarc\u00eda -S\u00e1nchez, L. Alfonso Ure\u00f1a- L\u00f3pez, Rafael Valencia- Garc\u00eda  ........................................................ 265 \nOverview of QuALES at IberLEF 2022: Ques tion Answering Learning from Examples in Spanish  \nAiala Ros\u00e1, Luis Chiruzzo, Luc\u00eda Bouza, Alina Dragonetti, Santiago Castro, Mathias Etcheverry, Santiago G\u00f3ngora, Santiago Goycoechea, Juan Machado, Guillermo Moncecchi, Juan Jos\u00e9 Prada, Dina \nWonsever  .................................................................................................................................................. 273 \nOverview of ReCoRES at IberLEF 2022: Reading Comprehension and Reasoning Explanation for \nSpanish  \nMarco Antonio Sobrevilla Cabezudo, Diego Diestra, Rodrigo L\u00f3pez, Erasmo G\u00f3mez, Arturo Oncevay, Fernando Alva- Manchego ........................................................................................................................  281 \nOverview of Rest -Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid \nSemaphore Prediction for Mexican Tourist Texts  \nMiguel \u00c1. \u00c1lvarez -Carmona, \u00c1ngel D\u00edaz -Pacheco, Ram\u00f3n Aranda, Ansel Y. Rodr\u00edguez -Gonz\u00e1lez, Daniel \nFajardo- Delgado, Rafae l Guerrero -Rodr\u00edguez, L\u00e1zaro Bustio- Mart\u00ednez  ................................................  289 \n \nInformaci\u00f3n General  \nInformaci\u00f3n para los autores  ....................................................................................................................  303 \nInformaci\u00f3n adicional  ...............................................................................................................................  304 \n \n \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalArt\n\u00edculos  A computational psycholinguistic evaluation of the\nsyntactic abilities of Galician BERT models at the\ninterface of dependency resolution and training time\nUna evaluaci\u00b4 on psicoling\u00a8 u\u00b4 \u0131stico-computacional de las\ncapacidades sint\u00b4 acticas de los modelos BERT para el gallego\nen la intersecci\u00b4 on entre la resoluci\u00b4 on de dependencias y el\ntiempo de entrenamiento\nIria de-Dios-Flores, Marcos Garcia\nCentro Singular de Investigaci\u00b4 on en Tecnolox\u00b4 \u0131as Intelixentes (CiTIUS)\nUniversidade de Santiago de Compostela\n{iria.dedios, marcos.garcia.gonzalez}@usc.gal\nAbstract: This paper explores the ability of Transformer models to capture subject-\nverb and noun-adjective agreement dependencies in Galician. We conduct a series\nof word prediction experiments in which we manipulate dependency length together\nwith the presence of an attractor noun that acts as a lure. First, we evaluate the over-\nall performance of the existing monolingual and multilingual models for Galician.\nSecondly, to observe the effects of the training process, we compare the different de-\ngrees of achievement of two monolingual BERT models at different training points.\nWe also release their checkpoints and propose an alternative evaluation metric. Our\nresults confirm previous findings by similar works that use the agreement prediction\ntask and provide interesting insights into the number of training steps required by\na Transformer model to solve long-distance dependencies.\nKeywords: BERT models, Galician, targeted syntactic evaluation, agreement de-\npendencies.\nResumen: Este trabajo analiza la capacidad de los modelos Transformer para cap-\nturar las dependencias de concordancia sujeto-verbo y sustantivo-adjetivo en gallego.\nLlevamos a cabo una serie de experimentos de predicci\u00b4 on de palabras manipulando\nla longitud de la dependencia junto con la presencia de un sustantivo intermedio que\nact\u00b4 ua como distractor. En primer lugar, evaluamos el rendimiento global de los mod-\nelos monoling\u00a8 ues y multiling\u00a8 ues existentes para el gallego. En segundo lugar, para\nobservar los efectos del proceso de entrenamiento, comparamos los diferentes grados\nde consecuci\u00b4 on de dos modelos monoling\u00a8 ues BERT en diferentes puntos del entre-\nnamiento. Adem\u00b4 as, publicamos sus puntos de control y proponemos una m\u00b4 etrica\nde evaluaci\u00b4 on alternativa. Nuestros resultados confirman los hallazgos anteriores de\ntrabajos similares que utilizan la tarea de predicci\u00b4 on de concordancia y proporcionan\nuna visi\u00b4 on interesante sobre el n\u00b4 umero de pasos de entrenamiento que necesita un\nmodelo Transformer para resolver las dependencias de larga distancia.\nPalabras clave: Modelos BERT, Gallego, evaluaci\u00b4 on sint\u00b4 actica dirigida, depen-\ndencias de concordancia.\n1 Introduction\nCurrent language models (LMs) based on\ndeep neural network architectures obtain im-\npressive performance on most NLP tasks, in-\ncluding semantic and syntactic applications\n(Devlin et al., 2019). In fact, it has been\nargued that LSTM and Transformer modelsmay encode syntactic information captured\nin an unsupervised manner from unlabeled\ntext (Lin, Tan, and Frank, 2019; Hewitt and\nManning, 2019).\nTo explore the syntactic capabilities of\nLMs, various studies have probed their gram-\nmatical competence by analyzing the reso-\nlution of long-distance dependencies using a\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 15-26\nrecibido 31-03-2022 revisado 19-05-2022 aceptado 23-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-1\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalword prediction task, often known as tar-\ngeted syntactic evaluation or TSE (Linzen,\nDupoux, and Goldberg, 2016; Gulordava et\nal., 2018). Inspired by classical psycholin-\nguistic experiments on human sentence pro-\ncessing, this task consists on comparing the\nmodel probabilities for a correct and incor-\nrect alternative in the context of the targeted\nsyntactic phenomena. For instance, given the\nwell-known sentence in psycholinguistic re-\nsearch \u201cThe key to the cabinets is|*are on\nthe table\u201d, a model that correctly identifies\nthe dependency between the subject (key)\nand the verb should assign a higher proba-\nbility to the singular form (is) than to the\nplural form (are), despite the presence of an\nintervening plural attractor noun (cabinets).\nFirst studies have shown that recurrent\nneural networks (RNNs) are able to solve\nmost cases of the agreement prediction task\nin several languages (though mostly in En-\nglish) and scenarios (Bernardy and Lappin,\n2017; Gulordava et al., 2018; Kuncoro et al.,\n2018b). The interest in the assessment of\nsyntactic abilities of language models grew\nwith the popularization of Transformer archi-\ntectures (Vaswani et al., 2017), which learn\nrelations between words using a self-attention\nmechanism, and seem to perform better than\nmodels based on RNNs (Devlin et al., 2019).\nIn this respect, recent works are putting\nthe focus on the training procedure: P\u00b4 erez-\nMayos, Ballesteros, and Wanner (2021) mea-\nsure the impact of the amount of training\ndata on syntactic probing, while Wei et al.\n(2021) analyze the influence of word fre-\nquency on subject-verb number agreement.\nHowever, to the best of our knowledge, there\nare still no studies exploring the models\u2019 per-\nformance along the training process, i.e., how\nmany training steps do they need to solve\nlong-distance dependencies. This is one of\nthe goals of the present work.\nOn the evaluation side, it has been argued\nthat instead of comparing the probability of\na single correct|incorrect pair (as is|are in the\nexample above), these experiments should\nuse large lists of pairs representing the same\nphenomena (e.g., exist|exists) to observe the\nmodel\u2019s systematicity (i.e., in how many pairs\na model succeeds) and its likely behaviour\n(the probability of generating a correct inflec-\ntion) (Newman et al., 2021). Nevertheless,\nthis type of evaluation requires large sets of\ntarget pairs and, what is more substantial, itassumes a total independence between syntax\nand semantics \u2014something which is contro-\nversial from a linguistic and psycholinguistic\npoint of view.\nTaking the above into account, in this\npaper we investigate the ability of Trans-\nformer models to capture fundamental lin-\nguistic operations such as dependency reso-\nlution in a less studied language, Galician.\nFollowing previous research inspired by both\ncomputational modeling and psycholinguis-\ntic research, we conduct a series of word\nprediction experiments using a dataset that\ntargets two types of agreement dependen-\ncies (subject-verb dependencies and noun-\nadjective dependencies) in two experimental\nconditions (short and long-distance depen-\ndencies) while also manipulating the presence\nof an attractor noun that acts as a lure (e.g.\n\u201cOneno que xogaba onte al\u00b4 \u0131 coa nena \u00b4 e\nalto|*alta\u201d).1First, we evaluate the over-\nall performance of the existing monolingual\nand multilingual models for Galician. Sec-\nondly, in order to observe the effects of the\ntraining process, we compare the different\ndegrees of achievement of two monolingual\nBERT models (which vary on the number of\nhidden layers, vocabulary size, and initializa-\ntion) at various training steps.\nIn addition, we propose an alternative\nevaluation metric of accuracy, which puts the\nfocus on the probability distance between the\ncorrect and the incorrect alternatives for a\ngiven semantic plausible word. This met-\nric allows us to explore a model\u2019s confidence\nin producing syntactically and semantically\nwell-formed expressions and to compare mod-\nels with different vocabulary sizes.\nOur contributions are the following: (i) 34\ncheckpoints of two BERT models for Galician\nwhich allow to explore the effects of the train-\ning steps on different tasks; (ii) a novel met-\nric for targeted syntactic evaluation which fo-\ncuses on the probability distribution between\ncorrect and incorrect alternatives; (iii) a care-\nful comparison of the models\u2019 performance on\ntwo agreement dependencies, analyzing the\nimpact of the learning steps, the amount of\ntraining data, the model initialization, and\nthe depth of the neural network.\n1Here alto (\u2018tall\u2019 in masculine) agrees in its gen-\nder features with the correct antecedent neno (\u2019boy\u2019),\nwhile alta (\u2018tall\u2019 in feminine) agrees in gender with\nthe structurally irrelevant noun nena (\u2019girl\u2019), hence\nproducing an ungrammatical dependency.\n16\nIria de-Dios-Flores, Marcos Garcia Our results confirm previous findings by\nsimilar works using the agreement prediction\ntask and provide interesting insights into the\nnumber of training steps required by a Trans-\nformer model to solve long-distance depen-\ndencies, as they already achieve high perfor-\nmance at early checkpoints when trained on\nenough data.\nThe rest of this paper is organized as\nfollows: Section 2 introduces previous work\nabout targeted syntactic evaluation on neu-\nral language models. Then, in Section 3, we\npresent the main characteristics of the mod-\nels used for the experiments and the differ-\nent checkpoints provided by our study. In\nSection 4 we describe our methodology, in-\ncluding the rationale behind the evaluation\nmetric proposed here. Finally, the results are\npresented and discussed in Section 5, while\nSection 6 draws the conclusions of the work.\n2 Background\nLinzen, Dupoux, and Goldberg (2016) intro-\nduced the number prediction task to evaluate\nthe performance of language models on long-\ndistance agreement, and their results sug-\ngested that even if LSTM models seem not to\ngeneralize syntactic structures, they identify\nsubject-verb agreement dependencies. In-\nspired by this paper, various studies explored\nthe behaviour of LSTMs models on a variety\nof languages and syntactic phenomena, ar-\nguing that these networks may achieve near-\nhuman performance in some agreement ex-\nperiments (Bernardy and Lappin, 2017; Gu-\nlordava et al., 2018; Kuncoro et al., 2018b)\neven though they may be alternative explana-\ntions (such as surface-based heuristics) that\nexplain the models\u2019 success (Kuncoro et al.,\n2018a; Linzen and Leonard, 2018). Moving\nforward, Marvin and Linzen (2018) published\na new dataset in English which includes not\nonly subject-verb agreement items, but also\nother dependencies (e.g. anaphora, negative\npolarity items) and more complex construc-\ntions. Their results showed that although\nthe behavior of various RNNs on this dataset\nis far from human performance, they ob-\ntain competitive results in various settings.\nTaking a different approach, Lakretz et al.\n(2019) were able to identify individual cells\non a LSTM model which encode information\nabout grammatical number and plurality in\nEnglish, suggesting that the network effec-\ntively captures some morphosyntactic infor-mation from raw text.\nThe growing interest in this research area\nmotivated the release of SyntaxGym2, an\nonline platform for targeted evaluation of\nlanguage models (Gauthier et al., 2020),\nas well as datasets in different languages,\nsuch as Mueller et al. (2020) (for En-\nglish, French, German, Hebrew, and Rus-\nsian), P\u00b4 erez-Mayos et al. (2021) (for Span-\nish), or Garcia and Crespo-Otero (2022) (for\nGalician and Portuguese), which is the one\nused in this work.\nUnlike LSTMs, Transformer architectures\n(Vaswani et al., 2017) use a non recurrent\nneural network which learns relations be-\ntween words using a self-attention mecha-\nnism, and they can be interpreted as induced-\nstructure models (Henderson, 2020). On a\ncomparison of LSTM and Transformer ar-\nchitectures, Tran, Bisazza, and Monz (2018)\nfound that the former slightly outperform\nTransformers on English subject-verb agree-\nment. In this respect, the release of large\nTransformer-based models, such as BERT\n(Devlin et al., 2019) and its variants, gave rise\nto a larger interest in exploring their linguis-\ntic abilities. Given that training these mod-\nels is computationally expensive, most stud-\nies explore publicly available resources (Gold-\nberg, 2019; Mueller et al., 2020). Among oth-\ners, P\u00b4 erez-Mayos, Ballesteros, and Wanner\n(2021) have shown that more training data\nyields better performance in most syntac-\ntic tasks in English, and P\u00b4 erez-Mayos et al.\n(2021) compared multilingual and monolin-\ngual models in English and Spanish: the re-\nsults seem to indicate that the syntactic gen-\neralization of each model type is language-\nspecific, as some multilingual architectures\nwork better than the monolingual ones in\nsome scenarios and vice-versa. More recently,\nGarcia and Crespo-Otero (2022) evaluated\na variety of BERT models for Galician and\nPortuguese and found that monolingual ones\nseem to properly identify some agreement de-\npendencies across intervening material such\nas relative clauses but struggle when dealing\nwith others, like inflected infinitives.\nMore related to our project, Wei et al.\n(2021) trained BERT models for English con-\ntrolling the training data, and found that\nword frequency during the learning phase\ninfluences the prediction performance of a\n2https://syntaxgym.org/\n17\nA computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface \n of dependency resolution and training time model on subject-verb agreement dependen-\ncies (something which was previously sug-\ngested by Marvin and Linzen (2018)). How-\never, to the best of our knowledge, there are\nno studies analyzing the impact of training\ntime on the syntactic abilities of Transformer\nmodels, possibly because intermediate train-\ning checkpoints are not often available (al-\nthough Sellam et al. (2022) just released sev-\neral checkpoints at different training steps of\nBERT models for English).\nThis paper presents a detailed comparison\nbetween monolingual and multilingual BERT\nmodels for Galician. On the one hand, we\nexplore the models\u2019 behaviour regarding lin-\nguistic properties of the test items, such as\nthe length of the target dependency or the\npresence of attractors. On the other hand,\nwe compare the models\u2019 performance taking\ninto account several parameters, such as the\nnumber of hidden layers, the amount of train-\ning data, their initialization, or the number\nof their training steps.\n3 Galician BERT models\nIn our experiments we compare the following\nmonolingual and multilingual models:\n\u2022mBERT , which is the official multilin-\ngual BERT (base, with 12 layers).\n\u2022Bertinho-small (with 6 hidden layers)\nandBertinho-base (with 12 hidden\nlayers) published by Vilares, Garcia, and\nG\u00b4 omez-Rodr\u00b4 \u0131guez (2021). These two\nmodels have a vocabulary of 30k tokens,\nand have been trained on the Galician\nWikipedia (with about 45M words).\n\u2022BERT-small (6 hidden layers) and\nBERT-base (12 layers) released by\nGarcia (2021), both trained on a corpus\nof about 550M tokens. BERT-small has\na vocabulary size of 52k tokens and has\nbeen trained during 1M steps. BERT-\nbase has been initialized from mBERT\n(which includes Galician as on of its 102\nlanguages). It has a vocabulary size of\n119,547 and has been trained during 600\nsteps.\nAdditionally, we release the checkpoints\nof the latter two monolingual BERT mod-\nels (BERT-small and BERT-base) mentioned\nabove (Garcia, 2021). These models have\nbeen trained on a corpus which combines the\nGalician Wikipedia (April 2020 dump), SLIGalWeb (Agerri et al., 2018, composed by\ncrawled texts from various domains), CC-\n100 (Wenzek et al., 2020), and other data\ncrawled from online newspapers. It was semi-\nautomatically cleaned by removing duplicate\nsentences, and utterances with many symbols\nand punctuations. The models were trained\nwith a masked language modeling objective\non a single Titan XP GPU (12GB), with\nbatch sizes of 208 (small) and 198 (base), and\nusing the transformers library (Wolf et al.,\n2020). For each model, we saved a check-\npoint every 25k steps (about 12h and 26h for\nthe small and base models respectively) up\nto 425k steps.3To avoid confusion with the\noriginally published models BERT-small and\nBERT-base, these newly released checkpoints\nwill be referred to as Check-small and Check-\nbase.\n4 Methodology\n4.1 Research questions and\nexperimental design\nThis work aims to explore the following re-\nsearch questions:\n\u2022Q1: Are Galician BERT LMs able to\nresolve agreement dependencies?\n\u2022Q2: Does model accuracy vary as a\nfunction of dependency type?\n\u2022Q3: Are Galician BERT LMs subject\nto interference effects from structurally-\nirrelevant attractor nouns?\n\u2022Q4: Does accuracy improve with train-\ning time?\nTo provide an (at least tentative) answer\nto these questions, we created a word predic-\ntion task that we run in the different mod-\nels under evaluation (i.e. mBERT, Bertinho-\nsmall and base, BERT-small and base and\nthe different checkpoints of Check-small and\nbase. Our task had a factorial design which\nmanipulated the type of dependency (noun-\nadjective vs. subject-verb agreement), the\namount of intervening material (short vs.\nlong) and the presence or absence of an inter-\nvening but structurally irrelevant noun that\nmismatches in agreement features with the\nhead word (no attractor vs attractor). A\n3All checkpoints are available at https://github.\ncom/marcospln/galician_bert_checkpoints\n18\nIria de-Dios-Flores, Marcos Garcia dep length attr example\nnoun-adjshortno Oneno que xogaba onte al\u00b4 \u0131 \u00b4 e alto|*alta.\nyes Oneno que xogaba onte al\u00b4 \u0131 coa nena \u00b4 ealto|*alta.\nlongno Oneno que xogaba no parque inaugurado recentemente \u00b4 e\nalto|*alta.\nyes Oneno que xogaba no parque inaugurado recentemente\ncoa nena \u00b4 ealto|*alta.\nsubj-verbshortno Oneno que xogaba onte al\u00b4 \u0131 aparece|*aparecen na tele-\nvisi\u00b4 on.\nyes Oneno que xogaba onte al\u00b4 \u0131 cos outros nenos\naparece|*aparecen na televisi\u00b4 on.\nlongno Oneno que xogaba no parque inaugurado recentemente\naparece|*aparecen na televisi\u00b4 on.\nyes Oneno que xogaba no parque inaugurado recentemente\ncos outros nenos aparece|*aparecen na televisi\u00b4 on.\nTable 1: Sample set of the experimental conditions for noun-adjective and subject-verb agree-\nment dependencies. depis the target dependency, and attrefers to the presence/absence of an\nattractor word. Words in bold are in a dependency relation, and underlined words are attractors,\nwhich agree with the wrong alternative marked with *. The base sentence (i.e. short without\nattractor) for noun-adjective dependencies means \u201cThe boy who was playing there yesterday\nis tall\u201d, and for subject-verb agreement dependencies means \u201cThe boy who was playing there\nyesterday appears on TV\u201d.\nsample set of the experimental conditions is\nshown in Table 1.\nWe will pay particular attention to the\nmodels\u2019 accuracy for the experimental con-\nditions at different steps in the training pro-\ncess by testing checkpoints at every 25k steps\nup to 425k for both Check-small and base.\nThis will also allow us to investigate not only\nthe effects of the training steps (on the var-\nious checkpoints) but also to make, among\nothers, the following comparisons: (a) the\nimpact of the training data and vocabulary\nsize, comparing BERT-small and Bertinho-\nsmall; (b) the influence of the hidden lay-\ners, using Bertinho-base and Bertinho-small;\n(c) the effects of fine-tuning on monolingual\ndata, comparing mBERT with our BERT-\nbase (initialized from mBERT and fine-tuned\nin Galician). This inquiry is possible thanks\nto the public availability of the dataset de-\nscribed in the next section.\n4.2 The dataset\nIn order to run the experiments described\nabove, we have used a subset of the dataset\nreleased by Garcia and Crespo-Otero (2022)4\n\u2014the first and only available dataset for tar-\ngeted syntactic evaluation for Galician and\nPortuguese (number and gender) agreement\n4https://github.com/crespoalfredo/\nPROPOR2022-gl-ptdependencies. We limit ourselves to a sub-\nset of the dataset by choosing items with the\nstructure of those in Table 1.\nFor noun-adjective agreement dependen-\ncies, the dataset contains 2,112 sentences. In\norder to avoid possible confounds, gender was\nconterbalanced so that half of the items had\na feminine target and the other half a mas-\nculine one. For subject-verb agreement de-\npendencies, the dataset contains 4,368 sen-\ntences. Similarly, number was counterbal-\nanced so that half of the items had a sin-\ngular target and the other half a plural one.\nIt is worth noting that, overall, there are\nless experimental items without an attractor\nthan those with an attractor. This is because\nthe original dataset contained variants of the\nattractors to avoid potential lexical biases.\nFurther details on the dataset building pro-\ncedure are available in Garcia and Crespo-\nOtero (2022), but it must be noted that to\ncreate the dataset, the authors selected as\ntarget (masked) words only those forms ap-\npearing in the vocabulary of the (monolin-\ngual and multilingual) models in order to al-\nlow the evaluation of all models using the\nsame number of experimental items.\n4.3 Evaluation metrics\nBefore moving into the results of the experi-\nments, some notes on evaluation procedures\n19\nA computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface \n of dependency resolution and training time Sentence\nAsnenas que xogaban onte al\u00b4 \u0131 co outro neno *ten|te\u02dc nen fame.\n\u2018The girls who were playing there yesterday with the other boy *is| are hungry.\u2019\nModel Prob Corr Prob Wrong 0/1 accuracy PD accuracy\nmBERT 0.0007 0.0005 1 0.236\nBertinho-base 0.1856 0.0030 1 0.968\nTable 2: Example of a sentence where both models (mBERT and Bertinho-base) give higher\nprobability to the correct inflection (Prob Corr vsProb Wrong ).\ndeserve mentioning. Contrary to the stan-\ndard evaluation procedure which considers\nthat a model succeeds if it assigns a higher\nprobability to the correct than to the incor-\nrect form (assigning thus either a 0 or a 1),\nNewman et al. (2021) propose the use of a\nlarge set of correct/incorrect pairs on each\nsentence to probe a model. In this vein, they\nmeasure the model\u2019s systematicity (in how\nmany pairs per sentence the model prefers\nthe correct alternative) and likely behaviour\n(the probability of generating a correct inflec-\ntion). In this method, large lists of verbs are\ngathered using corpus frequencies, which are\nthen used to replace the original pairs of each\ncontext. Hence, using this strategy involves\nthe evaluation on many potentially semanti-\ncally infelicitous (or implausible) sentences.\nBy contrast, we put the focus on the\nprobability distance between the correct and\nthe incorrect alternatives provided by the\ndataset, and propose a new metric dubbed\nProbability Distance (PD accuracy). It is cal-\nculated by normalizing the probabilities of\nthe correct and incorrect targets (e.g. te\u02dc nen\nvsten) via percentages, and then substract-\ning the percentage of the incorrect form from\nthe percentage of the correct one. The ratio-\nnale behind this metric is motivated by the\nobservation that traditional binary metrics\nobscure possible effects because subtle differ-\nences such that between 0.51 vs 0.49 would\nbe immediately translated to 1 or 0 (thus ob-\ntaining the same accuracy as for large dif-\nferences such as 0.85 vs 0.15). Instead, PD\naccuracies are circumscribed to a probabil-\nity space which only includes a semantically\nplausible target pair, and accuracy is calcu-\nlated within that narrowed probability space,\nkeeping the original distance between cor-\nrect and incorrect words. To demonstrate\nthis, Table 2 shows the results for a subject-\nverb long agreement dependency with an at-\ntractor for which mBERT and Bertinho-basegive a higher probability for the correct in-\nflection. Nonetheless, while 0/1 accuracy\nfocuses on the most likely alternative, and\nwould thus assign a 1 in both cases, PD ac-\ncuracy brings the distance between the cor-\nrect and incorrect alternatives to the fore.\nTo further demonstrate this, Figure 1 shows\nthe mean accuracy for BERT-base\u2019s long sen-\ntences with an intervening attractor (i.e. ex-\namples such as the one in Table 2). We are\nonly showing BERT-base results for long sen-\ntences with an attractor for the sake of sim-\nplicity, as these are the cases where models\ntend to fail. What can be observed here is\nthat binary metrics clearly overestimate the\nsystematicity of language models, as accu-\nracy drops once PD accuracy is calculated.\nCritically, PD accuracy also provides a bet-\nter threshold for comparison between models\nwith different vocabulary sizes.\nFigure 1: Mean accuracy for BERT-base\u2019s\nlong sentences with an intervening attractor\nusing 0-1 and PD accuracy metrics.\n5 Results and Discussion\nWe will first report and discuss the accuracy\nprovided by the five available models and\nthen, we will focus on the results for Check-\nsmall and Check-base at different training\ncheckpoints.\n5.1 Published models\nOverall accuracy: Figure 2 shows the\noverall accuracy for each model for noun-\n20\nIria de-Dios-Flores, Marcos Garcia adjective and subject-verb agreement depen-\ndencies, regardless of dependency length of\nthe presence/absence of an attractor. Sev-\neral ideas related to our research ques-\ntions can be tackled at this point: first,\nthere is a decline in accuracy for the\nmonolingual models (BERT-base>BERT-\nsmall>Bertinho-base>Bertinho-small), sug-\ngesting that the amount of training data\nheavily influences the models\u2019 performance.\nInterestingly, mBERT\u2019s results resemble\nthose from Bertinho-base, most possibly be-\ncause they have been trained with the same\nGalician data (Wikipedia) and have a simi-\nlar architecture (same number of hidden lay-\ners and dimensionality). BERT-base and\nBERT-small show a relatively acceptable\nperformance, while the other three models\n(Bertinho-base, Bertinho-small and mBERT)\nare closer to chance performance (see Q1).\nThis is particularly true for subject-verb\nagreement dependencies, while all models ex-\ncept Bertinho-small provide better results for\nnoun-adjective dependencies (see Q2). In line\nwith the results of Goldberg (2019) for En-\nglish, Bertinho-small obtained slightly better\naccuracy than its base variant on subject-\nverb agreement.\nFigure 2: Mean accuracy by dependency type\nfor the five models under investigation.\nAccuracy per condition: Figure 3 pro-\nvides a closer picture of the models\u2019 perfor-\nmance for noun-adjective and subject-verb\ndependencies when looking at the four dif-\nferent experimental conditions. These re-\nsults tap directly into the possible presence of\nagreement attraction effects (see Q3). Based\non previous studies, out of the four exper-\nimental conditions, short sentences with no\nattractor were predicted to be the easiest\nones, while long sentences with an attrac-\ntor were predicted to be the hardest ones.\nThis was borne out in the results, confirm-\nFigure 3: Mean accuracy by dependency type\nand experimental condition for the five mod-\nels under investigation.\ning that Galician BERT models are lured\nby structurally irrelevant mismatching nouns\nthat intervene in agreement dependencies be-\ntween the head and the target. Critically,\nthe emergence of attraction effects is medi-\nated by the distance between the head and\nthe target such that longer dependencies are\nmore prone to give rise to attraction effects.\nNonetheless, it must be noted that not all\nmodels show equally strong attraction effects\n(aligning with the accuracy decline described\nabove), and that attraction effects are steeper\nin subject-verb agreement dependencies than\nin noun-adjective agreement dependencies \u2014\nsomething which was foreseeable on the basis\nof Figure 2.\n5.2 Learning curves\nOverall accuracy: Moving now into the\nanalyses by training checkpoints, Figure 4\nshows the overall accuracy for Check-small\nand Check-base for the two dependencies at\nevery checkpoint, from 25k to 425k steps.\n21\nA computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface \n of dependency resolution and training time Figure 4: Mean accuracy by dependency type and checkpoint for Check-base and Check-small.\nThe horizontal red lines indicate the overall accuracy mean for Bertinho-base, Bertinho-small\nand mBERT for ease of comparison (cf. Figure 2).\nCheck-small: Focusing on the small\nmodel (with 6 layers and trained from\nscratch) represented by the dark line, the re-\nsults show that it needs relatively few check-\npoints to surpass the average accuracy by\nBertinho and mBERT. On noun-adjective de-\npendencies, Bertinho-small is surpassed at\ncheckpoint 75k, while it needs between 150k\nand 175k steps to outperform Bertinho-base\nand mBERT, both with 12 layers. This is\neven clearer on subject-verb dependencies, as\nthe second checkpoint (75k) already shows\nbetter results than any of the three men-\ntioned models. When comparing with the re-\nsults of the published BERT-base and BERT-\nsmall (see Figure 2), it is worth noting that\nthese models do not obtain notoriously better\nresults even though they have been trained\nfor a longer period of time. These results sug-\ngest that the amount of (monolingual) train-\ning data, rather than training time, is cru-\ncial to generalize the target dependencies, as\nCheck-small obtains better results at check-\npoint 75k than Bertinho-base at 1.5M train-\ning steps.\nCheck-base: Moving now into Check-\nbase, represented by the dark line, it should\nbe reminded that this model has been ini-\ntialized with the weights of mBERT, and at\nthe first checkpoint (25k) it already obtains\ncomparable results to that of the final BERT-\nbase model on subject-verb agreement (see\nFigure 2). On noun-adjective dependencies,\nthe model keeps a more constant learning\nrate, but it achieves similar performance than\nBERT-base at around 400k steps. In this\ncase, we may hypothesize that the model is\ntaking advantage of the linguistic properties\nof other languages covered by mBERT, and itadapts the model to Galician on early steps.\nContrarily to the constant learning rate ob-\nserved for noun-adjective dependencies, the\npanorama learning curve for subject-verb de-\npendencies seems to be much more unstable.\nIndeed, no improvement is observed for nei-\nther Check-base nor Check-small. Although\naccuracy improves with time, subject-verb\nagreement dependencies experience ups and\ndowns in intermediate checkpoints.\nAccuracy per condition: Finally, Fig-\nure 5 shows the curves for each experimen-\ntal condition for Check-small and Check-\nbase at the different training steps. As ex-\npected, short contexts and sentences with-\nout attractors are easily solved by both mod-\nels on the two dependencies. As previously\nshown (Figure 4), Check-base overtakes the\nperformance of mBERT on the first check-\npoints, but then the raise of the learning\ncurve is very small, namely for subject-verb\nagreement. The small variant, especially on\nnoun-adjective dependencies, shows a con-\nstant learning process which seems to stabi-\nlize around checkpoint 300k. Interestingly,\nsubject-verb agreement dependencies are eas-\nily solved by Check-small at 50k steps in the\nabsence of attractors. However, when attrac-\ntors are present, Check-small is sensitive to\nthem even in short contexts, where it pre-\nserves the performance again about check-\npoint 300k. Finally, on long-distance depen-\ndencies with attractors, none of the mod-\nels seem to have a stable behaviour, as the\nperformance of both of them varies unpre-\ndictably. This is more noticeable for Check-\nbase, which solves most cases properly but\nstruggles with the more complex structures\n(i.e. long sentences with attractors).\n22\nIria de-Dios-Flores, Marcos Garcia Figure 5: Mean accuracy of Check-small (top) and Check-base (bottom) by dependency type,\nexperimental condition, and checkpoint.\n6 Conclusions and future work\nThis paper has presented a multidimensional\nevaluation of a variety of BERT models\nfor Galician on two types of agreement de-\npendencies, noun-adjective and subject-verb.\nWe compared the performance of multilin-\ngual and monolingual models with diverse\nproperties, including 6 and 12 layers variants,\ndifferent sizes of training data and vocabu-\nlaries, and two initializations: training from\nscratch, and fine-tuning a multilingual BERT\non Galician data.\nOur results show a gradient in the abil-\nity of Galician BERT LMs models to re-\nsolve agreement dependencies, with BERT-\nbase being the most accurate and Bertinho-\nsmall the least accurate. Furthermore, we ob-\nserved that accuracy varied as a function of\ndependency type, with noun-adjective agree-\nment dependencies being easier to handle\nthan subject-verb agreement dependencies.\nInterestingly, BERT LMs are subject to inter-\nference effects from structurally-irrelevant at-\ntractor nouns, and the degree of fallibility to\nattraction effects is inverse to accuracy (i.e.\nless accurate models show more attraction ef-\nfects). Last and most important, although\ntraining time does seem to have a small ef-\nfect on the models\u2019 accuracy, this factor is far\nfrom being comparable with the influence of\nthe size of the training corpus.\nBesides the results and analyses of the per-formed experiments, we contribute with new\n34 checkpoints of BERT models for an under-\nstudied language, Galician, which are freely\nreleased with this paper and can hopefully\ncontribute to foster research on languages dif-\nferent from English.\nThis exploratory work has opened many\nlines of inquiry that we aim to explore in fu-\nture research. On the one hand, we plan to\ncreate new datasets in Galician that do not\nonly overcome some shortcomings observed\nin the one released by Garcia and Crespo-\nOtero (2022) but also incorporate new types\nof linguistic relations. On the other hand, we\nplan to compare the results obtained for Gali-\ncian with other languages in order to observe\ncross-linguistic differences and similarities.\nAcknowledgements\nThis research was funded by the project\n\u201cN\u00b4 os: Galician in the society and econ-\nomy of artificial intelligence\u201d (Xunta de\nGalicia/Universidade de Santiago de Com-\npostela), by grant ED431G2019/04 (Galician\nGovernment and ERDF), by a Ram\u00b4 on y Ca-\njalgrant (RYC2019-028473-I), and by Grant\nED431F 2021/01 (Galician Government).\nReferences\nAgerri, R., X. G\u00b4 omez Guinovart, G. Rigau,\nand M. A. Solla Portela. 2018. Devel-\noping new linguistic resources and tools\n23\nA computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface \n of dependency resolution and training time for the Galician language. In Proceed-\nings of the Eleventh International Con-\nference on Language Resources and Eval-\nuation (LREC 2018), Miyazaki, Japan,\nMay. European Language Resources As-\nsociation (ELRA).\nBernardy, J.-P. and S. Lappin. 2017. Us-\ning deep neural networks to learn syntac-\ntic agreement. In Linguistic Issues in Lan-\nguage Technology, Volume 15, 2017. CSLI\nPublications.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. In Proceedings of\nthe 2019 Conference of the North Amer-\nican Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies, Volume 1 , pages 4171\u20134186,\nMinneapolis, Minnesota. Association for\nComputational Linguistics.\nGarcia, M. 2021. Exploring the repre-\nsentation of word meanings in context:\nA case study on homonymy and syn-\nonymy. In Proceedings of the 59th Annual\nMeeting of the Association for Computa-\ntional Linguistics and the 11th Interna-\ntional Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Pa-\npers), pages 3625\u20133640, Online, August.\nAssociation for Computational Linguis-\ntics.\nGarcia, M. and A. Crespo-Otero. 2022.\nA Targeted Assessment of the Syntac-\ntic Abilities of Transformer Models for\nGalician-Portuguese. In International\nConference on Computational Processing\nof the Portuguese Language (PROPOR\n2022), pages 46\u201356. Springer.\nGauthier, J., J. Hu, E. Wilcox, P. Qian,\nand R. Levy. 2020. SyntaxGym: An\nonline platform for targeted evaluation\nof language models. In Proceedings of\nthe 58th Annual Meeting of the Associa-\ntion for Computational Linguistics: Sys-\ntem Demonstrations, pages 70\u201376, Online,\nJuly. Association for Computational Lin-\nguistics.\nGoldberg, Y. 2019. Assessing BERT\u2019s\nSyntactic Abilities. arXiv preprint\narXiv:1901.05287.\nGulordava, K., P. Bojanowski, E. Grave,\nT. Linzen, and M. Baroni. 2018. Colorlessgreen recurrent networks dream hierarchi-\ncally. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the\nAssociation for Computational Linguis-\ntics: Human Language Technologies, Vol-\nume 1 (Long Papers) , pages 1195\u20131205,\nNew Orleans, Louisiana, June. Associa-\ntion for Computational Linguistics.\nHenderson, J. 2020. The unstoppable rise\nof computational linguistics in deep learn-\ning. In Proceedings of the 58th Annual\nMeeting of the Association for Computa-\ntional Linguistics, pages 6294\u20136306, On-\nline, July. Association for Computational\nLinguistics.\nHewitt, J. and C. D. Manning. 2019. A\nstructural probe for finding syntax in\nword representations. In Proceedings of\nthe 2019 Conference of the North Ameri-\ncan Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers) , pages 4129\u20134138, Minneapolis,\nMinnesota, June. Association for Compu-\ntational Linguistics.\nKuncoro, A., C. Dyer, J. Hale, and P. Blun-\nsom. 2018a. The perils of natural be-\nhaviour tests for unnatural models: the\ncase of number agreement. Learning Lan-\nguage in Humans and in Machines, 5(6).\nhttps://osf.io/9usyt/.\nKuncoro, A., C. Dyer, J. Hale, D. Yogatama,\nS. Clark, and P. Blunsom. 2018b. LSTMs\ncan learn syntax-sensitive dependencies\nwell, but modeling structure makes them\nbetter. In Proceedings of the 56th Annual\nMeeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Pa-\npers) , pages 1426\u20131436, Melbourne, Aus-\ntralia, July. Association for Computa-\ntional Linguistics.\nLakretz, Y., G. Kruszewski, T. Desbordes,\nD. Hupkes, S. Dehaene, and M. Baroni.\n2019. The emergence of number and\nsyntax units in LSTM language models.\nInProceedings of the 2019 Conference of\nthe North American Chapter of the As-\nsociation for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers), pages 11\u201320,\nMinneapolis, Minnesota, June. Associa-\ntion for Computational Linguistics.\n24\nIria de-Dios-Flores, Marcos Garcia Lin, Y., Y. C. Tan, and R. Frank. 2019.\nOpen sesame: Getting inside BERT\u2019s lin-\nguistic knowledge. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks\nfor NLP, pages 241\u2013253, Florence, Italy,\nAugust. Association for Computational\nLinguistics.\nLinzen, T., E. Dupoux, and Y. Goldberg.\n2016. Assessing the ability of LSTMs\nto learn syntax-sensitive dependencies.\nTransactions of the Association for Com-\nputational Linguistics, 4:521\u2013535.\nLinzen, T. and B. Leonard. 2018. Distinct\npatterns of syntactic agreement errors in\nrecurrent networks and humans. In Pro-\nceedings of the 40th Annual Conference\nof the Cognitive Science Society. arXiv\npreprint arXiv:1807.06882.\nMarvin, R. and T. Linzen. 2018. Targeted\nsyntactic evaluation of language models.\nInProceedings of the 2018 Conference\non Empirical Methods in Natural Lan-\nguage Processing, pages 1192\u20131202, Brus-\nsels, Belgium, October-November. Associ-\nation for Computational Linguistics.\nMueller, A., G. Nicolai, P. Petrou-Zeniou,\nN. Talmina, and T. Linzen. 2020. Cross-\nlinguistic syntactic evaluation of word pre-\ndiction models. In Proceedings of the 58th\nAnnual Meeting of the Association for\nComputational Linguistics , pages 5523\u2013\n5539, Online, July. Association for Com-\nputational Linguistics.\nNewman, B., K.-S. Ang, J. Gong, and J. He-\nwitt. 2021. Refining targeted syntactic\nevaluation of language models. In Pro-\nceedings of the 2021 Conference of the\nNorth American Chapter of the Associa-\ntion for Computational Linguistics: Hu-\nman Language Technologies, pages 3710\u2013\n3723, Online, June. Association for Com-\nputational Linguistics.\nP\u00b4 erez-Mayos, L., M. Ballesteros, and L. Wan-\nner. 2021. How much pretraining data\ndo language models need to learn syn-\ntax? In Proceedings of the 2021 Con-\nference on Empirical Methods in Natu-\nral Language Processing , pages 1571\u20131582,\nOnline and Punta Cana, Dominican Re-\npublic, November. Association for Com-\nputational Linguistics.P\u00b4 erez-Mayos, L., A. T\u00b4 aboas Garc\u00b4 \u0131a, S. Mille,\nand L. Wanner. 2021. Assessing the\nsyntactic capabilities of transformer-based\nmultilingual language models. In Find-\nings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021 , pages\n3799\u20133812, Online, August. Association\nfor Computational Linguistics.\nSellam, T., S. Yadlowsky, J. Wei, N. Saphra,\nA. D\u2019Amour, T. Linzen, J. Bastings,\nI. Turc, J. Eisenstein, D. Das, I. Ten-\nney, and E. Pavlick. 2022. The Multi-\nBERTs: BERT Reproductions for Ro-\nbustness Analysis. In The Tenth Inter-\nnational Conference on Learning Repre-\nsentations (ICLR 2022). arXiv preprint\narXiv:2106.16163.\nTran, K., A. Bisazza, and C. Monz. 2018.\nThe importance of being recurrent for\nmodeling hierarchical structure. In Pro-\nceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Pro-\ncessing, pages 4731\u20134736, Brussels, Bel-\ngium, October-November. Association for\nComputational Linguistics.\nVaswani, A., N. Shazeer, N. Parmar,\nJ. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin. 2017. At-\ntention Is All You Need. arXiv preprint\narXiv:1706.03762.\nVilares, D., M. Garcia, and C. G\u00b4 omez-\nRodr\u00b4 \u0131guez. 2021. Bertinho: Galician\nBERT Representations. Procesamiento\ndel Lenguaje Natural, 66:13\u201326.\nWei, J., D. Garrette, T. Linzen, and\nE. Pavlick. 2021. Frequency effects\non syntactic rule learning in transform-\ners. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural\nLanguage Processing, pages 932\u2013948, On-\nline and Punta Cana, Dominican Repub-\nlic, November. Association for Computa-\ntional Linguistics.\nWenzek, G., M.-A. Lachaux, A. Conneau,\nV. Chaudhary, F. Guzm\u00b4 an, A. Joulin,\nand E. Grave. 2020. CCNet: Ex-\ntracting high quality monolingual datasets\nfrom web crawl data. In Proceedings of\nthe 12th Language Resources and Evalua-\ntion Conference , pages 4003\u20134012, Mar-\nseille, France, May. European Language\nResources Association.\n25\nA computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface \n of dependency resolution and training time Wolf, T., L. Debut, V. Sanh, J. Chau-\nmond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Davi-\nson, S. Shleifer, P. von Platen, C. Ma,\nY. Jernite, J. Plu, C. Xu, T. Le Scao,\nS. Gugger, M. Drame, Q. Lhoest, and\nA. Rush. 2020. Transformers: State-\nof-the-art natural language processing.\nInProceedings of the 2020 Conference\non Empirical Methods in Natural Lan-\nguage Processing: System Demonstra-\ntions, pages 38\u201345, Online, October. As-\nsociation for Computational Linguistics.\n26\nIria de-Dios-Flores, Marcos Garcia Information fusion for mental disorders detection:multimodal BERT against fusioning multiple BERTs\nFusi\u00b4 on de informaci\u00b4 on para detecci\u00b4 on de transtornos\nmentales: BERT multimodal contra m\u00b4 ultiples BERTs\nfusionados\nMario Ezra Arag\u00b4 on1, A. Pastor L\u00b4 opez-Monroy2,\nLuis C. Gonz\u00b4 alez-Gurrola3, Manuel Montes-y-G\u00b4 omez1\n1Instituto Nacional de Astrof\u00b4 \u0131sica, \u00b4Optica y Electr\u00b4 onica, Puebla, Mexico\n2Centro de Investigaci\u00b4 on en Matem\u00b4 aticas A.C., Guanajuato, Mexico\n3Universidad Aut\u00b4 onoma de Chihuahua, Chihuahua, Mexico\n{mearagon,mmontesg}@inaoep.mx, pastor.lopez@cimat.mx, lcgonzalez@uach.mx\nAbstract: Given the increasing number of modalities that modern classification\nproblems provide, recently a multimodal BERT transformer (MMBT) was proposed.\nAn interesting opportunity to evaluate the effectiveness of such model is posed by\nthe problem of timely detection of mental disorders of social media users. For this\nproblem, a multi-channel perspective involves extracting from each user post differ-\nent types of information, such as thematic, emotional and stylistic content. This\nstudy evaluates the suitability of tackling this problem by the apparently ad-hoc\nMMBT, moreover, we further evaluate if regular BERT models could be combined\nor fused in such a way that could have a chance in a multi-channel arena. For the\nevaluation, we use recent public data sets for three important mental disorders: De-\npression, Anorexia, and Self-harm. Results suggest that BERT models can get on\ntheir own a data representation that could be later fusioned and boost the classifi-\ncation performance by at least 5% in F1 measure, even surpassing the MMBT.\nKeywords: Multichannel information, Transformers, Mental disorders.\nResumen: Dado el creciente n\u00b4 umero de modalidades que ofrecen los problemas de\nclasificaci\u00b4 on modernos, recientemente se ha propuesto un transformer BERT mul-\ntimodal (MMBT). Una oportunidad interesante para evaluar la eficacia de dicho\nmodelo la plantea el problema de la detecci\u00b4 on oportuna de los trastornos mentales\nde usuarios de las redes sociales. Para este problema, una perspectiva multicanal\nimplica extraer de cada post de los usuarios diferentes tipos de informaci\u00b4 on, como\nsu contenido tem\u00b4 atico, emocional y estil\u00b4 \u0131stico. Este estudio eval\u00b4 ua la idoneidad de\nabordar este problema mediante el aparentemente ad-hoc MMBT, adem\u00b4 as, evalu-\namos si los modelos BERT regulares podr\u00b4 \u0131an combinarse o fusionarse de tal manera\nque pudieran tener una oportunidad en un escenario multicanal. Para la evaluaci\u00b4 on,\nutilizamos conjuntos de datos p\u00b4 ublicos recientes para tres importantes trastornos\nmentales: Depresi\u00b4 on, Anorexia y Autolesiones. Los resultados sugieren que los mod-\nelos BERT pueden obtener por s\u00b4 \u0131 solos una representaci\u00b4 on de los datos que podr\u00b4 \u0131a\nfusionarse posteriormente y aumentar el rendimiento de la clasificaci\u00b4 on en al menos\nun 5% en la medida F1, superando incluso al MMBT.\nPalabras clave: Multicanal, Transformers, Trastornos Mentales.\n1 Introduction\nOver the last few years, millions of people\naround the world have been affected by one\nor more mental disorders, for example, in2018 a study of mental disorders in Mexico\nreveals that 17% of people in the country\nhave at least one mental disorder and one\nin four will suffer a mental disorder at least\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 27-38\nrecibido 30-03-2022 revisado 20-05-2022 aceptado 23-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-2\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalonce in their life (Renteria-Rodriguez, 2018).\nUnfortunately, this phenomenon causes inter-\nference in their daily life, especially affect-\ning their behavior and thinking. Regularly,\nthe self-awareness of having a mental disorder\ncauses emotional and physical damage that\ncould make people feel fear to the idea of\nbeing vulnerable to criticism, judgment, or\nopposing opinions. Mental disorders may be\nrelated to a particular event that generated\nexcessive stress on the affected person or to\na series of different stressful events (World\nHealth Organization, 2019). For instance,\nsome of the causes are environmental stress,\ngenetic factors, or different difficult life situa-\ntions. There are several common mental dis-\norders such as depression, anorexia, or self-\nharm affecting people worldwide (Kessler et\nal., 2017).\nA reality nowadays, is that for some peo-\nple their social life does not occur in their\nsurroundings or immediate environment, but\ntakes place in a virtual world created by so-\ncial media platforms like Facebook, Twitter,\nor Reddit (Baer, 2021). In other words, so-\ncial media has become a vital link for some\nof us. This scenario presents opportunities\nto study and analyze, given the availability\nof data, how people communicate, and more\nspecifically, how this communication could be\nassociated to possible mental health issues\nthat people are experimenting, contributing\nin this way to attract attention to silent dis-\norders.\nPrevious studies have shown that texts\nshared by users in social networks have dif-\nferent evidence or types (channels or pseudo\nmodalities) of information that may be rel-\nevant for the detection of mental disorders\n(Guntuku et al., 2017), for example, their\ntopics of interest, emotional state, or their\nwriting style. This has motivated us to pro-\npose a method that considers these pieces of\ninformation and to study how to combine or\nfusion them. With this in mind, we evalu-\nate the plausibility of using the multimodal\nBERT (MMBT)(Kiela et al., 2019) for this\ntask, being this the first time. In addition,\nwe state the question: whether MMBT is the\nbest option to handle this sensitive task or\nif multiple BERTs can better exploit the na-\nture of the data. Thus, we compare the per-\nformance of MMBT against different archi-\ntectures based on early and late fusion ap-\nproaches of the popular BERT model. Inthis study we take the text modality and di-\nvide it into three channels1that focus on dif-\nferent aspects of the users\u2019 communication.\nThe first channel captures the thematic in-\nformation used for contextual analysis. The\nsecond channel indicates the manifested emo-\ntions, attempting to capture emotional topics\nrelated to mental disorders. Finally, the third\nchannel focuses on the writing style, where\nwe want to capture the use of personal ex-\npressions and verbs tense, among other as-\npects. As could be observed, our hypothesis\nis that people that present some mental dis-\norder tend to express differently, at different\ndimensions, regarding the control group.\nIt is widely known that the BERT model\n(Devlin et al., 2019) has led to important\nimprovements in representation learning for\nnatural language processing and text classi-\nfication problems. In a recent work (Kiela et\nal., 2019), the authors demonstrate that su-\npervised bidirectional transformers with uni-\nmodal pre-trained components obtain good\nperformance in multimodal fusion. They\nfound that learning to map dense multimodal\nfeatures to BERT\u2019s token embedding space\nis easy to extend to different modalities. In-\nspired by their findings, we adapt their Mul-\ntimodal BERT (MMBT) module with our\nchannels as modalities to create a multichan-\nnel contextualized representation. The main\nidea of our work is to find out the best way\nto combine the different types of information\nand see if using multimodal BERT is better\nthan considering a fusion of multiple BERTs,\neach one specialized in a different channel.\nWe can summarize the contributions of\nour work as follows:\n1. We adapt a Multimodal BERT (MMBT)\nfor the detection of mental disorders,\nconsidering three channels of informa-\ntion: thematic, emotional and stylistic.\n2. We explore different strategies to com-\nbine these information, considering early\nand late fusion approaches.\n3. We analyze and evaluate in detail these\nthree information channels and the im-\nportance of their fusion, then conclud-\ning about its feasibility of integration to\nboost classification performance.\n1In this work, we define a channel as a different\nproperty or view from the same modality (Qianli et\nal., 2017).\nMario Ezra Arag\u00f3n, A. Pastor L\u00f3pez-Monroy, Luis C. Gonz\u00e1lez-Gurrola, Manuel Montes-y-G\u00f3mez2 Related work\n2.1 Mental disorders detection\nIn the last few years, the study of public men-\ntal health through social media has increased.\nThis is mainly because these media provide\na source of support for those who suffer from\na mental health disorder, like for example, a\nsense of community, relatedness, and under-\nstanding (Hilton, 2016; Dyson et al., 2016).\nIn general, for the construction of corpora,\nresearchers identify a group of users who ex-\npressed in one of their publications having\nbeen clinically diagnosed with a mental dis-\norder and then download all or part of their\nposts (De Choudhury, Counts, and Horvitz,\n2013; Wang et al., 2017).\nRecent works (Trifan and Oliveira, 2019;\nVan Rijen et al., 2019), explored the analysis\nof the posts\u2019 content. In these works, the au-\nthors consider different features, such as word\nand char n-grams, and then apply a classi-\nfication algorithm to make a decision. The\nshortcoming with that strategy is the high\noverlap in the vocabulary used by the con-\ntrol and positive users, which generates sev-\neral missclassifications. Another well-known\nstrategy consists of counting the number of\noccurrences of positive, negative, and neu-\ntral words in texts (Kang, Yoon, and Kim,\n2016), or on measuring how similar are their\nwords to some reference negative and pos-\nitive lexicons (Htait, Fournier, and Bellot,\n2017). On the other hand, analyzing sen-\ntiments has shown interesting results since\nit has been found that negative comments\nare more abundant in people with a declared\nmental health disorder than in comments\ngenerated from a control group (Cooper-\nsmith, Dredze, and Harman, 2014; Preotiuc-\nPietro et al., 2015). Other works have used\na LIWC-based representation (Tausczik and\nPennebaker, 2010), which consists of a set\nof psychological categories that aim to rep-\nresent users\u2019 posts by features of social rela-\ntionships, thinking styles and individual dif-\nferences (Coppersmith et al., 2015).\n2.2 Fusion approaches for mental\ndisorders detection\nHow to effectively combine information is\nchallenging and has a long history in machine\nlearning (Baltrusaitis, Ahuja, and Morency,\n2019). In particular, some recent works on\nmental disorders detection have considered\nthe use of ensemble approaches to combinebag of words representations, LIWC features,\nand different deep neural models (Trotzek,\nKoitka, and Friedrich, 2018). In (Ragheb\net al., 2019), the authors combine the tem-\nporal mood variation and Bayesian inference\nfor their detection. The first phase uses an\nattention-based deep model to construct a\nrepresentation for the mood variation. Then,\nin the second phase, the model uses Bayesian\ninference to detect clear signs of mental dis-\norders and then give a decision based on their\ncombination. In (Ji et al., 2020), the authors\napply an attention model combined with sen-\ntiment and topic analysis to detect suicidal\nideation. A recent work (Uban, Chulvi, and\nRosso, 2021), explored the evolution of emo-\ntion expression in relation to cognitive styles\nand found specific patterns in users with\nmental disorders.\nThe performance shown by ensemble ap-\nproaches suggests the suitability of adapt-\ning advanced techniques to fusion informa-\ntion from different channels in a more effec-\ntive way. For that reason, we decided to im-\nplement our multi-channel BERT-based ap-\nproach as a new way to combine the the-\nmatic, emotional and stylistic views of the in-\nformation shared by social media users. This\nproposed model takes inspiration from multi-\nmodal BERT (Kiela et al., 2019) in order to\nhave an adapted version that is able to model\nindividual channels. As our experimental\nevaluation will show, the proposed strategy\nimproves the classification performance.\n3 Information channels\nrepresentation\nThe fusion strategies that we are going to ex-\nplore are implemented on the basis of BERT,\ntherefore, the three channels of information\nare captured by three different representa-\ntions of the words. The main idea of this\napproach is to have different views of the con-\ntent of the users\u2019 posts. We achieve this by\ngenerating three embeddings for each word in\nthe posts, capturing or emphasizing the the-\nmatic, emotional and style information, re-\nspectively. In the following subsections, we\nbriefly describe how to generate these three\nrepresentations.\n3.1 Thematic embeddings\nIn order to capture the thematic content re-\nlated to each word, we consider vanilla GloVe\nembeddings (Pennington, Socher, and Man-\n29\nInformation fusion for mental disorders detection: multimodal BERT against fusioning multiple BERTs ning, 2014). However, since this type of\nembeddings does not take into account the\ncontext of the words, we decided to also\nuse some contextualized word embeddings,\nin particular the BERT embeddings (Devlin\net al., 2019). For our experiments, we used\nboth separately and evaluated which one con-\ntributes the most to the final representation.\nWith contextualized embeddings, words\nthat have similar meaning or show some se-\nmantic relation are closer to each other. For\nexample, the words \u201cinsecure\u201d and \u201cworried\u201d\nhave similar embeddings, as do the words\n\u201ctherapy\u201d and \u201ctreatment\u201d.\n3.2 Emotion-based embeddings\nFor this work, we use the emotion-based word\nembeddings that were originally proposed in\n(Aragon et al., 2019). In short, to construct\nthese vectors, first, we generated groups of\nfine-grained emotions for each general emo-\ntion that belong to the EmoLEX lexicon (Mo-\nhammad and Turney, 2013). We achieved\nthis by representing each word of the lexicon\nwith its FastText embedding (Bojanowski et\nal., 2016) and then applying a clustering al-\ngorithm on them. After obtaining the fine-\ngrained emotions, which are groups of words\nthat capture specific topics related to the\nsame emotion, we represented each of them\nby means of the average vector of its words.\nSubsequently, and as the last step, for each\nword in the vocabulary we measured its co-\nsine similarity with all fine-grained emotions,\nand assigned to each one of the embedding\nfrom its closest fine-grained emotion.\nAccording to the process described above,\nthe words \u201caccident\u201d and \u201ccrash\u201d will have\nthe same embedding because both belong\nto the same subgroup of the Surprise emo-\ntion, whereas the word \u201cmagician\u201d will have\na slightly different embedding since it corre-\nsponds to a different subgroup of the same\nemotion. On the other hand, the words \u201cac-\ncomplish\u201d and \u201cachieve\u201d will have a com-\npletely different embedding as they belong to\nthe Joy emotion.\n3.3 Style-based embeddings\nThe third representation of the words aims to\ncapture particular characteristics of the writ-\ning style of social media users. Its idea is\nto capture how users with mental disorders\ntend to talk, for example, referring to past\nevents or to uncertainties about the future.To capture the stylistic information, we pro-\npose a new word representation inspired by\nthe successful use of character n-grams in au-\nthor profiling tasks.\nTo define the style-based embedding of\neach word from the users\u2019 posts, we carried\nout the following process:\n1. Divide the word into character 3-grams.\n2. Compute, for each 3-gram, its embed-\nding using FastText (Bojanowski et al.,\n2016), as well as its discriminative score\naccording to its chi2distribution in the\ntwo given classes (positive and control\nusers).\n3. Obtain the embedding vector of the\nword by applying a weighted sum of the\nvectors of its character 3-grams, consid-\nering as weights their chi2values.\nTake for example the word \u201cdepression\u201d,\nits style-based embedding is obtained by the\nweighted sum of the vectors corresponding to\nits character 3-grams \u201cdep\u201d, \u201cepr\u201d, ..., \u201cion\u201d.\nIt is important to notice that the style-based\nembeddings are similar for words that have\nsimilar spelling rather than meaning. For\nexample, words in superlative resemble each\nother, as well as regular verbs in past tense,\nor words with the same root. Take for in-\nstance the word \u201cmental\u201d some of their clos-\nest words would be \u201cdental\u201d, \u201cmentality\u201d or\n\u201cdecremental\u201d.\n4 On the fusion of the three\nchannels\nThe objective of our work is to compare dif-\nferent ways of combining information from\ndifferent channels. This is a key stage in the\nclassification process, and have the intuitive\nidea of learning the relevance of each channel\nin an automatic way. We use two main strate-\ngies, firstly, one based on multimodal BERT\nwhose idea is to learn a joint representation\nof the three types of information, and sec-\nondly, different architectures that treat each\nchannel separately and apply different early\nand late fusion techniques.\n4.1 Multimodal BERT (MMBT)\nIt is a recently proposed supervised mul-\ntimodal bitransformer model for classifying\nimages and text (Kiela et al., 2019). The\nMMBT model starts with pre-trained BERT\nMario Ezra Arag\u00f3n, A. Pastor L\u00f3pez-Monroy, Luis C. Gonz\u00e1lez-Gurrola, Manuel Montes-y-G\u00f3mezweights, and takes their contextual embed-\ndings as input. These contextual embeddings\nare obtained as the sum of the segment, po-\nsition, and token embeddings of each word.\nThen, the model weights them and project\neach of the embeddings to a token input. In\nFigure 1, we can appreciate the components\nof the architecture of the MMBT model. Al-\nthough proposed for only two modalities, this\narchitecture can be generalized to any num-\nber of modalities, assigning a different seg-\nment id to each of them.\nFigure 1: Multimodal bitransformer architec-\nture proposed in (Kiela et al., 2019).\nIn the original work, the authors took\ncontextual embeddings as input, learned the\nweights, and projected each image\u2019s embed-\ndings to a dimensional token input. Instead\nof image embeddings, we used the sequence\nof the words for the fine-tuning with our\ndifferent channels as embeddings. Once we\nfine-tuned the model with the channels, we\ntook advantage of the contextual informa-\ntion learned for classifying the users\u2019 posts.\nFor this purpose, we used the first output of\nthe final MMBT layer as input to a Convo-\nlutional Neural Network (CNN) for feature\nextraction, and then add a dense layer for\nachieving the classification. Figure 2 presents\nthe general architecture of this approach.\n4.2 Combining information using\nmultiple BERTs\nThe authors of MMBT (Kiela et al., 2019)\nnoted that their method is compatible with\nscenarios where not every modality is present\nand can be generalized to an arbitrary num-\nber of modalities. Then, for the second\nmodel, we decided to train individual BERTs\nand fine-tuning them with each channel sepa-\nrately. After the training, similar to the first\napproach, we used the first output of the final\nlayer of each BERT and concatenate them\nas input for a CNN layer, we will refer to\nFigure 2: General diagram of the Model\n1: Multimodal BERT with vectors of three\nchannels, then a CNN layer, and a classifica-\ntion layer.\nthis model as BERT-CNN. In Figure 3, we\npresent the general diagram for this process.\nFigure 3: General diagram of the BERT-\nCNN model: Each channel separately enters\nto a BERT model, then join vectors feed a\nsingle CNN layer, and a classification layer.\nFor the third model, instead of concate-\nnating the vectors and using a single CNN\nlayer, we separate them into different con-\nvolutional layers and used the output for a\ndense layer. With this approach, the model\nobtains for each channel different feature\nmaps of each region and concatenates them\ntogether to form a single feature vector. This\ncan be interpreted as summarizing the local\ninformation to find patterns, and then com-\nbining the information. The hypothesis that\nthe local information per channel is impor-\ntant and should be extracted before it is com-\nbined, we call this model BERT-3CNN. Fig-\nure 4 presents the general diagram for this\nmodel.\n31\nInformation fusion for mental disorders detection: multimodal BERT against fusioning multiple BERTs Figure 4: General diagram of the BERT-\n3CNN model: Each channel separately enters\na BERT model and a CNN layer, then their\noutputs are concatenated and fed to a single\nclassifier.\nOne of the challenges of this work is the\nproblem of fusing information. A simple so-\nlution is to concatenate the representations\nof each channel into one vector or perform\nan operation like adding or taking the prod-\nuct. However, the use of these operations as-\nsumes that all channels have the same rele-\nvance, which is usually not the case. In re-\ncent work (Arevalo et al., 2019), the authors\nproposed a novel type of hidden unit called\nGated Multimodal Unit (GMU). This unit\nworks similarly to the control flow mechanism\nin gated recurrent units. The gates in the\nunit let the model regulate the flow of infor-\nmation into the next one. Figure 5 presents\na general overview of the GMU module we\nused, where the xiinputs represent the fea-\nture vectors associated with each modality,\nand the ziweights indicate their relevance.\nFigure 5: Overview of GMU module (Arevalo\net al., 2019). Where xirepresents the ithin-\nput modality. The final fused representation\nof all modalities is represented by hat the\ntop.\nMotivated by the outstanding results ofthe GMU module in different multimodal\ntasks, our fourth model takes advantage of\nit. That is, after the feature extraction, we\nimplement a Gated Multimodal Unit (GMU)\nmodule to learn the relations between each\nchannel feature vector. Then, apply a dense\nlayer to classify the final vector with the in-\nformation of the three channels, we call this\nmodel BERT-GMU. Figure 6 describes the\nprocess for this model.\nFigure 6: General diagram of the BERT-\nGMU model: Each channel separately enters\na BERT model and a CNN layer, then their\noutputs are combined by a GMU module, and\nfed to a single classifier.\n5 Experimental settings\n5.1 Data sets\nWe performed experiments over data sets\nfrom the eRisk 2019 and 2020 evaluation\ntasks (Losada, Crestani, and Parapar, 2019;\nLosada, Crestani, and Parapar, 2020). These\ndata sets consist of the detection of depres-\nsion, anorexia, and self-harm, and contain\nthe post history of several users from the\nReddit platform. For each mental disorder,\nwe have two types of users: 1) the control\ngroup, people collected who do not suffer\nfrom any mental disorder; and 2) positive\nusers, a group composed of people affected\nby either depression, anorexia, or self-harm.\nIn the tasks of anorexia and self-harm, the\npositive class is composed of users who explic-\nitly mentioned that they were diagnosed by a\nmedical specialist or that they had commit-\nted self-harm. On the other hand, the control\nclass for both tasks is composed of random\nusers from the Reddit platform. However, to\nadd realism to the data sets and make the\nMario Ezra Arag\u00f3n, A. Pastor L\u00f3pez-Monroy, Luis C. Gonz\u00e1lez-Gurrola, Manuel Montes-y-G\u00f3mezdetection of positive users challenging, the\ncontrol group also contains users who often\ninteract in the threads of anorexia and self-\nharm.\nFor depression, the organizers of the\nshared task asked the participants to predict,\nfor each user, the possible answer to each in-\nput of the BDI questionnaire (Beck et al.,\n1961), which contains 21 questions that allow\nassessing the level of severity of the depres-\nsion. In contrast to them, in this work we\nexclusively consider a binary prediction task,\ni.e., to distinguish between positive and con-\ntrol users. In particular, the positive class is\ncomposed of users that obtained 21 points or\nmore in the final result of the questionnaire\n(presence of moderate or severe depression),\nwhereas the control class is formed by the\nrest of the users, having 20 points or fewer in\ntheir final result.\nTable 1 shows how classes distribute\nwithin these data sets as well as some general\ninformation regarding the collections. For\nthe depression task, we used for training the\ndata set from eRisk 2018 (Losada, Crestani,\nand Parapar, 2018), this data set was con-\nstructed similarly to anorexia and self-harm\ndata sets.\nData set Train Test\nP C P C\nAnorexia\u201919 61 411 73 742\navg. num. posts 407.8 556.9 241.4 745.1\navg num. words 37.3 20.9 37.2 21.7\navg. days 800 650 510 930\nDepression\u201920 214 1493 40 49\navg. num. posts 440.9 660.8 493.0 543.7\navg num. words 27.5 22.75 39.2 45.6\navg. days 686 663 642 1015\nSelf-harm\u201920 41 299 104 319\navg. num. posts 169.0 546.8 112.4 285.6\navg num. words 24.8 18.8 21.4 11.9\navg. days 495 500 270 426\nTable 1: Data sets used for experimentation,\nwhere P indicates the positive users and C is\nused for control users.\n5.2 Preprocessing\nThe texts were normalized by lowercasing all\nwords and removing special characters like\nURLs, emoticons, and #; the stopwords were\nkept. Our decision to keep stopwords was\ncompletely experimental, we performed ex-\nperiments removing them before masking the\ntexts, but consistently we got slightly lower\nperformances.5.3 Classification\nThe main goal is to classify users into one\nof the two classes (Depressed / Control,\nAnorexia / Control, or Self-harm / Control).\nWe separate each post history into Nparts.\nWe select the Nvalue empirically, testing rec-\nommended sizes of sequences in the litera-\nture, i.e., N={25,35,50,100}. For training,\nwe process each part of the post history as\nan individual input and train the model. For\nthe test, each part receives a label of 1 or 0;\nthen, if the majority of the posts are posi-\ntive, the user is classified as showing a men-\ntal disorder. The main idea is to consistently\ndetect the presence of major signs of depres-\nsion, anorexia, or self-harm through all the\nuser posts.\n5.4 Baselines\nThe results are compared to the traditional\nBag-of-Words representation combined with\na SVM classifier. This representation was\ncreated using word unigrams and n-grams;\nthese are common baseline approaches for\ntext classification. For both approaches, we\nselected the same number of features us-\ning tf-idf representation and chi2distribution\nX2\nk. We also add some baselines based on\ndeep learning approaches, using a CNN and\na Bi-LSTM. The neural networks used 100\nneurons, an adam optimizer, and word2vec\nand Glove embeddings with a dimension of\n300. For the CNN we use 100 random fil-\nters of sizes 1, 2, and 3 (parameters recom-\nmended in literature). We also add a BERT\nmodel with a fine-tuning over the training\ndata set. Additionally, the obtained results\nare compared against the top-three partic-\nipants of the eRisk evaluation tasks. For\nall these comparisons, we considered the F1\nscore, precision, and recall over the positive\nclass (Losada, Crestani, and Parapar, 2018).\n6 Evaluation and Analysis\nFor the evaluation, besides the baselines, we\nalso performed experiments using indepen-\ndently the proposed thematic, emotion and\nstyle representations. Table 2 presents the\nresults in terms of F1score, precision, and re-\ncall over the positive class to detect Anorexia\n(eRisk\u201919), Depression (eRisk\u201920) and Self-\nharm (eRisk\u201920). We organize the results\nin three groups: baseline methods, our pro-\nposal but limited to only one channel, and\nour proposal using all information channels\n33\nInformation fusion for mental disorders detection: multimodal BERT against fusioning multiple BERTs Method Anor Dep SH\nBaselines\nF1 P R F1 P R F1 P R\nBoW-unigrams 0.67 0.85 0.55 0.58 0.56 0.60 0.50 0.95 0.34\nBoW-Ngrams 0.66 0.83 0.55 0.57 0.55 0.59 0.50 0.92 0.33\nBag of char 3grams 0.67 0.85 0.55 0.58 0.56 0.60 0.52 0.97 0.36\nRNN-word2vec 0.65 0.95 0.49 0.57 0.62 0.53 0.55 0.60 0.51\nCNN-word2vec 0.66 0.94 0.52 0.60 0.57 0.62 0.56 0.54 0.59\nRNN-GloVe 0.65 0.92 0.51 0.58 0.59 0.57 0.57 0.62 0.53\nCNN-GloVe 0.67 0.93 0.52 0.61 0.56 0.68 0.57 0.62 0.53\nRNN-Attention 0.66 0.94 0.52 0.50 0.67 0.40 0.58 0.76 0.47\nBest eRisk participants\n1st 0.71 0.64 0.79 - - - 0.75 0.82 0.69\n2nd 0.68 0.77 0.60 - - - 0.62 0.62 0.62\n3rd 0.68 0.67 0.68 - - - 0.62 0.59 0.65\nOur methods: Single-channel\nThematic 0.77 0.70 0.85 0.62 0.55 0.72 0.60 0.44 0.94\nEmotion 0.70 0.85 0.60 0.61 0.62 0.61 0.63 0.68 0.59\nStyle 0.69 0.86 0.57 0.62 0.64 0.61 0.64 0.70 0.59\nOur methods: Multi-channel Bi-transformers\nMMBT 0.76 0.72 0.84 0.65 0.51 0.91 0.65 0.75 0.58\nBERT-CNN 0.82 0.81 0.82 0.70 0.54 0.96 0.70 0.73 0.68\nBERT-3CNN 0.80 0.81 0.79 0.68 0.53 0.95 0.70 0.69 0.71\nBERT-GMU 0.81 0.80 0.81 0.70 0.55 0.95 0.73 0.73 0.74\nTable 2: F1, precision and recall results over the positive class in three eRisk\u2019s tasks.\ncombined by the multimodal transformer as\nwell as by different architectures based on\nmultiple BERTs.\nFrom this evaluation, we observe that\nmost of our proposals outperform the base-\nline results. Firstly, the single-channel repre-\nsentations obtain an improvement in compar-\nison with baselines, in particular those based\non style and emotion information. Unex-\npectedly, for the baselines, the performance\nof deep learning models applied over word-\nbased representations is closer to traditional\napproaches using a Bag-of-Words. We think\nthis could be due to the small size of the\ndata set and the intersection of thematic con-\ntent. Something interesting to notice is that\nCNN networks obtain better performance\nthan RNN networks. The latter could be be-\ncause CNN networks search for the presence\nof specific local information important for the\ndetection of these disorders.\nFor our representations based on the fu-\nsion of information, their performance is\nhigher in comparison with the other mod-\nels, suggesting the relevance of combining the\ninformation from different channels. Some-\nthing interesting to notice is that all models\nusing multiple BERTs outperformed the mul-timodal BERT model in F1, indicating that,\nfor this particular task, and with this way of\nrepresenting the channels, it is better to rep-\nresent each channel independently and com-\nbine them later. In general, the model that\nuse the GMU module showed the best aver-\nage performance, which suggest that weight-\ning the information helps to create a better\nrepresentation of the posts and the users.\nFrom these experiments, we highlight the\nfollowing observations:\n1. Most single-channel representations ob-\ntain better results than the baselines.\n2. The architectures using multiple BERTs\nobtained better performance than the\nmultimodal BERT.\n3. These results confirms our intuition that\nlearning to combine different types of\ninformation is very relevant to capture\nsigns of mental disorders in users.\n4. In general, our models obtain an har-\nmonic result between precision and re-\ncall deriving in a better F1 score.\nMario Ezra Arag\u00f3n, A. Pastor L\u00f3pez-Monroy, Luis C. Gonz\u00e1lez-Gurrola, Manuel Montes-y-G\u00f3mez6.1 Comparison against the eRisk\nparticipants\nTo expand our analysis and add context to\nthe results, we also include a comparison\nagainst the original participants of the eRisk\ntasks. These evaluation forum considers a\ntotal of 54 models for the anorexia detection\ntask and 57 for the self-harm detection task in\neRisk-19 and 20 editions (Losada, Crestani,\nand Parapar, 2019; Losada, Crestani, and\nParapar, 2020). It is important to mention\nthat the participants focused on obtaining\nearly and accurate predictions on the users,\nwhile our approach focuses exclusively on de-\ntermining accurate classifications.\nWe observe that our models achieve com-\npetitive results in both tasks; they sur-\npassed the first place results in Anorexia, and\nshowed a slightly lower performance than the\nfirst place in Self-harm. For the depression\ntask, organizers changed the evaluation strat-\negy. While our approach focuses on binary\nclassification, the eRisk task considered the\nassessment of the level of depression sever-\nity for each user. For this reason, we cannot\ndirectly compare our results against the par-\nticipants.\n6.2 Contribution of each\ninformation channel\nTo understand how each information chan-\nnel contributes to the final decision we will\nutilize the GMU units in the fourth model\n(BERT-GMU) and analyze the weighting of\nthe gates, where the gates in the unit let the\nmodel regulate the flow of information into\nthe next one. The main idea in a GMU is that\nthe unit learns to weigh the modalities (chan-\nnels for us) and fuse them according to their\nrelevance. A GMU works similar to a neural\nnetwork layer and finds an intermediate rep-\nresentation based on the different modalities.\nFor this analysis, we obtained the gates\u2019\nzivalues of the GMU module correspond-\ning to the test set posts. Figure 7 presents\nthe results for the three tasks, where each\nvalue already takes into account the aver-\nage of all posts. For anorexia, we can ap-\npreciate that the thematic information con-\ntributes the most to the final decision, fol-\nlowed by emotion and style information. For\ndepression, we can observe that the thematic\ninformation is also the most important and\nthe value for the style information is higher\nthan the emotional value. Finally, for theself-harm task, the thematic information ob-\ntains the lowest value and the style informa-\ntion the highest. In general, it can be no-\nticed how the activation for each channel are\ndifferent depending on the mental disorder.\nSomething interesting is that the thematic\nchannel presents the highest variation, with\nthe lowest value in self-harm and the highest\nvalue in anorexia. We think that this vari-\nation indicates that the posts of users who\nsuffer from anorexia are probably more ho-\nmogeneous than those who suffer self-harm.\nFor a further analysis of the GMU, Table\n3 presents the posts of the depression data\nset with the highest zivalue for each chan-\nnel. We can notice that the posts are related\nto personal opinions, different topics, and in\ngeneral express negative emotions even when\nthey are not directly related to mental disor-\nders. Take for example the emotion channel\nwhere the post is related to regrets in life and\nfeelings, or the style channel where the post\ntalks about a mental illness, but also contains\nwords such as \u201cdon\u2019t\u201d and \u201cnothing\u201d.\nFigure 7: Average zivalue for the three men-\ntal disorders over the test set instances.\n7 Conclusion and future work\nIn this work, we explored the detection of\nusers that suffer anorexia, depression, or self-\n35\nInformation fusion for mental disorders detection: multimodal BERT against fusioning multiple BERTs Channel Post\nthematic \u201d...I have no idea what either of\nthem were trying to communicate\ntbh i was having a really good\ndayand then you had to bring up\nhughes...\u201d\nemotion \u201d...take the chance and have no\nregrets in life, its always bet-\nter to know if the other person\nfeels something so that you are\nnot wasting your time...\u201d\nstyle \u201d...these days parents don\u2019t know\na whole lot about mental illness\nthey were told it was nothing so\nthat\u2019s all...\u201d\nTable 3: Posts with highest zivalue for each\nchannel over the depression task.\nharm. For this task, we used the users\u2019 the-\nmatic interests, emotions and writing style.\nWe tested different strategies to combine\nthese information channels inspired by the\nusage of transformers to learn contextual\nknowledge. Our results suggest that enrich-\ning emotional and style data using a trans-\nformer improves the detection of users with\nmental disorders. Moreover, a striking re-\nsult is the superiority of using a combina-\ntion of multiple BERTs over the recent mul-\ntimodal BERT transformer; this finding by\nits own opens an opportunity to explore\nmodels inspired by transformers to create\nnew representations and continue improving\nthe performance in the detection of people\nwith mental disorders. The results outper-\nform traditional and state-of-the-art base-\nlines and are competitive with the perfor-\nmance of top eRisk participants. We believe\nthat it is important to mention that these\nmodels, although they obtain better results,\nare extremely resource-consuming (proces-\nsor, memory, energy, etc.) in comparison\nwith simple models. For future work, we\nwant to explore more sophisticated combi-\nnation techniques that could improve the re-\nsults and understanding of mental disorders\ndetection, for example, multi-modal trans-\nformers. We note that most of the analysis\nof mental disorders has been made for the\nEnglish language, then, one of our interests\nlies in the expansion of this study to Spanish\nlanguage.References\nAragon, M. v., A. Lopez-Monroy,\nL. Gonzalez-Gurrola, and M. Montes-y\nGomez. 2019. Detecting depression in\nsocial media using fine-grained emotions.\nProceedings of the 2019 Conference of\nthe North American Chapter of the As-\nsociation for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers).\nArevalo, J., T. Solorio, M. Montes-y G\u00b4 omez,\nand F. Gonz\u00b4 alez. 2019. Gated multi-\nmodal networks. Neural Computing and\nApplications.\nBaer, J. 2021. Multimodal machine\nlearning: A survey and taxonomy.\nhttps://www.convinceandconvert.com/social-\nmedia-research/social-media-usage-\nstatistics/.\nBaltrusaitis, T., C. Ahuja, and L. Morency.\n2019. Multimodal machine learning: A\nsurvey and taxonomy. IEEE Transactions\non Pattern Analysis and Machine Intelli-\ngence, 41(2):423\u2013443 .\nBeck, A., C. Ward, M. Mendelson, J. Mock,\nand J. Erbaugh. 1961. An inventory for\nmeasuring depression. JAMA Psychiatry\n4(6), 561\u2013571 .\nBojanowski, P., E. Grave, A. Joulin, and\nT. Mikolov. 2016. Enriching word vectors\nwith subword information. Transactions\nof the Association for Computational Lin-\nguistics.\nCoopersmith, G., M. Dredze, and C. Har-\nman. 2014. Quantifying mental health\nsignals in twitter. Workshop on Compu-\ntational Linguistics and Clinical Psychol-\nogy.\nCoppersmith, G., M. Dredze, C. Harman,\nand K. Hollingshead. 2015. From adhd\nto sad: analyzing the language of mental\nhealth on twitter through self-reported di-\nagnoses. Proceedings of the 2nd Workshop\non Computational Linguistics and Clinical\nPsychology , pages 1\u201310.\nDe Choudhury, M., S. Counts, and\nE. Horvitz. 2013. Social media as a\nmeasurement tool of depression in popu-\nlations. In Proceedings of the 5th Annual\nACM Web Science Conference .\nMario Ezra Arag\u00f3n, A. Pastor L\u00f3pez-Monroy, Luis C. Gonz\u00e1lez-Gurrola, Manuel Montes-y-G\u00f3mezDevlin, J., M. Chang, K. Lee, and\nK. Toutanova. 2019. Bert: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. NAACL-HTL.\nDyson, M., L. Hartling, J. Shulhan,\nA. Chisholm, A. Milne, P. Sundar,\nS. Scott, and A. Newton. 2016. A system-\natic review of social media use to discuss\nand view deliberate self-harm acts. PLOS\nONE.\nGuntuku, S., D. Yaden, M. Kern, L. Ungar,\nand J. Eichstaedt. 2017. Detecting de-\npression and mental illness on social me-\ndia: an integrative review. Current Opin-\nion in Behavioral Sciences , pages 43\u201349.\nHilton, C. 2016. Unveiling self-harm be-\nhaviour: what can social media site twit-\nter tell us about self-harm? a qualitative\nexploration. Journal of clinical nursing.\nHtait, A., S. Fournier, and P. Bellot.\n2017. Lsis at semeval-2017 task 4: Using\nadapted sentiment similarity seed words\nfor english and arabic tweet polarity clas-\nsification. Proceedings of the 11th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval-2017).\nJi, S., X. Li, Z. Huang, and E. Cambria.\n2020. Suicidal ideation and mental disor-\nder detection with attentive relation net-\nworks. arXiv:2004.07601.\nKang, K., C. Yoon, and E. Kim. 2016. Iden-\ntifying depressive users in twitter using\nmultimodal analysis. In Big Data and\nSmart Computing (BigComp), 2016 Inter-\nnational Conference on. IEEE, 231\u2013238.\nKessler, R., E. Bromet, P. Jonge, V. Shahly,\nand Marsha. 2017. The burden of depres-\nsive illness. Public Health Perspectives on\nDepressive Disorders.\nKiela, D., S. Bhooshan, H. Firooz, E. Perez,\nand D. Testuggine. 2019. Supervised mul-\ntimodal bitransformers for classifying im-\nages and text. Visually Grounded Inter-\naction and Language (ViGIL), NeurIPS\n2019 Workshop.\nLosada, D., F. Crestani, and J. Parapar.\n2018. Overview of erisk 2018: Early risk\nprediction on the internet (extended lab\noverview). Proceedings of the 9th Inter-\nnational Conference of the CLEF Associ-\nation, CLEF 2018, Avignon, France.Losada, D., F. Crestani, and J. Parapar.\n2020. Overview of eRisk 2020: Early\nRisk Prediction on the Internet. Exper-\nimental IR Meets Multilinguality, Multi-\nmodality, and Interaction Proceedings of\nthe Eleventh International Conference of\nthe CLEF Association (CLEF 2020).\nLosada, D. v., F. Crestani, and J. Parapar.\n2019. Overview of erisk 2019: Early risk\nprediction on the internet. Experimental\nIR Meets Multilinguality, Multimodality,\nand Interaction. 10th International Con-\nference of the CLEF Association, CLEF\n2019, Lugano, Switzerland .\nMohammad, S. and P. Turney. 2013. Crowd-\nsourcing a word-emotion association lexi-\ncon. Computational Intelligence .\nPennington, J., R. Socher, and C. Manning.\n2014. Glove: global vectors for word rep-\nresentation. In Proceedings of the Con-\nference on Empirical Methods on Natural\nLanguage Processing.\nPreotiuc-Pietro, D., J. Eichstaedt, G. Park,\nM. Sap, L. Smith, V. Tobolsky,\nH. Schwartz, and L. Ungar. 2015.\nThe role of personality, age and gender\nin tweeting about mental illnesses. In\nProceedings of the 2nd Workshop on\nComputational Linguistics and Clinical\nPsychology .\nQianli, M., S. Lifeng, C. Enhuan, T. Shuai,\nW. Jiabing, and C. Garrison. 2017. Walk-\ning walking walking: Action recognition\nfrom action echoes. Twenty-Sixth Inter-\nnational Joint Conference on Artificial In-\ntelligence .\nRagheb, W., J. Aze, S. Bringay, and M. Ser-\nvajean. 2019. Attentive multi-stage learn-\ning for early risk detection of signs of\nanorexia and self-harm on social media.\nProceedings of the 10th International Con-\nference of the CLEF Association, CLEF\n2019, Lugano, Switzerland .\nRenteria-Rodriguez, M. 2018. Salud men-\ntal en mexico. NOTA-INCyTU N \u00b4UMERO\n007.\nTausczik, Y. and J. Pennebaker. 2010. The\npsychological meaning of words: Liwc\nand computerized text analysis methods.\nJournal of Language and Social Psychol-\nogy, pages 24\u201354.\n37\nInformation fusion for mental disorders detection: multimodal BERT against fusioning multiple BERTs Trifan, A. and J. Oliveira. 2019.\nBioinfo@uavr at erisk 2019: delving\ninto social media texts for the early\ndetection of mental and food disorders.\nProceedings of the 10th International\nConference of the CLEF Association,\nCLEF 2019, Lugano, Switzerland.\nTrotzek, M., S. Koitka, and C. Friedrich.\n2018. Word embeddings and linguistic\nmetadata at the clef 2018 tasks for early\ndetection of depression and anorexia. Pro-\nceedings of the 9th International Con-\nference of the CLEF Association, CLEF\n2018, Avignon, France.\nUban, A., B. Chulvi, and P. Rosso. 2021. An\nemotion and cognitive based analysis of\nmental health disorders from social media\ndata. Future Generation Computer Sys-\ntems.\nVan Rijen, P., D. Teodoro, N. Naderi, L. Mot-\ntin, J. Knafou, M. Jeffryes, and P. Ruch.\n2019. A data-driven approach for measur-\ning the severity of the signs of depression\nusing reddit posts. Proceedings of the 10th\nInternational Conference of the CLEF As-\nsociation, CLEF 2019, Lugano, Switzer-\nland.\nWang, T., M. Brede, A. Ianni, and\nE. Mentzakis. 2017. Detecting and\ncharacterizing eating-disorder communi-\nties on social media. In Proceedings of\nthe Tenth ACM International conference\non web search and data mining.\nWorld Health Organization, W.\n2019. Mental health: Fact sheet.\nhttps://www.euro. who.int/en/health-\ntopics/noncommunicable-\ndiseases/mental-health .\nMario Ezra Arag\u00f3n, A. Pastor L\u00f3pez-Monroy, Luis C. Gonz\u00e1lez-Gurrola, Manuel Montes-y-G\u00f3mezUn redactor asistido para adap tar textos administrativos  \na lenguaje claro \nA writing assistant to adapt administrative texts  into plain  language  \nIria da Cunha  \nUniversidad Nacional de E ducaci\u00f3n a  Distancia  \niriad@flog.uned.es  \n \nResumen:  El lengu aje claro ab oga por que los textos dirigidos  a los ciudadanos est\u00e9n \nredactados en un lenguaje m\u00e1s sencillo y transparente, para que estos puedan entender \nf\u00e1cilmente el mensaje que se les quiere transmitir . En e ste contexto , nuestro  objet ivo es \ndesarrollar un redactor asistido para el espa\u00f1ol que ayude al personal de la \nAdministraci\u00f3n p\u00fablica a escribir en lenguaje claro los textos que dirige a la ciudadan\u00eda. \nEl sistema, gratuito y en l\u00ednea , integra diferentes herramientas de Procesamiento  de \nLenguaje Natural (PLN ) para  detectar en los textos escritos por los usuarios los rasgos \nling\u00fc\u00edsticos que interfieren con las recomendaciones sobre lenguaje claro . Asimismo, \nofrece al usuario informaci\u00f3n para hacer m\u00e1s sencillo su texto. Para ev aluar los \nalgoritmos se emple \u00f3 un corpus anotado manualmente , y las medidas de precisi\u00f3n y \ncobertura . Los resultados son muy positivos, aunque tambi\u00e9 n ref lejan algunos aspectos  \nque se pueden mejorar en el fut uro. \nPalabras clave:  Lenguaje claro, redacci\u00f3n  asistida,  Procesam iento de Lenguaje Natu ral \n(PLN ), Admi nistraci\u00f3n  p\u00fablica . \nAbstract: Plain language  advocates t hat texts  addressed to citizens should be written in  \nsimpler and more transparent language, so that they can e asily  understand the message to \nbe co nveyed. In this context, our  aim is to develop an assisted writing tool  for Spanish to \nhelp Public Administration staff to write texts addressed  to citizens  in plain  language . The \nsystem  is free and online . It integrates different Natural Language Processing (NLP) tools \nto detect the linguisti c features that interfere with plain language recommendations  in the \ntexts written by users. It also provides users with information to make their text clearer. A manually annotated corpus , and pr ecision and recall me asures were used to evalua te the \nalgori thms. The results are very positive, although they also hig hlight some aspects that \ncould be improved in the future.  \nKeywords:  Plain Language, Assisted  Writing , Natural Language Processing (NLP), \nPublic Administration. \n \n1 Introducci\u00f3n  \nEl lenguaje claro es u na corriente que aboga por \nque los textos que se dirijan a los ciudadanos \nest\u00e9n redactados en un lenguaje m\u00e1s sencillo y \ntransparente, para que estos puedan entender el mensaje que se les quiere transmitir y, as\u00ed , \ncontribui r a que ejerzan sus derechos y a que \ncumplan con sus obligaciones. Seg\u00fan el sitio web de la International Plain Language \nFederation ,\n1 la comunicaci\u00f3n en lenguaje claro \ntiene unos rasgos concretos: \u201cA commu nication \n1 https://www.iplfederation.org/plain -language/  is in plain  langua ge if its wording, struc ture, \nand design are so clear that the intended readers can easily find what they need, understand what \nthey find, and use that information\u201d.  \nEsta corriente adquiere una relevancia \nespecial en el marco de la modernizaci\u00f3n del  \nlenguaje jur\u00eddico y administra tivo, uno de los \ngrandes retos de la Administraci\u00f3n espa\u00f1ola \n(Cassany, 2005; Montol\u00edo, 2012; Carretero y Fuentes, 2019; Montol\u00edo y Tasc\u00f3n, 2020; Bay\u00e9s, 2021). El reto es cambiar una tradici\u00f3n en la Administraci\u00f3n que hace que los textos \nque recibe la ciudadan\u00eda (notificaciones, \nrequerimientos, resoluciones, multas, etc.) \nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 39-49\nrecibido 31-03-2022 revisado 20-05-2022 aceptado 30-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-3\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturaltengan unas caracter\u00edsticas ling\u00fc\u00edsticas que \n\u201coscurecen\u201d el texto, como p\u00e1rrafos largu\u00edsimos, oraciones con m\u00faltiples subordinadas e incis os, verbos en voz pa siva, en \ngerundio y en participio, formas verbales arcaicas, ter minolog\u00eda y fraseolog\u00eda muy \ncompleja, multitud de negaciones y formas subjetivas, entre otros rasgos (De Miguel, 2000; Alcaraz, Hugues y G\u00f3mez, 2014).  \nPor e sta raz\u00f3n , en l os \u00faltim os a\u00f1os se h an \nelaborado gu\u00edas y manuales para ayudar a \nredactar en lenguaje claro los textos que se dirigen a los ciudadanos. Algunos ejemplos son las publicaciones de la Comisi\u00f3n Europea (2015), Carretero et al. (2017), y Mont ol\u00edo y \nTasc\u00f3n (2017) . Sin embargo, a pesar de estas \nmerito rias aportaciones, son escasos los \ntrabajos que  establecen sinergias entre el \nlenguaje claro en espa\u00f1ol y la tecnolog\u00eda.  \nEn este contexto, el objetivo de la presente \ninvestigaci\u00f3n  es desarrollar el primer redactor \nasistid o para el espa\u00f1ol que ayude al personal \nde la Administraci\u00f3n p\u00fablica a escribir en lenguaje claro los textos que dirige a la ciudadan\u00eda. Este redactor asistido  se llama \n\u201carText claro \u201d, puede utilizarse grat uitamente  y \nest\u00e1 disponible e n l\u00ednea  desde la direcci\u00f3n: \nhttp://sistema -artext.com/ . El sistema  integra \ndiferentes herramientas de Procesamiento de Lenguaje Na tural (PLN), gracias a las  cuales \nlogra detectar en los textos escritos por los usuarios los rasgos ling\u00fc\u00edsticos que interfieren con las recomendaciones sobre lenguaje claro \nm\u00e1s habituales. Asimismo, ofrece al usuario \ninformaci\u00f3n para hacer m\u00e1s claro y sen cillo su \ntexto. Las recomendaciones que integra el sistema est\u00e1n relacionadas con tres niveles de la \nlengua: discursivo, morfosint\u00e1ctico y l\u00e9xico.  \nEn el apartado 2 se incluye un breve e stado \nla cuesti\u00f3n sobre herramientas que tienen que ver con la rev isi\u00f3n de  textos, especialmente, en \nrelaci\u00f3n con el lenguaje claro. En el apartado 3 se muestran las fases de la metodolog\u00eda de la investigaci\u00f3n . En el apartado 4 se detallan las \nfuncionalida des y la implementaci\u00f3n del \nredactor asistido. En el apartado 5 se explica  la \nevaluaci\u00f3n del sistema. Finalmente, en el apartado 6, se exponen las conclusiones y se plantean algunas l\u00edneas de trabajo futuro. \n2 Estado de la cuest i\u00f3n \nActualment e, existen herra mientas tecn ol\u00f3gica s \nque ayudan a revisar y corregir textos en \nespa\u00f1ol . Estas herramientas incluyen diferentes  funcionalidades  relacionadas con diversos \nniveles de la lengua . Estos niveles pueden ir  \ndesde el m\u00e1s simple, como el ortogr\u00e1fico, hasta el m\u00e1s co mplejo, como el discursivo o \npragm\u00e1tico, pasando por el l\u00e9xico y el sint\u00e1ctico. A dem\u00e1s del cl\u00e1sico corrector d e los \nprocesadores de textos como W ord y \nOpenOf fice, hay otros sistema s destacables que \nse han desarrollado en los \u00faltimos a\u00f1os, como, \npor ej emplo , LanguageTool ,\n2 OutWrite ,3 Stilus4 \ny Estilector .5 Sin embargo, ninguno de  estos \ncorrectores e st\u00e1 dise\u00f1ado  espec\u00edficamente para \nayudar a redactar en lenguaje c laro. \nPara la l engua inglesa, s\u00ed existen algunos \nejemplos muy recient es. Por e jemplo, los \nsistemas comerciales con demos gratuitas Hemingway Editor\n6 y VisibleThread .7 Estos  \nsistem as otorgan una puntuaci\u00f3n a la legibilidad \no comprensibilidad del texto. Para asignar  estas \npuntuaciones,  tiene n en cuenta dife rentes \ncuestiones gramaticales y de estilo que hacen \nque los textos resulten m\u00e1s claros. Por ejemplo, \nrecomienda n limitar el uso de los adverbios y \nde la voz pasiva; propone n alternativas m\u00e1s \nsimples para algunas expresiones utilizadas en \nel texto; y se\u00f1alan las oraciones que resultan \ndif\u00edciles de leer para que el usuario las acorte o \ndivida.  Cada una de estas cuestiones se marca \nen el texto con un color diferente.  \nEn el \u00e1mbito de  las tecnolog\u00edas de la lengua  \nen espa\u00f1ol, hay a\u00fan mucho camino por recorrer  \nen relaci\u00f3n con el lenguaje claro . Por ejemplo , \nes de destacar la herramienta en versi\u00f3n beta \nClara,8 gratuita y en l\u00ednea, que permite realizar \nun test de claridad a textos en espa\u00f1ol, \nprincipalmente documentos  administrativos y \ncontratos de servicios. A partir de un texto \nescrito por el usuario, de un m \u00ednimo  de 40 \npalabras y un m\u00e1ximo de 120, la herramienta \nofrece un porcentaje global de claridad . \nAsimism o, indica un porcentaje espec\u00edfico de \nclaridad para cada una de las m\u00e9tricas que \nincluye  y ofrece propuesta s de mejora. La \nherramienta  inclu ye nueve  m\u00e9tricas de \nevaluaci\u00f3n  (Torrijos y Oquendo, 2021: 123 -\n124):  \n \n \n \n2 https://www.langua getool.org  \n3 https://es.outwrite.com/  \n4 http://www.mystilus.com/  \n5 http://www.estilector.com/  \n6 https://hemingwayapp.com / \n7 https://www.visiblethread. com/  \n8 https://cla ra.comunicacionclara.com/  \n40\nIria da Cunha   \u2022 M\u00e9trica 1: uso de palabras f uera del \ndiccionario.  \n\u2022 M\u00e9trica 2: uso de conectores \ndiscursivos.  \n\u2022 M\u00e9trica 3: uso de la puntuaci\u00f3n.  \n\u2022 M\u00e9trica 4: citas y referencias a leyes.  \n\u2022 M\u00e9trica  5: uso de la voz pasiva . \n\u2022 M\u00e9trica 6: uso de nexos subordinados.  \n\u2022 M\u00e9trica  7: uso de tecnicismos \nfinancieros  y administrativos . \n\u2022 M\u00e9tr ica 8 : uso de palabras del ranking \nde las 1000 m\u00e1s comunes en espa\u00f1ol . \n\u2022 M\u00e9trica 9: n\u00famero medio de palabras \npor fras e. \n \nNo obsta nte, Clara no marca  en el texto \nescrito por el  usuario  las cuestiones espec\u00edficas \nque interfieren con el lenguaje claro, ya  que no \nes un redactor asistido ni un corrector, sino un \nsistema de medici \u00f3n de la claridad textual . \n3 Metodolog\u00eda \nLa metodolog\u00eda de esta investigaci\u00f3n incluye  \ntres fases, que se detallan a continuaci\u00f3n:  \n \nFase 1 . B\u00fasqueda de fuentes bibliogr\u00e1ficas  \nsobre leng uaje c laro en el \u00e1mbito \nadministrativo en espa\u00f1ol . Para seleccionar las \nrecomendaciones sobre  lenguaje claro que \nintegra el redactor asistido , partimos del t rabajo \nde Da Cunha y E scobar (2021) . En este estudio \nse analiz an las principales fuentes sobre \nlenguaje  claro en  el \u00e1m bito jur\u00eddico-\nadministrativo en espa\u00f1ol  peninsular (Vilches y  \nSarmiento, 2010;  Ministerio de Justicia, 2011 ; \nComis i\u00f3n Europea , 2015; Jim\u00e9n ez Y\u00e1\u00f1ez, \n2016; Carret ero et al., 2017; Montol\u00edo y  Tasc\u00f3n, \n2017; Carretero, 2019) y se cuantifican  las \nrecomendaciones que recoge n, para seleccionar  \nlas m\u00e1s frecuentes. A continuaci\u00f3n, se dividen  \nlas recomendaciones seleccionadas en funci\u00f3n \nde los tres niveles de la lengua mencionados en \nel apartado  1:  \n\u2022 Nivel discursivo . Ejemplo de \nrecomendaci\u00f3n : \u201cSe recomienda \nredactar ora ciones cortas\u201d.  \n\u2022 Nivel morfosint\u00e1ctico. Ejemplo de \nrecomendaci\u00f3n : \u201cSe recomienda \nutilizar la voz activa en vez de la voz \npasiva\u201d.  \n\u2022 Nivel l \u00e9xico . Ejemplo de \nrecomendaci\u00f3n : \u201cSe recomienda evitar \nlos arca\u00edsmos \u201d. \n \n Fase 2 . Dise\u00f1o e implementaci\u00f3n de l sistema . \n \nFase 2a. Selecci\u00f3n de herramientas de PLN en \nabierto para la lengua espa\u00f1ola que permitan un \nprocesamiento ling\u00fc\u00eds tico del texto escrito por \nel usuario, concretamente:  \n \n\u2022 Lematizaci\u00f3n . \n\u2022 An\u00e1lisis Part of Speech  (POS). \n\u2022 Segmentaci\u00f3n oracional. \n\u2022 Segmentaci\u00f3n discu r siva i ntraoracional.  \n \nFase 2b. D ise\u00f1o de algoritmos que detect en en \nel texto escrito por el usuario  los rasgos \nling\u00fc\u00edsticos  que interfi eren con el lenguaje \nclaro. Para ello se tienen en cuenta las \nrecomendaciones seleccionadas en la Fase 1 y el resultado del procesamiento ling\u00fc\u00edstico del \ntexto  de la Fase 2a.  Por ejemplo, en el caso de \nla recomendaci\u00f3n \u201cSe recomienda redactar \noraciones cortas\u201d, el algoritmo detecta tod as las \noraciones del texto y contabiliza las palabras  \nque incluye cada una . Si una oraci\u00f3n  supera las \n25 palabras, la subraya  en amarillo en el texto \nescrito por el usuario y le ofrece la recomendaci\u00f3n correspondiente para ada ptarla a \nlenguaje claro. En el caso de la r ecomendaci\u00f3n \n\u201cSe recomienda utilizar la voz activa en vez de \nla voz pasiva\u201d, el algoritmo marca en el texto \ndel usuario los verbos en voz pasiva obtenidos \ngracias al analizador POS y le ofrece la recomendaci\u00f3n cor respondiente.  \n \nFase 2c. Reda cci\u00f3n de las recomendaciones \nsobre lenguaje claro ofrecidas al usuario . En \ncada recomendaci\u00f3 n se in cluye la siguiente \ninformaci\u00f3n:  \n \n\u2022 T\u00edtulo de la recomendaci\u00f3n.  \n\u2022 Sugerencia para la adaptaci\u00f3n a \nlenguaje clar o. \n\u2022 Ejemplo (en caso de que se considere \nneces ario para que el usuario entienda \nla recomendaci\u00f3n ). \n \nEn el An exo 1 se detallan todas las \nrecomendaciones que ofrece el sistema.  \n \nFase 2d. Implementaci\u00f3n  del redactor asistido  \n(los detalles t\u00e9cnicos se incluyen en el  apartado \n4). \n \nFase 3. Evaluaci\u00f3n del  sistema . Para evaluar el \nsistema, se compi la un corpus de textos del \n\u00e1mbito de la Administraci\u00f3n , con cretamente, \nresoluciones del BOAM (Bolet\u00edn Oficial del Ayuntamiento de Madrid) . Una persona con \nformaci\u00f3n en li ng\u00fc\u00edstica y experiencia en \n41\nUn redactor asistido para adaptar textos administrativos a lenguaje claro anotaci\u00f3n de corpus  anota manualmente en cada \ntexto los diferentes rasgos ling\u00fc\u00edsticos \ncorrespondientes a las rec omen dacion es que se  \nquieren  evaluar (detallados en el apartado 5).  A \ncontinuaci\u00f3n, se aplic a el sistema sobre los \ntextos del corpus para obtener autom\u00e1ticamente \nlas recomendaciones sobre lenguaje claro. \nFinalmente, se calcula la precisi\u00f3n y cobertura \nde los resultados del sistema en contraposici\u00f3n \ncon la anotaci\u00f3n manual.   \n4 Funcionalidades e implementaci\u00f3n \ndel redactor asistido \nEl sistema se desarroll\u00f3  en un entorn o Linux \ncon un servidor Apache. T ambi\u00e9n se usaron  \ndistintos  recursos en el back-end  (Bash, Perl y \nPHP, c on un entorno de trabajo Laravel) y en el \nfront -end (HTML, CSS, JavaScript, con AJAX \ny jQuery). El sistema est\u00e1 optimizado para \nutilizarse con el navegador Google Chrome.  \nEl redactor  integra dos herramientas de PLN \nexistentes para el espa\u00f1o l que perm iten procesar \nling\u00fc\u00edsticamente el texto escrito por el usuario : \n \n\u2022 El analizador morfosint\u00e1ctico de \nFreeling ( Atserias et al. , 2006), \nmedi ante el  cual se lematizan todas las \nunidades l\u00e9xicas del texto y se asigna \nuna categor\u00eda  gramatical a cada una de \nellas.  Este analizador permite detectar \nrasgos ling\u00fc\u00edsticos que son utilizados \npor los algoritmos del sistema en las recomendaciones que tienen que ver \nprincipalmen te con el nivel \nmorfosint\u00e1ctico,  como,  por ejemplo, los \nverbos en voz pasiva, los gerundios, los partici pios, los verbos en futuro de \nsubjuntivo, o  los verbos en 1.\u00aa persona \ndel singular  y del  plural . \n\u2022 Un segmen tador discursivo ( Da Cun ha \net al. , 2010, 2012b ), que permite dividir \nel texto en oraciones y,  adem\u00e1s, en \nsegmentos discursivos intraoracionales, \na partir de la d efinici\u00f3n de Tofiloski et \nal. (2009, p.77):  \u201c Discourse \nsegmentation is the process of decomposing discourse into elemen tary \ndiscourse units ( EDUs), which may be \nsimple  sentences or clauses in a \ncomple x sentence,  and from which \ndiscourse trees are constructed\u201d.  Este \nsegmentador  permite detectar los rasgos \nutilizados  por los algoritmos en las \nrecomend aciones relativas al niv el \ndiscursivo, como la  segmentaci\u00f3n discursiva de las oraciones largas y la sugerencia de conectores alternativos.  \n \nAdem\u00e1s de integrar est as do s herramien tas, \nel sistema incluye diversos algoritmos  \ndesarrollados en el marco de nuest ra \ninvestigaci\u00f3n. Estos algoritmos toman como \nentrada el texto procesado ling\u00fc\u00edsticamente  por \nlas dos herramientas de PLN mencionadas y \ndetectan en el texto d el usuari o los rasgo s \nling\u00fc\u00edsticos  necesarios para poder ofrecer las \nrecomendaciones asociadas a cada uno de ellos . \nEstos  rasgos son: \n \n\u2022 P\u00e1rrafos-oraci\u00f3n.  \n\u2022 P\u00e1rrafos largos, con un umbral de 135 \npalabras (teniendo en cuenta las fuentes \nrecopiladas en la F ase 1 ). \n\u2022 Oracio nes largas, con un umbral de 25 \npalabras (teniendo en cuenta las fuentes recopiladas en l a Fase 1 ). \n\u2022 Conectores discursivos interoracion ales \ne interoracionales que evidencian  ocho \nrelaciones discursivas: ant\u00edtesis, causa, \nconcesi\u00f3n, condici\u00f3n, contrast e, \nprop\u00f3sito, reformulaci\u00f3n y resumen  \n(Da Cunha et al. , 2012a). \n\u2022 Nominal i\n zaciones verbales. \nConcretamente, el algoritmo  detecta los \nsustantiv os acabados en -ci\u00f3n  (en \nsingular y plural)  que comienzan por \nmin\u00fascula,  excepto los incluidos en una \nlista de exclusi\u00f3n predefinida  que \nintegra t\u00e9rminos del \u00e1mbito de la \nAdministraci\u00f3n, como \u201c licita ci\u00f3n\u201d y \n\u201cnotificaci\u00f3n \u201d (Da Cunha , 2022). \n\u2022 Unidades que expresan negaci\u00f3n  de una \nlista predefinida , que incluye unidades \ncomo \u201c no\u201d, \u201cni\u201d y \u201c ninguno\u201d. \n\u2022 Unidades l\u00e9xicas que indican \nsubjetividad,  como ciertos adjetivos (ej. \n\u201cbueno\u201d), adverbios (ej.  \n\u201cevidentemente\u201d) y frases (ej. \u201csin ninguna  duda\u201d), extra\u00eddas de Otao la \n(1988). \n\u2022 Siglas propias  (Girald o, 2008) y sus \ncorrespondientes t\u00e9rminos desplegados. \nPara hacer la correlaci\u00f3n  entre la sigla y \nsu t\u00e9rmino  desplegado, se tiene en \ncuenta que la letra inicial de las \nunidades  l\u00e9xicas incluidas en el t \u00e9rmino  \n(excepto las stopwords ) se \ncorrespondan, en el mismo orden,  con \nlas mismas letras que incluye la sigla. \nEj. \u201cPlan de Emergencias Invernales \n42\nIria da Cunha   del Ayuntamiento de Madrid\u201d > \n\u201cPEIA M\u201d. \n\u2022 T\u00e9rminos dif\u00edciles de entender que \ntienen una variante sinon\u00edmica m\u00e1s \nsencilla de una lista predefinida  (Da \nCunha , 202 2). Por ejemplo, la varian te \nm\u00e1s s encilla del verbo \u201ca dverar\u201d es \n\u201ccertificar\u201d y la variante m\u00e1s sencilla \ndel sustantivo \u201caquiescencia\u201d es \n\u201cconsentimiento \u201d. \n\u2022 Expresiones dif\u00edciles de ent ender que \ntienen una var iante sinon\u00edmica m\u00e1s \nsencilla de una lista predefinida  (Da \nCunha , 2022). Por ejemplo, el latinismo \n\u201cad va lorem \u201d tiene como variante en \nespa\u00f1 ol \u201cseg\u00fan en valor \u201d y la expresi\u00f3n \narcaica \u201c a tenor de \u201d tiene como variante \nm\u00e1s clara \u201c seg\u00fan\u201d. \n\u2022 Palabras poco precisas de una  lista \npredefi nida (Da Cunha, 2022), c omo \n\u201ccosa\u201d, \u201cvarios \u201d, \u201calguno\u201d, \u201c muy\u201d y \n\u201cpoco\u201d. \n\u2022 Expresiones redundantes de una lista \npredefinida  (Da Cunha , 2022), como \n\u201cest\u00e1 claro que\u201d , \u201cmi op ini\u00f3n personal \u201d \ny \u201ccomo es bien sabido \u201d. \u2022 Palabras largas que tiene n una variante \nsinon\u00edm ica m\u00e1s breve de una lista \npredefinida  (Da Cunha , 2022), como \n\u201cgratuitamente/g ratis\u201d y \n\u201cencomendar/encargar \u201d. \n \nEn total, el redactor incluye 22 \nrecomendaciones  sobre lenguaje claro . Como se \nha indicado, e n el Anexo 1 se detallan todas  \nellas, divididas en funci\u00f3n de  los tres niveles de \nla lengua mencionados  en el apartado 3.  \nEn la Figura 1 se ofrece una captura de \npantalla  de \u201carText claro \u201d en donde se mues tra \nla recomendaci\u00f3n sobre p\u00e1rrafos largos e n un  \ntexto del corpus de evaluaci\u00f3n.  \nEn relaci\u00f3n  con la exportaci\u00f3n  e importaci\u00f3n \nde documentos, por una cuesti\u00f3n de protecci\u00f3n  \nde datos, se decidi\u00f3  que el sistema no guardase \nen su  servidor los textos escritos por los \nusuarios. Para ello, existen varias opciones de \nexportaci\u00f3n  de documentos  en local: .doc, .pdf, \n.txt, .html, etc. Para poder  importar un texto \nposteriormente en el redactor,  debe  utilizare un \nformato creado espec\u00edficamente para este \nsistema: .artext. \n \n \n \n \nFigura  1: Captura de pantalla  de \u201carText claro \u201d en donde se mues tra la recomendaci\u00f3n sobre p\u00e1rrafos \nlargos  en un texto del corpus de evaluaci\u00f3n . \n \n5 Evaluaci\u00f3n \nComo se avanzaba en el apartado 3, una vez \nimplementado el sistema, se llev\u00f3 a cabo una \nevaluaci\u00f3n data-driven  utilizando un corpus \nformado por 10 resoluciones del BOAM publicadas en el  a\u00f1o 2021, que  en total suman 8.052 palabras.  El texto m\u00e1s corto tiene 436 \npalabras y el texto m\u00e1s largo, 1398. En el \nAnexo 2 se recogen  los t\u00edtulos de las \nresolu ciones empleadas.  \nPara comparar los resultados del sistema  \ncon los resultados de la anotaci\u00f3n manual, se \ncalcul \u00f3 la precisi\u00f3n y cobertura de los rasgos \n43\nUn redactor asistido para adaptar textos administrativos a lenguaje claro ling\u00fc\u00edsticos anotados en los textos del corpus . \nLos rasgos anotados se incluyen en la Tabla 1.  \n \nNivel d e  Rasgos anotados   \n \n \n \n \n \nDiscursivo a1. P\u00e1rrafos -oraci\u00f3n \na2. P\u00e1rrafos lar gos \na3. P\u00e1rrafos  que no comienzan por \nun conector discursivo  \na4. Oraciones largas  \na5. Oraciones largas que pueden \ndividirse en segmentos discursivos  \na6. Conectores  discursi vos que \naparecen 3 veces o m\u00e1s   \na7. Listas  \n \n  \nMorfo- \nsint\u00e1ctico  b1. Verbo s en voz  pasiva  \nb2. Gerundios  \nb3. Participios  \nb4. Verbos en futuro de subjuntivo  \nb5. Verbos en 1 .\u00aa persona  del plural \ny del singular  \nb6. Nom inalizaciones verbales   \nb7. Unidades que expresan negaci\u00f3n   \n \n \n \n \n \n \n \n \n \nL\u00e9xico  c1. Unidades que expresan \nsubjetividad  \nc2. Siglas  que no aparecen con su \ncorrespondiente t\u00e9rmino desplegado \nla 1.\u00aa vez que aparecen en el texto  \nc3. T\u00e9rminos desplegados para los \nque se introduj o su sigla  \npreviamente en el texto  \nc4. T\u00e9rminos dif\u00edciles de e ntender \nque tienen una  variante sinon\u00edmica  \nm\u00e1s sencilla  \nc5. Expresiones dif\u00edciles de entender  \nque tienen una variante m\u00e1s sencilla  \nc6. Palabras poco precisas  \nc7. Expresiones redundantes  \nc8. Palabras largas que tienen una \nvariante sinon\u00edmica m\u00e1s bre ve  \n \n \nTabla  1: Rasgos ling\u00fc\u00edsticos anotados \nmanualmente en el corpus de evaluaci\u00f3n.  \n \nLos resultados de la evaluaci\u00f3n  de cada \nrecomendaci\u00f3n  se muestran en la Tabla 2 \n(\u201cID\u201d se refiere al identificador de la \nrecomendaci\u00f3n).  \n \nID Preci si\u00f3n Cobertura  \na1 0,99 0,74 \na2 0,7 0,8 \na3 0,84 0,71 \na4 1 0,7 \na5 1 0,89 \na6 1 1 \na7 1 0,83 \nb1 1 0,67 \nb2 1 1 \nb3 0,86 1 \nb4 1 1 b5 0,17 1 \nb6 1 1 \nb7 1 1 \nc1 Rasgo sin  ocurrencias en el corpus  \nc2 0,6 1 \nc3 1 1 \nc4 1 1 \nc5 1 1 \nc6 1 1 \nc7 Rasgo sin  ocurrencias en el corpu s \nc8 1 1 \n \nTabla 2: Resultados de la evaluaci\u00f3n del \nsistema.  \n \nComo puede apreciarse , los resultados \nobtenidos son, en general , positivos para la \nmayor parte  de las recomendaciones evaluadas. \nDestaca especialmente que el sistema obtie ne \nla m\u00e1xima precisi\u00f3n  y cobertura en relaci\u00f3n \ncon la detecci\u00f3n  de 10 ra sgos:  los conectores  \nque aparecen  m\u00e1s de tr es veces en el texto, los \ngerundios, los verbos en futuro de subj untivo, \nlas nominaliza ciones verbales, las unidades  \nque expresan negaci \u00f3n, los  t\u00e9rminos \ndesplegado s para los que se in trodujo su sigla \npreviamente en el texto , los t\u00e9rminos dif\u00edciles \nde entender que tienen una variante sinon\u00edmica \nm\u00e1s sencilla , las expresiones dif\u00edciles de \nenten der que tienen una variante m\u00e1s sencilla  y \nlas palabras poco precisas.  \nEn cuanto a las recomendaciones  \nrelacio nadas con los p\u00e1rrafos ( a1, a2, a3), \ntambi\u00e9n se o btienen resultados positivos , \naunque bajan ligeramente.  Esto se debe, \nprincipalmente , a signos de  puntuaci\u00f3n que \ninterfieren en la segmentaci\u00f3n  oracional , lo \ncual tiene consecuencias en la detecci\u00f3n correcta de p\u00e1rrafos: \n \n- Citas a art\u00edculos y leyes. Ej. \u201c los \nart\u00edculos 8.1 y 46.1 de la Ley\u201d. \n- Elementos  numerados con puntuaci \u00f3n \nEj. \u201c1.\u201d, \u201c2. \u201d. \n- Enlaces web . Ej. \u201cen la \nintranet /extranet municipal  ayre \n(https://ayre.madrid.es ) y en la web  \nhttps://jubilacion.madrid.es\u201d. \n \nEn cuanto a las recomend aciones \nrelacionadas con la detecci\u00f3n de o raciones \nlargas y segmentaci\u00f3n discusiva (a4, a5), la \nprecisi\u00f3n es muy alta , pero en cambio baja \nligeramente la cobertura. El motivo es que en \neste tipo de textos administrativo s en ocasiones \nhay oraciones que acaban con una coma o \ndirectamente sin ning\u00fan signo de puntuaci\u00f3n. \n44\nIria da Cunha   Por tanto, el sistema no  logra recuperarlas. Por \nejemp lo: \n \n(1) \u201cEn su virtud, de conformidad con el \nAcuerdo de 27 de junio de 2019 [\u2026] la \ncompetencia para la ejecuci\u00f3n de los \nplanes y programas de formaci\u00f3n de los empleados y directivos del Ayuntami ento de Madr id,\u201d \n(2) \u201cEn virtud de las facultades que me \nhan sido conferidas por [..], esta \nGerencia\u201d \n \nEn cuanto a la detecci\u00f3n de ra sgos \nmorfosint\u00e1cticos, los que no a lcanzan la \nm\u00e1xima precisi\u00f3n  y cobertura son los \nsiguientes: \n \n\u2022 Verbos en voz pasiva (b1). En este  caso, \nlos errores en la cobertu ra (0,67) provienen \ndel procesamiento ling\u00fc\u00edstico  con Freeling. \nPor ejemplo , no se detectan las f ormas  \n\u201chubiera sido cesado \u201d o \u201chan sido \nconferidas\u201d. \n\u2022 Partici pios (b3). En e sta ocasi\u00f3n el \nproblema es de precisi\u00f3n ( 0,86) y se debe a \nla desambiguaci\u00f3n de Free ling. Por \nejemplo, se detecta  \u201cpropuesta\u201d  y \u201cpuesto\u201d \ncomo participio s cuando en el texto tiene n \nfunci\u00f3n de  sustanti vo (\u201c la propue sta\u201d, \u201cel \npuesto de trabajo \u201d). \n\u2022 Verbos en 1.\u00aa persona del plural y del \nsingular  (b5) . Este ha sido el rasg o que ha \nobtenido  peores resultados en la \nevaluaci\u00f3n , con un  0,17 de pre cisi\u00f3n , \ndebido tambi\u00e9n a problemas en la \ndesambiguaci\u00f3n . Por ejemp lo, se detectan \ncomo  1.\u00aa persona del si ngular  las formas \n\u201csea\u201d y \u201c haga \u201d, cuando en real idad en el \ntexto so n formas de  3\u00aa per sona si ngula r \n(\u201csea esta accidental o intencio nada \u201d, \n\u201ccuando la previsi\u00f3n meteorol\u00f3gica haga \nprevisible \u201d). \n \nEn relaci\u00f3n con el  nivel l\u00e9xico, como se ha \nvisto, se obtiene n muy buenos resultados. \n\u00danicamente  baja la precisi\u00f3n (0,6) en la \ndetecci\u00f3n de siglas  que no aparecen con su \ncorrespondiente t\u00e9rmino desplegado la primera \nvez que aparecen en el texto  (c2). El mo tivo \nprincipal es que en estos documento s suele \nhaber n\u00fameros roma nos (ej. \u201cIII\u201d , \u201cIV\u201d) y el \nsistema los marc a err\u00f3neamente como siglas. \nTamb i\u00e9n se\u00f1ala otras secuen cias de letras en \nmay\u00fa scula  (como los acr\u00f3nimos ) que s\u00ed \naparecen con su t\u00e9rmino  desplegado  y que, por \ntanto , no ser\u00eda pertinente marcar  en el t exto del usuario . Ej. \u201cPlan Territorial Superior de la \nComunidad de Madrid ( PLATERCAM )\u201d. \nFinalm ente, como se ob serva en la Tabla 2, \nhay dos recomendaciones del nivel l\u00e9xico que \nno se han podido evaluar (c1 y c 7), porque no \nse han det ectado  en el corpus uni dades que \nexpresan subjetividad ni e xpresiones \nredundantes . \n6 Conclusione s y l\u00edneas de traba jo \nfuturo  \nComo se ha vi sto en este trabajo, el lenguaje \nclaro es un tema de investigaci\u00f3n en el que a\u00fan \nqueda mucho por explorar, especialmente \ndesde el punto de vist a del PLN. El objetivo de \neste trabajo ha sido desarrollar  el primer \nredactor asistido para el espa\u00f1ol para escribir  \ntextos administrativos en lenguaje claro. La \nherramienta desarrollada tiene un gran \npotencial de aplicaci\u00f3n en las diferentes \ndependencias de la Administraci\u00f3n p\u00fablica, \ncomo ayuntamientos, diputaciones, consejer\u00edas, ministerios, etc. Los empleados p\u00fablicos dispondr\u00e1n, as\u00ed, de una herramienta \ntecnol\u00f3gica que les ayudar\u00e1 a revisar sus textos \ny adaptarlos al lenguaje claro. \nLa evaluaci\u00f3n de \u201c arText claro \u201d ofrece \nbuenos resultados, aunque a\u00fan quedan \ncuestiones que se pueden  mejorar, c omo la \nprecisi\u00f3n en la detecci\u00f3n de siglas, o las \ninterferencias con l os signos de puntuaci\u00f3n que \nprovocan errores en la detecci\u00f3n de p\u00e1rrafos y \noraciones largas. Tambi\u00e9n ser\u00eda interesante \nampliar el corpus de evaluaci\u00f3n con m\u00e1s textos anotados manualmente , de otros g\u00e9neros \ntextuales del \u00e1mbito admin istrativo, con el \nobjetivo de  validar los resultados obtenidos en \nesta investiga ci\u00f3n.  \nCon r especto a la metodolog\u00eda, una posible \nl\u00ednea de trabajo futuro ser\u00eda la aplicaci\u00f3n de estrategias de aprendizaje autom\u00e1t ico. Este \nenfoque ya es t\u00e1 siendo utilizado en  algunas \ninvestigaciones, como es el caso del sistema de \nmedici\u00f3n de la claridad textual  Clara (Torrijos \ny Oque ndo, 2021) , mencionado en el apartado \n2. La principal dificultad de este enfoque , sin \nembargo,  es la necesidad de contar co n corpus \nmuy extensos  de textos originales y sus \ncorrespondientes textos clari ficados.  \nOtra de nuestras l\u00edneas de investigaci\u00f3n \nser\u00e1 llevar a cabo estudios de evaluaci\u00f3n de la \npercepci\u00f3n de la claridad y de la comprensi\u00f3n \nde los textos escritos con la herr amienta por \nparte de l os destinatarios. Finalmente, nuestro \n45\nUn redactor asistido para adaptar textos administrativos a lenguaje claro objetivo es implementar el sistema en las \ndependencias de la Administr aci\u00f3n p\u00fablica \nespa\u00f1ola.  \nAgradecimientos \nEste trabajo se deriva del proyecto de \ninvestigaci\u00f3n titulado \u201c Tecnolog\u00edas de la \nInformaci\u00f3n y la Comunicaci\u00f3n para la e-\nAdministraci\u00f3n: hacia la mejora de la \ncomunicaci\u00f3n entre Administraci\u00f3n y ciudadan\u00eda a trav\u00e9s del lenguaje claro (TIC -\neADMIN)\u201d, financiado por el Ministerio de Ciencia, Innovaci\u00f3n e Universidades en  la \nconvocatoria 2018 de Proyectos I+D del Subprograma Estatal de Generaci\u00f3n de \nConocimiento (referencia PGC2018-099694-A-I00), y desarrollado en el Departamento de \nFilolog\u00edas Extranjeras y sus Ling\u00fc\u00edsticas de la Facultad de Filolog\u00eda de la Universidad \nNacional de Educaci\u00f3n a Distancia (UNED) , \nen el marco del g rupo de investigaci\u00f3n \nACTUALing  y en colaboraci\u00f3n con el grupo \nIULATERM (IULA-UPF).  \nBibliograf\u00eda  \nAlcaraz, E. , B. Hugue s, y A. G\u00f3mez. 2014. El \nespa\u00f1ol jur\u00eddico. 3.\u00aa edici\u00f3n. Ariel , \nBarcelona. \nAtserias, J. , B.  Casas, E. Co melles, M. \nGonz\u00e1lez,  Ll. Padr\u00f3, y M.  Padr\u00f3 . 2006 . \nFreeLing 1.3. Syntactic and semantic \nservices in an open -source NLP library . En \nLREC 2006 Proceedings. 5th Edition of the \nInternational Conference on Language Resources and Evaluation , p\u00e1ginas 48-55, \nEuropean Language Resources Association \n(Par\u00eds).  \nDa Cunha, I. (Ed.). 2022 . Lenguaje claro y \ntecnolog\u00eda en la Administraci\u00f3n . Granada, \nComares.  En prensa . \nDa Cunha, I . y M. \u00c1 . Escobar . 2021 . \nRecomendaciones sobre lenguaje claro en \nespa\u00f1ol en el \u00e1mbito jur\u00eddico -\nadmin istrativo: an\u00e1lisis y clasificaci\u00f3n . \nPragmaling\u00fc\u00edstica , 29:129 -148. \nDa Cunha, I ., E. SanJuan, J-M. Torres-\nMoreno, M. T. Cabr\u00e9, y G. Sierra . 2012 a. A \nSymbolic Approach for Automatic Detection of Nuclearity and Rhetorical \nRelations am ong Intra -sentence Disco urse \nSegments in Spani sh.  Lecture Notes in \nComputer Science, 7181:462 -474.  Da Cunha, I ., E. SanJuan , J-M. Torres-\nMoreno, M. Lloberes,  e I. Castell\u00f3n . 2012b . \nDiSeg 1.0: The First System for Spanish Discourse Segmentation . Expert Sys tems \nwith Applications , 39(2) :1671-1678.  \nDa Cunha, I ., E. SanJuan , J-M. Torres-\nMoreno, M. Lloberes,  e I. Castell\u00f3n . 2010. \nDiSeg: Un segmentador discursivo autom\u00e1tico para el espa\u00f1ol . Procesamiento \ndel Lenguaje Natural , 45:145-152.  \nBay\u00e9s, M. 2021. An\u00e1lisis del impacto de una \nselecci\u00f3n de (meta)indicaciones de redacci\u00f3n clara en la percepci\u00f3n de claridad de un documento adminis trativo: es tudio de \ncaso. Tesis doctoral , Universitat de \nBarcelona.  \nCarretero, C. 2019. Comunicaci\u00f3n para \njuristas . Tirant lo Blanch , Valencia. \nCarretero, C. , y J. C. Fuentes. 2019 . La \nclaridad del lenguaje jur\u00eddico . Revista del \nMinisterio Fiscal , 8:7-40.  \nCarrete ro, C. , J. M.  P\u00e9rez,  L. Lanne -Lenne , y \nG. de los Reyes. 2017. Lenguaje Claro. \nComprender y hacernos entender. Instituto de Lectura F\u00e1cil y Clarity, Se villa, Madrid . \nCassany , D. 2005 . Pla\n in Language in Spain . \nClarity , 53:41-44.  \nComisi\u00f3n Europea. 2015. C\u00f3mo escribir con \nclaridad. Oficina de Publicaciones de la \nUni\u00f3n Europea , Luxemburgo.  \nDe Miguel, E. 2000. El texto jur\u00eddico-\nadmini strativo: an\u00e1lisis de un a orden \nministerial. C\u00edrculo de ling\u00fc\u00edstica aplicada \nal a comunicaci\u00f3n, 4: e n l\u00ednea.  Disponible \nen: \nhttps://webs.ucm.es/info/circulo/no4/demiguel.htm  \nGiraldo, J. J.  2008. An\u00e1lisis y descripci\u00f3n de \nlas siglas en el discurso especializado de genoma humano y medio ambiente . Tesis \ndoctoral . Universitat Pompeu Fabra, Institut \nde Ling\u00fc\u00edstica Aplicada (IULA) , Barcelona. \nJim\u00e9nez Y\u00e1\u00f1ez, R. M. 2016. Escribir bien es \nde justicia . Aranzadi , Cizur Menor . \nMinisterio de Justicia . 2011. Informe de la \nComisi\u00f3n de modernizaci\u00f3n del lenguaje jur\u00eddico . Ministerio de Justicia , Madrid . En \nl\u00ednea.  \n46\nIria da Cunha   Montol\u00edo, E. 2012. La modernizaci\u00f3n del \ndiscurso jur\u00eddico espa\u00f1ol impulsada por el \nMinisterio de Justicia. Presentaci\u00f3n y  \nprincipales aportacio nes del Informe sobre \nel lenguaje escrito . Revista de Llengua  i \nDret, 57:95-121.  \nMontol\u00edo, E. y M.  Tasc\u00f3n . 2020 . El derecho a \nentender: la comunicaci\u00f3n clara, la mejor defensa de la ciudadan\u00eda. La Catarata, \nMadrid. \nMontol\u00edo, E. y M. T asc\u00f3n . 2017. \nComunicac i\u00f3n Clara. Gu\u00eda Pr\u00e1ctica . \nAyuntamiento de Madrid, Madrid . \nOtaola, C.  1988. La modalidad (con especial \nreferencia a la lengua espa\u00f1ola) . Revista de \nFilolog\u00eda Espa\u00f1ola , 68(1) :97-117.  \nTofiloski, M. , J. Brooke, y M. Taboada . 2009. \nA syntactic and lexical -based  discourse \nsegmenter. En Proceedings of the ACL -\nIJCNLP 2009 Confe rence Short Papers,  \np\u00e1ginas 77-80, Association for \nComputational Linguistics  (Singapur ). \nTorrijos , C. y S. Oque ndo. 2021. \u00a1Hola! Soy \nclara y mido  la claridad de tu texto \u201d \nArchiletras Ci ent\u00edfica , Vol. V I:119-133.  \nVilches, F.  y R. Sarmiento. 2010 . Manual de \nlenguaje Jur\u00eddico-Administrativo . \nDykinson, Madrid . \nA Anexo 1:  Recomendaciones que \nofrece \u201carText cl aro\u201d \na) Recomendaciones del nivel discursiv o \n \na1. T\u00edtulo de la recome ndaci\u00f3n: Revisi\u00f3n de \np\u00e1rrafos -oraci\u00f3n . \nTexto de la recomendaci\u00f3n:  Parece que los \np\u00e1rrafos marcados en el texto solo incluyen una \noraci\u00f3n. Ten en cuenta que suele recomendarse que \ncada p\u00e1rrafo incluya al menos dos oraciones.  \n \na2. T \u00edtulo de la recomendaci\u00f3n:  Revisi\u00f3n de \np\u00e1rrafos largos . \nTexto de la recomendaci\u00f3n: Parece q ue los \np\u00e1rrafos marcados en el texto son bastante largos. Te recomendamos que los dividas en otros m\u00e1s \ncortos. Recuerda que cada p\u00e1rrafo debe tratar un tema diferent e. \n \na3. T\u00edtulo de la recom endaci\u00f3n:  Introducci\u00f3n de \nconectores al inicio de p\u00e1rrafos .  \nTexto de la recomendaci\u00f3n: Parece que los \np\u00e1rrafos marcados en el texto no comienzan con \nuna marca expl\u00edcita que los relacione con su p\u00e1rrafo anterior. Te recomendamos que enlaces los \ndiferentes p\u00e1rrafos a trav\u00e9s de c onectores \ndiscursivos. Haz clic en cada acc i\u00f3n para ver \nsugerencias de conectores que pueden servirte de ayuda para introducir p\u00e1rrafos:  \n \nIntroducir un tema nuevo  \nMarcar un orden  \nDistinguir  \nSeguir el mismo te ma \nEnfatizar y reformular  \nDetallar  \nResumir \nTermi nar \nIndicar causa  \nIndicar consecuencia  \nIndicar oposici\u00f3n  \nIndicar objeci\u00f3n Mostrar elementos en forma de lista  \n \na4. T\u00edtulo de la recomendaci\u00f3n:  Revisi\u00f3n de \noraciones largas . \nTexto de la recomendaci\u00f3n: Las orac iones \nmarcadas son muy lar gas. Te recomendamos que \nlas revises. Por ejemplo, podr\u00edas dividi r la oraci\u00f3n \nen otras m\u00e1s cortas o eliminar la informaci\u00f3n poco \nrelevante.  \n \na5. T\u00edtulo de la recomendaci\u00f3n:  Divisi\u00f3n de \noraciones largas . \nTexto de la recomendaci\u00f3n: Parece que las \noraciones marcadas podr\u00edan dividi rse en otras m\u00e1s \ncortas. Te recomendamos qu e lo hagas. Haz clic en \ncada oraci\u00f3n para ver d\u00f3nde podr\u00edas  segmentarla. \nSi decides dividir la oraci\u00f3n en otras m\u00e1s cortas, te recomendamos que utilices conectores. Haz clic en las unidades m arcadas en rojo en el texto para ver \nsugerencias de conectores al ternativos.  \n \na6.\n T\u00edtulo de la recomendaci\u00f3n:  Variaci\u00f3n de \nconectores . \nTexto de la recomendaci\u00f3n: Los conectores de \nla lista sig uiente se repiten varias veces en el t exto. \nHaz clic en cada con ector para ver sugeren cias de \nconectores alternativos.  \n \na7. T\u00edtul o de la recomendaci\u00f3n:  Inclusi\u00f3n de listas . \nTexto de la recomendaci\u00f3n: Parece que no has \nincluido ninguna lista en tu texto utilizando las \nopciones disponibles en la  barra superior de \nherrami entas (numeraci\u00f3n o vi\u00f1etas). Recuerda que \nlas listas, si est\u00e1n b ien construidas, son muy \neficaces para transmitir informaci \u00f3n de manera \nclara. Te recomendamos que a\u00f1adas alguna lista a tu tex to. \nEjemplo:  \nPara presentar su solicit ud, debe enviar cuatro \ndocumentos:  \n1. El formulario  de solicitud cumplimentado.  \n2. Una fotocopia de su DNI.  \n47\nUn redactor asistido para adaptar textos administrativos a lenguaje claro 3. Su acta de nacimiento.  \n4. Su certificado de empadronamiento.  \n \nb) Recomendaciones del nivel morfosint\u00e1ctico  \n \nb1. T\u00edtulo de la recomendaci\u00f3n:  Uso de la voz \npasiva. \nTexto de la recomenda ci\u00f3n: Las unidades \nmarcadas parecen verbos en voz pasiva. Ten en \ncuenta que en este tipo de textos es m\u00e1s habitual la \nvoz ac tiva. \nEjemplo:  \nLa oraci\u00f3n \"El suministro que se contrata fue \naprobado por este organismo\" podr\u00eda sustituir se \npor \"Este organismo aprob\u00f3 el suministro que se contrata\".  \n \nb2. T\u00edtulo de la recomendaci \u00f3n: Revisi\u00f3n de \ngerundios . \nTexto de la recomendaci\u00f3n: Las u nidades \nmarcadas en el texto p arecen verbos en gerundio. \nTe recomendamos que evites estas formas verbales,  \nya que pueden causar ambi g\u00fcedades, alargar \ndema siado la oraci\u00f3n y hacer dif\u00edcil la \ncomprensi\u00f3n.  \nEjemplo:  \nEn la siguiente oraci\u00f3n, no queda claro qui\u00e9 n es \nel sujeto del gerundio:  \"El aspirante a la plaza dio \nsu documentaci\u00f3n al empleado del registro \nsolicitando una copia.\"  \nSi el su jeto es el aspirante, ser\u00eda m\u00e1s \nconveniente decir:  \"El aspirante a la plaza dio su \ndocumentaci\u00f3n al empleado del registro, a  quien \nsolicit\u00f3 una copia.\"  \nEn cambio, si el sujeto es el empleado del \nregistro, ser\u00eda m\u00e1s adecuado decir,  por ejemplo: \n\u201cEl aspirante a la plaza dio su documentaci\u00f3n al \nempleado del registro y solicit\u00f3 una copia.\"  \n \nb3. T\u00edtulo de la recomendaci\u00f3n:  Revisi\u00f3n de \nparticipios . \nTexto de la r ecomendaci\u00f3n: Las unidades \nmarcadas en el texto parecen verbos en participio.  \nA no ser que sea impresci ndible utilizarlos, te  \nrecomendamos que evites estas formas verba les, ya \nque pueden causar ambig\u00fcedades, alargar demasiado l a oraci\u00f3n y hacer dif\u00edcil la \ncomprensi\u00f3n.  \nEjemplos: En las siguientes oraciones, no queda claro \nqui\u00e9n es el sujeto del participio:   \n\"Enviada la resoluci\u00f3n , el aspirante tiene diez d\u00edas \npara reclama r.\"  \n\"Presentado el documento, se cerr\u00f3 el expediente.\" \nPodr \u00eda especificarse de la siguiente manera, \npor ejemplo:  \n\"Una vez el comit\u00e9 evaluador env\u00ede la resoluci\u00f3n, el aspirante tiene diez d\u00eda s para reclamar.\"  \n\"Cua ndo el solicitante present\u00f3 el documento, se  \ncerr\u00f3 el expediente\".   \nb4. T\u00edtulo de la recomendaci\u00f3n:  Eliminaci\u00f3n de \nformas verb ales arcaicas . \nTexto de la recomendaci\u00f3n: Las formas \nverbales marcadas en el text o est\u00e1n en desuso y son \ninnecesarias. Te recomendamos que las evites y que \nutilices en su lug ar otras formas m\u00e1s sencillas y \nactuales.  \nEjemplos: En vez de \u201csi resultare neces ario\u201d, podr\u00eda \ndecirse \u201csi resulta necesario\u201d, \u201csi resultara necesario\u201d o \u201csi resul tase necesario\u201d.  \nEn vez de \u201csi hubiere sido neces ario\u201d, podr\u00eda \ndecirse \u201csi hubiera sido neces ario\u201d o \u201csi hubiese \nsido necesario\u201d.  \n \nb5. T\u00edtulo de la reco mendaci\u00f3n:  Sistematici dad en \nel uso de verbos en 1.\u00aa persona.  \nTexto de la recomendaci\u00f3n: Las unidades \nmarcadas en verde parecen ver bos en 1.\u00aa  persona \ndel singular y las marcadas en azul parecen ver bos \nen 1.\u00aa persona del plural. Si estas formas verbales \nse ref ieren al emisor del te xto, te recomendamos \nque optes por el singular o el plural para que el \ntexto sea sistem\u00e1tico.  \n \nb6. T\u00edtulo de la recomendaci\u00f3n:  Revisi\u00f3n de \nnominalizaciones verbales . \nTexto de la recomendaci\u00f3n: Parece que algunas \nde las palabras marcadas en el texto son nom bres \nderivados de verbos y, por tanto, pueden resultar \ndif\u00edciles de entender. A  no ser que sean t\u00e9rminos de \neste \u00e1mbito que no s e puedan cambiar, te \nrecomendamos que las su stituyas por sus \ncorrespondientes verbos para hacer el texto m \u00e1s \nclaro y din\u00e1mico.  \nEjemplos: La oraci\u00f3n \"Se llevar\u00e1 a cabo una evaluaci\u00f3n \nde los riesgos\" podr\u00eda sustituirse por \"Se evaluar\u00e1n  \nlos riesgos\" . \nLa orac i\u00f3n \"Se efectu\u00f3 la instalaci\u00f3n de los \nprogra mas inform\u00e1ticos\" podr\u00eda sustituirse por \"Se \ninstalaron los pr ogramas inform\u00e1ticos\".  \n \nb7. T\u00edtulo de la recomendaci\u00f3n:  Reformulaci\u00f3n de \nideas expresadas en negativo . \nTexto de la recomendaci\u00f3n : Parece que las \npalab ras marcadas en el texto indican negaci\u00f3n. T e \nrecomendamos que optes por la formulaci\u00f3n afirmativa de tus ideas en  caso de ser posible, \npuesto que as\u00ed se favorece la legibilidad y la interpretaci\u00f3n del mensaje.  \nEjemplos: \nLa oraci\u00f3n \u201cN o es infrecuente que s e acepten \nnuevos proyectos\u201d podr\u00eda formulars e en positivo de \nla siguiente manera: \u201cEs habitual que se acep ten \nnuevos proyectos\u201d.  \n   \n48\nIria da Cunha   c) Recomendaciones del nivel l\u00e9xico  \n \nc1. T\u00edtulo de la recomendaci\u00f3n:  Uso de i ndicadores \nde subjetivida d. \nTexto de la recomen daci\u00f3n: Las unidades \nmarcadas podr\u00edan indicar subjetividad. Ten en \ncuenta que este tipo de textos suelen ser objetivos. \nTe recomendamos que revises estas unidades para \nconfirmar que son adecuadas en tu texto.  \n c2. T\u00edtulo de la recomen daci\u00f3n:  Introducci\u00f3n de \nsiglas . \nTexto de la recomendaci\u00f3n: Las unidades \nmarcadas parecen siglas. Si es as\u00ed, ten en cuenta \nque la primera vez que se utiliza una sigla en un \ntexto suele ir acompa\u00f1ada del t\u00e9rmino desplegado.  \nEjemplos: \nUniversidad Nacional de Educaci\u00f3n a Distancia \n(UNED)  \nEPOC (enfermedad pulmonar obstructiva \ncr\u00f3nica)  \n \nc3. T\u00edtulo de la recomendaci\u00f3n:  Sistem aticidad en \nel uso de siglas . \nTexto de la recomendaci\u00f3n: Las unidades \nmarcadas parecen el t\u00e9rmino desplegado de siglas que utilizas en el texto. Si es as\u00ed, ten en cuenta que, \nuna vez se introduce una sigla en un texto, se suele seguir utilizando la sigla y  no el t\u00e9rmino \ndesplegado.  \n \nc4. T\u00edtulo de la recomendaci\u00f3n:  Utilizaci\u00f3n de \nt\u00e9rminos m\u00e1s transparentes.  \nTexto de la r ecomendaci\u00f3n: Los t\u00e9rmino s de la \nlista siguient e aparecen en tu texto y pueden \nresultar dif\u00edc iles de entender. Te recomendamos \nque los sustituyas por otros  m\u00e1s transparentes o \nque los aclares entre par\u00e9ntesis la primera vez que \naparecen.  Haz clic en cada u no de ellos para ver \nsugerencias de t\u00e9rminos al ternativos m\u00e1s claros.  \n c5. T\u00edtulo de la recom endaci\u00f3n:  Sustituci\u00f3n de \nexpresiones dif\u00edciles de entender . \nTexto de la recomendaci\u00f3n: Las expresiones de \nla lista siguiente aparecen en tu texto y pueden resultar  dif\u00edciles de entender, p or ser formas \narcaicas , en desuso o latinismos. Te recomendamos \nque las sustituyas por otras m\u00e1s claras o, si no es \nposible, que l as expliques en el texto la primera vez \nque aparecen (entre par\u00e9ntesis o en una nota, por ejemplo).  Haz clic en cada una de el las para ver \nsugerenci as de variantes alternativas m\u00e1s \ncomprensibles  o una explicaci\u00f3n de su significado.  \n \nc6. T\u00edtulo de la recome ndaci\u00f3n:  Sustituci\u00f3n de \npalabras poco precisas . \nTexto de la recomendaci\u00f3n: Parece que las \npalabras marcadas en el texto son poco precisas. \nTe recome ndamos que las elimines o las sustituyas \npor otras palabras m\u00e1s precisas.  Ejemplos: \nEn vez de \u201cSe trataron a lgunos temas en la \nreuni\u00f3n\u201d, ser\u00eda m\u00e1s preciso decir \u201cSe trataron \ncuatro temas en la reuni\u00f3n\u201d.  \nEn vez  de \u201cDijeron que hab\u00eda que hacer una \nreuni\u00f3n\u201d, ser\u00eda m\u00e1s preciso decir \u201cDijeron que \nhab\u00eda que  convocar una reuni\u00f3n\u201d.  \n \nc7. T\u00edtulo de la recomendaci\u00f3n:  Elimi naci\u00f3n de \nexpresiones redundantes . \nTexto de la recomendaci\u00f3n: Las expresiones \nmarcadas en el texto in cluyen informaci\u00f3n \nredundante. Te recomendamos que las elimines \npara hacer el texto m\u00e1s breve . \n \nc8. T\u00edtulo de la recomendaci\u00f3n:  Revisi\u00f3n de \npalabras largas . \nTexto de la recomendaci\u00f3n: Las palabras de la \nlista siguiente aparecen en tu texto y tienen \nvariant es alternativas m\u00e1s corta s. Te \nrecomendamos que  las utilices para favorecer la \nlegibilidad.  Haz clic en cada palabra para ver cu\u00e1l \nes su alternativa m\u00e1s co rta. \nB Anexo 2: Resoluciones del BOAM \nincluidas en el corpus de evaluaci\u00f3n  \n1. BOAM 8933 (19/07/2021) . Resoluci\u00f3n 1927.  \n2. BOAM 8980 (22/09/2021) . Resoluci \u00f3n 2393.  \n3. BOAM 9001 (22/10/2021) . Resoluci\u00f3n  2773.  \n4. BOAM 9030 (07/12/2021) . Resoluci\u00f3n  3277.  \n5. BOAM 8919 (29/06/2021) . Resoluci\u00f3n  1691.  \n6. BOAM 9027 (01/12/2021) . Resoluci\u00f3n  3241.  \n7. BOAM 9023 (25/11/2021) . Resoluci\u00f3n  3167.  \n8. BOAM 9005 (28/10/202 1). Resoluci\u00f3n  2831.  \n9. BOAM 8997 (18/10/2021) . Resoluci\u00f3n  2728.  \n10. BOAM 9021 (23/11/2021) . Resoluci\u00f3n  3120.  \n \n49\nUn redactor asistido para adaptar textos administrativos a lenguaje claro 50\nExploiting user-frequency information for mining\nregionalisms in Argentinian Spanish from Twitter\nExplotando informaci\u0013 on de frecuencia de usuarios para minar\nregionalismos del espa~ nol de Argentina en Twitter\nJuan Manuel P\u0013 erez,1;2Dami\u0013 an E. Aleman,1\nSantiago N. Kalinowski,3Agust\u0013 \u0010n Gravano4;2\n1Universidad de Buenos Aires, Argentina\n2Consejo Nacional de Investigaciones Cient\u0013 \u0010\fcas y T\u0013 ecnicas (CONICET), Argentina\n3Academia Argentina de Letras, Buenos Aires, Argentina\n4Universidad Torcuato Di Tella, Buenos Aires, Argentina\nfjmperez,dalemang@dc.uba.ar, s.kalinowski@aal.edu.ar, agravano@utdt.edu\nAbstract: The task of detecting regionalisms (expressions or words used in certain\nregions) has traditionally relied on the use of questionnaires and surveys, heavily\ndepending on the expertise and intuition of the surveyor. The emergence of social\nmedia and microblogging services has produced an unprecedented wealth of content\n(mainly informal text generated by users), opening new opportunities for linguists\nto extend their studies of language variation. Previous work on the automatic detec-\ntion of regionalisms depended mostly on word frequencies. In this work, we present\na novel metric based on Information Theory that incorporates user frequency. We\ntested this metric on a corpus of Argentinian Spanish tweets in two ways: via man-\nual annotation of the relevance of the retrieved terms, and also as a feature selection\nmethod for geolocation of users. In either case, our metric outperformed other tech-\nniques based on word frequency, suggesting that measuring the amount of users that\nuse a word is an informative feature. This tool has helped lexicographers discover\nseveral unregistered words of Argentinian Spanish, as well as di\u000berent meanings as-\nsigned to registered words.\nKeywords: Lexical dialectology, Social media, Spanish variants, Entropy.\nResumen: La tarea de detectar regionalismos (expresiones o palabras utilizadas en\ndeterminadas regiones) se ha basado tradicionalmente en el uso de cuestionarios y\nencuestas, dependiendo en gran medida de la pericia e intuici\u0013 on del investigador.\nEl surgimiento de las redes sociales y los servicios de microblogging ha producido\nuna riqueza de contenido sin precedentes (principalmente textos informales gener-\nados por usuarios), lo cual ha abierto nuevas oportunidades para el estudio de la\nvariaci\u0013 on ling\u007f u\u0013 \u0010stica. Estudios previos de la detecci\u0013 on autom\u0013 atica de regionalismos\ndependen sobre todo de la frecuencia de palabras. En este trabajo presentamos una\nm\u0013 etrica novedosa basada en la Teor\u0013 \u0010a de la Informaci\u0013 on, que incorpora la frecuencia\nde usuarios. Ponemos a prueba esta m\u0013 etrica en un corpus de Tweets en espa~ nol\nargentino de dos maneras: a trav\u0013 es de la anotaci\u0013 on manual de la relevancia de los\nt\u0013 erminos recuperados, y tambi\u0013 en us\u0013 andola como un m\u0013 etodo de selecci\u0013 on de carac-\nter\u0013 \u0010sticas para la geolocalizaci\u0013 on autom\u0013 atica de usuarios. En ambos casos, nuestra\nm\u0013 etrica super\u0013 o otras t\u0013 ecnicas basadas en la frecuencia de palabras, lo que sugiere\nque medir la cantidad de usuarios que usan una palabra es una caracter\u0013 \u0010stica in-\nformativa. Esta herramienta ha ayudado a lexic\u0013 ografos a descubrir varias palabras\nno registradas del espa~ nol argentino, as\u0013 \u0010 como signi\fcados nuevos de palabras ya\nregistradas.\nPalabras clave: Dialectolog\u0013 \u0010a l\u0013 exica, Redes sociales, Variantes del espa~ nol, En-\ntrop\u0013 \u0010a.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 51-62\nrecibido 08-03-2022 revisado 20-05-2022 aceptado 23-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-4\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introduction\nLexicography has been aided and enriched\nin the past 30 years by tools and resources\nfrom Computational Linguistics, mainly in\nthe form of corpora of selected texts (Atkins\nand Rundell, 2008). Statistical analyses of\ncorpora usually result in evidence to support\nthe addition of a word to a dictionary, its re-\nmoval, or its marking as dated or as unused\nor as regional, among other decisions.\nIn the process of compiling dictionaries,\ndi\u000berences emerge between dialects, where\nfrequently certain words or meanings do not\nspan all speakers. Since languages are ideal\nconstructs based on the observation of di-\nalects, it is of paramount importance to es-\ntablish which words are likely shared by an\nentire linguistic community and which are\nused only by smaller groups. In the lat-\nter case, word usage descriptions can pro\ft\nconsiderably from information as precise as\npossible, about geographical extension (re-\ngion, province, district, city, even neighbor-\nhood), registry (colloquial, neutral, formal),\nfrequency (current, past or a combination of\nboth depending on the chronological span of\nthe corpus), and other such variables.\nRegionalisms (words used mainly in a par-\nticular subregion, such as che ormetegol\nin Argentinian Spanish1) are commonly de-\ntected through surveys or transcriptions, us-\ning methods that depend more or less on the\nintuition and expertise of linguists (Almeida\nand Vidal, 1995; Labov, Ash, and Boberg,\n2005). The results of this methodology are of\ngreat value to lexicographers, who need evi-\ndence to support the addition of a word into\na regional dictionary, as well as the indica-\ntion of where it is used. Information gathered\nwith such methods has been used as lexical\nvariables to compute similarities between di-\nalects (Kessler, 1995; Nerbonne et al., 1996).\nThe emergence of social media and mi-\ncroblogging services has produced an un-\nprecedented wealth of content, with a clear\ntendency towards informal or colloquial text\ngenerated by users. This fact has opened\nmany opportunities for linguists due to the\npossibility of accessing geotagged contents,\nwhich provide valuable information about\nthe location of users. In this sense, social\nmedia texts have been used to aid lexical\n1Che: interjection for getting the interlocutor's at-\ntention; metegol : mechanic game that emulates foot-\nball (futbol\u0013 \u0010n ) (Academia Argentina de Letras, 2008).dialectology , for example to establish \\con-\ntinuous\" isoglosses (Gon\u0018 calves and S\u0013 anchez,\n2014; Huang et al., 2016) or to study the\ndi\u000busion of lexical change (Eisenstein et al.,\n2014), inter alia.\nA problem closely related to lexical di-\nalectology is geolocation, which maps words\ninto regions or locations (Eisenstein, 2014).\nA possible way to evaluate dialectological\nmodels is to use them in geolocation algo-\nrithms; regionalisms can be seen as location-\nindicative words (Han, Cook, and Baldwin,\n2012). Most previous work in word-centric\ngeolocation algorithms (and lexical dialec-\ntology) relies on the observation of the fre-\nquency of a certain word, ignoring the num-\nber of users producing them. Also, to our\nknowledge very little work has been per-\nformed in Spanish on these topics.\nIn this work, we present an information-\ntheoretic measure to detect regionalisms in\nsocial media texts, particularly on Twitter,\nand we test it against a dataset of tweets\nin Argentinian Spanish. Our contributions\nare twofold: a) we introduce a new metric\nbased on Information Theory which can be\nseen as a mixture of TF-IDF and Informa-\ntion Gain; and b) we show that measuring\nthe dispersion of users is a strong indicator\nof relevance, for both lexical dialectology and\ngeolocation. We conduct our experiments\non a dataset of tweets in Argentinian Span-\nish, with 81M tweets, 56K users, all balanced\nacross the country's 23 provinces.\n2 Previous Work\nMost previous work in lexical dialectology\nconsists in measuring the usage of words\nthat are known a priori to be regional vari-\nants. These studies typically use information\ngathered from sources such as web searches\n(Grieve, Asnaghi, and Ruette, 2013) and\nmanually-collected regionalisms (Ueda and\nRuiz Tinoco, 2003; Kessler, 1995). Even pa-\npers that analyze data from Twitter (Huang\net al., 2016; Gon\u0018 calves and S\u0013 anchez, 2014)\nstill rely on words already known for the dis-\ncovery of dialectal patterns.\nLanguage evolves so quickly that it is im-\nportant to detect these contrastive words\nautomatically { or at least, to alleviate\nthe e\u000borts needed to detect them. Two\ntypes of approaches exist for this problem:\nmodel-based approaches and metric-based\napproaches (Rahimi, Baldwin, and Cohn,\n52\nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano Total Mean SD\nWords 647M 28.14M 6.64M\nTweets 80.9M 3.51M 0.91M\nUsers 56.2K 2.44K 0.04K\nVocabulary 7.5M 0.32M 0.04M\nTable 1: Dataset summary. Total \fgures,\nalong with province-level means and stan-\ndard deviations.\n2017). Model-based approaches use gener-\native models to detect topics and regional\nvariants (Eisenstein et al., 2010; Ahmed,\nHong, and Smola, 2013). Typically, these\nare computationally expensive, which limits\nthe amount of data that may be processed.\nMetric-based approaches compute statistics\nfor each word or expression, and use them\nto create rankings (Cook, Han, and Bald-\nwin, 2014; Chang et al., 2012; Jimenez et al.,\n2018; Monroe, Colaresi, and Quinn, 2008).\nThese rankings are subsequently evaluated\nby checking external sources of regionalisms,\nsuch as dictionaries. In the following section,\nwe compare our metrics to those proposed\nby Han, Cook, and Baldwin (2012): Term-\nFrequency Inverse Location Frequency (TF-\nILF) and Information-Gain Ratio.\nText-based geolocation can be seen as the\ninverse problem of lexical dialectology: while\ndialectology maps regions into text, geolo-\ncation maps text into regions (Eisenstein,\n2014). Thus, a reasonable way of assessing\nthe performance of a method for discover-\ning regional words is to use it as a feature-\nselection method for a geolocation classi\fer,\nas proposed by Han, Cook, and Baldwin\n(2012). In the present work, we use provinces\nas our unit of study (see Section 3), but \fner\ngrained geolocation could be performed by\nusing an adaptive grid (Roller et al., 2012).\nRahimi, Cohn, and Baldwin (2017) pro-\npose a di\u000berent approach to this problem.\nThey train a multilayer perceptron with a\nbag-of-words as input to geolocate users. In-\ntermediate layers serve as vector representa-\ntions to perform lexical analysis by analyzing\nproximities in the embedding space.\nInformation Theory is the basis for many\nof these methods (Han, Cook, and Baldwin,\n2012; Roller et al., 2012; Chang et al., 2012).\nOther uses of information theoretic measures\ninclude telling whether a hashtag is promoted\nby spammers by analyzing its dispersion in\ntime and in users (Cui et al., 2012; Ghosh,Surachawala, and Lerman, 2011), and also to\ndiscover valuable features from user messages\non Twitter for sentiment analysis and opin-\nion mining (Pak and Paroubek, 2010). The\nmetrics discussed in the next section use this\nconcept of measuring the entropy of the users\nof a particular word.\n3 Materials\nThe territory of Argentina is divided into 23\nprovinces and the autonomous city of Buenos\nAires, with populations ranging from 127,000\n(Tierra del Fuego Province) to 15 million\n(Buenos Aires Province), according to the\n2010 National Census.2Provinces are further\nsubdivided into departments, which in some\ncases are called partidos orcomunas.\nTo gather our data, we \frst collected infor-\nmation of all departments in Argentina from\nthe 2010 National Census and conducted a\nlookup through the Twitter API for users\nwith location matching those departments.\nEven though location \felds in Twitter are\nnot very reliable (Hecht et al., 2011), given\nthat we restrict our search to a \fxed number\nof department names, we observe that most\nof the potential noise is reduced. We used\nthe Python library tweepy to interact with\nthe Twitter API.3\nFor each of the retrieved users, we suc-\ncessfully downloaded their entire tweetlines.\nTweets were tokenized using NLTK (Bird,\nKlein, and Loper, 2009). Hashtags and men-\ntions to users were removed; the remaining\nwords were downcased; and identical con-\nsecutive vowels were normalized up to three\nrepetitions (\\woaaa\" instead of \\woaaaaaa\").\nTable 1 summarizes the collected dataset,\nand Figure 1 shows the distributions of tweets\nper user and tweet length.\nIt is well known that the Twitter vocabu-\nlary tends to be very noisy with lots of con-\ntractions, non-normal spellings (e.g., vocal-\nizations), typos, etc. (Kaufmann and Kalita,\n2010). For this reason, we decided to take\ninto account only words occurring more than\n40 times and used by more than 25 users\n(these values were chosen empirically). This\nremoves about 1% of the total words and\nshrinks the vocabulary from 2.3 million words\nto around 135,000 words.\n2https://www.indec.gov.ar\n3https://www.tweepy.org\n53\nExploiting user-frequency information for mining regionalisms in Argentinian Spanish from Twitter Figure 1: Dataset distributions: Number of tweets per user (left) and words per tweet (right).\n4 Method\nWe can think of a regionalism as a word\nwhose usage is not uniform across the terri-\ntory { i.e., whose concentration is higher in a\nspeci\fc region. With this in mind, we aim to\nmeasure these disorders in word usage { or,\nmore precisely, the entropy of words (Shan-\nnon, 1948).\nIn general, words with high entropy are\nmore likely to be pronouns, connectors\nand other closed-class words, whereas their\nlow-entropy counterparts are usually nouns,\nverbs, adjectives and adverbs with fuller se-\nmantic content (Montemurro and Zanette,\n2002; Montemurro and Zanette, 2010). Also,\nwords with high entropy (i.e., high disorder)\ncan be regarded as used evenly across the\ncountry. On the other hand, low-entropy\nwords are used with higher frequency in a\nfew speci\fc locations.\nLetl1; l2; : : : l Nbe our locations, and !1;\n!2; : : : ! Mour vocabulary. If Ojrefers to\nthe event of occurrence of word !j, then\np(lijOj) denotes the probability that word wj\noccurred in location li.\nWe next de\fne the word-count entropy as\nHwords(!j) =\u0000NX\ni=1p(lijOj)\u0001logp(lijOj):(1)\nNote that this measure does not take into\naccount the actual frequency of words. For\ninstance, if two words !1and!2occur only in\none particular location, but !1is much more\nfrequent than !2, both words will still have\nthe same entropy according to Equation 1.\nIn a similar fashion to tf-idf and inspired\nby Montemurro and Zanette (2010) and Han,\nCook, and Baldwin (2012), we de\fne measure\nIwords(!) for word !as follows:\nIwords(!) =p(!)\u0001(logN\u0000Hwords(!));(2)where log Nis the maximum possible value of\nHwords(!) (Shannon, 1948), and p(!) is the\nrelative frequency of !in the corpus (0 \u0014\np(!)\u00141). In this way, Iwords(!) will be high\nfor frequent words that accumulate in just a\nfew locations.\nAnother important aspect of a word is\nthe amount of people that use it (Cui et al.,\n2012). Assuming we now sample Twitter\nusers, let Ujbe the event that a particular\nuser uses word !j. Then p(lijUj) denotes the\nprobability that the location of a user is li\ngiven the fact that s/he uses word !j. We\nde\fne the user-count entropy as\nHusers(!j) =\u0000NX\ni=1p(lijUj)\u0001logp(lijUj) (3)\nand the following metric of !,\nIusers(!) =q(!)\u0001(logN\u0000Husers(!));(4)\nwhere q(!) is the proportion of users who\nmentioned !in the corpus (0 \u0014q(!)\u00141).\nNote that Iusers(!) will be high for words\nmentioned by several users who accumulate\nin just a few locations.\nAccording to Zipf's Law, the counts of\nthe most frequent words are orders of magni-\ntude higher than the counts of the remaining\nwords { a phenomenon that is also true when\ncounting users of words. So the p(!) and\nq(!) terms in Equations (2) and (4) become a\nproblem as words with high frequencies over-\ncome their low entropies. To alleviate this,\nwe performed a normalization on the word\nfrequency as follows. Let M!be the most\nfrequent word, that is,\nM!= arg max\n!2W#!; (5)\nwhere # !denotes the total number of occur-\nrences of !in our dataset. Then, the Nor-\n54\nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano malized log-frequency of word occurrences is\nde\fned as\nnwords(!) =log(#! )\nlog(#M w): (6)\nWords with very high frequency di\u000ber lit-\ntle in their values of nwords(!). We de\fne\nanalogously the Normalized log-frequency of\nuser mentions nusers. Hence, we rewrite\nEquations (2) and (4) and arrive at the \fnal\nde\fnition of our two metrics as\nIwords(!) =nwords(!)(log( n)\u0000Hwords(!))\n(7)\nIusers(!) =nusers(!)(log( n)\u0000Husers(!))\n(8)\nWe call the \frst metric Log-Term Frequency\nInformation Gain (LTF-IG) and the second\none Log-User Frequency Information Gain\n(LUF-IG). Summing up, words with high\nvalues of LTF-IG or LUF-IG are canti-\ndates for being regionalisms { words that\noccur much more often in a certain region\nthan in the rest of the country.\nWe subsequently sort all words in our\ndataset relative to these metrics, thus obtain-\ning two word rankings: Word-Count Ranking\nand User-Count Ranking. The words that\nappear in the \frst positions of a ranking are\nthose with high values for the metric, and\nthus more likely to be regionalisms.\n4.1 Lexicographic Validation\nWith these rankings, a team of lexicogra-\nphers from Academia Argentina de Letras\nperformed a linguistic validation of the \frst\nthousand words according to each metric.\nThis qualitative analysis consisted in a de-\ntailed study, word by word, to determine if\nthe word in question is part of the lexical\nrepertoire of a community of speakers.\nProper and place names (toponyms) were\nexcluded {as is usual in lexicography{ al-\nthough many words in this class had high\nvalues for our metrics. Potential toponyms\nwere automatically highlighted to facilitate\ntheir manual exclusion by lexicographers.\nTo perform the linguistic validation, lex-\nicographers were provided with tables con-\ntaining counts for each word and province:\nnumber of users, number of occurrences and\nnormalized frequency (occurrences per mil-\nlion words). Also, samples of tweets contain-\ning these words were provided when neces-\nsary. The goal of this manual validation wasto identify not only words used exclusively or\nmainly in a region, but also words used there\nwith a di\u000berent meaning.\nAs a result of this process, every word in\nthe top-1000 of each ranking was annotated\nwith `1' if it had lexical relevance as a re-\ngionalism, or `0' if it had not. Lastly, lex-\nicographers performed a characterization of\nthe words marked as regionalisms, according\nto the linguistic phenomenon they represent.\nThe outcome of these procedures is described\nin Section 5.\n4.2 Feature Selection for\nGeolocation\nTo indirectly assess the usefulness of our\nmetrics, we used each as a feature-selection\nmethod to train geolocation classi\fers. This\nmeans that, instead of using the entire bag-\nof-words as input for a geolocation algorithm,\nwe consider a smaller subset of the vocab-\nulary. This dimensionality reduction of the\nfeature space is aimed at boosting the classi-\n\fer performance.\nThis approach to geolocation can be de-\nscribed as \\word-centric\", as it uses lexical\ninformation from tweets to predict a location\n(Zheng, Han, and Sun, 2018). But we empha-\nsize that we are interested in user geoloca-\ntion, not tweet geolocation. Thus, the units\nconsidered here are all the tweets from indi-\nvidual users. We randomly selected 10,000\nusers from our dataset { 7,500 for training\nand 2,500 for testing.\nFor reference, we compare our results to\nthose obtained using the Information Gain\nRatio (IGR) metric (Han, Cook, and Bald-\nwin, 2012; Cook, Han, and Baldwin, 2014):\nifLis a random variable denoting the loca-\ntion of a given occurrence of word !i, then\ntheInformation Gain of!iis\nIG(! i) =H(L)\u0000H(Lj! i)\n/P(!i)mX\nj=1P(cjjwi) logP(cjjwi)\n+P(wi)mX\nj=1P(cjjwi) logP(cjjwi)\nwhere P(!i) denotes the probability that !i\ndoes not occur. Then, IGR(!i) is de\fned as\nIGR(!i) =IG(! i)\nIV(!i)(9)\n55\nExploiting user-frequency information for mining regionalisms in Argentinian Spanish from Twitter Rank Word User\n1 ushuaia chivil\n2 rioja ush\n3 chivilcoy poec\n4 bragado malpegue\n5 viedma aijue\n6 logro~ no tolhuin\n7 chepes vallerga\n8 ober\u0013 a yarca\n9 cldo blv\n10 tdf portho\n11 riojanos jumeal\n12 bre~ nas sinf\n13 choele plottier\n14 gallegos kraka\n15 tiemposur fsa\n16 fueguinos bombola\n17 chilecito yarco\n18 blv sanagasta\n19 ush wika\n20 merlo obera\nTable 2: Top 20 words for the two metrics.\nWords in bold have lexicographic interest as\nregionalisms.\nwhere IGis normalized by\nIV(!) =\u0000P(!) logP(!)\u0000P(!) logP(!)):\nWe also calculate IGR with respect to\nthe user frequencies of a word (which we ab-\nbreviate \\user frequencies\" for the sake of\nsimplicity), in a similar way to Equation 4.\nAs a baseline for our feature selection meth-\nods, we also calculate Term-Frequency In-\nverse Location Frequency (TF-ILF) , which\nconsists in sorting our terms \frst by Location\nFrequency (in ascending order) and then by\nTerm-Frequency (in descending order).\nSumming up, \fve feature selection meth-\nods are tested as feature selection for geoloca-\ntion: TF-ILF, LTF-IG ,LUF-IG, basic IGR,\nand User IGR. We train Multinomial Lo-\ngistic Regressions using the top N% words\nas features, and test against the 2.5K held\nout users. Performance is assessed using ac-\ncuracy and mean distance between capital\ncities of each province { a fairly good esti-\nmate, since most of the population concen-\ntrates around those cities.\n5 Results\nTable 2 shows the top-20 words calculated\nwith each metric. Many are toponyms:chivil, ush, blv, tolhuin, kraka, sanagasta,\nwika refer to towns, cities and local clubs.\nAlso, some words refer to gentilics ( riojanos,\nfueguinos ), or local institutions ( POEC ).\nSome of these words emerge as region-\nalisms: yarca/yarco, aijue, sinf, cldo, bom-\nbola, malpegue . We observe that the two\nrankings even share many words: User-Count\nand Word-Count have an overlap of 63% in\nthe top thousand words.\nFigure 2 shows four three-dimensional\nscatter plots. A dot in these plots corre-\nsponds to an individual word in our cor-\npus, and is placed along the horizontal\naxes according to its word- or user-count\nentropy (H words(!) and Husers(!), respec-\ntively). Along the vertical axes, each dot is\nlocated following its corresponding word or\nuser frequency (n words(!) and nusers(!)). Ad-\nditionally, each dot is colored according to\nthe position of the word in one of our rank-\nings using a chromatic scale, such that the\nlighter the dot, the higher the word's rank.\nFor clearer visualization, word rankings are\nalso shown in logarithmic scale.\nFigure 2a shows that words higher in the\nWord-Count Ranking (in lighter color) tend\nto appear closer to the upper-left corner of\nthe plot { that is, such words are more fre-\nquent and their mentions are concentrated in\nfewer regions. Figure 2d shows a very simi-\nlar thing, now with respect to the number of\nusers that mention the words: words higher\nin the User-Count Ranking are mentioned by\na larger number of users from fewer regions.\nThese two \fgures display a gradient from the\nupper-left corner (words ranked higher, in\nlighter color) to the lower-right corner (words\nranked lower, in darker color).\nFigure 2b uses horizontal and vertical axes\ncorresponding to users ( Husers andnusers),\nbut colors each word with respect to the\nWord-Count Ranking. Here we can observe\na slight perturbation in the gradient: there\nare words far from the left-corner that have\nlight colors. From this, we understand that\nthere are words with high Word-Count Rank-\ningthat have low User-Count Ranking.\nLikewise, Figure 2c uses User-Count\nRanking to color the points, and word axes\nHuserandnuser. The perturbation in the gra-\ndient is clearer in this plot; many words ap-\npear high in the Word-Count Ranking (closer\nto the top-left corner, see Figure 2a) but low\ninUser-Count Ranking (darker color).\n56\nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano (a) Color scale: Word-Count Ranking\n (b) Color scale: Word-Count Ranking\n(c) Color scale: User-Count Ranking\n (d) Color scale: User-Count Ranking\nFigure 2: Scatter plots showing words (dots) along three dimensions. Horizontal axes: word-\ncount entropy Hwords (left plots) or user-count entropy Husers (right plots). Vertical axes: nor-\nmalized log word frequencies nwords (left plots) or user frequencies nusers (right plots). Color:\nlog word rank according to Word-Count (top plots) or to User-Count (bottom plots); lighter\ncolor means higher rank.\nTo further inspect this phenomenon, we\nsearched for words that have large di\u000ber-\nences in the logarithm of Word-Count Rank-\ningandUser-Count Ranking. The logarithm\nreduces the di\u000berence between words ranked\nvery high (e.g., between the word at position\n10,000 and another in position 20,000) and\nampli\fes the di\u000berence when one of the ranks\nis low and the other is high. A close examina-\ntion of these words and their tweets showed\nthat they were produced by bots (news and\nmetheorological accounts, or accounts using\ntools to gain more followers) or in small\nniches of fans of some celebrity. From the\ntop-100 words sorted by this di\u000berence, only\none ranks higher in users than in words.\nSumming up, when a word has a high\nUser-Count Ranking, it also tends to have\na high Word-Count Ranking. The reverse is\nnot true, however, as words produced by a\nsmall number of accounts would not rank well\nwith respect to users. Thus, the User-Count\nRanking successfully discards words coming\nfrom automatic agents, as already done in\nCui et al. (2012).Word Word Rank User Rank\nrioja 2 2499\nvto 27 28179\nhoa 81 83717\ncontextos 88 71290\ncardi 32 23756\nagraden 107 75042\nhemmings 59 40227\nushuaia 1 565\ntweeted 43 21342\nprecipitaci\u0013 on 66 31042\nTable 3: Top 10 words with the largest gaps\nbetween log word rank and log user rank.\n5.1 Lexicographic Validation\nThe \frst thousand words in the Word-Count\nRanking were manually analyzed by the lex-\nicographers, who marked 21.9% as likely re-\ngionalisms. Likewise, from the \frst thousand\nwords in the User-Count Ranking, 30.2%\nwere marked as being lexicographically rel-\nevant. This validation suggests that\nconsidering user-frequency dispersion\nis more relevant when assessing a word\nas a regionalism.\n57\nExploiting user-frequency information for mining regionalisms in Argentinian Spanish from Twitter Lexical characterization is illustrated in\nTable 4, which displays a few examples of\ngroups of regionalisms found thanks to this\nmethodology. A special note is reserved for\nthe group of indigenisms, where a number of\nwords were found coming from the Guaran\u0013 \u0010\nlanguage (for instance, mita\u0013 \u0010, ang\u0013 a, anga\u0013 u,\nnderakore ) and also from Quechua (ura).\nIt is worth mentioning that the regions of\nthe words derived from Guaran\u0013 \u0010 { spoken\nin Northeastern Argentina, Paraguay, Bolivia\nand Southwest of Brazil { coincide with the\nregion delimited by Vidal de Battini (1964).\nColloquialisms\nWord Region Meaning\nculiado C\u0013 ordoba asshole\nchombi Mendoza poor in quality\ncarnasas Neuqu\u0013 en not classy, inele-\ngant\nbolasear Cuyo to bullshit\naprontar E. R\u0013 \u0010os to get ready\nIndigenisms\nura Northwest vagina (quechua)\nmita\u0013 \u0010 Guaranitic boy\nang\u0013 a Guaranitic unfortunate\nRegional realities\npiadinas San Juan roll (food)\ntarefero Misiones yerba mate worker\nPOEC Neuqu\u0013 en high School exam\nInterjections\naijue Formosa surprise\nyirr Corrientes joy\naiss Formosa annoy\njiaa Corrientes yeehay\nOrtographic variations\npesao Northwest pesado\nql Northwest culiado\nuaso C\u0013 ordoba guaso\nRegional Morpheme\nraraso C\u0013 ordoba very strange (raro)\ntardaso C\u0013 ordoba very late (tarde)\nTable 4: Examples of regionalisms found in\nthe manual analysis. Each group corresponds\nto a subjective category found by the lexicog-\nraphers during the annotation process.\n5.2 Feature Selection for\nGeolocation\nMoving on to the results of our second valida-\ntion procedure, Figure 3 displays the perfor-\nmance of the di\u000berent feature selection meth-ods when used to train a discriminative clas-\nsi\fer. Horizontal axes represent the percent-\nage of top words selected, and the vertical\naxes represent the mean distance error in 3a\nand the accuracy in the case of 3b.\nLUF-IG obtains the best performance in\nthe user geolocation task, and stabilizes in\na plateau at roughly 3.75% of top words\nused. It outperforms its word-frequency ver-\nsion LTF-IG and both IGR metrics. Table\n5 displays the results of using the full bag\nof words (baseline) versus using the di\u000ber-\nent feature selection methods with 5,000 top\nwords.\nWhen comparing our metrics, we note\nthat the ones based on user-frequencies ob-\ntain a better performance than their word-\nfrequency counterparts. This is more appar-\nent in the case of LTF-IG and LUF-IG, but\ncan also be observed for IGR metrics.\n6 Discussion\nOf the proposed metrics, User-Count Metric\nproved to be the most promising one. It suc-\ncessfully removed from the top of the ranking\nwords likely to come from automatic agents\nor from small niches of users, and a manual\nlexicographic validation con\frmed that this\nranking contained more regionalisms than\ntheWord-Count Metric . Further, using this\nmetric as a feature selection method for ge-\nolocating users also showed a signi\fcative\nimprovement over other metrics { both its\nword-frequency counterpart and IGR metrics\nfrom Han, Cook, and Baldwin (2012). This\nstrongly suggests that measuring the disper-\nsion of users of a certain word is a very in-\nformative indicator { both in lexicographic\nand in geolocation terms { backing what was\nalready proposed in previous work to detect\nspam on Twitter (Cui et al., 2012).\nThe proposed metric was developed in the\ncontext of analyzing regional colloquialisms.\nFeatures Accuracy Mean Distance\nAll 0.383 599.8\nTF-ILF 0.654 363.3\nIGR-Words 0.736 214.2\nIGR-Users 0.748 234.7\nLTF-IG 0.737 227.9\nLUF-IG 0.784 164.9\nTable 5: Performance of the di\u000berent feature\nselection methods when using the top-5000\nwords.\n58\nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano (a) Mean distance error in user geolocation\n (b) Accuracy in user geolocation\nFigure 3: Comparison of the metrics when used as feature selection methods for geolocation.\nVertical axes show the percentage of the top words used as features to train a Multinomial\nLogistic Regresion, and vertical axes display the performance of each respective classi\fer. Figure\na uses mean distance error as y-axis (less is better) and Figure b uses accuracy (more is better)\nThis area of the lexicon is most elusive, since\nits impact on any printed medium arrives no-\nticeably late { and in many cases it never\nreaches it at all. Colloquialisms are a class\nof words hardly found in any other media.\nOur best performing metric marked as rele-\nvant several words that were already listed\nin the Diccionario del Habla de los Argenti-\nnos(Academia Argentina de Letras, 2008), a\nfact that con\frms the usefulness of both our\nmetric and Twitter data in general for this\ntask.\nAn outstanding subgroup of words found\nin the analysis are those coming from\ntheGuaranitic region, in Northeastern Ar-\ngentina. In particular, three words have\nalready been proposed for addition to the\naforementioned dictionary: ang\u0013 a, anga\u0013 u,\nmita\u0013 \u0010 . This case is emblematic because it\nshows how this type of approach can help\novercome the intrinsic limitations of doing\nregional lexicography. When lexicographers\nare native to only one of the di\u000berent dialects\nof the region included in a projected dictio-\nnary, the probability of properly detecting\nand de\fning words of other dialects is slim or\ndepends on mere chance. As the team of lex-\nicographers expressed when confronted with\nthese three words related to Guaran\u0013 \u0010 her-\nitage, those very robust normalized frequen-\ncies across a signi\fcant portion of the terri-\ntory of Argentina would have otherwise re-\nmained unknown. Instead of including them\nin the next edition of the dictionary that at-\ntempts to describe all regional lexical itemsin the country, they would have remained un-\nregistered, thus perpetuating a serious omis-\nsion.\nAs our focus was in detecting lexical vari-\nations within provinces, we paid no attention\nto spatial granularity. If a better granularity\nwere necessary in the analysis, adaptive par-\ntitioning could be used (Roller et al., 2012)\nto improve geolocation and to \fnd localisms\nwithin provinces. Although previous work\n(Vidal de Battini, 1964) indicates that most\nprovinces do not have large dialectal vari-\nations within them, this is something that\nwould need to be explored and con\frmed in\nfuture work.\nAlso, these techniques should be tested\nagainst other datasets, such as those used in\n(Roller et al., 2012; Han, Cook, and Baldwin,\n2012), to further con\frm that they outper-\nform other feature selection methods.\n7 Conclusions\nIn this work, we developed and compared\ntwo novel metrics useful for detecting re-\ngionalisms in Twitter based on Informa-\ntion Theory. One was based on the word\nfrequency (Log Term Frequency-Information\nGain, LTF-IG ) and the other on the user\nfrequency of a word ( Log user frequency-\nInformation Gain, LUF-IG ). These met-\nrics may be seen as a mixture of previ-\nous information-theoretic measures and clas-\nsicTF-IDF .\nWe evaluated their performance in two\nways. First, a team of lexicographers man-\n59\nExploiting user-frequency information for mining regionalisms in Argentinian Spanish from Twitter ually assessed the presence of regionalisms in\nthe \frst thousand words ranked by each met-\nric. Second, we tested the metrics as feature-\nselection methods for geolocation algorithms,\nfor which we also tested against metrics from\nprevious works (Han, Cook, and Baldwin,\n2012; Cook, Han, and Baldwin, 2014). In\nboth evaluation types, the metric built upon\nuser frequencies ( LUF-IG ) yielded the best\nresults, suggesting that the number of users\nof a word is very informative { perhaps even\nmore than simple word frequency.\nThis method has aided lexicographers in\ntheir task, allowing them to propose the ad-\ndition of a number of words into the Dic-\ncionario del Habla de los Argentinos . The\nwork behind this particular dictionary relies\non a collaborative e\u000bort based on the in-\ntuition of scholars and lexicographers that\nidentify regionalisms used mainly (seldom\nexclusively) within Argentina's borders by\ncarefully parsing over a diversity of sources.\nTherefore, using Twitter to automatically\ndetect regionalisms does not limit itself to\navoiding most of this manual work, which,\nin and of itself, would already be a sizeable\ncontribution. Since a considerable portion of\nthe lexical repertoire of a community does not\nmake its way across to published materials\n(which make most of the 300 millions words\nincluded to date in, for example, CORPES\nXXI (Real Academia Espa~ nola, 2013)), the\npossibility of creating lists of words that are\nlikely to be regional, based on actual utter-\nances written by users, opens a way of shed-\nding light onto entire pockets of lexical items\nthat would remain otherwise chronically un-\nderrepresented in dictionaries. Even when a\nregional word is published, and then included\nin corpora, the task of appropriately isolat-\ning it remains largely unchanged, given that\nthe word has to be previously identi\fed in or-\nder to then take advantage of the statistical\ninformation available.\nThis work de\fnes Argentinian provinces\nas the regional units of analysis, but this\ncould be changed in order to repeat the anal-\nysis at di\u000berent granularity levels. In this\nway, it might be possible to study intra-\nprovincial dialectal di\u000berences (e.g., at the\ndepartment level, see Section 3), although the\nlimited precision of the geolocation of Twit-\nter users may complicate this task. And it\nwould de\fnitely be possible to detect con-\ntrastive words across larger regions, for ex-ample to study Spanish in all its geographical\nvariants.\nA further challenge triggered by this work\nis the detection of regions with di\u000berent di-\nalectal uses (Gon\u0018 calves and S\u0013 anchez, 2014)\nbut using features obtained in a semisuper-\nvised fashion with these metrics. This would\nallow to assess the validity of the dialectal\nregions of Argentina proposed by Vidal de\nBattini in 1964 (Vidal de Battini, 1964). Spa-\ntial and temporal information could be also\nexplored, particularly \fner-grained locations.\nRegarding geolocation, the proposed metrics\nshould also be tested against other datasets\nto evaluate its performance as a feature se-\nlection method.\nAcknowledgments\nThis work was funded in part by CONICET,\nUniversidad de Buenos Aires, and Universi-\ndad Torcuato Di Tella. We thank Edgar Alt-\nszyler, Mariela Sued, and Federico Plager for\nhelpful discussions, and our anonymous re-\nviewers for valuable suggestions.\nReferences\nAcademia Argentina de Letras. 2008.\nDiccionario del habla de los argentinos .\nEmec\u0013 e Editores.\nAhmed, A., L. Hong, and A. J. Smola. 2013.\nHierarchical geographical modeling of user\nlocations from social media posts. In Pro-\nceedings of the 22nd international confer-\nence on World Wide Web, pages 25{36.\nACM.\nAlmeida, M. and C. Vidal. 1995. Variaci\u0013 on\nsocioestil\u0013 \u0010stica del l\u0013 exico: un estudio con-\ntrastivo. Bolet\u0013 \u0010n de \flolog\u0013 \u0010a, 35(1):50.\nAtkins, B. S. and M. Rundell. 2008. The Ox-\nford guide to practical lexicography. Ox-\nford University Press.\nBird, S., E. Klein, and E. Loper. 2009.\nNatural language processing with Python:\nAnalyzing text with the natural language\ntoolkit. O'Reilly Media, Inc.\nChang, H.-w., D. Lee, M. Eltaher, and\nJ. Lee. 2012. @Phillies tweeting from\nPhilly? Predicting Twitter user locations\nwith spatial word usage. In Proceedings\nof the 2012 International Conference on\nAdvances in Social Networks Analysis and\nMining (ASONAM 2012), pages 111{118.\nIEEE Computer Society.\n60\nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano Cook, P., B. Han, and T. Baldwin. 2014.\nStatistical methods for identifying local\ndialectal terms from GPS-tagged docu-\nments. Dictionaries: Journal of the\nDictionary Society of North America ,\n35(35):248{271.\nCui, A., M. Zhang, Y. Liu, S. Ma, and\nK. Zhang. 2012. Discover breaking events\nwith popular hashtags in twitter. In Pro-\nceedings of the 21st ACM International\nConference on Information and Knowl-\nedge Management, CIKM 12, pages 1794{\n1798, New York, NY, USA. ACM.\nEisenstein, J. 2014. Identifying regional di-\nalects in online social media. In School\nof Interactive Computing Faculty Publica-\ntions. Georgia Institute of Technology.\nEisenstein, J., B. O'Connor, N. A. Smith,\nand E. P. Xing. 2010. A latent variable\nmodel for geographic lexical variation. In\nProceedings of the 2010 conference on em-\npirical methods in natural language pro-\ncessing, pages 1277{1287. Association for\nComputational Linguistics.\nEisenstein, J., B. O'Connor, N. A. Smith,\nand E. P. Xing. 2014. Di\u000busion of lex-\nical change in social media. PloS one ,\n9(11):e113114.\nGhosh, R., T. Surachawala, and K. Ler-\nman. 2011. Entropy-based classi\fcation\nof retweeting activity on Twitter. arXiv\npreprint arXiv:1106.0346 .\nGon\u0018 calves, B. and D. S\u0013 anchez. 2014. Crowd-\nsourcing dialect characterization through\nTwitter. PloS one, 9(11):e112074.\nGrieve, J., C. Asnaghi, and T. Ruette. 2013.\nSite-restricted web searches for data col-\nlection in regional dialectology. American\nspeech, 88(4):413{440.\nHan, B., P. Cook, and T. Baldwin. 2012. Ge-\nolocation prediction in social media data\nby \fnding location indicative words. Pro-\nceedings of COLING 2012, pages 1045{\n1062.\nHecht, B., L. Hong, B. Suh, and E. H. Chi.\n2011. Tweets from Justin Bieber's heart:\nthe dynamics of the location \feld in user\npro\fles. In Proceedings of the SIGCHI\nconference on human factors in comput-\ning systems, pages 237{246. ACM.Huang, Y., D. Guo, A. Kasako\u000b, and\nJ. Grieve. 2016. Understanding US re-\ngional linguistic variation with Twitter\ndata analysis. Computers, Environment\nand Urban Systems , 59:244{255.\nJimenez, S., G. Due~ nas, A. Gelbukh, C. A.\nRodriguez-Diaz, and S. Mancera. 2018.\nAutomatic Detection of Regional Words\nfor Pan-Hispanic Spanish on Twitter. In\nIbero-American Conference on Arti\fcial\nIntelligence , pages 404{416. Springer.\nKaufmann, M. and J. Kalita. 2010. Syntac-\ntic normalization of Twitter messages. In\nInternational conference on natural lan-\nguage processing, Kharagpur, India.\nKessler, B. 1995. Computational dialectol-\nogy in Irish Gaelic. In Proceedings of the\nseventh conference on European chapter\nof the Association for Computational Lin-\nguistics, pages 60{66. Morgan Kaufmann\nPublishers Inc.\nLabov, W., S. Ash, and C. Boberg. 2005.\nThe atlas of North American English:\nPhonetics, phonology and sound change .\nWalter de Gruyter.\nMonroe, B. L., M. P. Colaresi, and K. M.\nQuinn. 2008. Fightin'words: Lexical fea-\nture selection and evaluation for identify-\ning the content of political con\rict. Polit-\nical Analysis, 16(4):372{403.\nMontemurro, M. A. and D. H. Zanette. 2002.\nEntropic analysis of the role of words in\nliterary texts. Advances in complex sys-\ntems, 5(01):7{17.\nMontemurro, M. A. and D. H. Zanette. 2010.\nTowards the quanti\fcation of the seman-\ntic information encoded in written lan-\nguage. Advances in Complex Systems,\n13(02):135{153.\nNerbonne, J., W. Heeringa, E. Van den Hout,\nP. Van der Kooi, S. Otten, W. Van de Vis,\net al. 1996. Phonetic distance between\nDutch dialects. In CLIN VI: proceedings\nof the sixth CLIN meeting , pages 185{202.\nPak, A. and P. Paroubek. 2010. Twitter as a\ncorpus for sentiment analysis and opinion\nmining. In LREc, volume 10, pages 1320{\n1326.\nRahimi, A., T. Baldwin, and T. Cohn. 2017.\nContinuous representation of location for\ngeolocation and lexical dialectology using\n61\nExploiting user-frequency information for mining regionalisms in Argentinian Spanish from Twitter mixture density networks. arXiv preprint\narXiv:1708.04358.\nRahimi, A., T. Cohn, and T. Baldwin.\n2017. A neural model for user geolocation\nand lexical dialectology. arXiv preprint\narXiv:1704.04008.\nReal Academia Espa~ nola. 2013. Banco de\ndatos (CORPES XXI) [online]. Corpus\ndel espa~ nol del siglo XXI (CORPES).\nRoller, S., M. Speriosu, S. Rallapalli,\nB. Wing, and J. Baldridge. 2012. Su-\npervised text-based geolocation using lan-\nguage models on an adaptive grid. In\nProceedings of the 2012 Joint Conference\non Empirical Methods in Natural Lan-\nguage Processing and Computational Nat-\nural Language Learning , pages 1500{1510.\nAssociation for Computational Linguis-\ntics.\nShannon, C. E. 1948. A mathematical the-\nory of communication. The Bell system\ntechnical journal , 27(3):379{423.\nUeda, H. and A. Ruiz Tinoco. 2003. Var-\nilex, variaci\u0013 on l\u0013 exica del espa~ nol en el\nmundo: Proyecto internacional de inves-\ntigaci\u0013 on l\u0013 exica. In Pautas y pistas en\nel an\u0013 alisis del l\u0013 exico hispano (americano) .\nIberoamericana Vervuert, pages 141{278.\nVidal de Battini, B. E. 1964. El espa~ nol en la\nArgentina. Technical report, Argentina.\nZheng, X., J. Han, and A. Sun. 2018. A\nsurvey of location prediction on Twit-\nter. IEEE Transactions on Knowledge\nand Data Engineering , 30(9):1652{1671.\n62\nJuan Manuel P\u00e9rez, Dami\u00e1n E. Aleman, Santiago N. Kalinowski, Agust\u00edn Gravano Reflexive pronouns in Spanish Universal Dep endencies: from \nannotation to automatic morphosyntactic analysis  \nLos pronombres reflexivos en l as Universal Dependencies  en espa\u00f1ol: desde la \nanotaci\u00f3n hacia el an\u00e1lisis morfosint\u00e1ctico autom\u00e1tico  \nJasper Degraeuwe , Patrick Goethals  \nGhent University  (Belgium ) \n \nJasper.Degraeuwe@UGent.be  \nPatrick.Goethals@UGent.be  \n \nAbstract : In this follow -up article  of Degraeuwe and Goethals (2020) , we present the \nannotation scheme used to reannotate the 7298 potentially reflexive pro nouns included in \nthe Universal Dependencies Spanish AnCora v2.6 treebank, which resulted in significant \nmodifications  for the \u201cCase\u201d feature (100% changed) and dependency  relations  (87% \nchanged) . Next, we evaluate  the performance of spaCy v3.2.2 and Stanz a v1.3.0 (both \ntrained on AnCora v2.8, and thus based on our reannotations) on the AnCora v2.8 test  set, \nwhich yielded weighted F1 scores up to  0.88 and 0.98 for the \u201cCase\u201d  and \u201cReflex\u201d \nfeature s, respectively,  and up to 0.71 for the dependency relations . Finally, the error \nanalysis o f the spaCy results underline s the (generalisation) potential of the model , but \nalso reveal s some of the remaining issues in the automatic morphosyntactic analysis of \nreflexive pronouns in Spanish , such as  determining if expleti ve relations denote an \nimpersonal, passive or inherently reflexive use.  \nKeywords : reflexive pronouns, se, Universal Dependencies, morphosyntactic tagging \nand parsing . \nResumen : En est e art\u00edculo de seguimiento de Degraeuwe y Goethals (2020) , presentamos \nel esquema de anotaci\u00f3n utilizado para reanotar los 7298 pronombres potencialmente \nreflexivos incluidos en el Universal Dependencies Spanish AnCora v2.6 treebank , lo cual \nresult\u00f3 en un significativo n\u00famero de modificaciones  para la caracter\u00edstica ( feature ) de \n\u201cCase\u201d (el 100% cambiado) y las relaciones de dependencia (el 87% cambiado). A \ncontinuaci\u00f3n, evaluamos  el desempe\u00f1o de spaCy v3.2.2 y Stanza v1.3.0 (ambos \nentrenados en AnCora v2.8, y, por tanto, basados en nuestras reanotaciones) en el  set de \nprueba de An Cora v2.8, lo cual dio como resultado puntuaciones de F1 ponderado de \nhasta 0,8 8 y 0,98 para la s caracter\u00edstica s de \u201cCase\u201d y \u201cReflex\u201d , respectivamente,  y de \nhasta 0,71 para las relaciones de dependencia. Por \u00faltimo, el an\u00e1lisis de errores de los \nresultados  de spaCy subray a el potencial (generalizador) del modelo, pero tambi\u00e9n \ndesvel a algunos de los problemas pendientes en el an\u00e1lisis morfosint\u00e1ctico autom\u00e1tico de \nlos pronombres reflexivos en espa\u00f1ol, como por ejemplo determinar si las relaci ones de \ndependen cia expletiva s son de car\u00e1cter  impersonal, pasivo o inherentemente reflexivo.  \nPalabras clave : pronombres reflexivos, se, Universal Dependencies , etiquetado y an\u00e1lisis \ngramatical morfosint\u00e1ctico . \n \n \n1 Introduction  \nAs Natural Language Processing (NLP) tools \nsuch as spaCy ( spacy.io ) and Stanza \n(stanfordnlp.github.io/stanza/ ) are becoming more and more accessible (also for a non -expert \naudience, see e.g. Altinok (2021) and Vasiliev \n(2020) ), automatic morphosyntactic anal ysis \nhas been integrated into  a wide range of text-\nbased applications. By means of a simple \nprogramming script, for example, raw corpora \nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 63-72\nrecibido 31-03-2022 revisado 17-05-2022 aceptado 29-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-5\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalcan be transformed into intelligent resources \ncontaining morphosyntactic information . These \n\u201cenriched corpora\u201d can then be used as input for  \ncorpus query tools or  language learning \nenvironments , enabling  their users  to perform \nmuch more fine -grained queries.  \nTo train their (morpho syntactic ) taggers and \n(syntactic) parsers, NLP tools usually  make use \nof treebanks as reference data. One of t he most \nwell-known initiat ives concerned with the \nconstruction of such treebanks is the Universal \nDependencies (UD) project, established in \n2014.  In a nutshell, UD aims at developing \n\u201ccross -linguistically consistent treebank \nannotation for many languages, with the goal of \nfacilitat ing multilingual parser development, \ncross -lingual learning, and parsing research \nfrom a language typology perspective\u201d \n(https://universaldependencies.org/introduction, \nretrieved 22 February  2022; see also Nivre et al. \n(2016)) . Together with the growing nu mber of  \nlanguages included in the project (v2.9 contains \n217 treebanks in 122 languages), t he \nstandardised, cross -linguistically consistent \napproach  of UD has l ed to increasing usage of \nUD treebanks not only for the development of \nNLP tools, but also for r esearch purposes (see \nde Marneffe et al. (2021, p. 304) for an \noverview).  \nHowever, the UD initiative is also a \n\u201cconstantly improving effort\u201d (Mart\u00ednez Alonso \nand Zeman, 2016), implying that  the annotation \nguidelines are regularly updated and  fine-tuned \nover the successive releases of the treebanks. \nFurthermore , within UD there remain several \nannotation issues, which may be problematic \nfrom both a cross -linguistic and an intra -\nlinguistic perspective. In a previous article \n(Degraeuwe and Goethals, 2020 ), we a ddressed \none of those pending issues, namely the \nannotation of  the potentially reflexive pronouns \nme, te, nos, os and se (see also Markovi\u0107  and \nZeman, 2018) in the UD Spanish AnCora \ntreebank  (Mart\u00ednez Alonso and Zeman, 2016 ; \nTaul\u00e9, Mart\u00ed and Recasens , 2008 ). These are \nvery frequent items in Spanish, with se alone \noccurring in almost 30% of the sentences in the \nAnCora treebank  and bein g ranked eleventh in \nthe list of most common lemmas in CORPES \nXXI (Real Academia Espa\u00f1ola de la Lengua, \n2022). \nThe annotation proposal described in \nDegraeuwe and Goethals (2020)  was revised by \nthe UD contributor responsible for the AnCora \ntreebank (see sec tions 2.1.1  and 2.1.2  for the details of th e original and  revised annotation \nscheme s), after which all potentially reflexive \npronouns in the treebank were reannotated and \nthe resulting 7298 changes were pushed to the \nUD project (visible from v2.7 onwards).  In \n2021, both spaCy (with v3.0) and Stanza (with \nv1.2. 0) released thoroughly  updated versions  of \ntheir tools  trained on UD v2. 7 or higher  (thus \nincluding the reannotated  reflexive pronouns) . \nIn this follow -up contribution , we will carry out \nboth a quantit ative and qualitative analysis of \nhow well the tools perform on \nmorphosyntactically analysing Spanish \nreflexive pronouns , and we will identify the key  \nremaining issues  (see section 3). \n2 Literature overview  \n2.1 Reflexives in Spanish Universal \nDependencies  \nThe UD  framework provides three main \nannotation layers by which linguistic \nconstructions can be progressively defined and \ndifferentiated: a morphosyntactic Part -Of-\nSpeech (POS) tag (limited to a universal set of \nseventeen tags) , a syntactic dependency relation \nsuch as subject  or (in)direct object , and a \nfeature set containing additional lexical and \ngrammatical properties (e.g. number or person \nin the case of pronouns).  \n \n2.1.1 Annotation scheme as proposed in  \nDegraeuwe and Goethals (2020)  \nThe original proposal (see Tabl e 1) arose from \nthe notion that, as NLP tools become more \naccessible, more theoretical linguists will use \nthem and evaluate their linguistic accuracy and \ngranularity. Consequently , we not only focused \non improving annotation consistency in order to \nincreas e tagger/parser accuracy, but also took \ninto account the (cross -)linguistic analyses \nmade in non -computational linguistics ( Croft et \nal., 2017 ; Maldonado, 2008 ; Mendikoetxea, \n1999; Peregr\u00edn Otero, 1999).  \nFirst, the pronouns were disambiguated \naccording to their general reflexive character, \ndistinguishing between me veo (\u2018I see myself\u2019) \nand me ven  (\u2018they see me\u2019). In the latter group, \nthe dependency relation is defined as  \u201cobj\u201d or \n\u201ciobj\u201d ( me dieron algo , \u2018they gave me \nsomething\u2019).  \nSecondly, the reflexive use s were assigned \none of the dependency labels \u201cobj\u201d, \u201ciobj\u201d, \n\u201cexpl:impers\u201d , \u201cexpl:pass\u201d and \u201cexpl:pv\u201d. This \n64\nJasper Degraeuwe, Patrick Goethals   means that reflexive and non -reflexive \u201cobj\u201d \nand \u201ciobj\u201d have the same dependency label but \nare distinguished by the \u201cReflex\u201d  feature , which \nis absent  in the case of non -reflexives. \nReflexive \u201cobj\u201d and \u201ciobj\u201d are further \nsubdivided according to their genuine reflexive \nversus reciprocal use.  \nThirdly, the umbrella category \u201cexpl:pv\u201d \nconsists of three subgroups, namely \nconstructions with corresponding tran sitive \nverbs, constructions which show an alternation \nwith intransitive verbs, and constructions \nwithout corresponding (in)transitive verbs. The \nfirst group of \u201ctransitivity -based\u201d reflexive \nconstructions is then further subdivided by \nassigning different c ombinations of feature sets. \nThese feature sets overlap with other \u201cnon -\nexpl:pv\u201d constructions, showing their shared \ncharacteristics.  \nWith this annotation proposal, many \nannotation  inconsistencies  were resolved  (e.g. up to 30% and 60% of false positives of  \n\u201cexpl:pass\u201d and \u201ciobj\u201d, respectively) . \nFurthermore, the proposal  also provided a more \nfine-grained and informative categorisation , as \nthe previous taxonomy (AnCora \u2264 v2.6) did not \nallow distinguishing between , for example,  \npassive ( en este volumen se ofrecen textos \nsobre , \u2018in this volume texts are provided \nabout\u2019) and reflexive uses ( Mar\u00eda se ofrece \npara hacerse cargo del beb\u00e9 , \u2018Mar\u00eda offers \nherself to take c are of the baby\u2019) of the same \nverb, or between passive ( se incautaron las \narmas , \u2018the guns were seized\u2019) and inherently \nreflexive constructions ( la polic\u00eda se incauta de \nlas armas , \u2018the police seized the guns\u2019). In all \nthese cases, se was label led as \u201cobj\u201d , and no \ndifferences were to be found between the  \nfeature sets of the se instances, nor between the \nfeature sets of the ir verbal heads . \n \n Features  \n Pronoun  Verb  \nCase  Reflex  Voice  \nReflexive uses  \nexpl:pass  Acc Reflex  Pass (a) la noticia se public\u00f3  \nobj Acc Reflex  Act (b) Pedro se ve en el espejo  \nAcc Rcp Act (c) Pedro y Juan se vieron en la calle  \niobj Dat Reflex  Act (d) Pedro se quita la ropa  \nDat Rcp Act (e) Pedro y Juan se dieron la mano  \nexpl:impers  - Reflex  Act (f) se trabaja mucho  \nexpl:pv  With corresponding non -reflexive transitive verb  \nAcc Reflex  Pass (g) el fen\u00f3meno se manifiesta  \nAcc Reflex  Act (h) la gente se manifiesta  \nAcc Rcp Act (i) Pedro y Juan se ponen de acuerdo  \nDat Reflex  Act (j) Pedro se da cuenta  de que \u2026  \nDat Rcp Act [does not occur in Spanish]  \nCom  Reflex  Act (k) Pedro se llev\u00f3 el regalo  \nWith corresponding non -reflexive intransitive verb  \n- Reflex  Act (l) Pedro se muere  \nWithout corresponding non -reflexive verb  \nAcc Reflex  Act (m) Pedro se atreve a \u2026  \nNon-reflexive  uses \nobj Acc - - (n) me/te/nos/os ven  \niobj Dat - - (o) me/te/nos/os/se lo dijeron  \nTable 1: Overview  of the  annotation scheme for potentially reflexive pronouns in Spanish  as proposed \nin Degraeuwe and Goethals  (2020) . Translations: (a) \u2018the news was pub lished\u2019 , (b) \u2018Pedro sees himself \nin the mirror \u2019, (c) \u2018Pedro and Juan see each other on the street \u2019, (d) \u2018Pedro takes off his clothes \u2019, (e) \n\u2018Pedro and Juan shake hands\u2019 , (f) \u2018a lot of work is being done \u2019, (g) \u2018the phenomenon becomes clear \u2019, \n(h) \u2018people are demonstrating \u2019, (i) \u2018Pedro and Juan agree \u2019, (j) \u2018Pedro realises that \u2026 \u2019, (k) \u2018Pedro took \nthe present with him \u2019, (l) \u2018Pedro dies \u2019, (m) \u2018Pedro dares to \u2026 \u2019, (n) \u2018they see me/you/us/you \u2019, (o) \u2018they \ntold it to me/you/us/you/him/her \u2019. \n65\nReflexive pronouns in Spanish Universal Dependencies: from annotation to automatic morphosyntactic analysis 2.1.2 Annotation scheme  used for  AnCora  \n\u2265 v2.7  annotations  \nAs the annotation scheme presented in section \n2.1.1 included some drastic modifications, the \nproposed changes were first revised by the UD \ncontributor responsible for the AnCora treebank  \nbefore applying any reannotations . Based o n \nthis feedback, the reflexive \u2013 reciprocal \ndistinction  was dropped : since \u201cReflex\u201d is  \ncurrently  a Boolean feature  in all UD treebanks , \nthe addition of a \u201cRcp\u201d value would lower \ncross -linguistic  consistency.  Moreover, the \ndistinction also showed to be  a too subtle one to \nmake for machine learning methods  (tested with \ncustom models built on spaCy v3.2.2 and \nStanza v1.3.0 architectures).  \nSecondly, changing the \u201cVoice\u201d feature of \nthe verbal head of the potentially reflexive \npronoun was also discarded , again to give full \npriority to  cross -linguistic consistency. \nAlthough these characteristics do seem to be \nrecognisable for machine learning models \n(average weighted F1 test set score s of 0. 78 for \ncustom model trained with spaCy v3.2.2 \narchitecture and 0. 61 with St anza v1.3.0), t he \u201cVoice=Pass\u201d feature was primarily designed \nfor annotating verbal paradigms which \ndistinguish active from passive voice \nmorphologically, which is not the case i n \nSpanish . \nEven though  the modifications presented \nabove slightly decrease gra nularity, the \nannotation proposal  (see Table 2) remains very \ninformative (five different dependency labels, \naccusative/dative/comitative case distinction \nand reflexive/non -reflexive use distinction). \nMoreover, the new annotation scheme now \nadheres very str ictly to the UD principles and \nguidelines.  \nThe reannotation of the potentially reflexive \npronouns was implemented from AnCora v2.7 \nonwards. Since all \u201cCase\u201d values of pronouns \nwere labelled as \u201c{Acc, Dat}\u201d in the v2.6 \ntreebank, all of the 7298 pronouns pre sent in \nthe development, test and training sets received \na new \u201cCase\u201d value: 5933 instances were \nreannotated as \u201cAcc\u201d, 893 instances as \u201cDat\u201d \nand 472 instances as \u201cNA\u201d (non -applicable, for \nnon-cased instances).  The comitative case \n(\u201cCom\u201d) did not occur in the data.  \n \n Features   \nCase  Reflex  \nReflexive uses  \nexpl:pass  Acc Yes (a) la noticia se public\u00f3  \nobj Acc Yes (b) Pedro se ve en el espejo  \n(c) Pedro y Juan se vieron en la calle  \niobj Dat Yes (d) Pedro se quita la ropa  \n(e) Pedro y Juan se dieron la mano  \nexpl:impers  - Yes (f) se trabaja mucho  \nexpl:pv  With corresponding non -reflexive transitive verb  \nAcc Yes (g) el fen\u00f3meno se manifiesta  \n(h) la gente se manifiesta  \n(i) Pedro y Juan se ponen de acuerdo  \nDat Yes (j) Pedro se da cuenta  de que \u2026  \nCom  Yes (k) Pedro se llev\u00f3 el regalo  \nWith corresponding non -reflexive intransitive verb  \n- Yes (l) Pedro se muere  \nWithout corresponding non -reflexive verb  \nAcc Yes (m) Pedro se atreve a \u2026  \nNon-reflexive uses  \nobj Acc - (n) me/te/nos/os ven  \niobj Dat - (o) me/te/nos/os/se lo dijeron  \nTable  2: Overview of the annotation scheme for potentially reflexive pronouns in Spanish  used in  \nAnCora \u2265 v2.7 . \n \n66\nJasper Degraeuwe, Patrick Goethals   AnCora \u2265 v2.7  \nAncora v2.6  expl:impers  expl:pass  expl:pv  iobj obj Total  \n(Ancora v2.6)  \nexpl:pass  301 159 35 1 6 502 (6.9%) \niobj 1 17 253 152 43 466 (6.4%)  \nobj 54 2052  2927  628 665 6326  (86.7%)  \nother  0 3 0 1 0 4 (0,1%)  \nTotal  (AnCora \u2265 v2.7)  356 \n(4.9%)  2231 \n(30.6%)  3215  \n(44.1%)  782 \n(10.7%)  714 \n(9.8%)  7298 \nTable 3: Overview of the dependency relation changes in Spanish UD  AnCora \u2265 v2.7 compared to \nv2.6 (dev + test + train) . \nNext, the \u201cReflex\u201d value of 7108 pronouns \n(97%) remained unaltered: 6483 instances \nmaintained their reflexive annotation  (\u201cYes\u201d) \nand 625 instances their non -reflexive character  \n(\u201cNA\u201d). However, 49 pronouns  were  changed \nfrom reflexive to non -reflexive, while the value \nof 141 instances  was modified the other way \naround from non -reflexive to reflexive.  \nFinally, 6322 of the 729 8 potentially \nreflexive pronouns (almost 87%) received a \nnew dependency label . A detailed, quantitative \noverview of the corresponding changes is \npresented in  Table 3 (note that \u201cexpl:impers\u201d \nand \u201cexpl:pv\u201d do not occur in the v2.6 \ntreebank).  The statistics show a fundamental \nshift from \u201cobj\u201d as the predominant label to a \nmore dispersed distribution, with \u201cexpl:pv\u201d and \n\u201cexpl:pass\u201d being the most important labels. In \nother words,  the reannotation shows that \nreflexive pronouns usually express an expletive \nuse, more specifically an inherently reflexive \nuse (\u201cexpl:pv\u201d) or a passive one which blurs the \nsubject role (\u201cexpl:pass\u201d).  \n \n2.2 Reflexives in machine learning  \nTo our knowledge, to date no studies have been \nperformed which focus on the performance of \nmachine learning models at tagging potentially \nreflexive pronouns in Spanish based on UD \ntreeba nk data. On non -UD data, however, some \nexperiments have been carried out . In Aldama \nGarc\u00eda and Barbero Jim\u00e9nez  (2021 ), for \nexample, a machine learning approach is \nadopted to predict the dependency label of se in \na one -per-sentence setup, for which a custom  \n\u201cse corpus\u201d was compiled containing 2140 \nsentences from CORPES XXI (Real Academia \nEspa\u00f1ola de la Lengua, 202 2). The corpus was \nannotated according to a four -category \nannotation scheme, containing the \u201cse -mark\u201d \n(for cases of valency reduction, such as pass ive \nand impersonal constructions) , \u201cexpl\u201d  (for p ure pronominal predicates  or emphatic contexts ), \n\u201ciobj\u201d (for indirect objects) and \u201cobj\u201d (for direct \nobjects) labels.  Next, nine different machine \nlearning classifiers were applied to the test set \nof the corp us, with pre-trained language models \nbased on a transformers architecture  obtaining  \nthe best performance (macro F1 score of 0.7) . \nResults such as these indicate that, to a \ncertain extent, recent machine learning methods \nare able to successfully distinguish  different \nuses of the potentially reflexive pronoun se. To \nimplement them in real -life scenarios, \napproaches as in Aldama Garc\u00eda and Barbero \nJim\u00e9nez (2021), which are based on a language -\nspecific setup and require a self -compiled and \nannotated set of trai ning and test data, can be \nintegrated as a custom component for that \nspecific language in NLP tools such as spaCy  \nand Stanza . This way, the morphosyntactic \ninformation offered by the tool\u2019s tagger and \nparser can be complemented by the output of \nthe task -specific model.  \nHowever, the creation of such models is a \nvery time -consuming operation, especially for \nnon-computational linguists. Therefore, in \nsection 3, we will study the potential of the \ndefault taggers and parsers included in spaCy \nand Stanza  (which a re trained on UD data) , and \nanalyse if they would need to be complemented \nby a task -specific model and where exactly (i.e. \nfor which labels) issues arise . \n3 Automatic morphosyntactic analysis \nof potentially reflexives pronouns  \nFrom section 2.1.2 it can be co ncluded that, in \ntheory, any NLP tool trained on the reannotated \ntreebank as input data should be able to perform \na more fine -grained morphosyntactic analysis \nof potentially reflexive pronouns in Spanish.  To \nevaluate the validity of this claim, we apply th e \nlarge pretrained Spanish model of spaCy v3.2.2 \n(\u201ces_core_news_lg\u201d) and the default pretrained\n67\nReflexive pronouns in Spanish Universal Dependencies: from annotation to automatic morphosyntactic analysis  Case  Reflex  Dependency relation  \nAcc Dat NA Yes NA expl:impers  expl:pass  expl:pv  iobj obj \n#instances  452 47 42 504 37 30 171 254 36 50 \nspaCy  F1 0.94 0.62 0.57 0.99 0.88 0.51 0.75 0.8 0.6 0.37 \nmacro avg  0.71 0.93 0.6 \nweighted avg  0.88 0.98 0.71 \nStanza  F1 0.9 0.46 0 0.98 0.78 0.5 0.75 0.76 0.56 0.23 \nmacro avg  0.45 0.88 0.56 \nweighted avg  0.79 0.97 0.68 \nTable 4: Results (macro and weighted F1) of automa tic morphosyntactic analysis of potentially \nreflexive pronouns in  the AnCora v2.8 test set using spaCy v3.2.2 and Stanza v1.3.0.  \nSpanish model of Stanza v1.3.0, which are both \ntrained on the UD Spanish AnCora v2.8 \ntraining and development sets, to the \ncorresponding AnCora v2.8 test set. This test \ndata includes 668 potentially reflexive \npronouns, of which  127 instances are  clitic \nforms such as  se in la gente va a la calle a \nmanifestarse  (\u2018people go to the streets to \ndemonstrate\u2019). As spaCy does not include a \nmultiword tokeniser (which is required to split  \nwords with clitics into so -called  \u201csubword \ntokens\u201d and t hen analyse  these separate tokens \ninstead of the entire word form ), clitic forms \nwill be excluded from the evaluation in order to \nobtain comparable re sults. Table 4 presents a \ndetailed overview of the morphosyntactic \nanalysis , with F1 as the evaluation metric  and \nthe number of instances for each label included \nin the \u201c #instances \u201d row . \nFinally, some architectural characteristics of \nthe NLP tools should b e highlighted:  in the \nspaCy pipeline , the tagger and parser \ncomponents listen to the same word embedding \ncomponent but do not share any information \nbetween them, implying  that the features and \ndependency relations are predicted \nindependently of each other.  Stanza, however, \ndoes take into account information from the \ntagger when  training its dependency parser, \nwhich means that the Stanza dependency \nrelation predictions partially depend on the  \nfeature predictions . \nFor the \u201cCase\u201d and \u201cReflex\u201d features, \nsatisfy ing results are obtained, especially with \nspaCy  (weighted F1 scores of 0.8 8 for \u201cCase\u201d \nand 0.98 for \u201cReflex\u201d) . Stanza , however,  does \nnot seem to be able to recognise non -cased uses  \n(see \u201cNA\u201d column) , which correspond to \nreflexive pronouns with \u201cexpl:impers \u201d as the \ndependency label and to \u201cexpl:pv\u201d relations \nwith verbs for which a  corresponding non -reflexive intransitive counterpart exists (see \nalso Table 2). \nAs far as the dependency relations are \nconcerned, the automatic morphosyntactic \nanalysis achieves re latively good results  as \nwell, with weighted F1 scores of 0.71 for spaCy \nand 0.68 for Stanza . Compared to the top macro \nF1 score of 0.7 reached in Aldama Garc\u00eda and \nBarbero Jim\u00e9nez (2021) , both spaCy (0.6) and \nStanza (0.56) perform worse, although it shoul d \nbe observed that Aldama Garc\u00eda and Barbero \nJim\u00e9nez (2021)  distinguish only four instead of \nfive categories  and exclusively focus on se as \npotentially reflexive pronoun  (and not on me, \nte, nos and os). Next, the low scores for the \n\u201cexpl:impers\u201d and especi ally \u201cobj\u201d categor y \nhave to be highlighted.  A first possible \nexplanation for this lower performance could be \nthe limited number of training instances in the \ntraining and developments sets: 2 68 for \n\u201cexpl:impers\u201d (5.07%) and 444 for \u201cobj\u201d \n(8.4%).  To gain mor e in-depth insights into this \nmatter, and into the errors  made by NLP tools  \nin general , an additional analysis is performed \nbased on the contingency tables included in \nTable 5, which zero in on the performance of \nspaCy, the best -performing tool.  The error \nanalysis will also include a qualitative \ncomponent, with special attention to the \ngeneralisation potential of the tool (i.e. if it has \nlearnt to make predictions based on patterns, \nnot just to predict the most frequent label for \neach word  form).  \nFor the \u201cC ase\u201d errors, three main findings \ncan be extracted from the results:  \n1. Predicting the correct case of me, te, \nnos and os when they are used in \naccusative case is challenging (see (p) \nand (s) in Table 6 for some examples): \ntogether, these pronouns account for 29 \nof the 452 accusative instances, and 13  \n68\nJasper Degraeuwe, Patrick Goethals Case  \npredicted  \ncorrect  Acc Dat NA Total  \nAcc 435 14 3 452 \nDat 19 28 0 47 \nNA 23 1 18 42 \nTotal  477 43 21 541 \nReflex  \npredicted  \ncorrect  Yes NA Total  \nYes 496 8 504 \nNA 2 35 37 \nTotal  498 43 541 \nDependency rela tion \npredicted  \ncorrect  expl:impers  expl:pass  expl:pv  iobj obj other  Total  \nexpl:impers  14 9 5 1 1 0 30 \nexpl:pass  7 138 24 1 0 1 171 \nexpl:pv  4 39 201 7 3 0 254 \niobj 0 5 3 25 3 0 36 \nobj 0 7 16 14 13 0 50 \nTotal  25 198 249 47 21 1 541 \nTable 5: Results (contingency tables ) of automatic morphosyntactic analysis of potentially reflexive \npronouns in  the AnCora v2.8 test set using spaCy v3.2.2 . \nof those 29 cases received the wrong \n\u201cDat\u201d  prediction (which corresponds to \n13 of the 17 errors made for th e \naccus ative  label).  Importantly, 12 of \nthose instances had also received a \nwrong dependency label (namely \u201ciobj\u201d \ninstead of \u201cobj\u201d or \u201cexpl:pv\u201d) . \n2.Predicting the correct case of se when it\nis used in dative case also  entails\nchallenges  (see (q) in Table 6 for an\nexample) : se accounts for 24 of the 47\ndative instances, and 15 of those 24\ncases were labelled wrongly as\naccusative  (corresponding to 15 of the\n19 errors made for this label).\n3.As for the non -cased instances , the\nerrors  seem to indicate that the model\ndoes not just  na\u00efvely link labels to\nverbal heads , since  in sentences with\nirse/marcharse  (\u2018to leave\u2019), which\nfrequently occur in the training data and\nunder all circumstances  receive the\n\u201cNA\u201d label , 5 incorrect but also 4\ncorrect predictions were to be found.\nThis finding can be considered\nevidence that the model has developed\na kind of generalisation procedure ,although it thus results in the \nintroduction of some errors in the case \nof irse/marcharse . In this regard, it also \nappears that  the generalisation as such \nis not entirely successful  either, since  \nseveral of the \u201cCase\u201d errors correspond \nto reflexive pronoun \u2013 verbal head \ncombinations which (almost) do  not \noccur in the training (and development ) \ndata, as was the case with advertir . For \nthis verb,  which only occurs once as a \nverbal head (i.e. the verbal form on \nwhich the  potentially reflexive pronoun \nis syntactically dependent) in the \ntraining set , the test sentence (r) (see \nTable 6) was wrongly predicted as \n\u201cAcc\u201d , meaning that the model was not \nable to gen eralise, in this particular case \nat least, from similar examples with \nother verbal heads  (e.g. contratar  as in \nse contrata  a alguien  \u2018someone was \ngiven a contract \u2019, which occurs three  \ntimes in the training data).  \nNext, the (few) errors made for the \u201cRefle x\u201d \nfeature  (see (s) in Table 6) usually correspond \nto instances of me, te, nos and os for which the \n69\nReflexive pronouns in Spanish Universal Dependencies: from annotation to automatic morphosyntactic analysis model also wrongly predicted both the case \n(usually dative  instead of accusative case ) and \nthe dependency relation (usually \u201ciobj\u201d instead \nof \u201cexpl:pv\u201d or \u201c obj\u201d).  Especially the wrong \n\u201ciobj\u201d prediction provides a plausible \nexplanation for the error in the  \u201cReflex\u201d label, \nas in the training data  non-reflexive \u201ciobj\u201d \ninstances are twice as frequent as reflexive \n\u201ciobj\u201d instances.  \nThirdly , the error analysis of t he dependency \nrelation predictions led to again three  main \nfindings:  \n1. Errors in one of the \u201cexpl\u201d categories \nalmost always correspond to one of the \ntwo other \u201cexpl\u201d labels (14 of the 16 \nerrors in \u201cexpl:impers\u201d, 31 /33 in \n\u201cexpl:pass\u201d and 43 /53 in \u201cexpl:pv\u201d).  In \nother words, the model has no problem \nin identifying expletive uses of \npotentially reflexive pronouns, but \nassigning the right expletive \nsubcategory  seems to be a less \nstraightforward operation from a \nmachine learning point of view  (see \nsentences (q) an d (r) in Table 6). \n2. For \u201ciobj\u201d, 5 of the 11 errors are \n\u201cexpl:pass\u201d predictions . Looking at the \nsentences, it appears that the model is \nnot able to predict the \u201ciobj\u201d label for \nreflexive pronouns which co -occur with \nan explicit subject and direct object, as \nin the examples (t) and (u) in Table 6. \n3. Predicting the correct dependency label \nof me, te, nos and os when they are used as direct object also poses \nchallenges to spaCy\u2019s machine learning \nmodel  (see (p) and (s) in Table 6): \ntogether, these pronouns account  for 19 \nof the 50 \u201cobj\u201d instances, and 17 of \nthose 19 cases received a wrong \ndependency relation  (which \ncorresponds to 17 of the 37 errors made \nfor the \u201cobj\u201d label) . Moreover, all of \nthe 14 cases where an \u201c iobj\u201d instance \nwas predicted instead of  \u201cobj\u201d were  to \nbe found amongst those 17 errors, \nhighlighting the sometimes fuzzy \nboundary between me, te, nos and os \nacting as direct or indirect object.  \n \nFinally, as a general, overarching \nobservation it should be noted that the \ngeneralisation potential of the mode l, which \nwas already briefly addressed in the discussion \nof the \u201cCase\u201d errors, also comes to the fore with \npronoun \u2013 verbal head combinations which \nhave multiple possible uses.  A good case in \npoint is the combination with the verbal head \ntratar , which occu rs 111 times as \u201cexpl:impers\u201d \nand 2 times as \u201cexpl:pass\u201d in the training and \ndevelopment data. Despite the imbalanced \ndistribution in the training data, the test \nsentence containing los temas se tratar\u00e1n (\u2018the \ntopics will be treated\u2019) still got labelled \ncorrectly as \u201cexpl:pass\u201d, which hints at the fact \nthat the model has leveraged \u201cknowledge\u201d from \nother \u201cexpl:pass\u201d training examples to arrive at \nthis correct prediction. Still other evidence of\n \nSentence  Case  Reflex  Dependency relation  \ncorrect  predicted  correct predicted  correct  predicted  \n(p) [\u2026] nos trataron muy mal \n[\u2026] Acc Dat NA NA obj iobj \n(q) [\u2026] las carreteras catalanas se \ncobraron 16 vidas  [\u2026] Dat Acc Yes Yes expl:pv  expl:pass  \n(r) Si se hubiera a dvertido a la \ngente [\u2026]  NA Acc Yes Yes expl:impers  expl:pass  \n(s) [\u2026] no me enga\u00f1o a creer en \nla existencia de  [\u2026] Acc Dat Yes NA obj iobj \n(t) [\u2026] Beckenbauer se permite \nbromear [\u2026]  Dat Acc Yes Yes iobj expl:pass  \n(u) [\u2026] el affaire Cristo -Rey se \ntomaba un respiro  [\u2026] Dat Acc Yes Yes iobj expl:pass  \nTable 6: Selection of errors in automatic morphosyntactic analysis of potentially reflexive pronouns in \nthe AnCora v2.8 test set using spaCy v3.2.2 . Transl ations: (p) \u2018[\u2026] they treated us really badly  [\u2026]\u2019, \n(q) \u2018[\u2026] Catalan roads claimed 16 lives [\u2026] \u2019, (r) \u2018 If people had been warned [\u2026] \u2019, (s) \u2018 [\u2026] I don\u2019t \ndelude myself into believing in the existence of [\u2026] \u2019, (t) \u2018[\u2026] Beckenbauer affords himself to make \njokes  [\u2026]\u2019, (u) \u2018[\u2026] the Cristo -Rey affair took a breather  [\u2026]\u2019. \n70\nJasper Degraeuwe, Patrick Goethals the generalisation potential can be found in the \naccuracy rates the model obtain s for the 35 \npronoun \u2013 verbal head combinations which do \nnot occur at all in the training data : 73% for \n\u201cCase\u201d, 97% for \u201cReflex\u201d and 5 4% for the \ndependency relation s. \n4 Conclusion  \nIn this article, we built upon Degraeuwe an d \nGoethals (2020) , in which a proposal was \nformulated to reannotate the potentially \nreflexive pronouns ( me, te, nos, os and se) in \nthe Universal Dependencies Spanish AnCora \ntreebank. These items, and in particular se, \noccur very frequently i n Spanish, and have also \nreceived much attention in non-computational \nlinguistics ( Croft et al., 2017 ; Maldonado, \n2008 ; Mendikoetxea, 1999; Peregr\u00edn Otero, \n1999).  Taking into account that  treebanks are \nused as reference data to train  the models \noffered by  state-of-the-art NLP tools such as \nspaCy and Stanza, we aim to contribute to \nimproving  the NLP -driven morphosyntactic \nanalysis of potentially reflexive pronouns , and \nin doing so , also help creating higher -quality \n\u201cenriched  resources\u201d  which can b e used as \ninput for , among st other applications , corpus \nquery tools  and language learning \nenvironments.  \nWe presented  the slightly modified \nannotation scheme used to reannotate the \npotentially reflexive pronouns included in the \nAnCora v2.6 treebank (7298 items in total; \nchanges visi ble from v2.7 onwards), which \nresulted in label changes for all \u201cCase\u201d features, \n3% of the \u201cReflex\u201d features and 87% of the \ndependency relations.  The application of spaCy \nv3.2.2 and Stanza v1.3.0 (both trained on \nAnCora v2.8, and thus based on our \nreannota tions) to the AnCora v2.8 test set \nyielded promising results, hinting at the \npotential of using NLP -driven methods to \nperform fine -grained morphosyntactic analyses.  \nFinally, the error analysis on the spaCy \nresults revealed some of the remaining issues in \nthe automatic morphosyntactic analysis of \npotentially reflexive pronouns in Spanish (e.g. \ndetermining  the right subcategory  of expletive  \ndependency label s), but also underlined the \n(generalisation) potential  of the  underlying  \nmodel . \nAlthough more than satis factory \nperformance levels were achieved (weighted F1 \nup to 0.88 for \u201cCase\u201d, 0.98 for \u201cReflex\u201d and 0.71 for dependency relations), there is still \nroom for improvement, especially for the \nprediction of  dependency relations. Therefore, \nfuture work could cons ist in studying if a task-\nspecific model ( as in  Aldama Garc\u00eda and \nBarbero Jim\u00e9nez (2021) ) can complement the \ndefault taggers and parsers of NLP tools  in \norder  to push performance. Furthermore, it is \nworth considering to implement  rule-based \npredictions  for a fixed set of verb s which \nalways yield  the same label s when functioning \nas verbal head of a reflexive pronoun  (e.g. for \nirse and marcharse ), and to define rules which \ndetermine the feature values for a given \ndependency relation (e.g. if \u201cexpl:pv\u201d is \npredicted as the dependency relation then the \n\u201cReflex\u201d feature should always be \u201cYes\u201d) . In \nspaCy, for instance, such specific rules can be \neasily implemented thanks to the \u201cattribute \nruler\u201d component, which manages mappings \nand exceptions at token  level . \nAckno wledgements  \nThis research has been carried out as part of a \nPhD fellowship on the IVESS project (file \nnumber 11D3921N), funded by the Research \nFoundation \u2013 Flanders  (FWO) . \nReferenc es \nAltinok, D. 2021. Mastering spaCy: An end -to-\nend practical guide to imp lementing NLP \napplications using the Python ecosystem . \nPackt.  \nCroft, W., D. Nordquist, K. Looney, and M. \nRegan. 2017. Linguistic Typology meets \nUniversal Dependencies. In  Proceedings of \nthe 15th International Workshop on \nTreebanks and Linguistic Theories ( TLT15) , \npages 63 -75, Indiana University \n(Bloomington, Indiana) . \nde Marneffe, M -C., C.D. Man ning, J. Nivre and \nD. Zeman. 2021. Universal Dependencies.\nComputational Linguistics , 47(2): 255 -308.\nDegraeuwe, J., and P. Goethals. 2020. \nReflexive pronouns in Spa nish Universal \nDependencies. Procesamiento del Lenguaje \nNatural , 64: 77 -84. \nMaldonado, R. 2008. Spanish middle syntax: A \nusage -based proposal for gram mar teaching. \nIn S. De Knop and T. De Rycker (eds.) \nCognitive Approaches to Pedagogical \n71\nReflexive pronouns in Spanish Universal Dependencies: from annotation to automatic morphosyntactic analysis Grammar , 155 -196. Mouton De Gruyter , \nBerlin . \nMarkovi\u0107, S. and D. Zeman. 2018. Reflexives \nin Universal Dependencies.  In Proceedings \nof the 17th International Workshop on \nTreebanks and Linguistic Theories (TLT \n2018) , pages 131 -146, Oslo University \n(Oslo).  \nMart\u00ednez Alonso , H. and D. Zeman.  2016. \nUniversal Dependencies for the AnCora \ntreebanks . Procesamiento de l Lenguaje \nNatural , 57: 91-98. \nMendikoetxea, A. 1999 . Construcciones \ninacusativas y pasivas.  In Gram\u00e1tica \ndescriptiva de la lengua espa\u00f1ola , 2: 1575 -\n1629.  Espasa Calpe . \nNivre, J., M. -C. de Marneffe,  F. Ginter, Y. \nGoldberg, J. Haji\u010d, C. Manning, R. \nMcDonald, S. Petrov, S. Pyysalo, N. \nSilveira, R. Tsarfaty, and D. Zeman. 2016. \nUniversal dependencies v1: A multilingual \ntreebank collection.  In Proceedings of the \nTenth International Conference on \nLanguage  Resources and Evaluation \n(LREC'16) , pages 1659 -1666, European \nLanguage Resources Association (Portoro\u017e) . \nPeregr\u00edn Otero, C.  1999 . Pronombres reflexivos \ny rec\u00edprocos. In  Gram\u00e1tica descriptiva de la \nlengua espa\u00f1ola , 1: 1427 -1518. Espasa \nCalpe.  \nReal Academia  Espa\u00f1ola de la Lengua. 202 2. \nBanco de datos (CORPES XXI) [online]. \nCorpus del Espa\u00f1ol del Siglo XXI \n(CORPES) \nhttps://www.rae.es/recursos/banco -de-\ndatos/corpes -xxi. Accessed date: 2 2/02/2022 \nTaul\u00e9, M., M. A. Mart\u00ed, and M. Recasens. 2008. \nAnCora: Multilevel  annotated corpora for \nCatalan and Spanish. In Proceedings of the \nSixth International Conference on Language \nResources and Evaluation (LREC'08) , \nEuropean Language Resources Association \n(Marrakech) . \nVasiliev, Y. 2020. Natural Language \nProcessing with Python  and spaCy: A \nPractical Introduction.  No Starch Press.  \n \n \n \n72\nJasper Degraeuwe, Patrick Goethals Multi-label Text Classification for PublicProcurement in Spanish\nClasificaci\u00b4 on multi-etiqueta de textos de licitaciones p\u00b4 ublicas\nen espa\u02dc nol\nMar\u00b4 \u0131a Navas-Loro, Daniel Garijo, Oscar Corcho\nOntology Engineering Group, AI.nnovation Space, Universidad Polit\u00b4 ecnica de Madrid\nmnavas@fi.upm.es, daniel.garijo@upm.es, ocorcho@fi.upm.es\nAbstract: Public procurement accounts for a 14% of the annual budget of the\ndifferent governments of the European Union. In Europe, contracting processes are\nclassified using Common Procurement Vocabulary codes (CPVs), a taxonomy de-\nsigned to facilitate statistical reporting, search and the creation of alerts that can\nbe used by potential bidders. CPVs are commonly assigned manually by public\nemployees in charge of contracting processes. However, CPV classification is not a\ntrivial task, as there are more than 9,000 different CPV categories, which are often\nassigned following heterogeneous criteria. In this paper we have created a CPV\nclassifier that uses as an input the textual description of the contracting process,\nand assigns CPVs from the 45 top-level CPV categories. We work only with texts\nin Spanish, although our approach may be easily extended to other languages. Our\nresults improve the state of the art (10% F1-score improvement) and are available\nonline.\nKeywords: CPV, Multi-label Classification, Public Procurement, Hierarchical\nClassification.\nResumen: Las licitaciones p\u00b4 ublicas suponen el 14% del presupuesto anual de la\nUni\u00b4 on Europea. En Europa, los procesos de contrataci\u00b4 on se clasifican usando la\ntaxonom\u00b4 \u0131a Common Procurement Vocabulary (CPVs), dise\u02dc nada para facilitar la\ngeneraci\u00b4 on de estad\u00b4 \u0131sticas, las b\u00b4 usquedas y la creaci\u00b4 on de alertas que puedan uti-\nlizar los posibles licitadores. Los c\u00b4 odigos CPV suelen ser asignados manualmente\npor los empleados p\u00b4 ublicos encargados del proceso de contrataci\u00b4 on. Sin embargo, la\nclasificaci\u00b4 on de textos de acuerdo con estos c\u00b4 odigos no es trivial, pues existen m\u00b4 as de\n9000 CPVs y no siempre se siguen los mismos criterios para su asignaci\u00b4 on. En este\nart\u00b4 \u0131culo se propone un clasificador que utiliza como entrada la descripci\u00b4 on textual\ndel proceso de contrataci\u00b4 on, y produce c\u00b4 odigos de entre las 45 categor\u00b4 \u0131as de CPV\nm\u00b4 as generales de la jerarqu\u00b4 \u0131a. Trabajamos s\u00b4 olo con textos en espa\u02dc nol, aunque nue-\nstro enfoque puede extenderse f\u00b4 acilmente a otros idiomas. Los resultados obtenidos\nsuperan el estado del arte (10% de mejora en F1), y se encuentran disponibles online.\nPalabras clave: CPV, Clasificaci\u00b4 on Multi-etiqueta, Licitaciones P\u00b4 ublicas, Clasifi-\ncaci\u00b4 on Jer\u00b4 arquica.\n1 Introduction\nPublic authorities in the European Union\nspend around 14% of the yearly Gross Do-\nmestic Product (around 2 trillion euros) pur-\nchasing services, utilities and supplies.1Ac-\ncess to this data is crucial for enabling a sin-\ngle digital market in Europe, as well as for ac-\ncountability and transparency. Hence many\ngovernments provide this data in their open\n1https://ec.europa.eu/growth/\nsingle-market/public-procurement_endata portals as well as in data.europa.eu,\nand a number of platforms have been de-\nveloped to improve both the efficiency and\ntransparency in public procurement2(Soylu\net al., 2022).\nCommon Procurement Vocabulary codes\n(CPVs)3help classify public procurement\nprocesses in the European Union across dif-\n2https://opentender.eu/es/about/\nabout-opentender\n3https://simap.ted.europa.eu/web/simap/cpv\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 73-82\nrecibido 31-03-2022 revisado 18-05-2022 aceptado 20-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-6\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalferent languages. Thanks to CPVs, decision\nmakers can easily explore contracting pro-\ncesses across Europe, and companies from\ndifferent countries may use them to detect\nprocurement processes of interest, indepen-\ndently of the country of origin.\nEach public procurement process must be\nclassified with at least one CPV. However,\nmanual CPV classification presents three\nmain challenges. First, there are thousands\nof possible codes (more than 9000), some of\nthem with similar purposes, making it diffi-\ncult for those assigning or curating them to\ndecide which codes better suit a specific pro-\ncess. Second, countries with different official\nlanguages and countries with more than one\nofficial language, such as Spain or Belgium,\noften have offers in different languages (e.g.,\nCatalan, Basque, Castilian, etc.). Offices\nfrom different regions therefore follow differ-\nent classification guidelines. Third, CPVs\nare organized in a hierarchy, and thus an-\nnotated at different levels of granularity ac-\ncording to the annotator\u2019s or department\u2019s\ncriteria. For example, the CPV \u201cPharma-\nceutical products\u201d (3360000) shown in Fig-\nure 1 is often overgeneralized, instead of us-\ning more specific codes that shed more light\nin the type of purchase. This issue is in fact\nreflected in the European Union Policy Hand-\nbook, where the need of suggesting users to\nselect more specific CPV codes is stressed\n(European Commission, 2020).\nIn order to address these issues and ease\nthe assignment of CPV codes to procurement\nprocesses, this paper presents an approach\nto automatically assign high-level CPV codes\n(i.e., the 45 most general categories) to a pro-\ncurement process. In this paper, we assume\nthat we have the textual description of the\nprocess and that the text is in Spanish. Dif-\nferent methods have been tested to this end,\noutperforming the previous available results\nfor the Spanish language. We expect this re-\nsearch line will help public procurement prac-\ntitioners in assigning CPV codes in a more\nhomogeneous manner by providing sugges-\ntions that humans can use in their decision\nprocess.\nThe rest of the paper is organized as fol-\nlows. Section 2 introduces the CPV classifi-\ncation problem in detail, explaining the ratio-\nnale behind each part of the codes. Section 3\nsummarizes the related work done in the con-\ntext of multi-label text classification, as wellas existing approaches for CPV classification\nin Spanish. Section 4 describes how the cor-\npus used to train our classifier was developed,\nwhile in Section 5 we outline our approach.\nFinally, Section 6 details the results obtained\nby the different classification techniques used,\nand Section 7 concludes our work.\n2 Background\nThe Common Procurement Vocabulary\n(CPV) allows classifying public procurement\nprocesses with a homogeneous code that\nrepresents the need and main object of the\nrequested contract. Several CPV codes may\nbe used to describe a single offer. The format\nof these CPV codes follows a five-level tree\nstructure comprising the following digits:\n\u2022The first two digits identify the divisions\n(XX000000)\n\u2022The first three digits identify the groups\n(XXX00000)\n\u2022The first four digits identify the classes\n(XXXX0000)\n\u2022The first five digits identify the cate-\ngories (XXXXX000)\n\u2022The following three digits give a greater\ndegree of precision within each category\n(00000XXX)\nA ninth check digit serves to verify the\nprevious digits, and has no meaning by itself\n(00000000-Y).\nTherefore, the task of automatically clas-\nsifying CPVs increases in complexity the\nmore digits we aim to predict. The cur-\nrent official list of CPVs has 9454 possible\ncodes, grouped into 45 different divisions, 317\ngroups, 1321 classes and 3704 categories. In\nthis paper we focus in classifying CPVs at\nthe division level.\n3 Related Work\nWhile text classification has been widely ex-\nplored in the literature (Aggarwal and Zhai,\n2012; Minaee et al., 2021), multi-label clas-\nsification for the Spanish language has re-\nceived less attention so far. The main dif-\nference between the multi-label text classifi-\ncation case presented in this paper and other\npopular problems like sentiment analysis is\nthe amount of possible labels. Sentiment\nanalysis labels correspond to certain degrees\n74\nMaria Navas-Loro, Daniel Garijo, Oscar Corcho Figure 1: Excerpt of the tree-structure of CPV code 33600000, \u201cPharmaceutical products\u201d,\nextracted from http://www.cpv.enem.pl/en/33600000-6.\nof positive and negative emotions, or to a tax-\nonomy of emotions, whilst CPV labels may\ncontain up to thousands of possible options.\nIn order to target this kind of problems, a\nnew subtask has been defined inside multi-\nlabel text classification: extreme multi-label\ntext classification (XMTC) (Liu et al., 2017).\nXMTC addresses the problem of assign-\ning to a document its most relevant subset\nof class labels from an extremely large label\ncollection (Liu et al., 2017). The work by\nGargiulo et al. (2019) analyzes the impact\nof using different word embedding models in\nDeep Learning targeting extreme multi-label\nclassification. Their approach uses Convo-\nlutional Neural Networks (CNN) to classify\n27,775 hierarchical labels in the biomedical\ndomain. Similarly, Liu et al. (2017) com-\npared CNN to other approaches in XMTC,\nsuch as KNN-based approaches like SLEEC\n(Bhatia et al., 2015) or tree-based methods\nlike FastXML (Prabhu and Varma, 2014).\nFinally, Chang et al. (2020) proposed a\nscalable framework to fine-tune Deep Trans-\nformer models that performed well in differ-\nent XMTC datasets.\nRegarding specific previous work on CPV\nclassification, one of the main results was the\nmultilingual model built by Kaan G\u00a8 org\u00a8 un.4\nThis model categorizes public procurement\ndescriptions in multiple languages among 45\ndifferent division labels, with an F1 Score of\n0.68. Industrial approaches have also tar-\ngeted the CPV code classification problem,\nsuch as the solution developed by the data\nscience consultancy uData (Deloitte, 2020),\nusing a hierarchical nested approach consist-\ning of one model to predict the first two dig-\n4https://huggingface.co/MKaan/\nmultilingual-cpv-sector-classifierits of the CPV code, 50 models to predict the\nthird code (depending on the first model re-\nsults) and 250 additional models to predict\nthe fourth digit. Other approaches in the\nliterature include a deep learning sequence-\nprocessing regression algorithm (also contain-\ning several classifiers, considering different\naspects of CPVs) (Suta, 2019), or the ap-\nproach by Ahmia (2020), who used Linear\nSVMs in order to predict the first two dig-\nits of the CPV codes. SVMs were also used\nin Kayte and Schneider-Kamp (2019). Since\nthe only model available for reuse and evalua-\ntion for the Spanish language is the one from\nKaan G\u00a8 org\u00a8 un, we use it as a baseline for com-\nparison against our approach, making both\ntraining data and model results available to\nthe community.\n4 Creating a Spanish CPV\nCorpus\nWe created our training corpus with open\ndata from historical public procurement from\nthe Spanish Treasury\u2019s website (Hacienda5).\nWe decided to use data from 2019, in order to\navoid including later data that may have been\ninfluenced by public procurement related to\nCOVID19 pandemics. Procurement pro-\ncesses\u2019 metadata were processed from their\noriginal format (Atom Syndication Format6)\nusing different scripts available in our paper\nrepository (Navas-Loro, Garijo, and Corcho,\n2022).7Document pre-processing included\nthe following stages:\n1.Information extraction from all the\n5https://www.hacienda.gob.es/es-ES/\nGobiernoAbierto/Datos%20Abiertos/Paginas/\nLicitacionesContratante.aspx\n6https://www.w3.org/2005/Atom\n7https://github.com/oeg-upm/cpv-classifier\n75\nMulti-label Text Classification for Public Procurement in Spanish information contained in the Atom doc-\numents. We only retrieved the textual\ndescription of the offers and the differ-\nent CPV codes assigned to them. This\nis represented as a CSV file in order to\nease its further processing.\n2.Duplicate deletion and trim of the de-\nscriptions. Additionally, we only keep\ntexts in Spanish (to this aim we used\nfastText\u2019s language identification func-\ntionality8).\n3.Train/test dataset division , in order\nto make the dataset more manageable,\nwe split it into train and test sets (70/30)\nbefore uploading it to our public code\nrepository.\n4.In-code preprocessing. An additional\nset of scripts were used to remove rows\nwith no CPV code assigned and gener-\nalize CPV codes to the division level,\nwhich is the one we use in our experi-\nments.\nThe result of the first two steps are two\ncsv files, available in our repository. The\ncode used for all processing scripts can also\nbe found in the same location. Figure 2 shows\nthe distribution for each of the 45 division la-\nbels, which are clearly unbalanced. The most\nfrequent label (\u201845\u2019, that represents the divi-\nsion \u2018works\u2019) is present in 16128 instances of\nthe the training set, while label \u201876\u2019 is only\npresent in 13 instances.\n5 Approach\nWe addressed CPV classification in a hierar-\nchical manner: instead of creating a classifier\nfor nine thousand labels, we took advantage\nof the hierarchical structure of the CPVs and\ncreated a classifier for the 45 available divi-\nsions (first two digits). We believe this to\nbe a good first step due to the training data\navailable for most categories.\nThe only model openly available to per-\nform this task is the model from Kaan\nG\u00a8 org\u00a8 un (from now, MKaan) mentioned in\nthe Related Work section. This model also\ntargeted just the first two digits of the CPV\ncode, so we use it as a baseline to compare\nthe different approaches we have tested.\nIn order to perform multi-label classifica-\ntion, several approaches can be used. We can\n8https://fasttext.cc/docs/en/\nlanguage-identification.htmluse algorithms adapted to the task, such as\ndecision trees or random forests, or we can\nalso use binary classifiers like Na\u00a8 \u0131ve Bayes\nor SVM and then apply different strategies\nso that they serve for multi-label classifica-\ntion. Another option is to fine-tune exist-\ning transformers, as done in the approach by\nMKaan. We briefly present below the differ-\nent approaches we tested.\n5.1 Classical Techniques\nWe tried the following classifiers:\nNa\u00a8 \u0131ve Bayes (Minsky, 1961) has been\nwidely used for text classification ( \u02d9I\u00b8 sg\u00a8 uder-\nS \u00b8ahin, Zafer, and Adah, 2014), specially for\nsentiment analysis and SPAM classification.\nAlthough this algorithm relies on probability\nindependence, it works very well even when\nthis assumption is not met.\nSVM Support Vector Machines (SVM)\n(Boser, Guyon, and Vapnik, 1992) are lin-\near classifiers that define an hyperplane in\norder to discriminate among classes. SVM\nhave been frequently used for multiclass clas-\nsification tasks.\nSVM with RBF kernel Besides testing\nthe linear version of SVM, we also evaluated\nthe performance of an SVM with the Radial\nBasis Function as kernel, that is:\nrbf\u03b3=e\u2212\u03b3\u2225x\u2212x\u2032\u22252(1)\nwith parameter \u03b3\u22650.\nDecision Trees (Quinlan, 1986) are an in-\ntuitive way to classify instances. In our im-\nplementation we used the sklearn optimized\nversion of the CART algorithm.9\nRandom Forests (Breiman, 2001) are a\ntree-based ensemble approach to classifica-\ntion that overcomes most of the problems\nwith decision trees, such as high variance.\nDue to this robustness they have been fre-\nquently used for Extreme Multi-label Classi-\nfication (Siblini, Kuntz, and Meyer, 2018).\nK-Nearest Neighbours (K-NN) (Hand,\n2007) is widely used for multi-label classifica-\ntion (Zhang and Zhou, 2007). The idea be-\nhind K-NN is to check the K labeled instances\nthat are the closest to the new instance and\nclassify it with the most common label from\nthese neighbours.\n9https://scikit-learn.org/\nstable/modules/tree.html#\ntree-algorithms-id3-c4-5-c5-0-and-cart\n76\nMaria Navas-Loro, Daniel Garijo, Oscar Corcho 45\n50\n79\n71\n72\n34\n85\n92\n90\n33\n44\n55\n30\n48\n39\n60\n31\n09\n66\n80\n42\n98\n35\n38\n32\n18\n77\n15\n64\n63\n03\n37\n22\n70\n51\n24\n14\n73\n75\n43\n65\n16\n19\n41\n76\nDivision labels (first two digits)0200040006000800010000120001400016000Amount of instancestrain\ntestFigure 2: Bars (y axis) represent the amount of instances per division label (x axis). Blue\nbars represents the amount of labels in the training set, while red bars represent the number of\ninstances in the evaluation set.\nAdaBoost (Freund and Schapire, 1997) is\na meta-estimator that fits different versions\nof models using boosting (i.e., different\nversions of the training dataset). We used\nthe implementation defined in Hastie et al.\n(2009): AdaBoost-SAMME.\nFor all these approaches we used the Term\nFrequency - Inverse Document Frequency\n(TF-IDF) technique for vectorization, allow-\ning n-grams with n= 3. For those algo-\nrithms that do not support multi-label clas-\nsification, we decided to use the One-vs-the-\nrest (OvR) or One-vs-all strategy, frequently\nused for multiclass classification, where one\nbinary classifier per label is built in order to\ndecide if an instance should be classified with\nthat label or not.\n5.2 RoBERTa fine-tuned approach\nIn addition to the aforementioned classi-\ncal approaches, we also decided to fine-\ntune a transformed-based model for the\nSpanish language, namely RoBERTa-base-\nbne(Guti\u00b4 errez-Fandi\u02dc no et al., 2021), on a\ndataset derived from Spanish Public Procure-\nment documents from 2019.\nRoBERTa-base-bne is a transformer-based masked language model based on\nthe RoBERTa model and pre-trained us-\ning the largest Spanish corpus known to\ndate (570GB), compiled from the annual web\ncrawlings performed by the National Library\nof Spain (Biblioteca Nacional de Espa\u02dc na)\nfrom 2009 to 2019.10\nTable 1 summarizes the hyperparam-\neters used in the fine-tuning process,\nperformed using the HuggingFace trans-\nformers library. The whole training process\ncan be reproduced using the notebook\n\u2018fine-tuned-roberta-for-spanish-cpv-\ncodes.ipynb\u2019 in our code repository.\n6 Evaluation\nThis section describes how we evaluated\nthe results obtained with the different ap-\nproaches, and discusses them.\n6.1 Metrics\nWe use two sets of metrics in our evalua-\ntion. First, we use general metrics such as\nthe Area Under the ROC Curve (ROC AUC),\nF1-score and accuracy. Second, we use multi-\nlabel specific metrics, i.e., coverage error and\n10https://huggingface.co/PlanTL-GOB-ES/\nroberta-base-bne\n77\nMulti-label Text Classification for Public Procurement in Spanish Parameter Value\nlearning rate 2\u221710\u22125\ntrain batch size 8\neval batch size 8\nseed 42\noptimizer adam\nepochs 10\nTable 1: Summary of the hyperparameters\nused for training the RoBERTa fine-tuned\nmodel used in our analysis.\nLabel Ranking Average Precision. We briefly\ndescribe all these metrics below.\n6.1.1 General Metrics\nThe metrics used that are not specific to\nmulti-label classification are the following:\nArea Under the ROC Curve (AUC):\nmeasures the capability of a classifier to dis-\ntinguish between classes. The higher the\nAUC, the better the model can make the dis-\ntinction among classes.\nF1-score: harmonic mean between preci-\nsion and recall, widely adopted to monitor\nboth metrics at the same time.\nAccuracy: fraction of predictions that the\nmodel classified correctly.\n6.1.2 Coverage Error\nThe coverage error computes the average\nnumber of labels that have to be included in\nthe final prediction such that all true labels\nare predicted. That is, the average amount\nof ranked labels to take into account to miss\nno true label.\ncoverage (y,\u02c6f) =1\nnsns\u22121X\ni=0max\nj:yij=1rank ij(2)\nwith nlbeing the amount of labels, ns\nbeing the amount of samples, \u02c6f\u2208Rns\u00d7nl\nthe score associated with each label, y\u2208\n{0,1}ns\u00d7nlthe ground truth labels, rank ij=n\nk:\u02c6fik\u2265\u02c6fijo\n.\n6.1.3 Label Ranking Average\nPrecision\nLabel Ranking Average Precision (LRAP)\naverages over the ground truth labels as-\nsigned to each sample, ranking true labels\nhigher. This metric shows which ratio of\nhigher-ranked labels were true labels.LRAP (y,\u02c6f) =1\nnsns\u22121X\ni=01\n||yi||0X\nj:yij=1|Lij|\nrank ij\n(3)\nwith nlbeing the amount of labels,\nnsbeing the amount of samples, \u02c6f\u2208\nRns\u00d7nlthe score associated with each la-\nbel, y\u2208 {0,1}ns\u00d7nlthe ground truth la-\nbels, rank ij=n\nk:\u02c6fik\u2265\u02c6fijo\n,Lij=n\nk:yik= 1, \u02c6fik\u2265\u02c6fijo\n,|| \u00b7 || 0being the\n\u21130norm (which computes the amount of\nnonzero elements in a vector), and | \u00b7 |rep-\nresenting the cardinality of the set.\n6.2 Results and Discussion\nWe compare our results against the model by\nMKaan, since it is the only available model\nthat we have been able to find targeting the\nCPV code assignment problem in Spanish\n(besides other languages). Since no default\nthreshold or function is provided, we tested\ndifferent thresholds with the most common\nfunctions (softmax and sigmoid). Results are\nsummarized in Table 2 (using only 10% of\nthe dataset), and Table 3 (using the whole\ndataset).\nThe results clearly show that the\nRoBERTa fine-tuned model outperforms the\nrest of the approaches both when training\nusing just a fraction of the dataset and the\nfull dataset. The model by MKann shows\na good performance taking into account\nits multilingual nature (not specific for\nthe Spanish language). However, MKaan\nis matched and even outperformed by\nsome of the traditional algorithms in both\nexperiments.\nIn particular, classical approaches such\nas SVM, random forests and decision trees,\nproduce remarkably good results (0.69, 0.64\nand 0.63 F1 scores respectively on the full\ndataset). Given that these algorithms are\nusually less expensive to train, test and use\nthan transformer-based solutions, they are\nreasonable candidates for assisting in CPV\nclassification at a low cost. One possible ex-\nplanation for this good performance is that,\ndespite the presence of polysemous words\nthat can be problematic, both the hyper-\nplanes of SVM and the decisions of tree-\nbased methods allow to effectively discrimi-\nnate each label against all others (that is the\nstrategy usually used to adapt the algorithms\n78\nMaria Navas-Loro, Daniel Garijo, Oscar Corcho Approach ROC-AUC F1 Accuracy LRAP Cov. Error\nMultinomial NB 0.53 0.11 0.06 0.09 42.32\nSVM 0.66 0.47 0.33 0.36 30.19\nSVM (rbf) 0.66 0.47 0.33 0.36 30.19\nKNN 0.70 0.54 0.41 0.45 26.54\nDecision Tree 0.74 0.51 0.49 0.53 22.74\nRandom Forest 0.68 0.52 0.39 0.41 27.96\nAdaBoost 0.75 0.56 0.41 0.49 22.10\nRoBERTa fine-tuned (t=0.5) 0.84 0.74 0.68 0.73 14.13\nRoBERTa fine-tuned (t=0.6) 0.83 0.73 0.67 0.71 14.86\nRoBERTa fine-tuned (t=0.65) 0.82 0.73 0.67 0.70 15.41\nRoBERTa fine-tuned (t=0.7) 0.81 0.72 0.64 0.68 16.54\nMKaan (sigmoid, t=0.5) 0.80 0.13 0.0 0.07 17.38\nMKaan (sigmoid, t=0.7) 0.85 0.19 0.0 0.11 13.31\nMKaan (sigmoid, t=0.8) 0.86 0.24 0.0 0.15 12.21\nMKaan (sigmoid, t=0.9) 0.87 0.32 0.01 0.23 11.49\nMKaan (sigmoid, t=0.95) 0.87 0.42 0.06 0.34 11.64\nMKaan (softmax, t=0.01) 0.88 0.37 0.25 0.44 11.05\nMKaan (softmax, t=0.05) 0.86 0.55 0.43 0.59 12.48\nMKaan (softmax, t=0.1) 0.85 0.61 0.51 0.64 13.64\nMKaan (softmax, t=0.3) 0.81 0.65 0.61 0.66 16.63\nMKaan (softmax, t=0.5) 0.79 0.65 0.60 0.63 18.71\nTable 2: Results of the different approaches trained and tested on the 10% of the dataset (7243\ntraining samples, 3104 test samples).\nApproach ROC-AUC F1 Accuracy LRAP Cov. Error\nMultinomial NB 0.56 0.22 0.14 0.16 39.07\nSVM 0.78 0.69 0.58 0.62 18.89\nSVM (rbf) 0.78 0.69 0.58 0.62 18.89\nKNN 0.75 0.62 0.52 0.56 21.68\nDecision Tree 0.80 0.63 0.60 0.64 17.68\nRandom Forest 0.74 0.64 0.51 0.54 22.32\nAdaBoost 0.75 0.60 0.45 0.51 22.47\nRoBERTa fine-tuned (t=0.5) 0.89 0.79 0.74 0.80 10.32\nRoBERTa fine-tuned (t=0.6) 0.88 0.80 0.74 0.80 10.66\nRoBERTa fine-tuned (t=0.65) 0.88 0.79 0.74 0.79 10.95\nRoBERTa fine-tuned (t=0.7) 0.88 0.79 0.74 0.79 10.94\nMKaan (sigmoid, t=0.5) 0.81 0.13 0.0 0.07 17.19\nMKaan (sigmoid, t=0.7) 0.86 0.19 0.0 0.11 13.01\nMKaan (sigmoid, t=0.8) 0.87 0.24 0.0 0.15 11.91\nMKaan (sigmoid, t=0.9) 0.87 0.33 0.01 0.23 11.32\nMKaan (sigmoid, t=0.95) 0.87 0.42 0.06 0.34 11.50\nMKaan (softmax, t=0.01) 0.88 0.38 0.24 0.44 10.74\nMKaan (softmax, t=0.05) 0.86 0.55 0.43 0.59 12.25\nMKaan (softmax, t=0.1) 0.85 0.61 0.50 0.63 13.54\nMKaan (softmax, t=0.3) 0.81 0.66 0.61 0.66 16.46\nMKaan (softmax, t=0.5) 0.79 0.66 0.60 0.63 18.62\nTable 3: Results of the different approaches trained and tested on the whole dataset (72429\ntraining samples, 31042 test samples).\n79\nMulti-label Text Classification for Public Procurement in Spanish 45\n79\n50\n71\n72\n34\n85\n90\n92\n44\n33\n55\n30\n39\n48\n60\n31\n09\n80\n66\n42\n98\n35\n38\n77\n32\n64\n15\n18\n63\n03\n70\n22\n24\n37\n51\n14\n73\n75\n65\n43\n16\n19\n41\n760.00.20.40.60.81.0\nprecision\nrecall\nf1-score\nDivision labels (first two digits)Figure 3: Results of the RoBERTa fine-tuned model (t=0.5) per label. We preserve the order\npresented in Figure 2, from more represented labels (\u201845\u2019) to less represented labels (\u201876\u2019).\nto multiclass problems).\nA limitation of our approach is the lack\nof measures for balancing input data. Typi-\ncally, this would risk having our CPV classi-\nfier performing well only for the classes with\nmore representation. However, as shown in\nFigure 3, our CPV classifier shows an excel-\nlent performance for most categories, and has\nan acceptable performance for classes with\nless data available (except fpr extremely rare\ncategories \u201841\u2019 and \u201876\u2019). We suspect that in\naddition to the number of training instances,\nthe generality of the divisions and the over-\nlap between them also play a role in the dif-\nferences in performance. For example, divi-\nsions \u201842\u2019 and \u201843\u2019 represent \u201cIndustrial ma-\nchinery\u201d and \u201cMachinery for mining, quar-\nrying, construction equipment\u201d, respectively.\nWords similar to \u201cmachinery\u201d will therefore\nappear frequently in descriptions of both di-\nvisions, leading to false positives/negatives.\nIn Figure 3, we can in fact confirm that\nboth divisions have worse performance than\nthe immediate surrounding divisions having\na similar amount of instances.7 Conclusions and Future Work\nThis paper presents an approach to classify\nCPV code divisions for Spanish public pro-\ncurement descriptions. Our work evaluated\nclassical machine learning algorithms, show-\ning that SVM had an excellent performance,\nsurpassing the previous existing transformed-\nbased approach for the task. Additionally, we\nfine-tuned the RoBERTa transformed-based\nmodel trained on a corpus of the BNE (Span-\nish National Library), that outperformed all\nthe previous approaches. All data, data pro-\ncessing scripts and training notebooks have\nbeen made available through a public code\nrepository, Zenodo (Navas-Loro, Garijo, and\nCorcho, 2022)11and a Research Object12for\nthe sake of reproducibility. This material is\nalso planned to be used in the AI4Gov inter-\nnational master.13\nOur approach covers only CPV division\nclassification, and therefore it does not yet\naddress the CPV over-generalization problem\nwhen assigning CPVs to text (i.e., some codes\n11https://zenodo.org/record/6554843\n12https://w3id.org/dgarijo/ro/sepln2022\n13https://ai4gov-master.eu/\n80\nMaria Navas-Loro, Daniel Garijo, Oscar Corcho ...... ... ... ...\n......... ... ... ...First two digits\n45 labelsThird digit\nX\u226410 labels per \nclassifier(N-1)th digit\nX\u226410 labels per \nclassifierLast three digitsFigure 4: Hierarchical approach to the CPV\nclassification problem. The first classifier\nwould be responsible for categorizing the first\ntwo digits of the code, i.e., its division. The\nnext level would attempt to predict the next\ndigit based on the previous digits. For ex-\nample, if the first classifier determined that\na description corresponds to the labels \u201845\u2019\nand \u201848\u2019, that description would be passed to\nthe classifiers that determine the next digit\ntrained with examples of those two codes.\nare systematically not used in preference to\nmore generic codes, even though the specific\ncodes in disuse are much better suited to the\ntopic of the description). Our future work\nincludes designing a sequence of models that\nsuccessively classify the digits of CPVs, as de-\npicted in Figure 4, to be able to predict more\nspecific CPVs. Alternatively, we plan on as-\nsessing techniques based on sentence embed-\ndings against CPV descriptions, in order to\nsuggest more specific CPVs despite the lack\nof training instances. Designing more spe-\ncific classifiers will also require dealing with\nnoise in data, e.g., when annotators assign\ndifferent CPVs to the same contract descrip-\ntion or incorrect CPVs. We also plan to in-\ncrease the dataset, including contracting in-\nformation from several years and also retriev-\ning and making use of additional information\nfrom contracting processes. These include\nfeatures such as the cost, that could help in\nthe disambiguation of general words such as\n\u201cservice\u201d or \u201cwork\u201d, that can be used in very\ndifferent situations. Additionally, we will also\nenhance the preprocessing of the data in or-\nder to improve the quality in the dataset, awell-known problem in this kind of classifica-\ntion problem.\nOverall, our positive results are a step for-\nward towards the creation of a decision sup-\nport system to help in CPV classification, al-\nlowing a more transparent and efficient public\nprocurement in Spain and Europe.\nAknowledgments\nThis work has been supported by NextPro-\ncurement European Action (grant agreement\nINEA/CEF/ICT/A2020/2373713-Action\n2020-ES-IA-0255) and the Madrid Gov-\nernment (Comunidad de Madrid-Spain)\nunder the Multiannual Agreement with\nUniversidad Polit\u00b4 ecnica de Madrid in the\nline Support for R&D projects for Beatriz\nGalindo researchers, in the context of the V\nPRICIT (Regional Programme of Research\nand Technological Innovation). We also\nacknowledge the participation of Jennifer\nTabita for the preparation of the initial set of\nnotebooks, and the AI4Gov master students\nfrom the first cohort for their validation of\nthe approach. Source of the data: Ministerio\nde Hacienda.\nReferences\nAggarwal, C. C. and C. Zhai. 2012. A survey\nof text classification algorithms. In Min-\ning text data . Springer, pages 163\u2013222.\nAhmia, O. 2020. Assisted strategic monitor-\ning on call for tender databases using nat-\nural language processing, text mining and\ndeep learning . Ph.D. thesis, Universit\u00b4 e de\nBretagne Sud, 03.\nBhatia, K., H. Jain, P. Kar, M. Varma,\nand P. Jain. 2015. Sparse local em-\nbeddings for extreme multi-label classifi-\ncation. In C. Cortes, N. Lawrence, D. Lee,\nM. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Process-\ning Systems, volume 28. Curran Asso-\nciates, Inc.\nBoser, B. E., I. M. Guyon, and V. N. Vapnik.\n1992. A training algorithm for optimal\nmargin classifiers. In Proceedings of the\nfifth annual workshop on Computational\nlearning theory, pages 144\u2013152.\nBreiman, L. 2001. Random forests. Machine\nlearning , 45(1):5\u201332.\n81\nMulti-label Text Classification for Public Procurement in Spanish Chang, W.-C., H.-F. Yu, K. Zhong, Y. Yang,\nand I. S. Dhillon. 2020. Taming pre-\ntrained transformers for extreme multi-\nlabel text classification. In Proceedings\nof the 26th ACM SIGKDD International\nConference on Knowledge Discovery &\nData Mining, pages 3163\u20133171.\nDeloitte. 2020. Study on up-take of emerging\ntechnologies in public procurement. Tech-\nnical report, Deloitte.\nEuropean Commission. 2020. eForms : pol-\nicy implementation handbook . Publica-\ntions Office.\nFreund, Y. and R. E. Schapire. 1997. A\ndecision-theoretic generalization of on-line\nlearning and an application to boosting.\nJournal of computer and system sciences,\n55(1):119\u2013139.\nGargiulo, F., S. Silvestri, M. Ciampi, and\nG. De Pietro. 2019. Deep neural net-\nwork for hierarchical extreme multi-label\ntext classification. Applied Soft Comput-\ning, 79:125\u2013138.\nGuti\u00b4 errez-Fandi\u02dc no, A., J. Armengol-Estap\u00b4 e,\nM. P` amies, J. Llop-Palao, J. Silveira-\nOcampo, C. P. Carrino, A. Gonzalez-\nAgirre, C. Armentano-Oller, C. R. Pena-\ngos, and M. Villegas. 2021. Spanish lan-\nguage models. CoRR, abs/2107.07253.\nHand, D. J. 2007. Principles of data mining.\nDrug safety, 30(7):621\u2013622.\nHastie, T., S. Rosset, J. Zhu, and H. Zou.\n2009. Multi-class adaboost. Statistics and\nits Interface , 2(3):349\u2013360.\n\u02d9I\u00b8 sg\u00a8 uder-S \u00b8ahin, G. G., H. R. Zafer, and\nE. Adah. 2014. Polarity detection of\nturkish comments on technology compa-\nnies. In 2014 International Conference on\nAsian Language Processing (IALP) , pages\n136\u2013139. IEEE.\nKayte, S. and P. Schneider-Kamp. 2019.\nA mixed neural network and support\nvector machine model for tender cre-\nation in the european union ted database.\nInProceedings of the 11th International\nJoint Conference on Knowledge Discov-\nery, Knowledge Engineering and Knowl-\nedge Management, pages 139\u2013145. IN-\nSTICC, SciTePress.Liu, J., W.-C. Chang, Y. Wu, and Y. Yang.\n2017. Deep learning for extreme multi-\nlabel text classification. In Proceedings of\nthe 40th international ACM SIGIR con-\nference on research and development in in-\nformation retrieval , pages 115\u2013124.\nMinaee, S., N. Kalchbrenner, E. Cambria,\net al. 2021. Deep learning\u2013based text\nclassification: A comprehensive review.\nACM Comput. Surv. , 54(3), apr.\nMinsky, M. 1961. Steps toward artifi-\ncial intelligence. Proceedings of the IRE ,\n49(1):8\u201330.\nNavas-Loro, M., D. Garijo, and O. Corcho.\n2022. Code repository for multi-label text\nclassification for public procurement in\nspanish, May.\nPrabhu, Y. and M. Varma. 2014. Fastxml: A\nfast, accurate and stable tree-classifier for\nextreme multi-label learning. In Proceed-\nings of the 20th ACM SIGKDD interna-\ntional conference on Knowledge discovery\nand data mining , pages 263\u2013272.\nQuinlan, J. R. 1986. Induction of decision\ntrees. Machine learning , 1(1):81\u2013106.\nSiblini, W., P. Kuntz, and F. Meyer. 2018.\nCRAFTML, an efficient clustering-based\nrandom forest for extreme multi-label\nlearning. In J. Dy and A. Krause, editors,\nProceedings of the 35th International Con-\nference on Machine Learning , volume 80\nofProceedings of Machine Learning Re-\nsearch, pages 4664\u20134673. PMLR.\nSoylu, A., Corcho, B. Elves\u00e6ter, C. Badenes-\nOlmedo, F. Yedro-Mart\u00b4 \u0131nez, et al. 2022.\nData quality barriers for transparency in\npublic procurement. Information, 13(2).\nSuta, A. 2019. Multilabel text classifica-\ntion of public procurements using deep\nlearning intent detection. Master\u2019s thesis,\nKTH, Mathematical Statistics.\nZhang, M.-L. and Z.-H. Zhou. 2007. Ml-knn:\nA lazy learning approach to multi-label\nlearning. Pattern recognition, 40(7):2038\u2013\n2048.\n82\nMaria Navas-Loro, Daniel Garijo, Oscar Corcho Selecci\u00f3n de  colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un \nfiltro de interdisciplinariedad \nSelecting Spanish academic colloc ations  using a filter of interdisciplinarity \nEleonora Guzzi, Margarita Alonso Ramos \nUniversidade da Coru\u00f1a , CITIC  \neleonora.guzzi@udc.es, margarita.alonso@udc.gal \nResumen:  En este art\u00edculo se propone una metodolog\u00eda para compilar una lista de \ncolocaciones acad\u00e9micas con base nominal que se integra n en una herramienta l\u00e9xica  \n(Alonso -Ramos, Garc\u00eda-Salido y Garcia,  2017). Para ello, establecemos un filtro que mide \nla interdisciplinariedad de los nombres acad\u00e9micos a partir de los cuales se extraen las \ncolocaciones (Garc\u00eda-Salido , 2021), con el fin de  mantener los nombres frecuentes y bien \ndistribuidos en distintas disciplinas acad\u00e9micas , y descartar aquellos que se adscriben a la \nterminolog\u00eda  o que son m\u00e1s caracter\u00edsticos de la lengua general . Utilizamos tres criterios: \n(1)el\n IDF (Jones, 1972); (2) el an\u00e1lisis de la distribuci\u00f3n de colocaciones;  (3) el contraste\ncon lista s de vocabulario acad\u00e9mico ingl\u00e9s . Los resultados muestran que estos criterios  son\n\u00fa\ntiles para identificar los nombres  protot\u00edpicos del discurso acad\u00e9mico  y permite n filtrar  la\nlista de colocaciones acad\u00e9micas. No obstante , persiste e l problema de c\u00f3mo tratar l a\nd\nesambiguaci\u00f3n sem\u00e1ntica en relaci\u00f3n con las diferentes d isciplinas .\nP\nalabras clave:  discurso acad\u00e9mico, interdisciplinariedad , colocaciones acad\u00e9micas. \nAbstract: In this paper a methodology to compile  a list of noun- based academic \ncollocations that feed a lexical tool  (Author , 2017)  is proposed. To do so, a filter that \nmeasures the interdisciplinarity  of academic nouns  from which collocations are extracted \n(Garc\u00eda-Salido , 2021) is established . This filter is applied t o include  nouns  that are frequent \nand homogeneously distributed across different academic disciplines, and discard those \nascribed to terminology or are more prototypical of general language. T hree criteria were \nused: (1) the IDF  (Jones, 1972 ); (2) an  analysis of collocation distributions ; (3) a contrast \nwith vocabulary lists  of academic English. Results show that these criteria are useful for \nidentifying prototypical nouns  of academic discourse and allow for filtering the list of \nacademic collocations. However, the problem regarding how to deal with semantic disambiguation in different disciplines is still present. \nKeywords:  academic discourse, interdisciplinarity, academic collocations . \n1 Introducci\u00f3n  \nUno de los  principales objetivos  dentro del \n\u00e1mbito de las lenguas con fines acad\u00e9micos ha \nsido el de proporcionar listas de vocabulario \nacad\u00e9mico para ser utilizadas como recursos \npedag\u00f3gicos e integradas en la ense\u00f1anza de la \nescritura acad\u00e9mica. Este vocabulario incluye unidades y combi naciones l\u00e9xicas que son \nespec\u00edficas del g\u00e9nero acad\u00e9mico,  pero no son \nterminol\u00f3gicas.  A su vez, s e caracterizan por ser \nm\u00e1s frecuentes en el discurso acad\u00e9mico que en \nla lengua general o en otros g\u00e9neros . Siguiendo \na Tutin (2007a) podemos  definir el vocab ulario acad\u00e9mico como aquel vocabulario que hace \nreferencia a los procedimientos y actividades \ncient\u00edficas del discurso cient\u00edfico, esenciales en \nla argumentaci\u00f3n y en la estructuraci\u00f3n de los textos acad\u00e9micos (Dr ouin, 2007; Paquot y \nBestgen , 2009).  \nHasta el momento, se han propuesto varias \nlistas de unidades l\u00e9xicas acad\u00e9micas  \nespecialmente para el ingl\u00e9s y el franc\u00e9s: \nAcademic Vocabulary List (AVL, Gardner y \nDavies, 2013), Academic Word List  (AWL , \nCoxhead, 2000), Academic Keyword List  (AKL, \nPaquot , 2007, French Cross-disciplinary \nScientific Lexic on (Hatier  et al., 2016), Lexique \nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 83-94\nrecibido 31-03-2022 revisado 20-05-2022 aceptado 23-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-7\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalScientifique Transdisciplinaire  (LST, Drouin, \n200)7 , entre otras. Por otro lado, dentro de  las\nlistas de combinaciones l\u00e9xicas destacan la\nAcademic Collocation List  (ACL, Ackermann y\nChen , 2013) y  la Academic English  Collocation\nList (Lei y Liu, 2018), centradas espec\u00edficamente\nen las colocaciones, as\u00ed como  la lista de palabras\ny colocaciones acad\u00e9micas empleadas para la\nherramienta lexicogr\u00e1fica Collocaid\n(Frankenberg-Garcia et al., 2019) . En el \u00e1mbito\ndel espa\u00f1ol, se ha propuesto recientemente una\nlista de unidades l\u00e9xicas acad\u00e9micas, la Spanish\nAcademic Key Word List (SpAKW L, Garc\u00eda-\nSalido 202 1), que incluye 1.239 lemas  de\nnombres, adjetivos, verbos y adverbios . A partir\nde los nombres de esta lista, tambi\u00e9n se ha\nextra\u00eddo una primera versi\u00f3n de colocaciones\nacad\u00e9micas con base nominal que se integran en\nla Herramienta de Ayuda  a la Redacci\u00f3n de\nTextos Acad\u00e9micos (HARTA;  Alonso -Ramos,\nGarc\u00eda -Salido y Garcia,  2017; disponible en:\nhttp://www.dicesp.com:8083/).\nEn estos estudios se han aplicado varios \ncriterios estad\u00edsticos para identificar autom\u00e1ticamente el vocabulario interdisciplinar \nen el discurso acad\u00e9mico. Sin embargo, los \ncriterios pueden diferir entre las listas de \ndiferentes lenguas por los rasgos l\u00e9xicos de las mismas. Por ejemplo, c omo apuntan Cobb y \nHorst (2004) , existe un continuum entre el \nvocabulario acad\u00e9mico y no  acad\u00e9mico en \nlenguas  como el franc\u00e9s o el espa\u00f1ol  debido a  \nque, entre otras razones, el vocabulario greco -\nlatino  que es caracter\u00edstico de los textos \nacad\u00e9mico s, tambi\u00e9n se integra en dominios no \nacad\u00e9micos. Sin embargo,  esto no sucede en \ningl\u00e9s, donde los t\u00e9rminos greco -latino s tiene n \nmucha m\u00e1s presencia en el discurso acad\u00e9mico. \nEn el caso del espa\u00f1ol , adem\u00e1s, aunque una \npalabra se utilice en la lengua general, se podr\u00eda  \nincluir en las listas acad\u00e9micas si tambi\u00e9n es \nfrecuente en el discurso acad\u00e9mico, debido a \nque, por un lado,  los sentidos de las palabras \nacad\u00e9micas pueden diferir de  los utilizados en \nlos text os que no son acad\u00e9micos (Gilquin, \nGranger y Paquot, 2007), como, por ejemplo,  \ntrabajo (\u2018composici\u00f3n cient\u00edfica o literaria\u2019 \nfrente a trabajo como \u2018empleo \u2019, en la lengua general) y, por otro lado, porque dichas palabras \npueden formar parte de combinaciones  l\u00e9xicas \nque son m\u00e1s exclusivas de los textos acad\u00e9micos  \n(Garc\u00eda-Salido , 2021), como la colocaci\u00f3n  dato \ncuantitativo con la palabra dato, frente a dato \npersonal. El objetivo final a la hora de compilar \nlas listas de vocabulario acad\u00e9mico deber\u00eda ser  \ncrear un balance entre la especificidad del  \nvocabulario y su productividad. E sto es, se \ndeber\u00eda apuntar a la inclusi\u00f3n de unidades y \ncombinaciones l\u00e9xicas que son productivas para \nla redacci\u00f3n de textos acad\u00e9micos y que pueden \nestar en la intersecci\u00f3n, por ejemplo, entre la lengua general y la lengua acad\u00e9mica, pero que \nexcluyen lo que es propio de otros g\u00e9neros.  \nOtras dificultad es a la hora de recoger  este \nvocabulario argum entadas en Hyland y Tse \n(2007) son la falta de atenci\u00f3n a la posible \npolisemia y a la variedad disciplinar. La \npolisemia  implica que distintos sentidos sean \nutilizados en distintas disciplinas  acad\u00e9micas: \npor ejemplo, la palabra volumen puede significar \n\u2018capacidad\u2019 en F\u00edsica, \u2018ejemplar de libro\u2019  en \nBiblioteconom\u00eda o \u2018frecuencia ac\u00fastica\u2019  en \nIngenier\u00eda . Por otra parte, la variedad disciplinar \nse asocia a la posibilidad de que determinadas \nunidades y combinaciones l\u00e9xicas pueden ser \nacad\u00e9micas, p ero pueden utilizarse con m\u00e1s \nfrecuencia en algunas  disciplinas.  \nEn definitiva , no existe un consenso com\u00fan \nque apunte hacia una generalizaci\u00f3n de criterios sobre c\u00f3mo determinar la interdisciplinariedad  y \nobtener vocabulario caracter\u00edstico de textos acad\u00e9micos.  Como consecuencia, este trabajo se \npropone con un doble objetivo: por un lado, \nofrecer  una lista m\u00e1s refinada de nombres \nacad\u00e9micos en espa\u00f1ol y, por otro lado, utilizar \nla lista para filtrar las colocaciones acad\u00e9micas con base nominal que se integran en HARTA. \nPara alcanzar estos objetivos, seguimos  una \nmetodolog\u00eda  basada en el refinamiento de l os \nnombres de la SpAKWL , que implica el descarte \nde aquellos adscrit os a una disciplina espec\u00edfica \no que son significativamente m\u00e1s recurrentes en\nla lengua general.\nA continuaci\u00f3n, en la secci\u00f3n 2 presentamos \nel p\nroceso de extracci\u00f3n de la SpA KWL  y de las \ncolocaciones acad\u00e9micas  en espa\u00f1ol ; en la \nEleonora Guzzi, Margarita Alonso Ramos\n84secci\u00f3n 3 presentamos la metodolog\u00eda empleada \npara identificar los  nombres de la SpA KWL  que \nson interdisciplinares y filtrar las colocaciones que contienen dichos nombres ; en la secci\u00f3n 4 \nexponemos  los resultados obtenidos a partir de  \nlos distintos  an\u00e1lisis; y, en la secci\u00f3n 5, \ndiscutimos los resultados, seguidos  de las \nconclusiones finales.  \n2 Descripci\u00f3n de los datos:  SpAKW L y \ncolocaciones acad\u00e9micas \nEste estudio parte de  la lista de palabras \nacad\u00e9micas del espa\u00f1ol SpAKWL  (Garc\u00eda-\nSalido, 2021), extra\u00edda siguiendo criterios \nde especificidad y de distribuci\u00f3n a partir de un \ncorpus acad\u00e9mico, HARTA-Expertos (HE, Alonso -Ramos, Garc\u00eda-Salido y Garcia,  2017). \nHE contiene 413 art\u00edculos cient\u00edficos publicados, cuya mayor\u00eda proviene de la secci\u00f3n \nen espa\u00f1ol del corpus Spanish-English Research \nArticles Corpus (SERAC; P\u00e9rez -Llantada, \n2014) , y suma un total de 2.025.092 palabras. \nEst\u00e1 divido en 4 dominios principales (Artes y \nHumanidades, Biolog\u00eda y Medicina, Ciencias Sociales y Ciencias F\u00edsicas e Ingenier\u00eda) y 12  \nsubdominios , siguiendo la estructura del \nSERAC. El proceso de tokenizaci\u00f3n y \nlematizaci\u00f3n se llev\u00f3 a cabo mediante LinguaKit (Garcia y Gamallo, 2016)  y el de etiquetaci\u00f3n  \ncon FreeLing (Padr\u00f3 y Stanilovsky, 2012). Para \nanalizar sint\u00e1cticamente el corpus con \ndependencias universales (Nivre et al., 2016)  se \nutiliz\u00f3 UDPipe (Strak a, Hajic y Strakov\u00e1\n, 2016).  \nPara determinar la especificidad e identificar \nlas palabras espec\u00edficas del \u00e1mbito acad\u00e9mico \nfrente a un corpus de referencia, se emple\u00f3 el test \nestad\u00edstico log -likelihood a partir de las \nfrecuencias absolutas de HE y del corpus de \nreferencia, en este caso, la secci\u00f3n de ficci\u00f3n del \ncorpus  de narrativa LEXESP ( Sebasti\u00e1n -Gall\u00e9s \net al., 2000 ), de 5 millones de palabras.  Como \ncriterio de distribuci\u00f3n, se seleccionaron \naquellas palabras con ocurrencias en los 4 \ndominios y el 10% de palabras con una \ndistribuci\u00f3n m\u00e1s homog\u00e9nea en t\u00e9rminos de DP  \n(Deviation of Proportions, Gries , 2008).  La lista \nde palabras acad\u00e9micas resultante cuenta con 1.239 lemas que se corresponden con nombres, verbos, adverbios y adjetivos.  \nA partir de esta lista, se sel eccionaron los \nnombres (n=602) que  se emplearon como ba ses \npara extraer autom\u00e1ticamente una primera \nversi\u00f3n de colocaciones acad\u00e9micas, que est\u00e1n \nintegradas en HARTA.  Para este prop\u00f3sito, \ndefinimos las colocaciones, dentro del marco de \nla Lexicograf\u00eda Explicativa y Combinatoria \n(Mel\u2019\u010duk, 2012), como combinaciones l\u00e9xicas \ncon un significado composicional, que est\u00e1n \nformadas por una \u2018base\u2019, en este caso, un \nnombre,  y un \u2018colocativo\u2019, y cuyos elementos \ntienden a coocurrir , como, por ejemplo, alcanzar \nun objetivo .  \nPara analizar la interdisciplinariedad de las \nbases y de las colocaciones acad\u00e9micas en espa\u00f1ol, partimos de las colocaciones ya \nintegradas en HARTA y de un segundo grupo  \nm\u00e1s numeroso de colocaciones, que f ue extra\u00eddo  \na partir de una ampliaci\u00f3n del corpus  HE, \nHARTA-Expertos -Plus (HEP). Este corpus  \ncontiene 21.068.482 palabras procedentes de 3.870 art\u00edculos de investigaci\u00f3n: 19.043.390 \npalabras proceden del corpus acad\u00e9mico -\ncient\u00edfico Iberia  (Ahumada  et al., 2011) y \n2.025.092 palabras provienen del corpus H E. El \ncorpus HEP se divide en los mismos cuatro \ndominios principales que HE, a su vez divididos en subdominios . Para los nuevos art\u00edculos, se \naplic\u00f3 el mismo proceso de tokenizaci\u00f3n, lematizaci\u00f3n y an\u00e1lisis sint\u00e1ctico que se sigui\u00f3 \npara HE. Tras replicar los pasos seguidos para \nobtener  la primera versi\u00f3n de  colocaciones \nacad\u00e9micas,  en primer lugar, se extrajeron  \ncolocaciones de 5 relaciones de dependencias \nsint\u00e1cticas (N + N, V + Obj, Suj + V, N + Adj, \nN + Obl) . En segundo lugar , se realiz\u00f3 una \nextracci\u00f3n autom\u00e1tica, basada en medidas de asociaci\u00f3n estad\u00edsticas (log -likelihood, \nInformaci\u00f3n Mutua, entre otras), y en criterios de \nfrecuencia (\u22655 ocurrencias) ( Alonso -Ramos, \nGarc\u00eda -Salido y Garcia,  2017). En tercer lugar, \nun grupo de anotadores refin\u00f3 manualmente los \ncandidatos extra\u00eddo s autom\u00e1ticamente para \nobtener las colocaciones acad\u00e9micas,  siguiendo \ncriterios fraseol\u00f3gicos que se enmarcan dentro \nde la Teor\u00eda Sentido -Texto ( Mel\u2019\u010duk, 2012) . A \nSelecci\u00f3n de colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un  filtro de interdisciplinariedad\n85pesar del refinamiento manual  y de las medidas \nescogidas, tanto en la lista de nombres \nacad\u00e9micos como en las colocaciones \nseleccionadas se incluyen  casos m\u00e1s asociados a \nla terminolog\u00eda, como, por ejemplo, la palabra \ntejido usada con el sentido de Biolog\u00eda \u2018 cada uno \nde los  agregados de c\u00e9lulas de la misma \nnaturaleza\u2019 (DLE, s.m, def. 4), o la colocaci\u00f3n \ningresar paciente , que no son productiv as para \nlos varios dominios  acad\u00e9micos.  \n3 Metodolog\u00eda \nCon el fin de descartar  los nombres \nespecializados y  los que son m\u00e1s frecuentes en la \nlengua general o en otros g\u00e9neros, en primer lugar, aplicamos la medida  IDF (Inverse \nDocument Frequency, Jones 1972) a los 602 \nnombres  de la lista SpAKWL . Esta medida se \nbasa en el c\u00e1lculo de la proporci\u00f3n de documentos que contiene un determinado \nt\u00e9rmino y se utiliza frecuentemente en el campo \nde la Extracci\u00f3n de la Informaci\u00f3n p ara \nidentificar palabras clave, esto es, palabras \nespec\u00edficas de un conjunto de textos  que tienen \nun alto v alor de IDF . Tras calcular el IDF de los \nnombres, ordenamos los valores de mayor a menor para visualizar en la posici\u00f3n m\u00e1s alta los \ncandidatos m\u00e1s terminol\u00f3gicos. Calculamos la media de IDF (=0,659) y llevamos a cabo una \nrevisi\u00f3n exhaustiva de los nombres situados por \nencima y por debajo de la misma para seleccionar el punto de corte,  en este caso, la \npalabra potencia (IDF=0,900) . En este estudio, \nel IDF nos ayud \u00f3 a determinar precisamente las \npalabras que pueden ser descartadas (n=119) por \nser menos interdisciplinares y m\u00e1s \ncaracter\u00edstic as de algunos art\u00edculos, que se \ncorresponden con las ubicadas por encima del punto de corte (ver Anexo 1).  \nA partir de l resultado obtenido, seguimos un \nproceso de filtrado dividido en diferentes fases  \npara analizar con m\u00e1s profundidad los 119 nombres que se descartar\u00edan por el IDF . En la \nprimera fase (F1), un grupo de anotadores clasific\u00f3  los nombres  con IDF \u22650,900 en dos \ngrupos: los nombres que , tal y como indica esta \nmedida , se descartan por ser especializados o \nporque pertenecen a la lengua general (G1) y los nombres que deben seguir un proceso de revisi\u00f3n posterior (G2).  En la segunda fase (F2) , en la que \nse incluyen los  nombres clasificados en el  G2, \nidentificamos  las colocaciones extra\u00eddas que \ncontienen dichos  nombres como bases para \nanalizar su  distribuci\u00f3n y el n\u00famero de \ncolocativos con los que se combina . En este \nproceso , algunas bases se descart an, otras bases \nse reincluyen en la lista inicial y otro grupo de \nbases se seleccio na para revisar en  la siguiente \nfase. Por \u00faltimo, e n la tercera fase (F3) , \ncontrastamos los equivalente s de los nombres en \ncuatro listas de ingl\u00e9s acad\u00e9mico ( AVL, AWL, \nAKL, y la lista de palabras acad\u00e9micas de \nCollocaid). En  la Figura 1, exponemos  el \nproceso seguido:  \nFigura 1. Proceso para filtrar  los nombres  de la \nSpAKWL  \nEn las siguientes secciones, se explica en \ndetalle  el proceso que llevamos a cabo en las tres \nfases. \n3.1. C lasificaci\u00f3n  de las bases a partir del \nresultado del  IDF  \nUna vez identificados los 119 nombres que est\u00e1n \npor encima del punto de corte  del IDF, un grupo \nde anotadores conformado por tres ling\u00fcistas los \nclasific\u00f3 en dos grupos . En el G1 se incluyeron \nlos nombres que, tras observar los ejemplos en \ncontexto en el corpus HE y con la ayuda de los diccionarios para analizar los sentidos y las \nmarcas de especialidad, resultaron ser \nterminol\u00f3gicos o m\u00e1s asociados a un n\u00famero \nreducido de disciplinas. Tambi\u00e9n se incluyeron \naquellos nombres que presentan una frecuencia \nelevada en los corpus de lengua genera l, \nutilizando  herramientas de corpus para consultar \nsu frecuencia, como el corpus de esTenTen en IDF\nF1: Clasificaci\u00f3n de bases\nF2: An\u00e1lisis de colocaciones\nF3: Contraste con vocabulario \ningl\u00e9s\nEleonora Guzzi, Margarita Alonso Ramos\n86Sketch Engine (Kilgariff y Renau, 2013) . En el \nG2 se incluyeron los nombres que , en cambio, \nrequieren un an\u00e1lisis posterior  por no mostrar \nindicios claros sobre  su interdiscipli nariedad. En \nfunci\u00f3n de l descarte o mantenimiento, se les \nasignaron puntuaciones a los nombres . Por \nejemplo, encontramos casos como el de la \npalabra fracci\u00f3n , que se descarta (=0), e \nindicaci\u00f3n, que pasa a una siguiente fase  \n(=an\u00e1lisis) (Tabla 1) : \nFase 1 \nfracci\u00f3n  0 \nindicaci\u00f3n  an\u00e1lisis  \nTabla 1. Ejemplo de puntuaci\u00f3n asignada a nombres \na descartar  o analizar en la F1 . \n3.2.An\u00e1lisis de colocaciones  \nEn esta fase analizamos las bases clasificadas en \nel G2 (n=54) , con el fin de llevar a cabo  una \nvaloraci\u00f3n acerca del n\u00famero de colocativos, \nque se asocia  a la riqueza del vocabulario, as\u00ed \ncomo de la distribuci\u00f3n de las colocaciones que \nconforman,  relacionada con la \ninterdisciplinariedad . En este sentido, una base \nque se combina con varios colocativos y que \nforma colocaciones que est\u00e1n  bien distribuidas \nen los textos acad\u00e9micos deber\u00eda incluirse  en la \nlista de nombres acad\u00e9micos.  \nPara analizar la distribuci\u00f3n de las \ncolocaciones, se representa ron las frecuencias de \nlas colocaciones en cada dominio (AH, CS, CF, \nBM) en forma de porcentajes, se calcul\u00f3  la \ndesviaci\u00f3n est\u00e1ndar (DE), y se indic\u00f3  el n\u00famero \nde subdominios en los que aparece cada \ncolocaci\u00f3n. Los valores de la DE oscilaro n entre \n0,00 y 0,50: cuanto m\u00e1s bajo es el valor, m\u00e1s \nhomog\u00e9nea es la distribuci\u00f3n de la colocaci\u00f3n en los textos de los  cuatros dominios. En cuanto a \nlos criterios de an\u00e1lisis de las colocaciones que \ncontienen los nombres acad\u00e9micos, \nconsideramos los tres par\u00e1metros:  una \ncolocaci\u00f3n es interdisciplinar si presenta una DE \nentre 0,00 y 0,24, si se aproxima a un porcentaje \nde \u226520% en al menos tres dominios o bien en dos \ndominios, uno perteneciente a \u201cciencias duras\u201d \n(CF y BM) y otro a \u201cciencias blandas\u201d (CS y \nAH) y si aparece en  \u22653 subdominios. Por  lo tanto, si las colocaciones analizadas con un \nnombre del G2  est\u00e1n bien distribuidas, como, por \nejemplo, la colocaci\u00f3n alcanzar difusi\u00f3n (Tabla  \n2), el nombre, en este caso difusi\u00f3n,  recibe una \npuntuaci\u00f3n = 1 y se mantiene:  \nDE Dom.  Sub.  = \nalcanzar \ndifusi\u00f3n  0,27 AH 50%  \n5 Distri.  \nalta CS 12,5%  \nCF 12,5%  \nBM 25%  \nTabla 2. An\u00e1lisis de una colocaci\u00f3n con distribuci\u00f3n \nalta (homog\u00e9nea) . \nPor el contrario, si las colocaciones que se \nforman a partir de un nombre presentan una DE \nentre 0,36- 0,50,  aproximadamente, una \nfrecuencia de 0% en tres dominios o \u226590% en un \ndominio, y aparece solamente en uno o dos  \nsubdominios  (Tabla 3) , la base, en este caso, \nabundancia , con una puntuaci\u00f3n = 0, se descarta:  \nDE Dom.  Sub.  = \nabundancia \nmayor  0,47 BM 97%  2 Distr i. \nbaja CF 3%  \nTabla 3. An\u00e1lisis de una colocaci\u00f3n con distribuci\u00f3n \nbaja ( heterog\u00e9nea). \nLos nombres que pasan a la siguiente fase de \nrevisi\u00f3n (F3)  en la que se contrastan con las \nposibles equivalencias en las listas de ingl\u00e9s, y \nque reciben una puntuaci\u00f3n = an\u00e1lisis , son \naquellas bases que en este proceso presentan \ncolocaciones que oscilan  entre l os l\u00edmite s de una \ndistribuci\u00f3n homog\u00e9nea  y heterog\u00e9nea . Esto es, \nen este grupo se incluyen las bases cuyas \ncolocaciones presentan una DE entre  0,25 y \n0,35, aproximadamente, a parecen  en dos \ndominios de un \u00fanico grupo ( \u201cciencias \u201cduras\u201d /  \n\u201cciencias blandas\u201d), pero con porcentajes \nequilibrados , o en tres subdominios  de forma \ndesequilibrada  (Tabla 4):  \nDE Dom . Sub. = \nindicaci\u00f3n \nprecisa  0,33 CS 18%  \n3 Distr i. \nmedia  AH 9%  \nBM 73%  \nTabla 4. An\u00e1lisis de una colocaci\u00f3n  que requiere un \nan\u00e1lisis  posterior.  \nSelecci\u00f3n de colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un  filtro de interdisciplinariedad\n87Asimismo, pasan a la fase de revisi\u00f3n  3 un \nn\u00famero reducido de bases que no presentan \nning\u00fan colocativo productivo a nivel \nfraseol\u00f3gico  de entre todos los candidatos \nextra\u00eddos, como, por ejemplo, el nombre \ntipolog\u00eda . Ah\u00ed decidimos su mantenimiento o \ndescarte para formar parte de una nueva lista de \nnombres acad\u00e9micos.  \nEn la siguiente Tabla (5) mostramos las \npuntuaciones de tres bases que, tras el an\u00e1lisis \ncolocacional, reciben puntuaciones distintas: \nFase 1 Fase 2 \nabundancia  1 0 \nindicaci\u00f3n  1 an\u00e1lisis  \ndifusi\u00f3n  1 1 \nTabla 5. Ejemplos de puntuaciones asignadas a bases  \na descartar, analizar o  incluir  en la F2 . \n3.3. Contraste con listas de vocabulario de \ningl\u00e9s acad\u00e9mico  \nEn esta \u00faltima fase se analizaron  las bases que \npresentaron dudas tras pasar por el an\u00e1lisis \ncolocacional y aquellos nombres que no han \npodido analizarse en la fase 2 , debido a que no \npresentaron ning\u00fan colocativo.  \nSe seleccionaron cuatro listas de palabras \nacad\u00e9micas en ingl\u00e9s para el an\u00e1lisis: AKL, AVL, \nAWL y la lista de palabras acad\u00e9micas de \nCollocaid. A partir de los nombres seleccionados \nen la fase anterior, se realiz\u00f3 una comparativa \ncon las palabras acad\u00e9micas pertenecientes a las cuatro  listas para encontrar su equivalente en \ningl\u00e9s. En el proceso de b\u00fasqueda de los \nequivalentes, se consultaron dos  diccionarios, el \nOxford English Dictionary y el Cambridge \nDictionary, tanto la versi\u00f3n biling\u00fce espa\u00f1ol -\ningl\u00e9s como la monoling\u00fce , as\u00ed como el corpus \nparalelo Linguee  para observar ejemplos en \ncontexto. A la hora de buscar la equivalencia de cada nombre , se consideraron las diferentes \ntraducciones posibles debido a la presencia de \npolisemia. Se considera que una palabra coincide \ncon dos o m\u00e1s listas si en cada lista se presenta \nla traducci\u00f3n asociada a un mismo sentido, por \nejemplo,  la palabra cultura presenta su \ncorrespondencia en las cuatro  listas con el \nnombre en ingl\u00e9s culture. Sin embargo, se dan \notros casos en los que una palabra se traduce de distintas formas dependiendo del sentido que se \nadopte en cada una de ellas: por ejemplo, la \npalabra se\u00f1al se traduce en la AVL con el sentido \nasociado a signal, mientras que en la lista de \nCollocaid y en la AKL solamente se encuentra el \nequivalente de otro sentido, indication. En estos \ncasos, se considera el n\u00famero de listas coincidentes para cada sentido: s e\u00f1al puede \ncoincidir con una  lista (la AVL) o dos  listas \n(Collocaid  y AKL), en funci\u00f3n del sentido.  \nEn relaci\u00f3n con el criterio para filtrar la lista \nde nombres, se estableci\u00f3  un \u00edndice de \u22652, es \ndecir, si un nombre aparece en al menos dos de \nlas cuatro  listas se mantiene. Por el contrario, las \npalabras que coinciden \u00fanicamente con una  lista \no que no coinciden con ninguna finalmente sedescartan.  Fijamos  este \u00edndice debido a que la\nAWL  aplica criterios m\u00e1s restringidos y no\nincluye  palabras que tambi\u00e9n pertenecen a la\nlengua general, lo que provoca un porcentaje de\ncorrespondencia bajo  porque en el  vocabulario\nacad\u00e9mico  espa\u00f1ol se recogen palabras\ncompartidas con la lengua general.\nEn la  Tabla 6 , podemos observar los ejemplos  \ncon las respectivas puntuaciones de nombres que \nse analizan  en la fase 3 , que inclu yen tanto \nnombres que presentaron colocativos en la F2 \n(ej. indicaci\u00f3n  o especificaci\u00f3n ), como aquellos \nque no presentaron ning\u00fan colocativo (ej. \ntipolog\u00eda o almacenamiento ): \nFase1 Fase2 Fase3 = \nalmacenamiento  1 an\u00e1lisis  \n(no \ncoloc.)  0 1 \ntipolog\u00eda  1 an\u00e1lisis  \n(no \ncoloc.)  1 2 \nespecificaci\u00f3n  1 an\u00e1lisis  0 1 \nindicaci\u00f3n  1 an\u00e1lisis  1 2 \nTabla 6. Ejemplos de puntuaciones asignadas a \nnombres a descartar o incluir  en la F3 . \n4 Resultados \nLa lista  resultante se compone de 5 19 nombres \nacad\u00e9micos (Anexo 2) , en contraste con los 602 \nnombres iniciales. La clasificaci\u00f3n de los \nnombres en cada fase ha sido el resultado de las puntuaciones asignadas a cada uno de ellos. Una \nEleonora Guzzi, Margarita Alonso Ramos\n88  puntuaci\u00f3n final de 2 implic\u00f3 la reinclusi\u00f3n del \nnombre a la lista inicial y una puntuaci\u00f3n de 0 o \n1 conllev\u00f3 su descarte.  \nEn la primer a fase (F1), se descartaron 65 \nnombres,  con una puntuaci\u00f3n de 0,  entre los \ncuales encontramos ejemplos como emisi\u00f3n, \nfracci\u00f3n, tejido, geometr\u00eda, prevenci\u00f3n, infraestructura, etc. Por otra parte, se \nmantuvieron 54 nombres  que pasaron a una \nsiguiente revisi\u00f3n, con una puntuaci\u00f3n de 1, \ncomo  \u00e9nfasis, concordancia, fiabilidad, \npuntuaci\u00f3n, descenso, almacenamiento, \nprocesamiento, entre otros . \nEn cuanto a  la fase de revisi\u00f3n de las \ncolocaciones (F2), 6 bases se descartaron, como \nespecificaci\u00f3n, asignaci\u00f3n o resto ; 29 bases \npasaron a la fase 3, como trayectoria, gr\u00e1fico, indicaci\u00f3n o premisa ; y 19 bases se reincluyeron \nen la lista inicial por cumplir con el criterio de n\u00famero y distribuci\u00f3n de las colocaciones, como \nsesgo, producti vidad, predominio o est\u00e1ndar .  \nPor \u00faltimo, en  la fase 3, de los nombres que \nse contrastaron con las listas de palabras \nacad\u00e9micas en ingl\u00e9s, se descartaron l 2 nombres, \nentre los cuales encontramos ejemplos como correcci\u00f3n, concordancia  y fiabilidad, ya que \nsus posibles equivalente s en ingl\u00e9s no aparec\u00edan  \nen ninguna lista de ingl\u00e9s y almacenamiento, \nformulaci\u00f3n, asignaci\u00f3n, acumulaci\u00f3n, \nregulaci\u00f3n, procesamiento, trayectoria, \nafirmaci\u00f3n y barrera  porque aparec\u00edan  \n\u00fanicamente en una lista. Por ejemplo, de la \npalabra regulaci\u00f3n \u00fanicamente encontramos un \nposible equivalente en la AVL como adjustment.  \nSin embargo, se reincluyeron 17 nombres : \ngr\u00e1fico, indicaci\u00f3n, reproducci\u00f3n y experto,  que \naparec\u00edan  en dos  listas;  heterogeneidad, \npremisa, variante, diferenciaci\u00f3n, bibliograf\u00eda,  \ntipolog\u00eda, desempe\u00f1o, instancia, supuesto y \nuni\u00f3n, que aparec\u00eda n en tres listas, y paradigma, \nv\u00ednculo y rol, cuyos equivalentes aparecieron en \nlas cuatro  listas. Por ejemplo, la palabra \ninstancia aparece como instance  en la AVL, en la \nAKL y en la lista de Collocaid .  \nA modo de resumen, en la Tabla 7, \nmostramos el n\u00famero de nombres descartados, analizados o reincluidos en cada fase:   F1 F2 F3 TOTAL  \nDescarte  65 6 12 83 \nAn\u00e1lisis  54 29 - 83 \nReinclusi\u00f3n  - 19 17 36 \nTOTAL   54 29 119 \nTabla 7. N\u00ba de  nombres clasificados en cada fase . \n5 Discusi\u00f3n  \nLa medida IDF ha permitido detectar las \npalabras clave de determinados documentos del corpus y, en consecuencia, aquellos nombres \nempleados m\u00e1s espec\u00edficamente en algunas de \nlas \u00e1reas cient\u00edficas en las que est\u00e1 dividido el \ncorpus HE. Cabe destacar que la decisi\u00f3n del \npunto de corte en la palabra potencia (IDF \n=0,900) ha implicado que un n\u00famero reducido \nde nombres, como software o regresi\u00f3n , no se \ndescartaran  a pesar de presentar  un valor alto de \nIDF (cerca de 0,800) y de ser especializados. Sin embargo, hemos optado por no establecer un \npunto de corte m\u00e1s bajo debido a que se habr\u00eda \ndescartado una gran parte de nombres \nacad\u00e9micos relevantes, como disciplina, discurso, s\u00edntesis, entre otros.  \nDe los dos conjuntos de nombres obtenidos \ncon el IDF,  contrastamos la dispersi\u00f3n de \nalgunas pal abras con un alto valor de IDF con \nalgunas palabras con valores m\u00e1s bajos en los \nart\u00edculos de cada subdominio. En efecto, \nobservamos que la  frecuencia y distribuci\u00f3n no \nes proporcionada en el corpus en el primer caso  \n(IDF alto) , pero s\u00ed en el segundo (IDF bajo) . En \nla Figura 1, podemos observar este \ncomportamiento en una  muestra de seis nombres \ncon valores altos de IDF  (IDF>1 ,06), es decir, \nmenos distribuidos en los textos, y en la F igura \n2, seis nombres con valor es muy bajos  \n(IDF<0.05) . Para facilitar su lectura, \npresentamos la distribuci\u00f3n de los nombres por \nsubdominio en lugar de por art\u00edculos:  \nSelecci\u00f3n de colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un  filtro de interdisciplinariedad\n89Figura 2. Seis nombres con IDF >1,05 . \nF\nigura 3. Seis nombres con IDF <0,05 . \nC\nomo se puede apreciar , los nombres e n la \nFigura 3 presentan una dispersi\u00f3n  m\u00e1s \nhomog\u00e9nea que en la Figura  2. Por ejemplo , el \nnombre episodio presenta un gran predomi nio en \nlos subdominios de F\u00edsica y Qu\u00edmica y \nMedicina, y no aparece en 3 de los 12 \nsubdominios, mientras que el nombre an\u00e1lisis  \nest\u00e1 bien distribuido, con ocurrencias  \nproporcionales en los 12 subdominios.  \nEn la primera fase de clasificaci\u00f3n d e los \nposibles nombres que se descartar\u00edan  por el IDF , \nla mayor\u00eda se pudo clasificar en funci\u00f3n de su \nespecificidad en determinadas disciplinas , \ngracias al an\u00e1lisis de los contextos en los que aparecen dichas palabras en el corpus, as\u00ed como \na la ayuda de los diccionarios,  como, por \nejemplo, los nombres geometr\u00eda o motor . Sin \nembargo, en l\u00ednea con la problem\u00e1tica expuesta en la secci\u00f3n 1 , se llev\u00f3 a cabo una revisi\u00f3n m\u00e1s \nexhaustiva  especialmente de aquell os nombres  \nque se encuentran  en un continuum entre la \nlengua general y la lengua acad\u00e9mica, p.ej. rol, \nas\u00ed como de los nombres polis\u00e9micos como \nbarrera, que presenta un s entido m\u00e1s metaf\u00f3rico \nde \u2018obst\u00e1culo\u2019 , y otro sentido  de \u2018valla [\u2026] u \notro obst\u00e1culo semejante con que cierra el paso \u2019 \n(DLE, f.s., def. 1), con el fin de identificar  los \nsentidos m\u00e1s productivos en el discurso \nacad\u00e9mico.  \nEn relaci\u00f3n con el an\u00e1lisis de las colocaciones  \n(F2), un n\u00famero muy reducido de nombres \nacad\u00e9micos presentaron \u00fanicamente un \ncolocativo . En estos casos, si la colocaci\u00f3n no \npresent \u00f3 un nivel medio -alto de distribuci\u00f3n, la \nbase se elimin \u00f3. Por ejemplo, con el nombre \nespecificaci\u00f3n , \u00fanicamente se identific\u00f3 el \ncolocativo cumplir , y la colocaci\u00f3n present\u00f3 una \ndistribuci\u00f3n heterog\u00e9nea, con un 93% de ocurrencias en Ciencias F\u00edsicas ( ejemplo 1 ): \n(1)\u201cSe dise\u00f1\u00f3 la estructura de pavimentocon agregados de La Calera, por cumplir\ncon todas las especificaciones\u201d.\nP\nor otra parte, la gran mayor\u00eda  de nombres \nacad\u00e9micos presentaron \u22652 colocativo s, por lo \nque fue necesario un an\u00e1lisis  m\u00e1s detallado  de la \ndistribuci\u00f3n de cada colocaci\u00f3n para definir  una \nmedia. Por ejemplo, con la base productividad, se identificaron los colocativos aumento, alta y \nmayor, que tienen una distribuci\u00f3n medio-alta, \ncomo productividad alta, que presenta un 40% \nde ocurrencias en Ciencias F\u00edsicas, un 40% en Biolog\u00eda y Medicina y un 20% en Artes y \nHumanidades, una DE de 0,19, y aparece en 3 \nsubdominios.  Los casos que conllevaron m\u00e1s \ndudas se corresponden con aquellas bases que presentan 2 -3 colocati vos y una distribuci\u00f3n \nmedia de colocaciones. Por ejemplo, con la base \nindicaci\u00f3n se identificaron los colocativos clara \ny precisa : la colocaci\u00f3n indicaci\u00f3n precisa \npresenta una distribuci\u00f3n homog\u00e9nea, con una \nDE de 0,3 2, un porcentaje de aparici\u00f3n de un 9% \nen Artes y Humanidades, un 1 9% en Ciencias \nSociales y un 72% en Biolog\u00eda y Medicina, y una aparici\u00f3n en 4 subdominios; sin embargo, la \ncolocaci\u00f3n indicaci\u00f3n clara \u00fanicamente aparece 050100150200250300350400450\npremisa especialista sustrato\nd\u00e9ficit episodio exclusi\u00f3n\n05001000150020002500\nan\u00e1lisis resumen trabajo tipo forma resultado\nEleonora Guzzi, Margarita Alonso Ramos\n90  en los dominios de \u201cciencias blandas\u201d , con un \n22% de ocurrencias en Artes y Humanidades y \nun 78% en Ciencias Sociales, una DE de 0,37 y \nuna aparici\u00f3n en 3 subdominios. Debido a que \n\u00fanicamente se presentan dos colocaciones y la \nmedia de su distribuci\u00f3n no proporciona indi cios \ndefinitivos  sobre su inclusi\u00f3n  o descarte como \nbase acad\u00e9mica, en estos casos se contrasta el \nnombre  con las listas de vocabulario acad\u00e9mico \nen ingl\u00e9s. Cabe destacar que,  en esta fase, \ntambi\u00e9n  se identificaron 16 nombres que no \npresentaron ning\u00fan colocativo  y, por lo tanto , no \npudieron analizarse y  pasaron a la fase 3. Como \nhemos mencionado en la secci\u00f3n 1, a pesar de \nque el objetivo principal  del presente trabajo sea  \nfiltrar la lista de  colocaciones acad\u00e9micas, los \nnombres sin colocativos se incluyen  en este \nan\u00e1lisis con el  prop\u00f3sito de obtener tambi\u00e9n una \nlista completa y m\u00e1s refinada de nombres \nacad\u00e9micos del espa\u00f1ol.  \nEl an\u00e1lisis de las colocaciones tambi\u00e9n ha \nofrecido indicios sobre el contraste de uso de las colocaciones en la lengua general y la lengua \nacad\u00e9mica: la col ocaci\u00f3n jugar un rol  no \npresenta una frecuencia alta ni una buena \ndistribuci\u00f3n en el discurso acad\u00e9mico, pues su \nuso es m\u00e1s extendido en la lengua general, \nmientras que desempe\u00f1ar y ejercer un rol \npresentan una distribuci\u00f3n m\u00e1s homog\u00e9nea entre los dominios y una frecuencia ligeramente \nmayor en el discurso acad\u00e9mico. \nA su vez, hemos podido observar  casos en los \nque un nom bre puede combinarse con \ncolocativos y conformar colocaciones que est\u00e1n \ndistribuidas  de forma m\u00e1s homog\u00e9nea que con \notros colocativos: por ejemplo, con la base \npuntuaci\u00f3n, identificamos la colocaci\u00f3n otorgar \nuna puntuaci\u00f3n, que presenta una buena \ndistribuci\u00f3n, con un 14% de apariciones en AH, \nun 57% en CS , y un 39% en CF,  y con una  DE \nde 0,24;  contrariamente, la colocaci\u00f3n \npuntuaci\u00f3n m\u00ednima , presenta un distribuci\u00f3n \nheterog\u00e9nea, con un 14% de ocurrencias en CS y  \nun 86% en CF, una DE de 0,41 y con presencia \n\u00fanicamente en  2 subdo minios. Estos casos \nindican que la base debe ser incluida en la list a, \nya que es un  nombre utilizado frecuentemente en \ndistintos  textos acad\u00e9micos, pero  constituye colocaciones que se utilizan con m\u00e1s frecuencia en algunas disciplinas que  en otras, \nprobablemente debido a la variedad disciplinar.   \nEn definitiva, hemos observado que los \nnombres que han tenido que pasar por distintas \nfases de an\u00e1lisis se corresponden especialmente \ncon los nombres polis\u00e9micos, que poseen al \nmenos dos sentidos distintos (ej. barrera ) y los \nnombres que tambi\u00e9n se utilizan frecuentemente en la lengua general y, por lo tanto, no \nevidencian su especificidad en el discurso \nacad\u00e9mico, como los nombres uni\u00f3n o rol. \n6 Conclusiones  \nEn este art\u00edculo se ha presentado una \nmetodolog\u00eda para proponer una lista de nombres \ndel discurso acad\u00e9mico en espa\u00f1ol  a partir de la \nlista SpAKWL  (Garc\u00eda-Salido 2021) , aplicando \ncriterios que identifiquen mejor  la \ninterdisciplinariedad. Aunque el objetivo \nprincipal es obtener una lista de colocaciones \nacad\u00e9micas que se integrar\u00e1 en HARTA, como \nobjetivo secundario, hemos obtenido una lista de \nnombres acad\u00e9micos m\u00e1s protot\u00edpicos del \ndiscurso acad\u00e9mico. Partiendo de la medida de \nIDF que es  com\u00fanmente utilizada para \nidentificar de forma autom\u00e1tica los nombres m\u00e1s \nasociados a la terminolog\u00eda, hemos a plicado  \ndiferentes an\u00e1lisis para valorar su efectividad y corroborar que los nombres identificados  pueden \ndescartarse. \nLos resultados han demostrado que con esta \nmetodolog\u00eda es posible identificar la \ninterdisciplinariedad y establecer la lista de nombres acad\u00e9micos junto con una lista de \ncolocaciones que puedan ser integradas en una herramienta que ayude  a redactar textos \nacad\u00e9micos (HARTA) . Espec\u00edficamente, se ha \nobtenido un criterio para eliminar la terminolog\u00eda e identificar el vocabulario que \npuede ser utilizado independientemente de la \ndisciplina y que ayuda  a describir actividades y \nprocesos acad\u00e9mico -cient\u00edficos y a estructurar  la \nargumentaci\u00f3n. No obstante, los resultados \nsiguen remarcando  la necesidad de desambiguar \nlos sentidos de las palabras y la posibilidad de \nque, aunque  las bases sean interdisciplinares en \nSelecci\u00f3n de colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un  filtro de interdisciplinariedad\n91el discurso acad\u00e9mico, las colocaciones pueden \nutilizarse en unas disciplinas m\u00e1s que en otras. \nEl presente estudio forma parte de un \nproyecto de investigaci\u00f3n m\u00e1s amplio, en el cual, \na partir de este trabajo,  planteamos integrar la \nnueva versi\u00f3n de la  SpAKWL  en HARTA de \nmanera que, a partir de un texto, la herramienta \npueda detectar las palabras acad\u00e9micas y sugerir  \nlas colocaciones correspondientes, en l\u00ednea con \nlo que proponen herramientas como LEAD  \n(Paquot 2012) o Collocaid  (Frankenberg-Garci a \net al. 2019) para la escritura acad\u00e9mico -\ncient\u00edfica en ingl\u00e9s .  \nAgradecimientos \nEste estudio ha sido posible gracias a la financiaci\u00f3n del Ministerio de Ciencia e \nInnovaci\u00f3n (PID2019- 109683GB -C21) ; del \nCentro de Investigaci\u00f3n de Galicia \"CITIC\", financiado por la Xunta de Galicia y la Uni\u00f3n \nEuropea (FEDER GALICIA 2014 -2020), con la \nayuda ED431G 2019/01; y del Programa de \nAxudas \u00e1 Etapa predoutoral da Xunta de Galicia, FSE Galicia 2014 -2020. \nBibliograf\u00eda  \nAckermann, K.  y Chen, Y.H. 2013. D eveloping \nthe Academic Collocation List (ACL): A \ncorpus -driven and expert -judged approach. \nJournal of English for Academic Purposes , \n12(4) : 235\u2013247.  \nAhumada, I., Zamorano, J. P., Garc\u00eda, E. D. R. y \nLara, I. A. 2011 . Design and development of  \nIberia: a corpus of scientific \nSpanish.  Corpora, 6 (2): 145-158.  \nAlonso -Ramos, M., Garc\u00eda-Salido, M. y Garcia, \nM.2017. Exploiting a Corpus to Compile a\nLexical Resource for Academic Writing:\nSpanish Lexical Combinations . En I. Kosem ,\nJ.Kallas, C. Tiberius, S. Krek, M. Jakub\u00ed\u010dek\ny V. Baisa (Eds.),  Proceedings of eLex 2017 \nconference, p\u00e1ginas 571- 586. Leiden, the \nNetherlands.  \nCambridge Dictionary. Consultado el 27 de \nmarzo de 2022 \nen: https://dictionary.cambridge.org/us/dicti\nonary/.   Cobb, T., y Horst, M. 2004. Is there room for a n \nacademic word list in French?.  En P. \nBogaards y B. Laufer (E ds.), Vocabulary in a \nSecond Language. Selection, acquisition, and \ntesting , p\u00e1ginas 13-38,  John Benjamins  \n(Amsterdam/Philadelphia).  \nCoxhead, A. 2000. A new academic word list. \nTESOL Quarterly, 3 4(2): 213\u2013238. \nDrouin, P. 2007. Identification automatique du \nlexique scientifique transdisciplinaire. Revue fran\u00e7aise de linguistique appliqu\u00e9e , 7(2): 45-\n64.  \nFrankenberg-Garcia, A., Lew, R., Roberts, J. C., \nRees, G. P., y Sharma, N. 2019. Developing \na writing assistant to help EAP writers with \ncollocations in real time. ReCALL, 31(1): 23-\n39. \nGarc\u00eda -Salido, M. 2021. Compiling an \nAcademic Vocabulary List of Spanish. \nDisponible en:\nhttps://doi.org/10.13140/RG.2.2.27681.3312\n3. \nGarcia, M. y Gamallo, P. 2016. Yet another suite \nof multilingual NLP tools. En J. P. Leal J. L. SierraRodr\u00edguez et al. (Eds.), Languages, \nAppl ications and Technologies. \nCommunications in Computer and Information Science , p\u00e1ginas 65\u2013 75, \nSpringer ( Cham ). \nGardner, D., y Davies, M. 2013. A new academic \nvocabulary list. Applied Linguistics , 35(3): \n305\u2013 327.  \nGilquin, G., Granger, S., y Paquot, M. 2007 . \nLearner corpora: The missing link in EAP \npedagogy.  Journal of English for Academic \nPurposes, 6(4): 319-335.  \nGries, S. T. 2008. Dispersions and adjusted \nfrequencies in corpora.  International Journal \nof Corpus Linguistics , 13(4): 403\u2013437.  \nHatier, S., Augustyn, M., Tran, T. T. H., Yan, R., \nTutin, A., y Jacques, M. P. 2016. French \ncross-disciplinary scientific lexicon: \nextraction and linguistic analysis. \nEn Proceedings of EURALEX, p\u00e1ginas 355-\n366, Ivane Javakhishvili Tbilisi State \nUniversity (Tbilsi). \nEleonora Guzzi, Margarita Alonso Ramos\n92Hyland, K. y Tse, P. 2007 . Is there an \u201cacademic \nvocabulary\u201d?. TESOL quarterly , 41(2): 235-\n253. \nHyland, K. 2008 . As can be seen: Lexical \nbundles and disciplinary variation. English \nfor Specific Purposes, 27(1) : 4\u201321. \nJones, K. S. 1972 . A statistical interpretation of \nterm specificity and its application in \nretrieval.  Journal of documentation, 28(1): \n11-21.\nKilgarriff, A. y Renau, I. 2013. esTenTen , a vast \nweb corpus of Peninsular and American \nSpanish. Procedia -Social and Behavioral \nSciences, 95: 12-19.  \nKilgarriff, A., Baisa, V., Bu\u0161ta, J., Jakub \u00ed\u010dek, \nM., Kov \u00e1\u0159, V., Michelfeit, J. y  Suchomel, V. \n2014. The Sketch Engine: t en years on. \nLexicography , 1(1):  7\u201336. \nLei, L., y Liu, D. 2018. The academic English \ncollocation list: A corpus -driven \nstudy.  International Journal of Corpus \nLinguistics , 23(2): 216-243.  \nLINGUEE . Consultado el 28 de marzo de 2022 \nen: http://www.linguee.es. \nMel\u2019\u010duk, I. 2012. Phraseology in the language, \nin the dictionary, and in the \ncomputer.  Yearbook of phraseology , 3(1): \n31-56.\nNivre, J., Marneffe, M.-C. D., Ginter, F., \nGoldberg, Y., Manning, C. D., Mcdonald, R., \nPetrov, S., Pyysalo, S., Silveira, N., Tsarfaty, \nR. y Zeman, D. 2016. Universal\nDependencies v1: A Multilingual Treebank\nCollection. En Proceedings of the 10th\nInternati onal Conference on Language\nResources and Evaluation  (LREC 2016),\np\u00e1ginas  1659\u2013 1666, European Language\nResources Association (ELRA).\nOxford English Dictionary. Consultado el 27 de \nmarzo de 2022 en: y https://www.oed.com/ . \nPadr\u00f3, L. y Stanilovsky, E. 2012. Freeling 3.0: \nTowards wider multilinguality. En N. Calzolari et al., (Eds.), Proceedings of the 8th \nInternational Conference on Language \nResources and Evaluation  (LREC2012), \np\u00e1ginas 2473\u2013 2479,  European Language \nResources Association (ELRA).  Paquot, M. 2007. Towards a productivel y-\noriented academic word list. En J. Walinski, \nK. Kredens, y S. Gozdz Roszkowski (Eds.),\nPractical Applications in L anguage and\nComputers 2005, p\u00e1ginas 127\u2013140 . Peter\nLang (Frankfurt am main).\nPaq\nuot, M., y Bestgen, Y. 2009 . Distinctive \nwords in academic writing: A comparison of \nthree statistical tests for keyword extraction. \nLanguage and Computers , 68(1): 247 269. \nPaquot, M. 2012.  The LEAD dictionary -cum-\nwriting aid: An integrated dictionary and \ncorpus tool . En S. Granger  y M. Paquot \n(Eds.), Eletronic lexicography, p\u00e1ginas  161- \n186, Oxford University Press (Oxford). \nReal Academia Espa\u00f1ola:  Diccionario de la \nlengua espa\u00f1ola, 23.\u00aa ed., (versi\u00f3n 23.5 en \nl\u00ednea). Consultado el 25 de marzo de 2022 \nen: https://dle.rae.es.  \nSebasti\u00e1n -Gall\u00e9s, N., Mart\u00ed Anton\u00edn, M.A., \nCarreiras Vali\u00f1a, M. F., y Cuetos Vega, F. \n2000. LEXESP: L\u00e9xico informatizado del \nespa\u00f1ol . Barcelona: Edicions de la \nUniversitat de Barcelona.  \nStraka, M., Hajic, J. y Strakov\u00e1, J. 2016. Udpipe: \nTrainable pipeline for processing conll -u files \nperforming tokenization, morphological analysis, pos tagging and parsing. En \nProceedings of the 10th International Conference on Language Resources and \nEvaluation  (LREC 2016), p\u00e1ginas 1659\u2013\n1666,  European Language Resources \nAssociation (ELRA).  \nTutin, A.  2007a . Autour du l exique  et de la \nphras\u00e9ologie  des \u00e9crits scientifiques. Revue \nfran\u00e7aise de linguistique appliqu\u00e9e , 12(2), 5 -\n14. \nA Anexo 1: Nombres descartados a partir \ndel IDF  \npremisa, trayectoria, especialista, d\u00e9ficit, \nsustrato, concordancia, trabajador, exclusi\u00f3n, \nciudadano, episodio, fiabilidad, ejemplar, \nprioridad, geometr\u00eda, infraestructura, se\u00f1al, madurez, patolog\u00eda, creencia, amplitud, reproducci\u00f3n, paradi gma, procedencia, \nalmacenamiento, implantaci\u00f3n, complicaci\u00f3n, \ncorrecci\u00f3n, apertura, desplazamiento, motor, \ntipolog\u00eda, venta, puntuaci\u00f3n, consenso, \nSelecci\u00f3n de colocaciones acad\u00e9micas en espa\u00f1ol a trav\u00e9s de un  filtro de interdisciplinariedad\n93ejecuci\u00f3n, dispositivo, meta, asignaci\u00f3n, \nautonom\u00eda, lector, continuidad, carencia, compuesto, bibliograf\u00eda, supuesto, costo, normativa, emisi\u00f3n, correspondencia, variante, especificaci\u00f3n, instalaci\u00f3n, reto, experto, descenso, uni\u00f3n, formulaci\u00f3n, expectativa, \npuesta, mediana, motivaci\u00f3n, v\u00ednculo, \ninconveniente, departamento, productividad, desempe\u00f1o, incertidumbre, plataforma, tejido, tensi\u00f3n, experimento, diferenciaci\u00f3n, econom\u00eda, barrera, satisfacci\u00f3n, requerimiento, dosis, \nsesgo, acumulaci\u00f3n, entrevista, fundamento, \nregulaci\u00f3n, expansi\u00f3n, explotaci\u00f3n, transporte, abundancia, promoci\u00f3n, instancia, eliminaci\u00f3n, separaci\u00f3n, fracci\u00f3n, s\u00edntoma, heterogeneidad, \nefectividad, espectro, preferencia, difusi\u00f3n, presente, predominio, afirmaci\u00f3n, transferencia, \naceptaci\u00f3n, gr\u00e1fico, distinci\u00f3n, prevenci\u00f3n, \nsugerencia, dispersi\u00f3n, fragmento, \u00e9nfasis, varianza, canal, indicaci\u00f3 n, estadio, iniciativa, \nrol, procesamiento, transici\u00f3n, est\u00e1ndar, potencia. B Anexo 2: Nombres descartados tras las \ntres fases  \nemisi\u00f3n, fracci\u00f3n, tejido, geometr\u00eda, puesta, presente, prevenci\u00f3n, ciudadano, compuesto, sustrato, trabajador, infraestructura, transferencia, carencia, explotaci\u00f3n, estadio, transici\u00f3n, transporte, exclusi\u00f3n, varianza, ejemplar, venta, departamento, entrevista, \ns\u00edntoma , dosis, patolog\u00eda, episodio, creencia, \nmadurez, costo, abundancia, espectro, \nfragmento, iniciativa, econom\u00eda, aut onom\u00eda, \npotencia, prioridad, dispositivo, expectativa, incertidumbre, dispersi\u00f3n , preferencia, tensi\u00f3n , \ninconveniente, d\u00e9ficit, amplitud, desplazamiento, plataforma, requerimiento, expansi\u00f3n, separaci\u00f3n, implantaci\u00f3n, complicaci\u00f3n , concordancia, fiabilidad, \ntrayectoria,  resto, afirmaci\u00f3n, barrera, \ncorrecci\u00f3n, almacenamiento , formulaci\u00f3n , \nacumulaci\u00f3n, regulaci\u00f3n , procesamiento, \nespecificaci\u00f3n, efectividad, fundamento, distinci\u00f3n, asignaci\u00f3n.\n \nEleonora Guzzi, Margarita Alonso Ramos\n94Compilaci\u00f3n del corpus acad\u00e9mico de noveles en euskera \nHARTA eus y su explotaci\u00f3n para el estudio de la fraseolog\u00eda \nacad\u00e9mica \nCompilation of the academic corpus of novels in Basque HARTAeus and its \nexploitation for the study of academic phraseology  \nMar\u00eda Je s\u00fas Aranzabe,1 Antton Gurrutxaga,2 Igone Zabala1\n1 Centro HiTZ- Ixa, Universidad del Pa\u00eds Vasco (UPV/EHU)  \n2 Fundaci\u00f3n Elhuyar \n{maxux.aranzabe,igone.zabala}@ehu.eus, a.gurrutxaga@elhuyar.eus \nResu\nmen:  Se ha compilado un corpus acad\u00e9mico de noveles para el e uskera comparable \ncon el corpus HARTA- noveles para el espa\u00f1ol. A partir del corpus se ha extra\u00eddo una \nlista de vocabulario acad\u00e9mico para el euskera, y sendas listas de colocaciones y \nf\u00f3rmulas, a las que se les han asignado funciones discursivas. El objetivo \u00faltimo del \nproyecto HARTAes -vas, en el que se enmarca este trabajo , es dise\u00f1ar una herramienta de \nayuda a la escritura acad\u00e9mica para las dos lenguas centrada en las combinaciones l\u00e9xicas \nacad\u00e9micas, que integre diccionario y corpu s. \nPalabras clave:  corpus acad\u00e9mico, colocaciones, lexical bundles , funciones discursivas. \nAbstract: An academic corpus of novices was compiled for Basque, comparable to the \ncorpus HARTA- noveles for Spanish. A list of academic Basque vocabulary, collocations \nand formulas were e xtracted from the corpus, and then they were assigned discursive \nfunctions. The ultimate objective of the HARTAes -vas project , in which this work is \nframed , is to design a tool to help academic writing for Basque and Spanish focused on \nacademic lexical com binations, integrating lexicographic information and corpora . \nKeywords:  academic corpus, collocations, lexical bundles,  discursive functions.  \n1 Introducci\u00f3n \nLa introducci\u00f3n del euskera en \u00e1mbitos acad\u00e9micos , incluidos los de la educaci\u00f3n \nsuperior, que se produjo a principios de la d\u00e9cada de 1980, ha sido crucial para su \nrevitalizaci\u00f3n  (Zabala, 2019), ya que ha \ncontribuido de forma muy significativa al \naumento del n\u00famero de hablantes y al \ndesarrollo de los recursos expresivos necesarios \npara la comunicaci\u00f3n especializada. Sin \nembargo, \u00bfpodemos decir que el euskera ha \u201cconquistado\u201d los dominios acad\u00e9micos en el sentido de Laur\u00e9n et al. (2002)? Dicho en otras \npalabras, \u00bfel euskera ha desarrollado los \nrecursos expresivos necesarios para la \ncomunicaci\u00f3n acad\u00e9mica  en los diferentes \n\u00e1mbitos de especialidad? Laur\u00e9n et al. (2002) defienden que a esta pregunta se puede \nresponder de forma individual o de forma colectiva.   A nivel individual, los estudiantes \nuniversitarios adquieren los registros \nacad\u00e9micos necesarios p ara convertirse en \nmiembros de la comunidad de expertos de su \u00e1rea gracias a numerosas tareas en las que el \nlenguaje resulta crucial (Biber, 2006). Algunos autores defienden que los textos acad\u00e9micos se \nelaboran siguiendo esquemas discursivos \nprefabricados que utilizan unidades \nfraseol\u00f3gicas semiautom\u00e1ticas (Paquot, 2018),  \na las que nos referiremos de forma general \ncomo combinaciones l\u00e9xicas acad\u00e9micas \n(CLA). No es de extra\u00f1ar, por tanto, que las \nCLA  del ingl\u00e9s hayan sido el objeto de estudio \nde numerosas i nvestigaciones de ling\u00fc\u00edstica de \ncorpus con fines aplicados. Este auge es \nf\u00e1cilmente explicable teniendo en cuenta el rol \npredominante del ingl\u00e9s como lengua acad\u00e9mica internacional y el gran n\u00famero de \nhablantes, principalmente, hablantes no-nativos, \nque necesitan recursos de ayuda para la \nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 95-103\nrecibido 31-03-2022 revisado 20-05-2022 aceptado 30-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-8\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalredacci\u00f3n de art\u00edculos cient\u00edficos y de trabajos \nacad\u00e9micos en general. Posteriormente, tambi\u00e9n \nse han ido extendiendo los proyectos de \ncompilaci\u00f3n de corpus ac ad\u00e9micos y de estudio \nde las CLA  a otras lenguas como el fran c\u00e9s, \nportugu\u00e9s de Brasil, sueco, noruego, dan\u00e9s y espa\u00f1ol (Alonso et al., 2017), ya que numerosos \ntrabajos acad\u00e9micos se producen en las lenguas \nlocales.  \nSi bien podemos pensar que el ser hablante \nnativo de una lengua es un factor que puede facilitar la pr oducci\u00f3n de textos en dicha \nlengua, tambi\u00e9n est\u00e1 generalmente aceptado que no hay hablantes nativos de los registros \nacad\u00e9micos, y que \u00e9stos se adquieren gracias a \nla experiencia ling\u00fc\u00edstica de lectura y producci\u00f3n de textos acad\u00e9micos. Debido a la \ninterna cionalizaci\u00f3n de la comunicaci\u00f3n \nacad\u00e9mica y del uso cada vez m\u00e1s extendido del ingl\u00e9s como lengua de instrucci\u00f3n y de \nelaboraci\u00f3n de trabajos acad\u00e9micos en la educaci\u00f3n superior, existe la preocupaci\u00f3n de \nque los estudiantes universitarios, e incluso los \nexpertos tengan cada vez m\u00e1s dificultades para \nadquirir los recursos expresivos necesarios para \nla comunicaci\u00f3n acad\u00e9mica en L1 diferentes del ingl\u00e9s (Swales, 2000; G\u00f6rlach, 2002; Laur\u00e9n et \nal., 2002; Johansson Kokkinakis et al., 2012; \nGotti , 2012). Es por  esto que se hace necesario \nelaborar recursos y herramientas de ayuda a la \nescritura acad\u00e9mica tambi\u00e9n para otras lenguas. En el caso del euskera,  la idea generalizada es \nque no ha habido suficiente tiempo para el desarrollo y estabilizaci\u00f3n de los registr os \nacad\u00e9micos (Zabala et al., 2011; Zabala et al., \n2021), y la preocupaci\u00f3n por el impacto de la \ncreciente internacionalizaci\u00f3n es a\u00fan mayor.  \nLas CLA  son segmentos de palabras \nrecurrentes que pueden o no ser \nsem\u00e1nticamente composicionales y que cubren \nfunciones ret\u00f3ricas como a\u00f1adir informaci\u00f3n, \npresentar ejemplos o expresar posibilidad. Incluyen colocaciones ( ondorioak atera  \n\u201cextraer conclusiones\u201d),  locuciones ( oro har  \n\u201cen general\u201d) y f\u00f3rmulas, que coinciden en gran \nmedida con las denominadas en la literatura \nlexical bundles  (azpimarratu beharra dago \n\u201chay que remarcar\u201d).  La recurrencia es el \nresultado de su uso frecuente en discursos compartidos por la comunidad acad\u00e9mica y, por \nlo tanto, las CLA  constituyen un tipo de \nunidades privilegiadas para el estudio  del nivel \nde desarrollo de los registros acad\u00e9micos del euskera.  Este trabajo se enmarca en el proyecto \nHARTAes -vas, proyecto coordinado entre la \nUniversida de da Coru\u00f1a y la Universidad del \nPa\u00eds Vasco (UPV/EHU) y financiado por el Ministerio de Ciencia e  Investigaci\u00f3n. El \nequipo de la Coru\u00f1a cuenta con un corpus de expertos y otro de noveles en espa\u00f1ol, \ncompilados en un proyecto anterior, ha \nexplotado dichos corpus para la extracci\u00f3n y \nclasificaci\u00f3n de CLA y ha desarrollado una  \nherramienta de consulta de la fraseolog\u00eda \nacad\u00e9mica en espa\u00f1ol (Herramienta de Ayuda a \nla Escritura Acad\u00e9mica: HARTA )\n1 (Garc\u00eda -\nSalido et al., 2018). La colaboraci\u00f3n con el grupo de la Coru\u00f1a es fundamental para poder \ncontar con elementos de comparaci\u00f3n entre dos lenguas que se difer encian por su tipolog\u00eda y \npor su situaci\u00f3n socioling\u00fc\u00edstica. E n este trabajo \ndescribimos el corpus acad\u00e9mico de noveles \npara el euskera compilado dentro del proyecto \nHARTAvas  y su explotaci\u00f3n para el estudio de \nla fraseolog\u00eda acad\u00e9mica de cara a crear una \nherramienta de consulta coordinada con \nHARTAes , que ayude a la escritura acad\u00e9mica \nen euskera, y que contribuya al desarrollo y estabilizaci\u00f3n de los registros acad\u00e9micos en dicha lengua. Hemos elaborado un corpus \ncomparable con el  corpus HARTA de noveles \npara el espa\u00f1ol (Villayandre,  2018; Garc\u00eda -\nSalido et al., 2018) con el fin de poder \ncontrastar  los resultados obtenidos para el \neuskera, que es una lengua aglutinante en \nproceso de normalizaci\u00f3n, con los obtenidos \npara el espa\u00f1ol, lengua flexiva y bien \ndesarrollada.  \nEn el apartado 2 describimos la constituci\u00f3n \ndel corpus  de noveles  HARTAeus. El apartado \n3 lo dedicamos a la extracci\u00f3n , validaci\u00f3n  y \nan\u00e1lisis de las f\u00f3rmulas y colocaciones \nacad\u00e9micas a partir del corpus. Finalmente, los \napartados 4 y 5 recogen  los resultados y \nconclusiones obtenidos hasta el momento. \n2 Cons tituci\u00f3n del corpus HARTA eus \nEl corpus HARTAeus  de noveles para el vasco \nest\u00e1 constituido por Trabajos Fin de Grado \n(TFG) y Trabajos Fin de M\u00e1ster (TFM), por lo \nque se puede considerar una mues tra de la \nescritura acad\u00e9mica de los estudiantes universitarios. A l ser un corpus comparable con \nel corpus HARTA-noveles del espa\u00f1ol, su \ndise\u00f1o ha seguido los criterios definidos en la \n1 http://www.dicesp.com:8083/search  \n96\nMar\u00eda Jes\u00fas Aranzabe, Antton Gurrutxaga, Igone Zabala   creaci\u00f3n de \u00e9ste \u00faltimo (Villayandre, 2018). De \neste modo, los textos d el corpus HARTAeus \nest\u00e1n dividido s en cuatro secciones (Arte y \nHumanidades, Biolog\u00eda y Ciencias de la Salud, Ciencias F\u00edsicas y Ciencias Sociales) , que se \ndividen a su vez   en distintos dominios tem\u00e1ticos como, por ejemplo, Biolog\u00eda y \nMedicina en la secci \u00f3n de  Biolog\u00eda y Ciencias \nde la Salud ( ver A nexo 1  para la divisi\u00f3n del \ncorpus en  secciones y dominios).   \nEl proceso de compilaci\u00f3n ha comprendido \nseis fases: i) recolecci\u00f3n de documentos para el  \ncorpus: los documentos en formato PDF \nproceden en su mayor\u00eda del repositorio ADDI (Archivo Digital para la Docencia e \nInvestigaci\u00f3n) de la Universidad del Pa\u00eds Vasco/Euskal Herriko Unibertsitatea \n(UPV/EHU); ii) normalizaci\u00f3n: se ha realizado \nla conversi\u00f3n al formato DOCX de los \ndocumentos originales con el fin de realizar la \nlimpieza y ordenaci\u00f3n de las secciones y \np\u00e1rrafos de los textos , y eliminar las marcas \nsobrantes; iii) codificaci\u00f3n: se han introducido \nlas etiquetas que marcan el inicio y final de las \ndistintas secciones de los textos ( t\u00edtulo, \nresumen, presen taci\u00f3n, introducci\u00f3n, cuerpo, \nmetod olog\u00eda, resultados y discusi\u00f3n, \nconclusiones, agradecimientos, notas al pie de \np\u00e1gina y anexos); iv) se han incorporado de \nmanera autom\u00e1tica los textos en el  entorno de \ntrabajo Garaterm (Zabala et al,  2013) para su \nposter ior procesamiento; v) almacenamiento: se \nhan anotado los metadatos y las secciones de \nlos textos  del corpus de referencia con etiquetas \nXML. Asimismo, se ha adaptado el conversor de TEI existente en la plataforma Garaterm con \nel fin de mantener la estructura y las etiquetas \nde XML utilizadas para marcar los apartados de \nlos documentos originales, y vi) procesamiento: \nel corpus ha sido tokenizado, lematizado y \nanalizado morfol\u00f3gicamente por medio de \nEustagger (Alegr\u00eda et al., 2002), analizador \nmorfol\u00f3gico y etiquetador de partes del \ndiscurso para el euskera. Por medio de este \nproceso se obtiene la informaci\u00f3n del lema y los rasgos morfosint\u00e1cticos necesarios para poder \nextraer las combinaciones  de palabras \ncandidatas a colocaciones: N +N, N+V, N+Adj.  \nEl result ado de es te proceso ha sido la \ncreaci\u00f3n de un corpus acad\u00e9mico monoling\u00fce \nintegrado por 398 textos (71 % TFG y 29  % \nTFM) y 3.285.098 palabras distribuidas en cuatro \u00e1reas de conocimiento  (Tabla 1). La \ndistribuci\u00f3n en los distintos dominios tem\u00e1ticos  \npuede verse en el A nexo 1.  Secciones del \ncorpus  TFG  \nn\u00ba de \npalabras (n\u00ba \nde \ndocumentos)  TFM  \nn\u00ba de \npalabras (n\u00ba \nde \ndocumentos)  Total de \npalabras y \ndocumentos  \nArte y \nHumanidades  450.859  \n(62) \n 203.831  \n(12) \n 654.690  \n(74) \n \nBiolog\u00eda y \nCiencias de \nla Salud  271.932  \n(65) \n 153.533  \n(26) \n 425.465  \n(91) \n \nCiencias \nF\u00edsicas  1.035.599  \n(121)  378.685  \n(36) 1.414.284  \n(157)  \nCiencias \nSociales  559.791  \n(46) 230.868  \n(30) 790.659  \n(76) \nTotales  2.318.181  \n(294)  966.917  \n(104)  3.285.098  \n(398)  \n \nTabla 1: Distribuci\u00f3n de palabras y documentos por secciones y por tipolog\u00eda de textos (TGF y TFM).  \n \nComo se puede observar en la Tabla 1, el \nn\u00famero de TFM es menor  que el de TFG . Esto \nse debe a que el n\u00famero de TFM que se elaboran en euskera es muy peque\u00f1o, a que \nbastantes trabajos est\u00e1n protegidos po r \ncl\u00e1usulas de confidencialidad y a que muchos \nde ellos no se publican en la plataforma ADDI. \n3 Extracci\u00f3n y validaci\u00f3n de CLA   \nPara la extracci\u00f3n y validaci\u00f3n de las CLA \nhemos a\u00f1adido tres m\u00f3dulos al extractor de \nterminolog\u00eda para el euskera  Erauzterm \n(Alegr\u00eda et al., 2004): un m\u00f3dulo para la \nidentificaci\u00f3n del vocabulario acad\u00e9mico, un \nsegundo m\u00f3dulo para la identificaci\u00f3n de colocaciones acad\u00e9micas y un tercer m\u00f3dulo \npara la identificaci\u00f3n de f\u00f3rmulas acad\u00e9micas. \nDebido a las deficiencias del desarrollo d e los \nregistros acad\u00e9micos en euskera, enco ntramos \nCLA que supera n los umbrales de frecuencia y \ndispersi\u00f3n establecidos pero que pueden ser \nconsideradas como incorrectas o  no \u00f3ptimas. \nComo quiera que el \u00faltimo objetivo del proyecto es desarrollar una herramienta de \nayuda a la escritura, en el proceso de validaci\u00f3n \nhemos ido identificando las CLA incorrectas y \nelaborando una tipolog\u00eda de estas. \n \n3.1 Extracci\u00f3n y validaci\u00f3n del \nvocabulario acad\u00e9mico \nEl m\u00f3dulo para la elaboraci\u00f3n de la lista de vocabulario acad\u00e9mi co utiliza como contraste el \ncorpus Dabilena ,\n2 obtenido de la web \n                                                      \n2 https://dabilena.elhuyar.eus/  \n97\nCompilaci\u00f3n del corpus acad\u00e9mico de noveles en euskera HARTAeus y su explotaci\u00f3n para el estudio de la fraseolog\u00eda acad\u00e9mica (300.217.903 palabras): es el corpus mayor que \ntenemos para el euskera y  ha sido elaborado por \nElhuyar. Los candidatos se pueden filtrar seg\u00fan su categor\u00eda gramatical (N, V, Adj., Adv\u2026) y seg\u00fan varias medidas de frecuencia y \ndispersi\u00f3n: n\u00ba de dominios y partes del texto, \nporcentaje de textos del corpus en los que \naparecen, as\u00ed como log-likelihood, frecuencia y \numbrales de frecuencia esperada.  \nLa tarea de identificaci\u00f3n del vocabulario \nacad\u00e9mi co no es trivial, ya que se trata de \nidentificar los lemas caracter\u00edsticos del discurso acad\u00e9mico, pero descartando los t\u00e9rminos \nespec\u00edficos de una determinada \u00e1rea de \nespecialidad. Para que la lista obtenida para el \neuskera sea comparable con la obtenida para el \nespa\u00f1ol en el proyecto HARTA, se han probado \nalgunas de las medidas descritas en Garc\u00eda-\nSalido (2021). Se han realizado dos \nexperimentos con 3 condiciones comunes: \npresencia de los candidatos en las 4 secciones del corpus y en el 20 % de los documentos, y \nvalores de log- likelihood  positivos. En el \nsegundo experimento se ha a\u00f1adido la \ncondici\u00f3n de que la frecuencia no sea 3 veces \nsuperior a la frecuencia esperada en cada uno de las cuatro secciones del corpus. Los resultados \nobtenidos se resumen en l as Tablas 2 y 3.  \n \nExperimento 1  \n candidatos  validados  precisi\u00f3n  \nN 443 338 76,30  % \nV 167 165 98,80  % \nADJ 147 128 87,07  % \nADV  73 53 72,60 % \nTotal  830 684 82,41  % \n \nTabla 2 : Validaci\u00f3n de los candidatos para la \nlista de vocabulario acad\u00e9mico: presencia en las 4 secciones del corpus y en el 20  % de los \ndocumentos + log-likelihood  positiva. \n  \n \n \n  \n \n \n \n  \n                                                                               \n Experimento 2  \n candidatos  validados  precisi\u00f3n  \nN 160 116 72,50  % \nV 81 81 100 % \nAdj. 70 62 88,57  % \nAdv.  49 34 68,39  % \nTotal  360 293 81,39  % \n \nTabla 3: Va lidaci\u00f3n de los candidatos para la \nlista de vocabulario acad\u00e9mico: presencia en las \n4 secciones del corpus y en el 20  % de los \ndocumentos + log- likelihood  positiva + F< 3 \nFesperada  en cada secci\u00f3n . \n \nComo se puede ver en las Tablas 2 y 3, la \ncondici\u00f3n a\u00f1adida en el experimento 2, encaminada a descartar los t\u00e9rminos espec\u00edficos \nde las diferentes \u00e1reas de especialidad, no \naumenta la precisi\u00f3n y, adem\u00e1s, disminuye la cobertura en un 57 %. \n \n3.2 Extracci\u00f3n y validaci\u00f3n de \ncolocaciones acad\u00e9micas  \nEl m\u00f3dulo de extracci\u00f3n y validaci\u00f3n de \ncolocaciones acad\u00e9micas est\u00e1 conectado con el \nvocabulario acad\u00e9mico, de tal manera que permite filtrar los candidatos a colocaciones con \nun solo lema o con los dos lemas incluidos en la \nlista de vocabulario acad\u00e9mico. Las \ncombinaciones candidatas a colocaciones \nacad\u00e9micas se extraen utilizando las medidas de asociaci\u00f3n desarrolladas en Gurrutxaga et. al. \n(2011, 2018)  y atienden a los siguientes \npatrones sint\u00e1cticos: Sujeto -Verbo ( emaitzek \nerakutsi \u201cresultados mostrar\u201d),  Verbo -Objeto \n(helburu lortu  \u201cobjetivo conseguir = conseguir \nobjetivos \u201d; emaitzei erreparatu \u201c resultados-\nDATIVO  atender = atender a los resultados\u201d, \nlanetik atera \u201c trabajo -ABLATIVO = obtener a \npartir del trabajo \u201d), Nombre-Modificador \n(helburu nagusi \u201cobjetivo principal\u201d , fun tsezko \nelementu  \u201cfundamental elemento = elemento \nfundamental\u201d ), N-(posposici\u00f3n)- N ( lagin \ntamaina \u201c muestra tama\u00f1o = tama\u00f1o de \nmuestra\u201d ; laginaren tamaina \u201c muestra de l a \ntama\u00f1o = tama\u00f1o de la muestra\u201d).  \nSe han excluido los nombres ligeros mota  \n\u201ctipo\u201d, kopuru \u201cn\u00famero\u201d, falta \u201cfalta\u201d , multzo  \n\u201cconjunto\u201d, zati \u201cparte\u201d  y maila  \u201cnivel\u201d . \nTambi\u00e9n se han descartado los adjetivos bakar  \n\u201c\u00fanico\u201d, berdin \u201cigual\u201d , ezberdin/desberdin  \n\u201cdiferente\u201d y los modificadores prenominales \ngoiko  \u201csuperior \u201d, beheko  \u201cinferior\u201d , honako \n98\nMar\u00eda Jes\u00fas Aranzabe, Antton Gurrutxaga, Igone Zabala \u201ceste\u201d, horrelako  \u201csimilar\u201d, hurrengo  \n\u201csiguiente\u201d. La raz\u00f3n de este descarte es que, a \npesar de que dan lugar a combinaciones \nrecurrentes con nombres acad\u00e9micos, en la mayor\u00eda de los casos no se trata de \ncolocaciones. \nEn la Tabla 4  se resumen algunos de los \nresultados obtenidos. El n\u00famero tota l de tokens \nnormalizado es de 9.471  tokens por mill\u00f3n de \npalabras.  \nTypes  Tokens  \nN-Modif. 495 13.221 \nN(pos.)N  142 4.112 \nSujeto -V 11 192 \nV-Objeto 357 13.589 \nTotal      1.005 31.114 \nTa\nbla 4:  Colocaciones extra\u00eddas d el corpus \nHARTAeus. \n3.3 Extracci\u00f3n y validaci\u00f3n de f\u00f3rmulas \nacad\u00e9micas \nPara la detecci\u00f3n de f\u00f3rmulas se han extra\u00eddo n -\ngramas entre 2 y 5 elementos. Se han filtrado \u00fanicamente los que estaban presentes en las 4 \nsecciones del corpus y cuya frecuencia era igual  \no superior a 10 apariciones por mill\u00f3n de\npalabras, criterio generalmente utilizado para la\nidentificaci\u00f3n de lexical bundles (Biber et al.,\n1999). A la hora de validar los candidatos,hemos asignado una o m\u00e1s funciones\ndiscursiva s a cada n -grama validado , siguiendo\nla tipolog\u00eda usada en el proyecto HARTA parael espa\u00f1ol (Garc\u00eda- Salido et al., 2019), ya que,\ncon el fin de que los usuarios puedan encontrarlas f\u00f3rmulas f\u00e1cilmente en la herramienta deconsulta, es m\u00e1s detallada que la de Biber et al.\n(2004) y la de Hyland (2008). Adem\u00e1s, una de\nlas tareas principales del proyecto consiste encomparar las f\u00f3rmulas extra\u00eddas de los corpus\nde noveles vasco y espa\u00f1ol.\nEn el proceso de validaci\u00f3n y de asignaci\u00f3n \nde funci\u00f3n discursiva a los n- gramas, en \nalgunos casos hemos eliminado alg\u00fan elemento \nque no aportaba valor sem\u00e1ntico a la funci\u00f3n \ndiscursiva, como es la conjunci\u00f3n eta \u201cy\u201d. As\u00ed \npor, ejemplo, si un candidato validado era eta hala ere  \u201cy aun as\u00ed\u201d, lo hemos eliminado y \nhemos mantenido \u00fanicamente la f\u00f3rmula de dos \nelementos hala ere  \u201caun as\u00ed\u201d. Con este \nprocedimiento, hemos identificado algunas \nf\u00f3rmulas monol\u00e9xicas plurimorf\u00e9micas, que en principio no esper\u00e1bamos recoger. Por ejemplo, \nel n-grama eta ondorioz  \u201cy por consiguiente\u201d  lo \nhemos validado como la f\u00f3rmula  ondorioz  \u201cpor \nconsiguiente\u201d y le hemos asignado la funci\u00f3n \n\u201cexpresar consecuencia\u201d.  \nUna vez validados los n -gramas y asignadas \nlas funciones discursivas, hemos identificado \nlas variantes de una misma f\u00f3rmula. Por \nejemplo, aipatu den moduan \u201c como se ha \nmencionado\u201d y aipatu dugun moduan \u201ccomo \nhemos mencionado\u201d  son dos variantes de la \nmisma f\u00f3rmula, y lo mismo sucede con las \nf\u00f3rmulas horrek esan nahi du \u201ceso quiere decir\u201d \ny horrek ez du esan nahi  \u201ceso no quiere decir\u201d. \nEn estos casos, las variantes las hemos considera do como un solo type. Se han validado \ny clasificado 644 f\u00f3rmulas ( types ), 1.028 \nvariantes y 125.398 tokens (38.171 tokens por mill\u00f3n de palabras).  Como puede verse en la \nTabla 5, a falta de estrategias complementarias \npara la extracci\u00f3n de f\u00f3rmul as monol\u00e9xicas, las \nf\u00f3rmulas de 2 palabras son las m\u00e1s numerosas.  \nF\u00f3rmulas \nacad\u00e9micas  N\u00famero de \npalabras  Types  \n1 palabra  42 \n2 palabras  490 \n3 palabras  126 \n4 palabras  12 \n5 palabras  4 \nTotales  644 \nTab\nla 5: N\u00ba de f\u00f3rmulas acad\u00e9micas validadas \nuna vez analizada la variaci\u00f3n.  \n3.4 Identificaci\u00f3n y clasificaci\u00f3n de CLA \nincorrectas  \nAlgunas colocaciones y f\u00f3rmulas que llegan a \nlos umbrales de frecuencia y dispersi\u00f3n \nestablecidos, pueden considerarse como \nincorrectas o no \u00f3ptimas. Estas CLA las hemos recogido y clasificado para poder as\u00ed tenerlas en \ncuenta a la hora de dise\u00f1ar la herramienta de consulta ya que, aunque no son muy numerosas, \npresentan un importante grado de recurrencia y \ncompiten con forma s m\u00e1s correctas o genuinas.  \nEn la Tabla 6, ofrecemos una clasificaci\u00f3n \npreliminar y algunos ejemplos. \n99\nCompilaci\u00f3n del corpus acad\u00e9mico de noveles en euskera HARTAeus y su explotaci\u00f3n para el estudio de la fraseolog\u00eda acad\u00e9mica Ortografia no -est\u00e1ndar  \nkontu tan hartu behar da  \n\u201chay que tener en \ncuenta\u201d  kontuan hartu behar da  \ngutxi gora beher a \n\u201cpoco m\u00e1s o menos\u201d  gutxi gorabehera  \npaus uak eman  \n\u201cdar pasos\u201d  \npausu \u201cdescanso\u201d  paus oak eman  \npauso \u201cpaso\u201d  \nDemostrativo de 1er grado como an\u00e1fora  \nhonek  ez du esan nahi  \n\u201cesto no quiere decir \u201d  \nDemostrativo de 1er \ngrado (cat\u00e1fora)  horrek  ez du esan nahi  \nDemostrativo de 2\u00ba \ngrado (an\u00e1fora)  \nOrden de palabras inadecuado  \nLan honen helburua [\u2026] \nda \n\u201cEl ob jetivo de este \ntrabajo [\u2026] es\u201d  Lan honen helburua da \n[\u2026]  \nForma incorrecta de un conector  \nAlde batetik [\u2026] eta \nbeste aldetik , [\u2026] \n\u201cPor un lado [\u2026] y por \nel otro lado [\u2026]\u201d  Alde batetik [\u2026] eta, \nbestetik , [\u2026] \n\u201cPor un lado [\u2026] y por \nel otro [\u2026] \u201d \nAsignaci\u00f3n de una funci\u00f3n discursiva incorrecta  \nHau da  \u201cesto es\u201d  \n\u201cexpresar causa\u201d  Hau da  \n\u201creformular\u201d  \nColocaci\u00f3n incorrecta desde el punto de vista \nsem\u00e1ntico -sint\u00e1ctico  \ndatuek adierazi  \n\u201cdatos expresar\u201d  datuek erakutsi  \n\u201cdatos mostrar\u201d  \ndatu adierazga rri \n\u201cdato r epresentativo\u201d  datu esangarri  \n\u201cdato significativo\u201d  \nCalcos  \nbesteen artean \n\u201centre otros\u201d  besteak beste \n\u201centre otros\u201d  \nTa\nbla 6: Clasificaci\u00f3n y ejemplos de CLA \nincorrectas.  \n4 Resultados y discusi\u00f3n \nNo contamos con un corpus de expertos para el \neuskera comparable con el corpus HARTA de \nexpertos, por lo que, para analizar los datos obtenidos hasta el momento, los contrastaremos \ncon los ofrecidos en Garc\u00eda- Salido (2021) y en \nAlonso y Zabala (2022 ). \nLa lista de vocabulario acad\u00e9mico \ncompilada hasta el momento para el euskera \ncuenta con 684 lemas. Esta lista es bastante m\u00e1s \nreducida que la obtenida tras contrastar los \nresultados de diferentes t\u00e9cnicas para el espa\u00f1ol \n(Garc\u00eda -Salido, 2021): 833 lemas. La diferencia \npuede estar motivada por el descarte de los \nlemas con valores de log -likelihood  negativos, \nya que entre los lemas descartados puede haber algunos que son muy utilizados en textos generales pero que activan significados \nespec\u00edficos en los discursos acad\u00e9micos. \nNuestra idea es seguir completando la lista obtenida hasta el momento, probando otras \nt\u00e9cnicas y medidas.  \nSin embargo, la principal aplicaci\u00f3n de la \nlista de lemas acad\u00e9micos en el proyecto \nHARTAvas es la extracci\u00f3n de colocaciones \nacad\u00e9micas. Para ello, es fundamental la lista de \nnombres , ya que son los que constituyen las \nbases de las colocaciones, el n\u00famero de N \nobtenidos para el euskera es menor pero \ncomparable al del espa\u00f1ol: 338 eus / 358 es. \nEl n\u00famero de colocaciones acad\u00e9micas \nobtenidas es tambi\u00e9n comparable al obtenido a partir de l corpus HARTA de noveles para el \nespa\u00f1ol  (Alonso y Zabala, 2022): 1 .005 types \neus / 1.197 es. Aunque hay que tener en cuenta \nque el corpus del euskera es mayor que el del espa\u00f1ol, que cuenta con 2 M de palabras. Aun siendo menor el n\u00famero de colocaciones \nextra\u00eddas para el euskera, el n\u00famero de tokens \npor M de palabras es notablemente superior: \n9.471 tokens/M eus / 6.897 tokens/M es. Por lo \ntanto, como primera aproximaci\u00f3n se puede \ndecir que las colocaciones detectadas para el \neuskera son m\u00e1s recurrentes que las detectadas \npara el espa\u00f1ol en el corpus de noveles. \nEl n\u00famero de f\u00f3rmulas (types) extra\u00eddas \npara el euskera es notablemente superior al obtenido a partir del corpus de noveles en \nespa\u00f1ol: 644 types eus / 472 types es. Tambi\u00e9n \nexiste diferencia en la frecuencia de dichas f\u00f3rmulas: 38.171 tokens/M eus / 20 .474 \ntokens/M es. El mayor n\u00famero de f\u00f3rmulas detectadas en euskera podr\u00eda estar relacionado \ncon el menor grado de fijaci\u00f3n de las f\u00f3rmulas acad\u00e9micas en esta lengua, y con la variaci\u00f3n \nentre f\u00f3rmulas m\u00e1s genuinas y f\u00f3rmulas \ncalcadas del espa\u00f1ol: besteen artean  (calco) / \nbesteak beste ; orokorrean (calco) / oro har. El \nn\u00famero mayor de tokens, podr\u00eda indicar una \nmenor riqueza expresiva de los noveles vascos, \nque les har\u00eda recurrir m\u00e1s frecuentemente a las  \nmismas secuencias dando lugar a un discurso m\u00e1s repetitivo. De cualquier modo, se requiere \nun an\u00e1lisis m\u00e1s minucioso de los criterios de validaci\u00f3n que hemos utilizado para una y otra \nlengua, as\u00ed como de los datos, con el fin de \npoder hacer una comparaci\u00f3n m\u00e1s precisa de las \nCLA obtenidas en una y otra lengua de cara al \ndise\u00f1o de la herramienta de ayuda a la escritura acad\u00e9mica para las dos lenguas.  \n100\nMar\u00eda Jes\u00fas Aranzabe, Antton Gurrutxaga, Igone Zabala 5 Conclusio nes \nA pesar de que el euskera es una lengua \nminorizada que se introdujo hace solo unas \nd\u00e9cadas en l a ense\u00f1anza superior, hemos \nlogrado compilar un corpus de trabajos \nacad\u00e9micos en euskera (TFG y TFM) \ncomparable, e incluso algo m\u00e1s extenso, que el \ncorpus de noveles para el espa\u00f1ol.  \nA partir del corpus hemos obtenido una lista \nde vocabulario acad\u00e9mico en euskera (644 lemas), que aunque debe de considerarse \npreliminar, nos ha permitido identificar un gran n\u00famero de colocaciones acad\u00e9micas (1 .005 \ntypes).  \nHemos extra\u00eddo tambi\u00e9n n- gramas de 2, 3, 4 \ny 5 elementos, que hemos validado y a los que hemos asignado f unciones discursivas partiendo \nde la tipolog\u00eda utilizada para HARTAes. Los n -\ngramas nos han permitido detectar f\u00f3rmulas polil\u00e9xicas o lexical bundles  como ( kontuan \nhartu beharrekoa da \u201chay que tener en \ncuenta\u201d ), pero en el proceso de validaci\u00f3n, \nhemos podi do detectar tambi\u00e9n  42 f\u00f3rmulas \nmonol\u00e9xicas o morphemic bundles  como \nlaburbilduz  \u201cen resumen\u201d. As\u00ed hemos obtenido \n644 f\u00f3rmulas y 1.028 variantes, a las que les hemos asignado funciones discursivas. \nPor \u00faltimo, hemos desarrollado  una \ntipolog\u00eda de f\u00f3rmulas incorrectas,  de cara a \nelaborar su tratamien to lexicogr\u00e1fico en la \nherramienta de consulta.  \nEstamos implementando t\u00e9cnicas de \nsem\u00e1ntica distribucional, con el fin de utilizar \nlos corpus comparables del espa\u00f1ol y del \neuskera para la detecci\u00f3n de f\u00f3rmulas, sobre \ntodo f\u00f3rmulas monol\u00e9xicas, y equivalentes de colocaciones y f\u00f3rmulas entre las dos lenguas. \nAdem\u00e1s, hemos comenzado la tarea de \ncomparaci\u00f3n m\u00e1s minuciosa de las listas de CLA elaboradas para las dos lenguas, con el fin \nde obtener una clasificaci\u00f3n que tenga en cuenta las caracter\u00edsticas tipol\u00f3gicas del espa\u00f1ol \ny del euskera. Dicha comparaci\u00f3n nos servir\u00e1 \ntambi\u00e9n para decidir el dise\u00f1o de la herramienta de consulta para ambas lenguas.  \nAgradecimientos  \nEste trabajo es parte del proyecto  HARTAvas \n(PID2019-109683GB- C22), financiado por el \nMinisterio de Ciencia e Innovaci\u00f3n.  Bibliograf\u00eda \nAlegria, I., M.J. Aranzabe, A. Ezeiza A., N. \nEzeiza, y R. Urizar R. 2002.  Robustness and \ncustomisation in an analyser/lemmatiser for Basque. En Third International Conference \non Language Resources and Evaluation \n(LREC): Customizing Knowledge in NLP \nApplications- Strategies, Issues and \nEvaluation Workshop, p\u00e1 ginas 1 -6, Las \nPalmas de Gran Canaria (Spain).  \nAlegr\u00eda, I, A.  Gurrutxaga, P. Lizaso, X. \nSaralegi, S.  Ugartetxea, y R.  Uriza r. \n2004. \nAn Xml -Based Term Extraction Tool for \nBasque. En Proceedings of the Fourth \nInternational Conference on Language \nResources and Evaluation (LREC\u201904) , \np\u00e1ginas 1733- 1736, Lisboa (Portugal) . \nAlonso- Ramos, M., M. Garc\u00eda -Salido, y M. \nGarcia. 2017. Exploiting a Corpus to \nCompile a Lexical Resource for Academic Writing: Spanish Lexical Combinations. En \nKosem , I., J. Kallas, C. Tiberius, S. Krek, \nM.Jakub\u00ed\u010dek , V. Baisa (Eds). Electronic\nlexicography in the 21st century.\nProceedings of eLex 2017 conference,\np\u00e1ginas 571 -586, Leiden (the Netherlands).\nAlonso- Ramos, M. y I. Zabala . 2022. \nHARTAes -vas: Combinaciones l\u00e9xicas para \nuna Herramienta de ayuda a la redacci\u00f3n de \ntextos acad\u00e9micos en espa\u00f1ol y en vasco.  En \nProceedings of the Annual Conference of the Spanis h Association for Natural Language \nProcessin g: Projects and Demonstrations, \nSEPLN , September , A Coru\u00f1a ( Spain). \nBiber, D. 2006. University Language. A corpus -\nbased study of spoken and written registers. John Benjamins, Amsterdam.  \nBiber, D., E. Finegan, S. Johanson, S. Conrad, y \nG. Leech. 1999. Longman Grammar ofSpoken and Written Englis h. Longman,\nLondon.\nBiber, D., S. Conrad, y C. Viviana. 2004. If you \nlook at\u2026: lexical bund les in university \nteaching and textbooks. Applied Linguistics , \n25(3):371-405. \nGarc \u00eda-Salido, M ., M. Garc\u00eda, M., Villayandre, \ny M. Alonso- Ramos . 2018. A Lexical Tool \nfor Academic Writing in Spanish based on \nExpert and Novice Corpora. En Calzolari N. \net al. (Eds).  Proceedings of the Eleventh \nInternational Conference on Language \n101\nCompilaci\u00f3n del corpus acad\u00e9mico de noveles en euskera HARTAeus y su explotaci\u00f3n para el estudio de la fraseolog\u00eda acad\u00e9mica Resources and Evaluation (LREC 2018) , \np\u00e1ginas 260-265, Miyazaki (Japan). \nGarc\u00eda- Salido, M., M. Garcia , y M. Alonso-\nRamos. 2019. Identifying lexical bundles for \nan academic writing assistant in Spanish. En \nCorpas Pastor , G. y  R. Mitkov \n(Eds).  Computational and Corpus -Based \nPhraseology . Volume  11755 of Lecture \nNotes in Artificial Intelligence , p\u00e1ginas  144-\n158, Springer, Berlin.  \nGarc\u00eda- Salido, M. 2021. Compiling an \nAcademic Vocabulary List of Spanish. DOI: \n10.13140/RG.2.2.27681.33123  \nG\u00f6rlach, M. 2002. Still More Englishes . John \nBenjamins, Amsterdam.  \nGotti, M. 2012. Variation in Academic Texts. \nEn Gotti, M. (Ed). Academic Identity Traits. \nA Corpus Based Investigation, p\u00e1ginas 21-\n42, Peter Lang (Swi tzerland).  \nGurrutxaga, A. e I. Alegria. 2011. Automatic \nextraction of NV expressions in Basque: basic issu es on cooccurrence techniques. E n \nProceedings of the Workshop on Multiword Expressions: from parsing and generation to \nthe real world , p\u00e1ginas 2 \u20137, Portland, \nOregon (USA) . \nGurrutxaga, A., I. Alegria, y X. Artola. 2018. \nCaracterizaci\u00f3n computacional de la \nidiomaticidad: aplicaci\u00f3n a la combinaci\u00f3n nombre+verbo en euskera. En Ruiz \nMiyares, L. (Ed ). Estudios de Lexicolog\u00eda \ny Lexicograf\u00eda. Homenaje a Elo\u00edna  \nMiyares Berm\u00fadez.  Santiago de Cuba \n(Cuba). \nHyland, K. 2008. As can be seen : lexica l \nbundles and disciplinary varia tion. English \nfor Specific Porpuses, 27(1):  4-21. \nJohansson Kokkinakis , S., E. Sk\u00f6ldberg , B. \nHenriksen , K. Kinn, y J. Bondi Johannessen. \n2012. Developing Academic Word Lists for \nSwedish, Norwegian and Danish a Joint \nResearch Project. En Fjeld , R.V. y J.M. \nTorjusen (Eds ). Proceedings of the 15th \nEURALEX International Congress, p\u00e1ginas  \n563\u2013569, University of Oslo ( Norway ). \nLaur\u00e9n, Ch., J. Myking,  y H. Picht. 2002. \nLanguage and domains:  a proposal for a \ndomain dynamics taxonomy. LSP and \nProfessional Communication, 2(2):23 -30. \nPaquot, M. 2018. Phraseological Competence: \nA Missing Component in University Entrance Language Tests? Insights from A Study of EFL Learnes\u2019s Use of Statistical \nCollocations. Language Assessment \nQuarterly , 15(1):29-43. \nSwales, J. 2000. Language for Specific \nPurposes. Annual review of Applied \nLinguistics,  20:59-76. \nVillayandre, M. 2018. \u201c HARTA \u201d de noveles: \nun corpus de espa\u00f1ol acad\u00e9mico. \nCHIMERA: Revista De Corpus De Lenguas \nRomances Y Estudios  Ling\u00fc\u00edsticos , 5(1): \n131\u2013140. \nZabala, I., I. San Martin,  M. Lersundi,  y A. \nElordui . 2011. Graduate teaching of \nspecialized registers in a language in the \nnormalization process: Towards a comprehensive and interdisciplinary \ntreatment of academic Basque.  En Maruenda- Bataller , S. y B. Clavel -Arroita \n(Eds).  Multiple voices in academic and \nprofessional discourse , p\u00e1ginas 208\u02d7218, \nCambridge Scholars (Newcastle upon Tyne, \nUK).  \nZabala,   I.,  M. Le rsundi,   I. Leturia,   I. \nManterola,   y  G. Santander. 2013.  \nGARATERM:   euskararen   erregistro   \nakademikoen garapenaren ikerketarako lan -\ningurunea. En Alberdi, X. y P. Salaburu \n(Eds). Ugarteburu terminologia \njardunaldiak (V). Terminologia  naturala  \neta  terminologia  planifikatua   euskararen   \nnormalizazioari  begira , p\u00e1ginas 98-114, \nServicio Editorial de la UPV/EHU  (Bilbao). \nZabala, I. 2019. The elaboration of Basque in \nAcademic and Professional Domains. En Grenoble, L., P. Lane, y U. R\u00f8yneland \n(Editor- in-Chief), Igartua, I. y L. O\u00f1ederra \n(Basque Eds). Linguistic Minorities in \nEurope Online , De Gruyter Mouton. \nZabala, I., M.J. Aranzabe, y I. Aldezabal. 2021. \nRetos actuales del desarrollo y aprendizaje \nde los registros acad\u00e9micos orales y escritos \ndel eusker a. C\u00edrculo de Ling\u00fc\u00edstica Aplicada \na la Comunicaci \u00f3n, 88: 31-50. \nA Anexo 1: Corpus HARTA eus \nDistribuci\u00f3n de palabras y documentos (TFG y \nTFM) por secciones del corpus y dominios \ntem\u00e1ticos. \n102\nMar\u00eda Jes\u00fas Aranzabe, Antton Gurrutxaga, Igone Zabala Secciones del \ncorpus  Dominios \ntem\u00e1ticos  TFG  \nn\u00ba de \npalabras (n\u00ba \nde \ndocumentos ) TFM  \nn\u00ba de \npalabras (n\u00ba \nde \ndocumentos ) Total de \npalabras y \ndocumentos  \nArte y \nHumanidades  Arte 42.956  \n(6) 0 42.956  \n(6) \nLing\u00fc\u00edstica  207.443  \n(27) 203.831  \n(12) 411.274  \n(39) \nLiteratura  90.139  \n(12) 0 90.139  \n(12) \nHistoria y \nCultura  110.321  \n(17) 0 110.321  \n(17) \nBiblioteconom\u00eda \ny \ndocumentaci\u00f3n  0 0 0 \nTotales  450.859  \n(62) 203.831  \n(12) 654.690  \n(74 \nBiolog\u00eda y \nCiencias de \nla Salud  Biolog\u00eda  182.906  \n(44) 153.533  \n(26) 336.439  \n(70) \nMedicina  89.026  \n(21) 0 89.026  \n(21) \nTotales  271.932  \n(65) 153.533  \n(26) 425.465  \n(91) \nCiencias \nF\u00edsicas  Ciencias de la \nTierra  52.263  \n(7) 0 52.263  \n(7) \nF\u00edsica  113.027  \n(16) 0 113.027  \n(16) \nIngenier\u00eda  656.156  \n(69) 20.031  \n(1) 676.187  \n(70) \nInform\u00e1tica  75.160  \n(8) 332.982  \n(32) 408.142  \n(40) \nQu\u00edmica  138.993  \n(21) 25.672  \n(3) 164.665 \n(24) \nTotales  1.035.599  \n(121)  378.685  \n(36) 1.414.284  \n(157)  \nCiencias \nSociales  Econom\u00eda y \nEmpresa  158.433  \n(10) 0 158.433  \n(10) \nEducaci\u00f3n  102.335  \n(16) 230.868  \n(30) 333.203  \n(46) \nSociolog\u00eda  233.632  \n(16) 0 233.632  \n(16) \nDerecho  65.391  \n(4) 0 65.391  \n(4) \nTotales  559.791  \n(46) 230.868  \n(30) 790.659  \n(76) \n103\nCompilaci\u00f3n del corpus acad\u00e9mico de noveles en euskera HARTAeus y su explotaci\u00f3n para el estudio de la fraseolog\u00eda acad\u00e9mica 104\nExtraction  and  Semantic  Representation  of\nDomain-Specific  Relations  in Spanish  Labour  Law\nExtracci\u00b4 on y  representaci\u00b4 on de r elaciones  espec\u00b4\u0131ficas  de\ndominio  en la legislaci\u00b4 on laboral  espa\u02dcnola\nArtem  Revenko,1 Patricia  Mart \u00b4\u0131n-Chozas2\n1Semantic  Web  Compan y, Vienna,  Austria\n2Ontology  Engineering  Group,  Universidad  Polit\u00b4ecnica  de Madrid,  Madrid,  Spain  artem.revenko@semantic-web.com,  patricia.martin@upm.es\nAbstract: Despite the freedom of information and the development of various open\ndata repositories, the access to legal information to general audience remains hin-\ndered due to the difficulty of understanding and interpreting it. In this paper we\naim at employing modern language models to extract the most important infor-\nmation from legal documents and structure this information in a knowledge graph.\nThis knowledge graph can later be used to retrieve information and answer legal\nquestion. To evaluate the performance of different models we formalize the task as\nevent extraction and manually annotate 133 instances. We evaluate two models:\nGRIT and Text2Event. The latter model achieves a better score of \u00ab0.8F1score\nfor identifying legal classes and 0.5 F1score for identifying roles in legal relations.\nWe demonstrate how the produced legal knowledge graph could be exploited with\n2 example use cases. Finally, we annotate the whole Workers\u2019 Statute using the\nfine-tuned Text2Event model and publish the results in an open repository.\nKeywords: Information Extraction. Knowledge Graphs. Semantic Web. Legal\nDomain.\nResumen: A pesar de la actual libertad de informaci\u00b4 on y del desarrollo de difer-\nentes repositorios de datos abiertos, el acceso a la informaci\u00b4 on jur\u00b4 \u0131dica al p\u00b4 ublico\ngeneral sigue suponiendo un problema debido a la dificultad de comprensi\u00b4 on e in-\nterpretaci\u00b4 on de dicha informaci\u00b4 on. En este art\u00b4 \u0131culo, nuestro objetivo es emplear\nmodelos de lenguaje punteros para extraer informaci\u00b4 on relevante de documentos\njur\u00b4 \u0131dicos; as\u00b4 \u0131 como estructurar esta informaci\u00b4 on en un grafo de conocimiento, con el\nobjetivo de que este grafo pueda utilizarse m\u00b4 as adelante para recuperar informaci\u00b4 on\ny responder preguntas sobre el dominio jur\u00b4 \u0131dico. Para evaluar el rendimiento de los\ndiferentes modelos, hemos formalizado este proceso como una tarea como extracci\u00b4 on\nde eventos, y hemos anotado manualmente 133 instancias. Evaluamos dos modelos:\nGRIT y Text2Event. El \u00b4 ultimo modelo consigue mejores resultados, de \u00ab0.8F1para\nidentificar clases jur\u00b4 \u0131dicas y de 0 .5F1para identificar roles en relaciones jur\u00b4 \u0131dicas.\nAsimismo, ejemplificamos c\u00b4 omo el grafo producido podr\u00b4 \u0131a explotarse con diferentes\ncasos de uso. Finalmente, hemos anotado todo el Estatuto de los Trabajadores con\nel modelo Text2Event y publicado los resultados en un repositorio abierto.\nPalabras clave: Extracci\u00b4 on de Informaci\u00b4 on. Grafos de Conocimiento. Web\nSem\u00b4 antica. Dominio Jur\u00b4 \u0131dico.\n1 Introduction\nDue to its specific nature, the legal domain\nhas always been a complex area for non le-\ngal users. The challenges include finding the\ncorrect document for a purpose and inter-\npreting the document. With the recent rise\nof the data sharing and open data technolo-gies in the last decade, legal knowledge is\nmore accessible than ever. Well-known legal\npractitioners have already exposed their legal\ndata in open and machine readable formats,\ndeveloping platforms such as the European\nData Portal1, a platform funded by the Eu-\n1https://data.europa.eu/\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 105-116\nrecibido 31-03-2022 revisado 12-05-2022 aceptado 30-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-9\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalropean Union and managed by the Publica-\ntions Office that gathers legal data from dif-\nferent subdomains such as justice, legal sys-\ntem and government. At a national level,\none of the most important sources of legal\ndata in Spain is the Official State Gazette2,\nwhich is constantly being updated and ac-\ncessed by lawyers in Spain. It contains docu-\nments in the labour law domain, such as state\ncollective agreements and the Spanish Work-\ners Statute. This availability of legal data\nefficiently addresses the task for finding the\ncorrect documents and accessing them. Yet,\nthe interpretation of legal data and, there-\nfore, exploitation of the legal results by gen-\neral public remains an important challenge\n(Robaldo et al., 2019). We are driven by the\nidea of tackling this challenge and enabling\nmore human-friendly interfaces for accessing\nthis kind of information. We choose the\nlabour law sub-domain for our experimenta-\ntion as this domain is relevant for everyday\nuse by general audience and the tasks of en-\nabling natural language search, information\nretrieval, question answering over the labour\nlaw are highly demanded. To solve these kind\nof tasks and ease the access to legal informa-\ntion from general audience, we aim at struc-\nturing legal data into a knowledge graph.\nModern (multilingual) language models\nhave celebrated many successes on various\nNLP tasks (named entity recognition, rela-\ntion extraction, paraphrase detection, ques-\ntion answering, etc.). We employ these mod-\nels to tackle our challenge as well. We noticed\nthat most models are trained with general\ncorpora and, therefore, fail to identify the pe-\nculiarities of domain specific texts. The lack\nof domain specific annotated corpora and do-\nmain specific language resources published in\nmachine readable formats hinders the fine-\ntuning of the models.\nConsequently, the purpose of this paper is\ntwofold: on the one hand, we test different\nlanguage models on a domain specific corpus\nto extract relations amongst terms and, on\nthe other hand, we provide annotated data in\nthe labour law domain and structure the re-\nsults in Semantic Web formats so they can be\nreused in the future by upcoming researchers.\nThis work (code, input and output data)\nis openly available and published in a GitHub\nrepository3.\n2https://www.boe.es/\n3https://github.com/pmchozas/term_relex/1.1 A look at the Semantic Web\nMore than 20 years ago, the World Wide Web\nConsortium (W3C) promoted the publica-\ntion of data in structured, machine-readable\nand interlinked formats, in which the mean-\ning of data can be interpreted by machines to\nachieve more complex and effective data un-\nderstanding. This initiative is known as the\nSemantic Web or the Web of Data (Berners-\nLee, Hendler, and Lassila, 2001).\nThe most common format for publish-\ning data on the Semantic Web is the Re-\nsource Description Framework (RDF). RDF\nis at the core of the Linked Open Data\nparadigm for publishing information, based\non the Linked Data Principles (Berners-Lee,\n2006). According to these principles, re-\nsources need to be identified by a Uniform\nResource Identifier (URI) (a unique identi-\nfier that follows the HTTP standard web pro-\ntocols), and that resources need to contain\npointers to other resources.\nThe inner structure of Linked Data is\ndetermined by the ontology (also known\nasmodel orvocabulary ) that defines how\nto represent the concepts of a certain do-\nmain (Chandrasekaran, Josephson, and Ben-\njamins, 1999). These ontologies are com-\nposed of classes, relations, rules and restric-\ntions.\nIn this paper, we make use of RDF to\nstructure the extracted knowledge following\nthe Linked Data Principles, publishing the\nresults in a machine readable and linked\ndataset, also called knowledge graph .\n1.2 Motivation\nWe have already approached the task of cre-\nating legal knowledge graph in one of the pi-\nlots (Spanish labour law pilot) of the [Lynx]4\nproject, an Innovation Action funded by the\nEuropean Union\u2019s Horizon 2020, that was\nactive from 2017 until 2021. Although the\nproject already ended, this pilot served as an\ninspiration to work deeply on the extraction\nof knowledge over labour law documentation,\nsince several issues were spotted during its\ndevelopment:\n1. The labour law texts are highly domain\nspecific. This fact hinders the reuse of al-\nready pre-trained language models that\nare usually trained with texts from the\ngeneral domain.\n4https://lynx-project.eu/\n106\nArtem Revenko , Patricia Mart\u00edn-Chozas 2. Annotated corpora in this domain are\nscarce; even after annotating a part of\nthe Statute, the size of the resulting an-\nnotated corpus is not sufficient for a lan-\nguage model to obtain good results.\nTherefore, within this paper we tackle\nthose issues trying to untangle labour law\ndata, easing the understanding of duties and\nrights related to the labour domain to any-\none willing to know them. For instance, if\nwe suppose that a worker is in trouble with\na company for taking a leave from work, we\nmay want to answer questions such as:\nQ1) In which situations can a worker claim\nfor a right?\nQ2) When must a worker come back to work\nafter a leave from work?\nThe rest of the paper is divided as fol-\nlows: Section 2 explains the statement of the\ntask; Section 3 describes the corpus and the\nannotation process; Section 4 identifies the\nlanguage models applied in the experiment;\nSection 5 reports on the results and evalua-\ntion; Section 6 covers the conversion of the\nresults to Semantic Web formats; Section 7\nshows examples on how to exploit the result-\ning knowledge graph; Section 8 gathers re-\nlated work on event extraction and legal on-\ntologies and, finally, Section 9 collects con-\nclusions and future work.\n2 Task Statement\nAs described in previous work by the au-\nthors (Mart\u00b4 \u0131n-Chozas and Revenko, 2021),\nwe analysed the nature of conceptual rela-\ntions in legal texts and noticed that labour\nlaw texts are full of Hohfeld deontic relations,\npart of the Hohfeldian fundamental relations\n(Hohfeld, 1913), that are divided into two\nsets of relations:\n\u2022Deontic relations, that are those that\nmodify ordinary actions: Right, Duty,\nNo-Right and Priviledge.\n\u2022Potestative relations, that are those that\nmodify deontic relations: Power, Liabil-\nity, Disability and Immunity.\nIn this work, we focus on the extraction\nof deontic relations (see Figure 1), since they\nare the basis of the fundamental relations and\nthe most common within the Spanish labour\nlaw.\nRight Duty\nNo-right Privilegeopposite opposite\ncorrelativecorrelativeFigure 1: Hohfeld\u2019s Deontic Relations.\nHohfeld classes provide information about\nthe particular type of legal relation. How-\never, it is difficult to use this information\nalone. In fact, more complex use cases, such\nas question answering over legal texts or the\nmerge of different legal documents into a sin-\ngle knowledge graph, require extracting addi-\ntional information about the participants of\nthese relations.\nWe identify the following roles of the par-\nticipants: subjects of relations, objects of re-\nlations and complements of relations. These\nroles correspond to the classes identified by\nthe Provision Model (see Section 6):\n\u2022The subject is the agent of the action,\nwho performs the action.\n\u2022The object is the patient of the action,\nwho receives the action.\n\u2022The complement is the item which is\nhandled in the relation.\nConsequently, the model should be capa-\nble of (1) classifying a string from a legal\ntext into one of Hohfeld relation classes (if\nthere is one); (2) identifying the roles of par-\nticipants of the extracted relation. We for-\nmalise the task as sentence-level event extrac-\ntion5. Event extraction is an essential task\nfor natural language understanding, aiming\nto transform the text into structured event\nrecords (Doddington et al., 2004; Ahn, 2006).\nThese event records can further be trans-\nformed into a knowledge graph, see Section 6.\n5In preliminary experiments we also considered an\nalternative formalisation as a relation extraction task\n(Hendrickx et al., 2019). However, in that case, we\nneed to extract the entities in a separate step and\nthen use relation extraction to identify relation be-\ntween those entities. Moreover, the roles that we want\nto extract are better described as roles within a sen-\ntence rather than being in a relation with a particular\nentity. In sight of these difficulties, we refrained from\nusing relation extraction as the task formulation.\n107\nExtraction and Semantic Representation of Domain-Specific Relations in Spanish Labour Law We illustrate the event extraction task in Fig-\nure 2 with an example from Spanish labour\nlaw. From the sentence, we extract an event\nrecord of type \u201cRight\u201d corresponding to the\nHohfeld deontic class \u201cRight\u201d, together with\nthe roles of the different participants.\nGenerally, event extraction allows the def-\ninition of different sets of roles for each event.\nFor our use case we do not exploit this flex-\nibility of task formulation as we define the\nsame set of roles for all event types.\nConforme a lo establecido en dicha regulaci\u00f3n, el \ntrabajador podr\u00e1 solicitar de la Administraci\u00f3n p\u00fablica \ncompetente la expedici\u00f3n del correspondiente certificado \nde profesionalidad\u2026\nIn accordance with the provisions of such a regulation, the \nworker may request the Public Administration the issuance \nof the corresponding professional certificate from the \ncompetent public administration...\nEvent type Right\nTriggerpodr\u00e1 solicitar\nmay request\nSubjecttrabajador\nworker\nObjectAdministraci\u00f3n p\u00fablica\nPublic Administration\nComplementcertificado de \nprofesionalidad\nprofessional certificate\nFigure 2: Annotated sample of an event event\nfrom a sentence from Spanish labour law,\nwith approximate translations.\n3 Training Data\nAs mentioned before, this experiment is\nbased on the Spanish Workers\u2019 Statute, that\nis published in the Official State Gazette.\nThe text is the main legislative labour law\ndocument in Spain, therefore, it clearly cor-\nresponds to our aim. The Spanish Work-\ners\u2019 Statute is a representative example of a\ndomain-specific legal corpus, therefore, any\nobtained results could be extrapolated to\nother legal sub-domains.\nThe Statute is divided into three main sec-\ntions named as \u201ctitles\u201d. The first title cov-\ners individual labour relations; the second ti-tle covers the rights of collective representa-\ntion and workers\u2019 assemblies inside compa-\nnies, and the third title covers collective bar-\ngaining and collective agreements. In total,\nthe three sections gather 92 articles, contain-\ning approximately 50.000 tokens. With the\ncurrent state of analysis we estimate the den-\nsity of relations in the Spanish labour law to\nbe 3.65 relations per article.\nRegarding the manual annotation of this\ndocument, it started by identifying its most\nrelevant terms. To speed up this process,\nwe made use of an open source terminology\nextraction tool that extracts the most fre-\nquent terms in the statute. For more infor-\nmation about the evaluation of the tool\u2019s per-\nformance, we refer the reader to its research\npaper (Oliver and V` azquez, 2015).From those\nmost frequent terms, we identified those that\ncould hold the roles of subject and object of\na Hohfeld relation. This is, legal agents, such\nas worker or employer, and legal entities, such\nas company or worker union.\nHaving the document automatically anno-\ntated with these entities, we focused on dis-\ncovering Hohfeld relations amongst them and\nextracted the corresponding text excerpts.\nOptionally, these excerpts could also include\na complement, usually an object that takes\npart in the relation. Not only positive sam-\nples were annotated, but also negative sam-\nples: text excerpts with legal entities that\ndo not present any relation at all amongst\nthem. Corpus and annotated data statistics\nare shown in Table 1. Figure 2 shows an ex-\nample of a positive annotation. Regarding\nthe negative ones, we have identified 2 types:\n1) Annotations with entities but no relations\nand 2) Annotations with neither entities nor\nrelations. Examples of these types are shown\nbelow:\n1. Mediante los convenios colectivos, y en\nsu \u00b4 ambito correspondiente, los traba-\njadores(e1) y empresarios(e2) regulan\nlas condiciones de trabajo y de produc-\ntividad. (By means of the collective\nagreements, in their corresponding field,\nworkers(e1) and employers(e2) regulate\nworking and productivity conditions.)\n2. Igualdad de remuneraci\u00b4 on por raz\u00b4 on de\nsexo. (Equaly pay based on sex )\n108\nArtem Revenko , Patricia Mart\u00edn-Chozas Type of Element Total number\nSentences in the corpus 1568\nTokens in the corpus 54849\nAnnotated samples 133\nPositive samples 97\nNegative samples 36\nLegal agents 127\nLegal entities 86\nSubjects 90\nObjects 69\nComplements 100\nTable 1: Statistics of the corpus and the an-\nnotated data.\n4 Models\nFor this work we focused on joint multi-task\ndeep learning classification models as they\nachieve state of the art results on common\nevent extraction datasets, see also Section 8.\nAs these common datasets are in English, for\nthe final choice of the model it was impor-\ntant for us that the model code is publicly\navailable, so that we can reuse the model and\nthat the base model is either multilingual or\ncan be changed to multilingual. Finally, we\nproceed with two models: GRIT (Du, Rush,\nand Cardie, 2021) and Text2Event (Lu et al.,\n2021). For both models\nGRIT The model GRIT is a generative\nrole-filler transformer model, i.e. it is capable\nof identifying the (predefined) roles of entities\nin text. In order to apply GRIT to event ex-\ntraction task we found it necessary to declare\nthe trigger as a separate role and extend the\nmodel to also classify the class (event type) of\nthe input text. The extended model is a joint\ngenerative event extraction model. The base\nmodel of GRIT is BERT (Devlin et al., 2018),\nwe used the model with BETO (Ca\u02dc nete et al.,\n2020) that is a BERT model trained on a big\nSpanish corpus.\nText2Event The event extraction model is\nText2Event. This model relies on the de-\nscription or names of roles, more precisely\nthe names of roles are input to the language\nmodel together with the legal text. There-\nfore, we translated the names of the roles\nand also the names of the Hohfeld classes to\nSpanish. As recommended by the authors\nof the original paper, we have pre-trained\nthe model on ACE dataset, and only thenfine-tuned the model on our dataset6. The\nbase model of Text2Event is T5 (Raffel et\nal., 2020) pre-trained on English corpora, we\nused Text2Event with multilingual T5 (mT5)\n(Xue et al., 2020).\nRoles The final set of roles for both models\nconsists of trigger, subject, object, and com-\nplement roles as described in Section 2.\n5 Results and Evaluation\nFor the evaluation of the performance of our\nmodel we will use well established metrics\nsuch as precision (P ), recall (R ) and F1score.\nLet the gold standard be the correct manu-\nally annotated data. Let the true positives\n(TP) be all the correctly predicted relations;\nfalse positives (FP) \u2013 incorrectly predicted\nrelations; false negatives (FN) \u2013 those cases\nwhen a relation is not predicted, though it\ndoes exist in the gold standard; true neg-\natives (TN) \u2013 the relation is not predicted\nand it does not exist in the gold standard.\nThen P\u201cTP\nTP`FP,R\u201cTP\nTP`FNandF1\u201c\n2\u02daP\u02daR\nP`R. These measures are well established\nand widely used for evaluation of different\nclassification models, see also Section 8. It\nshould be noted that we use strict scores, i.e.\nonly exactly correct matches are counted as\ntrue predictions.\nFor computing the results we used the 126\nsamples from the training set in the follow-\ning split: 116 samples used for training and\n10 samples used for development set. The\ntest set consists of 20 samples, all the results\nare reported for the test set. This setup is\napplied to both models. The training was\nperformed with default parameters as set by\nthe authors of the original models, except for\nthe extension of GRIT and the change of the\nbase models as described in Section 4.\nThe comparison of the models is in Table\n2. The two columns present the F1scores for\nthe task of classifying all roles including \u201ctrig-\nger\u201d role and classifying the Hohfeld class,\nrespectively. The Text2Event model outper-\nforms the GRIT model by a significant mar-\ngin. Manual checks of the results confirm\nthis finding. A possible explanation of the\ndifference in performance could by the abil-\nity of the Text2Event model to include the\n6Though the roles and the language in the ACE\ndataset are quite different from our use case, we use\nACE in the pre-training to enable the model to learn\nthe constrained generative language as suggested by\nthe authors of the original Text2Event paper.\n109\nExtraction and Semantic Representation of Domain-Specific Relations in Spanish Labour Law model name F1roles F1Hohfeld class\nGRIT 0.26 0.65\nText2Event 0.47 0.82\nTable 2: Comparison of F1scores achieved by\nGRIT and Text2Event models. \u201c F1roles\u201d\nis the F1score of extracting all the differ-\nent roles, including the trigger. \u201cF 1Hohfeld\nclass\u201d is the F1of the legal relation classifi-\ncation.\ndata F1 p r\nall roles 0.47 0.45 0.50\n\u00ebtrigger 0.82 0.75 0.90\n\u00ebsubject 0.57 0.50 0.67\n\u00ebobject 0.00 0.00 0.00\n\u00ebcomplement 0.45 0.42 0.50\nHohfeld class 0.82 0.75 0.90\nTable 3: Detailed scores of Text2Event\nmodel.\nnames of roles and classes, i.e. \u201cderecho\u201d,\n\u201csujeto\u201d, etc., as input to the model. This\nability allows for pre-training on the large\nACE dataset and efficient knowledge trans-\nfer for the few-shot learning with our labour\nlaw dataset even despite the different types\nof events, different domain and different lan-\nguage (English) of the pre-training dataset.\nMore detailed results for the better per-\nforming Text2Event model are presented in\nTable 3. The model can classify and ex-\ntract triggers with good confidence, however,\nthe model experiences difficulties with other\nroles, in particular with identifying objects.\nManual investigation of the final results re-\nveals additional problems in identifying the\ncomplements. Moreover, model rarely pre-\ndicts other classes than \u201cRight\u201d and \u201cDuty\u201d.\nNevertheless, the Hohfeld classes, subjects,\nobjects, and triggers are predicted with F1\nscores in the range of 0.5-0.8, which we con-\nsider to be reasonably good.\n6 Triplification\nThe second part of our work is focused on the\npublication of the results obtained following\nSemantic Web formats. Normally, the results\nof this type of experiments are usually pub-\nlished in unstructured formats, such as txt,\nor semi-structured, such as csv.\nIn this case, we consider that it is highly\nimportant to publish these data in struc-\ntured, open and machine-readable formats,to support their reuse and update. This\nis possible thanks to the data models of\nthe Semantic Web. In this specific case,\nwe have combined linguistic data represen-\ntation models with legal information repre-\nsentation models, that are described in Sec-\ntion 6.1. Consequently, we have transformed\nour results following those vocabularies, as\nexplained in Section 6.2, and the resulting\ndataset is presented in Section 6.3.\n6.1 Ontology selection\nAs mentioned in the Motivation (Section\n1.2), the results of this experiment are to\nbe transformed into a labour law knowledge\ngraph, with the aim of generating a rich re-\nsource of concepts and relations that can be\napplied to other NLP tasks in the future.\nTherefore, to represent these relations, we\nchose the Provision Model mentioned in Sec-\ntion 8 since, although LegalRuleML also al-\nlows the representation of deontic operators,\nthe Provision Model contains classes corre-\nsponding to the potestative relations in case\nwe would like to include them in the future.\nOn the other hand, to represent the lin-\nguistic information apply SKOS7and la-\nbelling properties from RDF Schema8.\nIn this case, since the envisioned output\nis a rich terminological resource with many\ndifferent kind of data, we need both vocabu-\nlaries to represent the complexity of the in-\nformation contained. Additionally, we com-\nbine them with other ontologies such as the\nSchema data models9, to add extra informa-\ntion to the relation, as explained in the fol-\nlowing section.\n6.2 Schema\nThe heuristics behind the semantic represen-\ntation of this dataset are as follows:\n\u2022Every time we find a Hohfeld rela-\ntion, we create a new Hohfeld class\nwith the Provision Model, depending on\nthe nature of the relation. Therefore,\nthis would be prv:Right, prv:Duty,\nprv:Prohibition orprv:Permission.\n\u2022Following the nomenclature of the Pro-\nvision Model, every class needs to have\naBearer , aCounterpart and, optionally,\nanObject. These elements correspond\n7https://www.w3.org/TR/skos-reference/\n8https://www.w3.org/TR/rdf-schema/\n9https://schema.org/\n110\nArtem Revenko , Patricia Mart\u00edn-Chozas to the Subject, Object andComplement\nfrom our annotations, respectively.\nThese items are represented with the\nclass skos:Concept, and they are linked\nto the Hohfeld class with different\nproperties: prv:hasRightBearer,\nprv:hasRightCounterpart and\nprv:hasRightObject.\n\u2022The skos:Concepts are thought to be\nURIs, to assign unambiguous identifiers\nto each Hohfeld element. To repre-\nsent their labels, we use the rdfs:label\nproperty.\n\u2022Additionally, we also include the relation\ntrigger in this data model, that is rep-\nresented with the class schema:Action,\nand linked to the Hohfeld class with the\nproperties prv:hasRightAction,\nprv:hasDutyAction,\nprv:hasProhibitionAction or\nprv:hasPermissionAction.\n6.3 Dataset in RDF\nFirst, we have split the Statute into individ-\nual sentences, that were used one by one as\ninput to our fine-tuned Text2Event model.\nThen we recorded the results. For each sen-\ntence that is classified into one of Hohfeld\nclasses we created a subgraph as described in\nSection 6.2. The statistics of the resulting\ndataset are presented in Table 4.\nType of Element Total number\nHohfeld classes 791\nRight 578\nDuty 213\nSubjects 659\nObjects 31\nComplements 312\nTable 4: Statistics of the resulting dataset.\n7 Exploitation\nIn this section, we translate the questions in\nnatural language formulated at the end of\nSection 1.2 into SPARQL queries10, to exem-\nplify how to navigate through the generated\ngraph.\nFirst, in Listing 1, we collect all the pre-\nfixes that are needed to formulate the rest of\nthe queries. These prefixes identify the vo-\ncabularies that have been used to structure\n10https://www.w3.org/TR/rdf-sparql-query/the data: RDF and RDF Schema, Provision\nModel, Schema and SKOS.\nNow, Listing 2 formalises Question 1, as\nstated in Section 1.2: In which situations\ncan a worker claim for a right?. There-\nfore, we look for something (?s) that is a\nright (prv:Right), which has a right bearer\n(prv:hasRightBearer), whose ID we do not\nknow (?bearer), and that has right action\n(prv:hasRightAction), whose ID we do not\nknow either (?action). However, we do\nknow their labels (rdfs:label), which are\ntrabajador in Spanish (@es) for the bearer,\nand podr\u00b4 a reclamar in Spanish (@es) for\nthe action. The result of this query are\nthree URIs which are the identifiers of the\nright instances that satisfy these criteria:\nhttp://www.testuri.com/test hohfeld#1032;\nhttp://www.testuri.com/test hohfeld#762;\nhttp://www.testuri.com/test hohfeld#764.\nOn the other hand, Listing 3 formalises\nthe Question 2: When must a worker come\nback to work after a leave from work? . In\nthis case, we look for something (?s) that\nis a duty (prv:Duty), with a duty bearer\n(prv:hasDutyBearer) and a duty action,\nwhich is the trigger (prv:hasDutyAction).\nHere, the label of the duty bearer is tra-\nbajador, and the label of the duty action,\nwhich is of the type schema:Action, is de-\nber\u00b4 a reincorporarse. The result of this\nquery is a URI which is the identifier of\nthe right instance that satisfies this criteria:\nhttp://www.testuri.com/test hohfeld#1051\nAdditionally, since we already have ob-\ntained the ID of a given instance, we could\nmake a simple query to retrieve the textual\nexcerpt (with the property skos:note) from\nwhich the relation has been extracted, as ex-\nposed in Listing 4. The result of this query\nis the following excerpt: En los supuestos de\nsuspensi\u00b4 on por ejercicio de cargo p\u00b4 ublico rep-\nresentativo o funciones sindicales de \u00b4 ambito\nprovincial o superior, el trabajador deber\u00b4 a\nreincorporarse en el plazo m\u00b4 aximo de treinta\nd\u00b4 \u0131as naturales a partir de la cesaci\u00b4 on en el\ncargo o funci\u00b4 on.\n8 Related Work\nEvent Extraction In this work we fo-\ncus on sentence-level event extraction and\nconsider document-level extraction for future\nwork. Event extraction task has recently re-\nceived widespread attention (Liu, Min, and\nHuang, 2021). Most work in event extraction\n111\nExtraction and Semantic Representation of Domain-Specific Relations in Spanish Labour Law Model name Trigger F1Roles F1\nOneIE 72.8 54.8\nText2Event 71.8 54.4\nTable 5: State of the art results on ACE\ndataset.\nhas focused on the ACE sentence level event\ntask (Walker et al., 2006), which requires the\ndetection of an event trigger and extraction of\nits arguments from within a single sentence.\nThis dataset consists of 599 documents and\nincludes 8 event types and 6000 individual\nevents. Further important dataset is MUC-4\n(muc, 1992) with 1700 documents, 400 to-\nkens per document on average. Note that\nthese datasets are significantly larger than\nthe one we consider in this paper. Most exist-\ning event extraction dataset are available in\nEnglish language, no event extraction dataset\nin Spanish is known to us.\nThe most prominent approaches to solving\nthe task include\n1. Decomposition into subtasks such as en-\ntity recognition and argument classifica-\ntion (Ma et al., 2020; Zhang et al., 2020);\n2. Semantic grounding, i.e. mapping en-\ntities to external knwoledge sources\n(Zhang, Wang, and Roth, 2020; Huang\net al., 2018);\n3. Question-answering based approaches\n(Zhou et al., 2021; Liu et al., 2020);\n4. Joint multi-task classification models\n(Lin et al., 2020; Paolini et al., 2021; Du,\nRush, and Cardie, 2021; Lu et al., 2021).\nRecent state of the art results are achieved\nby the joint multi-task deep learning models.\nGRIT (Du, Rush, and Cardie, 2021) achieves\njoint (for classifying all roles) F1score of\n54.5 on MUC-4 dataset. The results on ACE\ndataset are collected in Table 5.\nLegal Ontologies Regarding the represen-\ntation of legal information in Semantic Web\nformats, we find many approaches depending\non the type of data and on the purpose of the\nontology. Likewise, the type of data depends\non the legal subarea to which a resource be-\nlongs (labour law, civil law, etc.) and on the\ntype of legal document (provisions, rules, li-\ncenses...). The purpose of the ontology is\nalso varied. Some are merely intended tostructure the information while others are de-\nsigned to reason over data and infer knowl-\nedge.\nAmongst the most used ontologies to\nstructure legal documents we find the ELI on-\ntology11, the European Legislation Identifier.\nThis vocabulary is widely used by the pub-\nlishers of legal data in the European Union to\nrepresent metadata of legislative documents\nas Linked Data. To complement ELI, the\nPublications Office of the European Union\nalso applies the Common Data Model (CDM)\nvocabulary and the Functional Requirements\nfor Bibliographic Records (FRBR) to repre-\nsent legal resources and their relationships.\nApart from those ontologies intended to\nrepresent legal documents, we also find well-\nknown vocabularies to represent common\ngeneral terms in the legal domain such as\nAkoma Ntoso, which was created as an XML\nstandard and afterwards evolved to an ontol-\nogy (Palmirani and Vitali, 2011), and Legal-\nRuleML (Athan et al., 2015), that is able\nto represent the particularities of the legal\nnormative rules with a rich, articulated, and\nmeaningful markup language. Similarly, we\nfind the Provision Model (Biagioli, 1996), to\nannotate rules and rule ammendments in nor-\nmative provisions, that was subsequently ex-\ntended in (Francesconi, 2016), to cover Ho-\nhfeld\u2019s relations (which are described in Sec-\ntion 2).\nIn this section, we have mentioned those\nontologies that are directly related to our\nwork. For more information about legal on-\ntologies, we refer the reader to more compre-\nhensive surveys such as (Valente, 2005) and\n(de Oliveira Rodrigues et al., 2019).\n9 Conclusions and Future Work\nIn this paper we experimented with pre-\ntrained multilingual language models for ex-\ntracting knowledge from a domain-specific\nlabour law corpus in Spanish. We for-\nmalised the task as sentence-level event ex-\ntraction and applied two models: GRIT\nand Text2Event. To train and evaluate the\nmodel, we annotated the Workers\u2019 Statute\nwith 133 individual sentences containing Ho-\nhfeld roles and relations. The latter model\n(Text2Event) outperforms the former by a\nsignificant margin of around 0.2 of F1scores\nboth on identifying the roles and classifying\n11https://op.europa.eu/es/web/\neu-vocabularies/eli\n112\nArtem Revenko , Patricia Mart\u00edn-Chozas into Hohfeld classes. Text2Event obtains a\nsatisfying results above 0.8 F1score for clas-\nsifying Hohfeld classes. The model also effi-\nciently extracts the triggers (F 1\u00ab0.75), but\nlooses the quality for other roles (F 1\u00ab0.5\nfor all roles including trigger).\nFurthermore, we split the Workers\u2019\nStatute into individual sentences and apply\nthe fine-tuned Text2Event model on this in-\nput. The results of this automatic informa-\ntion extraction tasks have been also triplified,\nfollowing existing Semantic Web vocabular-\nies, such as SKOS for concepts and the Provi-\nsion Model for Hohfeld relations. The result-\ning labour law knowledge graph is publicly\navailable for to be reused by the community.\nFuture Work In the short term, we plan\nto develop a post-processing script based on\nNLP rules to clean the results with the aim\nof improving precision. This idea is based\non the observation that the current models\nsometimes interchange relation agents with\nthe relation complement, which we think that\ncould be avoided with a role labeling task\nover the results. Furthermore, we focus on\nextracting sentence-level roles and relations\nas most relations are expressed in a single\nsentence. However, we note that the number\nof relations spanning over multiple sentences\nis not negligible. Hence, in the future work\nwe also plan to experiment with extracting\nrelations beyond sentence level.\nRegarding the representation in RDF, we\nplan to add more linguistic information to the\ngraph, linking it to existing legal resources\npublished in Semantic Web formats, such as\nEuroVoc12. We may also want to link our\nlabour law graph with more general resources\nsuch as Wikidata13, to extend the graph with\ninformation from a wider scope. To represent\nthis additional linguistic data we plan to use\nOntolex14to complement SKOS. This combi-\nnation is widely applied to represent language\nresources in the Semantic Web: while SKOS\nis used for thesauri and concept schemes, On-\ntolex is intended to represent lexical informa-\ntion such as dictionaries.\nFigure 3 shows an example of the seman-\ntic representation of a right relation amongst\nthe subject \u201ctrabajador\u201d (worker ), the ob-\nject \u201cadministraci\u00b4 on p\u00b4 ublica\u201d (public admin-\n12https://eur-lex.europa.eu/browse/eurovoc.\nhtml\n13https://www.wikidata.org/\n14https://www.w3.org/2016/05/ontolex/istration ) and the complement \u201ccertificado de\nprofesionalidad \u201d (professional certification),\nbeing these labels represented with the On-\ntolex vocabulary. In the example, terms are\nalso linked with matches in EuroVoc and\nWikidata.\nAcknowledgements\nThis work has been supported by the Eu-\nropean Union\u2019s Horizon 2020 research and\ninnovation programme through the Pr\u02c6 et-` a-\nLLOD15project, with grant agreement No.\n825182, and by COST (European Cooper-\nation in Science and Technology) through\nNexusLinguarum, the \u201cEuropean network for\nWeb-centred linguistic data science\u201d COST\nAction (CA18209)16.\nReferences\n1992. Fourth Message Uunderstanding Con-\nference (MUC-4): Proceedings of a Con-\nference Held in McLean, Virginia, June\n16-18, 1992.\nAhn, D. 2006. The stages of event ex-\ntraction. In Proceedings of the Workshop\non Annotating and Reasoning about Time\nand Events, pages 1\u20138, Sydney, Australia,\nJuly. Association for Computational Lin-\nguistics.\nAthan, T., G. Governatori, M. Palmirani,\nA. Paschke, and A. Wyner. 2015. Legal-\nruleml: Design principles and founda-\ntions. In Reasoning Web International\nSummer School, pages 151\u2013188. Springer.\nBerners-Lee, T. 2006. Design issues.\nBerners-Lee, T., J. Hendler, and O. Lassila.\n2001. The semantic web. Scientific amer-\nican, 284(5):34\u201343.\nBiagioli, C. 1996. Law making environment:\nmodel based system for the formulation,\nresearch and diagnosis of legislation. Ar-\ntificial Intelligence and Law .\nCa\u02dc nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u00b4 erez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. In PML4DC at ICLR 2020 .\nChandrasekaran, B., J. R. Josephson, and\nV. R. Benjamins. 1999. What are ontolo-\ngies, and why do we need them? IEEE\nIntelligent Systems and their applications .\n15https://pret-a-llod.eu/\n16https://nexuslinguarum.eu/\n113\nExtraction and Semantic Representation of Domain-Specific Relations in Spanish Labour Law prv:Right\nright_1prv:hasRightBearer\nprv:hasRightCounterpart\nprv:hasRightObject\nprv:hasRightActionhttp://eurovoc.europa.eu/4557skos:exactMatch\nhttp://eurovoc.europa.eu/77\nhttps://www.wikidata.org/wiki/Q196756skos:broadMatchskos:Concept\nterm_1\nskos:Concept\nterm_2skos:exactMatch\nskos:Concept\nterm_3writtenRep  \n\"trabajador\"@es  ontolex:Form ontolex:LexicalEntry\nlexical_entry_1\nontolex:LexicalEntry\nlexical_entry_2\nontolex:LexicalEntry\nlexical_entry_3writtenRep  \n\"administraci\u00f3n p\u00fablica\"@es  ontolex:Form\nwrittenRep \"certificado de\nprofesionalidad\"@es  ontolex:Form\nschema:Action\naction_1  \n\"podr\u00e1 solicitar\"@es  ontolex:isEvokedBy\nontolex:isEvokedBy\nontolex:isEvokedByontolex:form\nontolex:form\nontolex:formFigure 3: Example of a right modeled with SKOS and Ontolex, and linked with external knowl-\nedge bases.\nde Oliveira Rodrigues, C. M., F. L. G. de Fre-\nitas, E. F. S. Barreiros, R. R. de Azevedo,\nand A. T. de Almeida Filho. 2019. Legal\nontologies over time: A systematic map-\nping study. Expert Systems with Applica-\ntions, 130:12\u201330.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2018. BERT: Pre-training\nof Deep Bidirectional Transformers for\nLanguage Understanding. arXiv e-prints ,\npage arXiv:1810.04805, October.\nDoddington, G., A. Mitchell, M. Przy-\nbocki, L. Ramshaw, S. Strassel, and\nR. Weischedel. 2004. The automatic con-\ntent extraction (ACE) program \u2013 tasks,\ndata, and evaluation. In Proceedings\nof the Fourth International Conference\non Language Resources and Evaluation\n(LREC\u201904), Lisbon, Portugal, May. Eu-\nropean Language Resources Association\n(ELRA).\nDu, X., A. Rush, and C. Cardie. 2021.\nGRIT: Generative role-filler transformers\nfor document-level event entity extraction.\nInProceedings of the 16th Conference of\nthe European Chapter of the Association\nfor Computational Linguistics: Main Vol-\nume, pages 634\u2013644, Online, April. Asso-\nciation for Computational Linguistics.\nFrancesconi, E. 2016. Semantic model for\nlegal resources: Annotation and reasoning\nover normative provisions. Semantic Web,\n7(3):255\u2013265.Hendrickx, I., S. N. Kim, Z. Kozareva,\nP. Nakov, D. \u00b4O. S\u00b4 eaghdha, S. Pad\u00b4 o,\nM. Pennacchiotti, L. Romano, and S. Sz-\npakowicz. 2019. SemEval-2010 Task 8:\nMulti-Way Classification of Semantic Re-\nlations Between Pairs of Nominals. arXiv\ne-prints, page arXiv:1911.10422, Novem-\nber.\nHohfeld, W. N. 1913. Some fundamental le-\ngal conceptions as applied in judicial rea-\nsoning. Yale Lj, 23:16.\nHuang, L., H. Ji, K. Cho, I. Dagan, S. Riedel,\nand C. Voss. 2018. Zero-shot transfer\nlearning for event extraction. In Proceed-\nings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 2160\u2013\n2170, Melbourne, Australia, July. Associ-\nation for Computational Linguistics.\nLin, Y., H. Ji, F. Huang, and L. Wu. 2020. A\njoint neural model for information extrac-\ntion with global features. In Proceedings\nof the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages\n7999\u20138009, Online, July. Association for\nComputational Linguistics.\nLiu, J., Y. Chen, K. Liu, W. Bi, and X. Liu.\n2020. Event extraction as machine read-\ning comprehension. In Proceedings of the\n2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) ,\npages 1641\u20131651, Online, November. As-\nsociation for Computational Linguistics.\n114\nArtem Revenko , Patricia Mart\u00edn-Chozas Liu, J., L. Min, and X. Huang. 2021.\nAn overview of event extraction and\nits applications. arXiv e-prints , page\narXiv:2111.03212, November.\nLu, Y., H. Lin, J. Xu, X. Han, J. Tang,\nA. Li, L. Sun, M. Liao, and S. Chen. 2021.\nText2Event: Controllable sequence-to-\nstructure generation for end-to-end event\nextraction. In Proceedings of the 59th An-\nnual Meeting of the Association for Com-\nputational Linguistics and the 11th In-\nternational Joint Conference on Natural\nLanguage Processing (Volume 1: Long Pa-\npers), pages 2795\u20132806, Online, August.\nAssociation for Computational Linguis-\ntics.\nMa, J., S. Wang, R. Anubhai, M. Ballesteros,\nand Y. Al-Onaizan. 2020. Resource-\nenhanced neural model for event argument\nextraction. In Findings of the Association\nfor Computational Linguistics: EMNLP\n2020, pages 3554\u20133559, Online, Novem-\nber. Association for Computational Lin-\nguistics.\nMart\u00b4 \u0131n-Chozas, P. and A. Revenko. 2021.\nThesaurus enhanced extraction of ho-\nhfeld\u2019s relations from spanish labour law.\nInProceedings of the 2nd International\nWorkshop on Deep Learning meets On-\ntologies and Natural Language Process-\ning (DeepOntoNLP 2021) co-located with\n18th Extended Semantic Web Conference\n2021, volume 2918, pages 30\u201338. CEUR-\nWS.org.\nOliver, A. and M. V` azquez. 2015. Tbxtools:\na free, fast and flexible tool for automatic\nterminology extraction. In Proceedings of\nthe International Conference Recent Ad-\nvances in Natural Language Processing.\nPalmirani, M. and F. Vitali. 2011. Akoma-\nntoso for legal documents. In Legisla-\ntive XML for the semantic Web. Springer,\npages 75\u2013100.\nPaolini, G., B. Athiwaratkun, J. Krone,\nJ. Ma, A. Achille, R. ANUBHAI, C. N.\ndos Santos, B. Xiang, and S. Soatto.\n2021. Structured prediction as trans-\nlation between augmented natural lan-\nguages. In International Conference on\nLearning Representations.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li,and P. J. Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-\nto-text transformer. Journal of Machine\nLearning Research, 21(140):1\u201367.\nRobaldo, L., S. Villata, A. Wyner, and\nM. Grabmair. 2019. Introduction for\nartificial intelligence and law: special is-\nsue \u201cnatural language processing for legal\ntexts\u201d.\nValente, A. 2005. Types and roles of legal\nontologies. In Law and the semantic web .\nSpringer, pages 65\u201376.\nWalker, C., S. Strassel, J. Medero, and\nK. Maeda. 2006. Ace 2005 multilingual\ntraining corpus. Linguistic Data Consor-\ntium, Philadelphia , 57:45.\nXue, L., N. Constant, A. Roberts, M. Kale,\nR. Al-Rfou, A. Siddhant, A. Barua,\nand C. Raffel. 2020. mT5: A mas-\nsively multilingual pre-trained text-to-\ntext transformer. arXiv e-prints, page\narXiv:2010.11934, October.\nZhang, H., H. Wang, and D. Roth. 2020. Un-\nsupervised Label-aware Event Trigger and\nArgument Classification. arXiv e-prints,\npage arXiv:2012.15243, December.\nZhang, Z., X. Kong, Z. Liu, X. Ma, and\nE. Hovy. 2020. A two-step approach for\nimplicit event argument detection. In Pro-\nceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguis-\ntics, pages 7479\u20137485, Online, July. As-\nsociation for Computational Linguistics.\nZhou, Y., Y. Chen, J. Zhao, Y. Wu,\nJ. Xu, and J. Li. 2021. What the\nrole is vs. what plays the role: Semi-\nsupervised event argument extraction via\ndual question answering. Proceedings of\nthe AAAI Conference on Artificial Intel-\nligence , 35(16):14638\u201314646, May.\n115\nExtraction and Semantic Representation of Domain-Specific Relations in Spanish Labour Law PREFIX rdf:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX prv:<http://www.ittig.cnr.it/ontologies/def/ProvisionModel#>\nPREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>\nPREFIX schema:<https://schema.org/>\nPREFIX skos: <http://www.w3.org/2004/02/skos/core#>\nListing 1: Prefixes used in the SPARQL queries over the resulting dataset.\nSELECT ?s\nWHERE {\n?s rdf:type prv:Right;\nprv:hasRightBearer ?bearer ;\nprv:hasRightAction ?action .\n?bearer rdfs:label \"trabajador\"@es .\n?action rdfs:label \"podr\u00b4 a reclamar\"@es.\n}\nListing 2: Example of Question 1 translation into SPARQL.\nSELECT ?s\nWHERE {\n?s rdf:type prv:Duty;\nprv:hasDutyBearer ?bearer ;\nprv:hasDutyAction ?action .\n?bearer rdfs:label \"trabajador\"@es .\n?action rdf:type schema:Action ;\nrdfs:label \"deber\u00b4 a reincorporarse\"@es . }\nListing 3: Example of Question 2 translation into SPARQL.\nSELECT *\nWHERE {\n<http://www.testuri.com/test_hohfeld#1051> skos:note ?note .\n}\nListing 4: SPARQL query to retrieve the textual excerpt of a given duty.\n116\nArtem Revenko , Patricia Mart\u00edn-Chozas A Semantic-Proximity Term-Weighting Scheme\nf\nor Aspect Category Detection\nPonderaci\u00b4 on de T\u00b4 erminos basada en Proximidad Sem\u00b4 antica\npara la Detecci\u00b4 on de Categor\u00b4 \u0131as de Aspecto\nMonserrat V\u00b4 azquez-Hern\u00b4 andez, Luis Villase\u02dc nor-Pineda, Manuel Montes-y-G\u00b4 omez\nInstituto Nacional de Astrof\u00b4 \u0131sica, \u00b4Optica y Electr\u00b4 onica (INAOE), Puebla, M\u00b4 exico\n{mvazquez, villasen, mmontesg}@inaoep.mx\nAbstract: Aspect category detection is a subtask of aspect-level sentiment analysis,\nwhich aims at identifying the aspect categories present in an opinion. It is a di\ufb03cult\ntask because the category must be inferred from the terms of the opinion, and also\nbecause each opinion may include judgments for more than one aspect category. In\nrecent years, the use of attention mechanisms has improved performance in di\ufb00erent\ntasks, allowing the identi\ufb01cation and prioritization of terms that mostly contribute\nto the classi\ufb01cation. However, in multi-label problems, such as aspect category de-\ntection, di\ufb00erent terms must be selected based on each category, which is a drawback\nfor these models. Motivated by the same idea of identifying and highlighting the\nimportance of terms, this paper proposes a weighing scheme that emphasizes terms\nin an opinion based on their semantic proximity to each aspect category. The pro-\nposed scheme has been evaluated on di\ufb00erent SemEval datasets, demonstrating its\ne\ufb00ectiveness in this multi-label scenario. Moreover, it can be applied in scenarios\nwith limited training data and can be combined with di\ufb00erent classi\ufb01cation models,\nincluding deep neural networks.\nKeywords: semantic proximity, weighting of terms, aspect category detection.\nResumen: La detecci\u00b4 on de categor\u00b4 \u0131as de aspecto es una subtarea dentro del an\u00b4 alisis\ndesentimientos anivelde aspecto. Estasubtareaabordalaidenti\ufb01caci\u00b4 on de aquellas\ncategor\u00b4 \u0131asde aspecto presentes en una opini\u00b4 on. Se trata de una tareadesa\ufb01ante pues\nla categor\u00b4 \u0131a debe inferirse de los t\u00b4 erminos de la opini\u00b4 on, aunado a esto, una opini\u00b4 on\npuede incluir evaluaciones de m\u00b4 as de una categor\u00b4 \u0131a de aspecto. En los \u00b4 ultimos a\u02dc nos,\nel uso de mecanismos de atenci\u00b4 on ha permitido mejorar los resultados en distintas\ntareas, \u00b4 estos permiten identi\ufb01car y priorizar los t\u00b4 erminos clave que contribuyen a\nla clasi\ufb01caci\u00b4 on. Sin embargo, en problemas multi-etiqueta, como la detecci\u00b4 on de\ncategor\u00b4 \u0131as de aspecto, se deben seleccionar diferentes t\u00b4 erminos dependiendo de cada\ncategor\u00b4 \u0131a lo cual es un inconveniente para estos modelos. Motivados por esta misma\nidea de identi\ufb01car y destacar la importancia de t\u00b4 erminos clave, en este trabajo se\npropone un esquema que permite enfatizar los t\u00b4 erminos de una opini\u00b4 on en funci\u00b4 on\nde suproximidad sem\u00b4 antica a cada categor\u00b4 \u0131a de aspecto. El esquema propuesto\nse evalu\u00b4 o en distintos conjuntos de datos de SemEval demostrando su efectividad\nen este escenario multi-etiqueta. Adem\u00b4 as, es posible aplicarlo a pesar de contar\ncon pocos datos de entrenamiento, y puede combinarse con distintos modelos de\nclasi\ufb01caci\u00b4 on, incluyendo redes neuronales profundas.\nPalabras clave: proximidad sem\u00b4 antica, ponderaci\u00b4 on de t\u00b4 erminos, detecci\u00b4 on de\ncategor\u00b4 \u0131as de aspecto.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 117-127\nrecibido 31-03-2022 revisado 21-05-2022 aceptado 23-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-10\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introduction\nSe\nntiment Analysis aims to identify emo-\ntions, attitudes or opinions in a subjective\ntext about a product, service or topic of in-\nterest (Liu and Zhang, 2012). Di\ufb00erent con-\nsumers may have di\ufb00erent opinions about\nthe same product or service. Through their\nopinions, consumers express their approval\nor rejection on particular aspects that they\nwish to highlight, which poses the challenge\nof grouping the opinions into di\ufb00erent pre-\nde\ufb01ned aspect categories in order to iden-\ntify relevant groups (L\u00b4 opez Ramos and\nArco Garc\u00b4 \u0131a, 2019). This challenge is tack-\nled by the sub-task of Aspect Category De-\ntection.\nAspect Category Detection (ACD) at-\ntempts to identify the general concepts to\nwhich each of the di\ufb00erent aspects named in\nan opinion belong (Pontiki et al., 2016). For\nexample, given the opinion: \u201cthe spaghetti\nwas tasteless but the sta\ufb00 was nice\u201d, the\naspects named are \u201cspaghetti\u201d and \u201csta\ufb00\u201d,\nand the corresponding categories are \u201cfood\u201d\nand \u201cservice\u201d. ACD is a multi-label problem\nsince more than one aspect can be evaluated\nin an opinion and each one corresponds to a\nspeci\ufb01c category.\nIdentifying the terms associated with the\ndi\ufb00erent categories is a di\ufb03cult task because\nthe category must be inferred from the con-\ntext. One possibility for this is to observe\ndi\ufb00erent modi\ufb01ers in an opinion. Through\nthe nature of each category, it is possible to\ninfer the associated terms. For example, in a\nrestaurant, the term \u201ctasteless\u201d is used to de-\nscribe the \u201cfood\u201d but not the \u201csta\ufb00\u201d. In this\ncontext, a common approach used to address\nthis subtask is the use of lexicons (Mowlaei,\nSanieeAbadeh, andKeshavarz, 2020). Meth-\nods based on this approach perform category\ndetection using sets of words to identify the\ncorresponding categories. However, the con-\nstruction of these lexicons is di\ufb03cult, expen-\nsive, domain and language dependent.\nRecently, deep learning approaches using\nattention mechanisms have been applied to\naddress this task. These mechanisms exam-\nine the context of a sentence and identify and\nprioritize the most relevant terms for its clas-\nsi\ufb01cation. In same way, they are like lexicon-\nbased approaches in that they emphasize the\nmost relevant terms associated with a cate-\ngory, except that these approaches do so au-\ntomatically without relying on external re-sources (Chaudhari et al., 2021). Unfortu-\nnately, these approaches have some draw-\nbacks for this application. On the one hand,\nthis is a multi-label problem, so the terms\nrelated to all di\ufb00erent aspect categories men-\ntioned in a single text must be jointly iden-\nti\ufb01ed (Movahedi et al., 2019). On the other\nhand, like any deep learning approach, they\nrequire large training sets to achieve good re-\nsults (Chaudhari et al., 2021), and for this\nsubtask datasets are usually limited and also\nhighly imbalanced.\nSimilartopreviousworks, inthispaperwe\npropose a new term weighting scheme whose\naim is also to identify and prioritize terms\nassociated with each aspect category. Based\non a given set of category-oriented lexicons,\nwhich may have been manually or automat-\nically de\ufb01ned, the proposed scheme weights\neach term in an opinion according to its se-\nmantic proximity to the di\ufb00erent aspect cat-\negories. To do this, it considers pre-trained\nword embeddings; in a \ufb01rst step it computes\na representative vector for each category, and\nthen, in a second step, it measures the sim-\nilarity of each term vector with respect to\neach category vector. Accordingly, the terms\nthat contribute the most to identify each cat-\negory are highlighted before feeding the clas-\nsi\ufb01cation algorithm, acting as a kind of non-\nsupervised pre-attention mechanism. In this\nmanner, the solution proposed can deal with\nmulti-label problems, is less sensitive to data\nscarcity and distribution, and can be com-\nbined with di\ufb00erent classi\ufb01cation models, in-\ncluding neural networks.\nThe evaluation of the proposed approach\nwas carried out on datasets from SemEval\n(Pontiki et al., 2016), considering English\nand Spanish languages, two di\ufb00erent appli-\ncation domains, as well as several works for\ncomparison purposes.\nSummarizing, the two main contributions\nof this paper are:\n\u2022A new term weighting scheme specially\nsuited to the aspect category detec-\ntion task, which acts as a kind of non-\nsupervised attention mechanism.\n\u2022Adetailed study on the e\ufb00ectiveness and\nadaptability of the proposed weighting\nscheme, considering di\ufb00erent languages,\ndomains and classi\ufb01cation models.\nTheremainderofthepaperisorganizedas\n118\nMonserrat V\u00e1zquez-Hern\u00e1ndez, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez follows. Section 2 presents a brief overview of\npr\nevious work on aspect category detection.\nSection 3 describes the proposed weighting\nscheme. Sections 4 reports the experiments,\nresults, and their analysis. Finally, Section 5\npoints out our conclusions and future work.\n2 Related work\nAspect category detection is a subtask of\naspect-based sentiment analysis, which at-\ntempts to assign a subset of categories from a\nset of prede\ufb01ned aspect categories to a given\nopinion (L\u00b4 opez Ramos and Arco Garc\u00b4 \u0131a,\n2019). This subtask was introduced and de-\n\ufb01ned at the SemEval workshop: \u201cAn as-\npect category expresses, in a general way, the\ncharacteristics evaluated of an entity. Aspect\ncategories are usually not de\ufb01ned by terms\npresent in opinions instead they are inferred\nthrough terms used to evaluate di\ufb00erent as-\npects. Category detection is a challenging\nproblem due to the existence of overlapping\ncategories\u201d (Pontiki et al., 2016).\nPrevious research works in this topic can\nbe organized according to their classi\ufb01cation\nstrategy in: lexicon-based, unsupervised, su-\npervised and hybrid (Liu and Zhang, 2012).\nDespite the existence of successful lexicon-\nbased and unsupervised methods (Ghadery\net al., 2018), the majority of the proposed\napproaches follow a supervised learning ap-\nproach, considering hand-crafted representa-\ntions and using classi\ufb01cation algorithms such\nas SVM, K-NN, Logistic Regression or en-\nsembles of them (Xenos et al., 2016), (Hercig\net al., 2016), (Hetal and others, 2021).\nOver the last few years, deep learning\nmodels have brought signi\ufb01cant advances to\nthe aspect category detection task. For ex-\nample, in (Toh and Su, 2016) it is described\nthe construction of a set of binary classi-\n\ufb01ers, one for each category, considering a va-\nriety of lexical and syntactic features, along\nwith extra features learned from a Convo-\nlutional Neural Network (CNN). This ap-\nproach was the best performer at SemEval\n2016, and based on the analysis of its results,\ntheir authors concluded that the CNN out-\nput probabilities were the most relevant fea-\ntures, and that the combination of two dif-\nferent machine learning methods is a feasible\napproach for the task. In (Xue et al., 2017)\nthe multi-label aspect classi\ufb01cation was also\nhandled by multiple one-vs-all binary classi-\n\ufb01ers, implemented through a neural networkwith BiLSTM and CNN layers. In addition,\nthis work models the task as a multi-task\nlearning problem, jointly solving the detec-\ntion of aspect categories and the extraction\nof aspect terms. Its results showed an im-\nportant performance improvement, con\ufb01rm-\ning the synergy between both tasks. In (He\net al., 2017) it is presented a deep neural net-\nwork approach based on an attention mecha-\nnism, which was later modi\ufb01ed and improved\nin (Movahedi et al., 2019). In this last work,\ninstead of training several one-vs-all models,\nthe authors proposed a single model, namely\nTopic-Attention Network, which detects as-\npect categories of a given review sentence\nby attending to di\ufb00erent parts of the sen-\ntence based on di\ufb00erent topics. Their re-\nsults con\ufb01rmed that a single attention may\nnot be able to provide a good representation\nfor reviews containing multiple aspects, and,\ntherefore, pointed out the relevance of learn-\ning to weigh the terms based on the di\ufb00er-\nent categories. More recently, and in this\ndirection, in (Zhang et al., 2021) it is pre-\nsented a multilayer self-attention model to\ndeal with aspect category detection. Particu-\nlarly, it is a BERT-based multi-self-attention\nmodel, which uses multiple attentions to ob-\ntain relevant information of the multiple tar-\nget categories. Despite obtaining competi-\ntive results, its authors pointed out the dif-\n\ufb01culties of the attention model to correctly\nhandle short texts, since they provide very\nlimited contextual information.\nFollowing the previous ideas, our ap-\nproach seeks to emphasize the opinion terms\nby weighting them in accordance to their se-\nmantic proximity to each of the aspect cat-\negories, thus generating as many weights for\neach term as the number of categories. In\nthis way, the terms that could contribute the\nmost to identifying any given category are\nhighlighted prior to feeding the correspond-\ning binary classi\ufb01er, acting as a kind of non-\nsupervised attention mechanism.\n3 Proposed method\nFigure 1 shows the general diagram of the\nproposed method. It follows a one-vs-all ap-\nproach, which means that it uses as many\nclassi\ufb01ers as aspect categories in the training\nset. Themaincomponentsofourmethodare:\ni) the weighting of terms, ii) the construc-\ntion of the opinions\u2019 representations, and iii)\nthe classi\ufb01cation process. The following sub-\n119\nA Semantic-Proximity Term-Weighting Scheme for Aspect Category Detection sections detail the \ufb01rst two components since\nthe\ny represent the core contribution of our\nwork. For the classi\ufb01cation process we con-\nsider traditional as well as deep leaning mod-\nels, which are described in Section 4.\n3.1 New term weighting scheme\nThe purpose of the proposed term weighting\nscheme (named as SP for semantic proxim-\nity) is to emphasize the contribution of each\nopinion term for the detection of each aspect\ncategory. Accordingly, we calculate as many\nweightsforeachtermasthenumberofaspect\ncategories.\nGiven a set of aspect categories, C=\n{C1,C2,...,Cn}, where each category is rep-\nresentbyapre-de\ufb01nedlexiconorsetofterms,\nCi={ti1,...,tin}, and using pre-trained\nword embeddings from GloVe (Pennington,\nSocher, and Manning, 2014)1to represent\neachterm, with emb(t)indicatingtheembed-\nding vector of term t, we compute the seman-\ntic proximity weight of term tifor category\nCjas follows:\n1. De\ufb01ne the representative vector of the\ncategory Ci, referred as emb(Ci). This\nvector is computed as the average of the\nterm vectors of the category lexicon:\nemb(Ci) =1\n|Ci|/summationdisplay\n\u2200t\u2208Ciemb(t) (1)\n2. Measure the semantic proximity of term\ntiwith respect to category Cj. This\nproximityiscomputedbythecosinesim-\nilarity between the term and category\nembeddings:\nSPCj(ti) = cos(emb(ti),emb(Cj)) (2)\nAccording to this new term weighting\nscheme, the opinion\u2019s words that are strongly\nrelated to the lexicon of the aspect category\nthat is under analysis will have a greater\nweight than those from less related words.\nFigure 2 shows two opinions along with the\nsemantic proximity weights of their words\nrelative to the ambience-general and food-\nquality categories, respectively.\n1The experiments were carried out using pre-\ntrained embeddings on Wikipedia 300d and Twitter\n200d. Twitter GloVe embeddings were chosen due to\ntheir orientation towards language and text size in so-\ncial networks, Wikipedia GloVe were used to extend\nthe coverage of the vocabulary. Nonetheless, alterna-\ntive options could be considered.3.2 Opinion representations\nAs we previously mentioned, the proposed\nweighting scheme, which acts as a kind of\nunsupervised pre-attention mechanism, can\nbe used in combination with di\ufb00erent classi-\n\ufb01cation models, including traditional classi-\n\ufb01ers such as the SVM, as well as deep neu-\nral networks like a CNN. In the \ufb01rst case,\nthe SP weights are integrated under the Bag\nof Words representation, while in the second\ncase these weights are used to alter the em-\nbeddings that feed the networks. Both cases\nof opinion representation are described be-\nlow.\nRepresentation for a traditional clas-\nsi\ufb01er. In this case, opinions are represented\nusing a Bag of Words model. Accordingly,\neach opinion or document is represented by\na vector d=/an}bracketle{tw1,w2,...,wm/an}bracketri}ht, wheremis the\nsize of the training vocabulary and wiindi-\ncates the weight of term tiin the opinion. We\npropose to de\ufb01ne these weights as a combi-\nnation of the term frequency and the term\nsemantic proximity to the aspect category of\ninterest2Cjas follows:\nwi=tf(ti)\u00d7SPCj(ti) (3)\nRepresentation for a deep neural\nnetwork . In this case, opinions are repre-\nsentedbythearrayoftheirwordembeddings.\nThus, anopinionordocumenthaving kterms\nwill be represented by an array of the form\nd= [emb(t1),emb(t2),...,emb( tk)]. We pro-\npose to alter each of these embeddings by\nmultiplying them by a scalar that indicates\nthe relevance of each term, that is, by the se-\nmantic proximity SPof each term to the cat-\negory of interest Cj. Based on this, the new\nembedding of a term ti, denoted as emb\u2032(ti),\nis computed as indicated in Formula 4, and\nthe new representation of the opinion dis as\nindicated in Formula 5.\nemb\u2032(ti) =SPCj(ti)\u00d7emb(ti) (4)\nd= [emb\u2032(t1),emb\u2032(t2),...,emb\u2032(tk)] (5)\n2Please note that we follow a one-vs-all classi\ufb01ca-\ntion approach, and, thus, we have a di\ufb00erent classi\ufb01er\nfor each aspect category.\n120\nMonserrat V\u00e1zquez-Hern\u00e1ndez, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez Figure 1: The proposed method. The left part depicts its general view, which is based on a\none-vs-all classi\ufb01cation approach; the right part details the main components of each aspect\ncategory classi\ufb01er.\nFigure 2: Two opinions with their respec-\nti\nve SP weights. Words such as \u201cdecoration\u201d\nand \u201catmosphere\u201d are highly related to the\nambience-general category, whereas \u201cbeef\u201d\nand \u201cphenomenal\u201d to the food-quality cat-\negory.\n4 Experiments\n4.1 Datasets\nFor the experiments we considered two\ndatasets from SemEval 2016 (Pontiki et al.,\n2016). Particularly, we used the collections\nfor the restaurant domain in both English\nand Spanish. Table 1 describes the distri-\nbution of these collections. As pre-processing\noperations, we eliminated punctuation marks\nand special characters; when using the SVM\nclassi\ufb01er we also removed stopwords.\n4.2 Category-Oriented Lexicons\nThe category-oriented lexicons that our\nmethod takes as input can be manually or\nautomatically de\ufb01ned. For the experiments\nreported, we extracted these lexicons from\nthe training sets using the SS3 method re-\ncently proposed in (Burdisso, Errecalde, andMontes-y G\u00b4 omez, 2019). This method asso-\nciateseachvocabularytermwithacon\ufb01dence\nvalue for each of the categories. This value\nis a number in the interval [0,1] and repre-\nsents the degree of con\ufb01dence with which a\ntermisbelievedtoexclusivelybelongtogiven\ncategory. For example, the term \u201ctequila\u201d\nwill have a con\ufb01dence value close to 1 for the\n\u201cdrinks\u201d category, whereas the term \u201cof\u201d will\nhave a value equal or close to 0 because it is\nsimilarly distributed across all categories.\ninstances\nspanish english\nCategories tr aing test traing test\nambience#general 293 126 255 66\ndr\ninks#quality 31 10 47 22\ndr\ninks#style options 29 11 32 12\ndr\ninks#prices 14 10 20 4\nfo\nod#quality 845 291 849 313\nfo\nod#style options 192 69 137 55\nfo\nod#prices 127 41 90 23\nre\nstaurant#general 540 222 422 142\nre\nstaurant#miscellaneous 14 13 98 33\nre\nstaurant#prices 115 39 80 21\nse\nrvice#general 504 222 449 155\nlo\ncation#general 15 18 28 13\nTable 1: Aspect categories in restaurant\ndat\nasets.\nAs de\ufb01ned in the SemEval workshop,\naspect categories are formed by an entity\nand attribute pair (e.g., for the category\nfood#quality, the entity is \u201cfood\u201d and the\nattribute is \u201cquality\u201d). In particular, for the\nrestaurant data set there are 12 prede\ufb01ned\naspect categories which are listed in Table\n121\nA Semantic-Proximity Term-Weighting Scheme for Aspect Category Detection 1. From these categories, 6 di\ufb00erent enti-\nti\nes E ={ambience, drinks, food, restaurant,\nservice, location} and 5 di\ufb00erent attributes\nA ={general, miscellaneous, prices, quality,\nstyleoptions} are derived. We extracted lex-\ni\ncons for each one of the entities and for each\none of the attributes, and then we made the\ncorresponding unions to de\ufb01ne the lexicons\nfor the 12 aspect categories.\nAs stated above, the SS3 method (Bur-\ndisso, Errecalde, and Montes-y G\u00b4 omez, 2019)\ndetermines a con\ufb01dence value for all terms\nwith respect to all categories. In order to\nonly include in the lexicons the terms most\nstrongly associated with each category, we\npropose to \ufb01lter them using the following\ncriteria: we consider con\ufb01dences are nor-\nmallydistributed, andthuswekeeptheterms\nwhose con\ufb01dence values are equal or above \u03b2\nstandard deviations from the mean, consid-\nering\u03b2={1,2,3}. Table 2 shows \ufb01ve terms\nfor four di\ufb00erent lexicons, two in Spanish and\ntwo in English.\nambience#general food#quality\nterm value term value\nambiente 1.000 calidad 1.000\nme\nsas 1.000 caros 1.000\ntr\nanquilo 0.976 platos 0.977\nte\nrraza 0.921 excesivos 0.809\nboni\nto 0.846 proporcion 0.749\na) Category-oriented lexicons in Spanish\nlocation#general drinks#style options\nterm value term value\nlocated 1.000 champagne 1.000\nchar\nt 0.870 martinis 1.000\nbl\nock 0.870 well 0.922\nsi\ndewalk 0.870 generously 0.922\ncon\nveniently 0.870 guaranteed 0.802\nb) Category-oriented lexicons in English\nTabl\ne 2: Example of category-oriented lexi-\ncons. For each term its con\ufb01dence value ac-\ncording to SS3 is indicated.\n4.3 Experimental settings\nFor the experiments, we employed the\nmethod proposed in combination with two\nclassi\ufb01cation algorithms, a SVM and a CNN.\nFor each classi\ufb01cation model, we trained 12\nbinary classi\ufb01ers, one for each aspect cate-\ngory. For the sake of simplicity, for all clas-\nsi\ufb01ers we used the same settings; the used\nhyperparameter values are as follows:\n\u2022SVM: C = 2.5, kernel = linear, and De-gree = 2.5. For this particular case, the\nterms considered for the BOW represen-\ntation were those present in training set\nopinions from the category under analy-\nsis, without considering empty words3.\n\u2022CNN: We used a combination of ker-\nnels of sizes 1,2,3 in the convolutional\nlayer to create di\ufb00erent feature maps\n(Gehrmann et al., 2018). The rest of\nsettings for them are: activation=relu,\npool-size = max length - kernel-size + 1,\nst\nrides=1. The general settings of CNN\narchitecture are: epochs = 9, \ufb01lters =\n256, dropout = 0.5, activation function\n=sigmoide, lossfunction=binarycross-\nentropy, and optimizer = adam.\nDue to the fact that the datasets show a\nhigh level of imbalance, particularly because\nthe task was approached under the one-vs-all\napproach, we carried out additional exper-\niments applying oversampling over the mi-\nnority categories, which indeed correspond to\nthe aspect category under analysis for each\nbinary classi\ufb01er. In particular, we applied\nan oversampling technique that consists in\nrandomly replicate instances of the minority\nclass until reaching the size of the majority\nclass.\n4.4 Baseline results\nAs baseline results we used those obtained\nby the same two classi\ufb01ers but using the tra-\nditional representations. That is, for the\nSVM we used a BOW with tf-idf weigths,\nwithout including our SP weights. In the\ncase of the CNN, we fed it with the pre-\ntrained Glove embeddings without having al-\ntered them with our SP weights. We also\nconsiderthebaselineresultsreportedforeach\nSemEval 2016 task (Pontiki et al., 2016).\nAdditionally, we compared our results\nagainst those from state-of-the-art con-\nstrained4methods. In particular, we consid-\nered the top 3 results for the Spanish and the\nEnglish datasets. The works considered are:\n\u2022GTI(Alvarez-L\u00b4 opez et al., 2016). It\nuses a support vector machine, but also\n3Empty words de\ufb01ned in the Python NLTK li-\nbrary are considered.\n4Works that use external resources such as\ndatasets or dictionaries are classi\ufb01ed as U: Uncon-\nstrained and those that do not use any type of addi-\ntional resource are classi\ufb01ed as C: Constrained .\n122\nMonserrat V\u00e1zquez-Hern\u00e1ndez, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez a manually debugged list of words ob-\ntai\nned from the training set to remove\ninter-category noise.\n\u2022TGB(C \u00b8etin et al., 2016). It uses a two-\nlayer approach. In \ufb01rst layer considers a\none-vs-all classi\ufb01cation approach, where\nprobabilities for entities and attributes\nare computed. Then, in a second layer,\nthese probabilities are combined to un-\nderstand which is the best combination\nof entity and attribute in order to deter-\nmine the target aspect categories.\n\u2022UWB(Hercig et al., 2016). it im-\nplements a classi\ufb01er per category using\nmaximum entropy approach. A large\nnumberoffeaturesareconsideredforthe\nconstruction of classi\ufb01ers.\n\u2022BUTkn(Mach\u00b4 a\u02c7 cek, 2016). It uses a set\nof word n-grams manually compiled for\neach aspect category and then classi\ufb01ers\nthe opinions by looking for the occur-\nrence of these n-grams.\n\u2022XRCE(Brun, Perez, and Roux, 2016).\nIt adapts a component that extracts se-\nmantic information about entities and\nattributes. A dependency graph is cre-\nated in which the relationships of a\nterm with respect to categories are rep-\nresented. The classi\ufb01cation is performed\nby looking for word matches in the opin-\nions and their relationships with cate-\ngories.\n4.5 Experimental Results\nTable 3 presents a summary of the best re-\nsults obtained with our method as well as\nwith the baseline con\ufb01gurations. The sec-\nond and third columns refer to the con\ufb01gura-\ntion of the classi\ufb01er (kind and whether or not\noversampling was used), while the fourth and\nthird columns indicate the con\ufb01guration used\ntocalculatetheSPtermweights. Whencom-\nparing the results obtained with and with-\nout using the SP weights, the usefulness of\nthe proposed method is clearly appreciated.\nFor the Spanish collection, our best result is\nachieved with the SVM classi\ufb01er using over-\nsampling, and with \u03b2= 2 and using the\nTwitter Glove embeddings of 200 dimensions\nfor the computation of the SP weights. For\nEnglish, the best results were achieved with\ntheCNNarchitecturewithoversampling,and\nwith\u03b2= 2 and using Wikipedia Glove em-beddings of 300 dimensions for the computa-\ntion of the SP weights.\nRegarding the comparison against state-\nof-the-art methods, Table 4 shows our\nmethod\u2019s results as well as the best con-\nstrained results from SemEval 2016 in both\nSpanish and English. These comparisons ev-\nidence the relevance of the proposed method,\nsince, in addition to its simplicity and gen-\nerality, it shows competitive results in both\nscenarios; in particular it outperforms the\nbest results previously reported in the Span-\nish dataset.\n4.5.1 Discussion\nIn contrast to most previous works, the pro-\nposed method does not necessarily need to\nuse external resources for its implementa-\ntion. For example, the lexicons used by\nthe proposed weighting scheme can be au-\ntomatically extracted from the training set,\nas we did in the experiments. For those cat-\negories with a considerably reduced number\nof instances, the number of terms extracted\nfor their lexicons is also reduced. However,\nour experiments showed that the number of\nterms per lexicon is not directly related to\nthe results obtained per category. Tables 5\nand 6 contrast the number of instances, num-\nber of terms in lexicons, and results achieved\nfor each of the aspect categories of the two\ndatasets.\nDespite the small number of terms in\nthe lexicons, the proposed weighing scheme\ndemonstrates its usefulness by paying di\ufb00er-\nent levels of attention to those terms strongly\nrelated to the categories, which helps to de-\ntermine whether or not a category is present\nin an opinion. Analyzing the terms that con-\nstitute all category-oriented lexicons (refer to\nFigure 3), we observe that they are clearly\nrepresentative of the di\ufb00erent categories, de-\nspite they were automatically extracted. In\nconsequence, an advantage of this approach\nis that it can be easily adapted to other do-\nmains or languages, as was observed in the\nexperiments.\nTo demonstrate the generality of our\nmethod, we performed another experiment\nusing the English laptop dataset. The\nachieved results are shown in Table 7. In this\ncase, only the best constrained work is taken\nas a reference for comparison. Our best re-\nsultwasachievedusingtheSVMclassifer,ap-\nplying oversampling for class balancing, and\nconsidering \u03b2= 2 with GloVe Wikipedia em-\n123\nA Semantic-Proximity Term-Weighting Scheme for Aspect Category Detection Spanish Cl assi\ufb01er Oversampling Embeddings Threshold micro-F1\nOur method SVM Yes Twitter 200d \u03b2= 2 71.11\nBaseline SVM Yes - - 64.30\nOur method CNN Yes Twitter 200d \u03b2= 1 69.99\nBaseline CNN Yes Wikipedia 300d - 60.50\nBaseline SemEval - - - - 54.68\nEnglish Cl assi\ufb01er Oversampling Embeddings Threshold micro-F1\nOur method SVM Yes Twitter 200d \u03b2= 2 66.50\nBaseline SVM No - - 64.30\nOur method CNN Yes Wikipedia 300d \u03b2= 2 68.50\nBaseline CNN Yes Wikipedia 300d - 66.80\nBaseline SemEval - - - - 58.92\nTable 3: Micro-F1 results of the aspect category detection task , for the restaurant domain in\nSpanish and English. For each method the result of its best con\ufb01guration is included; the best\noverall result is highlighted in bold.\nmicro-f1 micro-f1\nSP+SVM 71.111 BUTK n. 71.494\nGTI 70.027 XRCE 68.701\nTGB 63.551 SP+CNN 68.500\nUWB 61.968 UWB 67.817\na) Spanish b) English\nTabl\ne 4: Comparison against SOTA results by constrained methods in the aspect category\ndetection tasks from SemEval 2016. The results of our method correspond to the SP+SVM and\nSP+CNN con\ufb01gurations for Spanish and English, respectively.\nCategoryTraining\nIntances#TermsTest\nIn\nstances%Errors\nambience#general 293 40 126 30.73\ndr\ninks#quality 31 17 10 90.00\ndr\ninks#style options 29 9 11 45.45\ndr\ninks#prices 14 8 10 55.55\nfo\nod#quality 845 17 291 20.66\nfo\nod#style options 192 62 69 58.06\nfo\nod#prices 127 8 41 20.00\nre\nstaurant#general 540 23 222 29.22\nre\nstaurant#miscellaneous 14 28 13 100.00\nre\nstaurant#prices 115 23 39 52.63\nse\nrvice#general 504 31 222 16.55\nlo\ncation#general 15 47 18 83.33\nTable 5: Number of opinions to classify and\nle\nxicon terms for each aspect category, as\nwell as the percentage of incorrectly classi-\n\ufb01ed instances by our best result in the Span-\nish dataset.\nbeddings of 300 dimensions for the computa-\ntion of the SP weights. As can be noticed,\nour result is signi\ufb01cantly higher than the re-\nported SemEval\u2019s baseline result, and it also\noutperformsthebestreportedconstrainedre-\nsult. It is important to highlight that, in\nspite of the large number of categories in this\ndataset, 67 aspect categories derived from 22\nentities and 9 attributes, for the experiments\nwe did not carried out any additional hyper-\nparameter adjustment.CategoryTraining\nInstances#TermsTest\nIn\nstances%Errors\nambience#general 255 17 66 78.79\ndr\ninks#quality 47 10 22 45.45\ndr\ninks#style options 32 9 12 58.33\ndr\ninks#prices 20 7 4 75.00\nfo\nod#quality 849 6 313 19.17\nfo\nod#style options 137 6 55 50.91\nfo\nod#prices 90 6 23 47.29\nre\nstaurant#general 422 4 142 42.14\nre\nstaurant#miscellaneous 98 4 33 78.79\nre\nstaurant#prices 80 10 21 47.62\nse\nrvice#general 449 4 155 21.29\nlo\ncation#general 28 1 13 53.85\nTable 6: Number of opinions to classify and\nle\nxicon terms for each aspect category, as\nwell as the percentage of incorrectly classi-\n\ufb01ed instances by our best result in the En-\nglish dataset.\nmicro-f1\nSP+SVM 62.10\nUW\nB 60.45\nBaseline SemEval 52.68\nTable 7: Micro F1 results in the aspect\ncat\negory detection using the English laptop\ndataset.\n4.5.2 Error analysis\nDoing a detailed analysis of the errors (refer\nto Tables 5 and 6), it can be noticed that\n124\nMonserrat V\u00e1zquez-Hern\u00e1ndez, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez Figure 3: Terms of three category-oriented\nle\nxicons of the restaurant domain.\nthere is no a clear relationship between the\ncharacteristicsofthedatasetsanderrorrates.\nFor the Spanish dataset, the correlation be-\ntween the number of training instances and\nthe percentage of errors is r=\u22120.690, while\nfor the English dataset r=\u22120.664. Sur-\nprisingly, these values indicate that there\nwas a slight tendency to misclassify instances\nfrom categories with many examples. This is\npartlybecausethesemorefrequentcategories\nare more diverse and also because they usu-\nally appeared together with others.\nOn the other hand, when correlating the\nsize of the categories\u2019 lexicons with the clas-\nsi\ufb01cation errors, for Spanish we obtained a\nr= 0.214 and for English r= 0.355, sug-\ngesting little in\ufb02uence of this variable on the\nresults. However, analyzing these lexicons in\ngreater depth, we found that although they\nseemed related and relevant to the di\ufb00erent\ncategories, they showed a high overlap. In\nthe Spanish dataset, for the smallest cate-\ngories more than 80% of their terms are also\nincluded in others. One interesting example\nis the entity category \u201cdrinks\u201d, for which any\nterm was unique; even more, 40% of its terms\nare also in the lexicons of four or more cat-\negories. For English, a similar behavior was\nobserved, but, in addition, we noticed that\nfor the category \u201clocation#general\u201d, with a\nsingle term but exclusive to this category, we\nobtained a better result than for other cat-\negories with larger lexicons. This suggests\nus the in\ufb02uence of the category-oriented lex-\nicons in whole process, as well as the need to\nde\ufb01ne them more precisely.\n5 Conclusions\nThemaincontributionofthisworkisthepro-\nposal of a new term weighting scheme spe-\ncially suited to the aspect category detection\ntask. It is based on the evaluation of the se-\nmantic proximity of each term in an opinion\nwith respect to the categories\u2019 description,acting as a kind of non-supervised attention\nmechanism.\nThe proposed weighting scheme relies on\nthe availability of a set of category-oriented\nlexicons, nonetheless, they can be automat-\nically extracted from the training dataset.\nThis latter characteristic makes the method\neasilyadaptabletodi\ufb00erentdomainsandlan-\nguages.\nFrom the results obtained, it is possible to\nconclude that the proposed term weighting\nscheme has a positive impact on the identi\ufb01-\ncation of the categories of aspects expressed\nin an opinion. Moreover, it has the advan-\ntage of being able to be combined with dif-\nferent classi\ufb01cation models, including tradi-\ntional machine learning classi\ufb01ers as well as\ndeep neural networks.\nOn the other hand, although the method\nis less sensitive to small and imbalanced\ndatasets than other supervised approaches, it\nis a\ufb00ected by these conditions. As it was ob-\nserved, the method achieved better results in\nSpanish than in English, being the latter the\ncollection with less instances and high imbal-\nance rates.\nAs working directions, we plan to evaluate\nthe method using di\ufb00erent kinds of category\nlexicons, both manually and automatically\ngenerated. Besides that, we seek to evalu-\nate our method in collections having di\ufb00erent\nvolumes of information, as well as using dif-\nferent contextual word embeddings such as\nthose from BERT. Furthermore, due to the\ngenerality of the proposed method, we plan\nto apply it in other text classi\ufb01cation tasks\nsuch as polarity classi\ufb01cation, author pro\ufb01l-\ning and fraud detection.\nAcknowledgments\nThe present work was supported by CONA-\nCyT/M\u00b4 exico (scholarship 756974 and grant\nCB-2015-01-257383). In addition, the au-\nthors thank CONACYT for the computa-\ntional resources provided by the Deep Learn-\ning Platform for Language Technologies.\nReferences\nAlvarez-L\u00b4 opez, T., J. Juncal-Mart\u00b4 \u0131nez,\nM. Fern\u00b4 andez-Gavilanes, E. Costa-\nMontenegro, and F. J. Gonz\u00b4 alez-Castano.\n2016. Gti at semeval-2016 task 5: Svm\nand crf for aspect detection and unsu-\npervised aspect-based sentiment analysis.\n125\nA Semantic-Proximity Term-Weighting Scheme for Aspect Category Detection InProceedings of the 10th interna-\nti\nonal workshop on semantic evaluation\n(SemEval-2016) , pages 306\u2013311.\nBrun, C., J. Perez, and C. Roux. 2016.\nXRCE at SemEval-2016 task 5: Feed-\nbacked ensemble modeling on syntactico-\nsemantic knowledge for aspect based sen-\ntiment analysis. In Proceedings of the\n10th International Workshop on Semantic\nEvaluation (SemEval-2016) , pages 277\u2013\n281, San Diego, California, June. Associ-\nation for Computational Linguistics.\nBurdisso, S. G., M. Errecalde, and\nM. Montes-y G\u00b4 omez. 2019. A text\nclassi\ufb01cation framework for simple and\ne\ufb00ective early depression detection over\nsocial media streams. Expert Systems\nwith Applications , 133:182\u2013197.\nC \u00b8etin, F. S., E. Y\u0131ld\u0131r\u0131m, C. \u00a8Ozbey, and\nG. Eryi\u02d8 git. 2016. Tgb at semeval-2016\ntask 5: multi-lingual constraint system for\naspect based sentiment analysis. In Pro-\nceedings of the 10th International Work-\nshop on Semantic Evaluation (SemEval-\n2016), pages 337\u2013341.\nChaudhari, S., V. Mithal, G. Polatkan, and\nR. Ramanath. 2021. An attentive sur-\nvey of attention models. ACM Transac-\ntions on Intelligent Systems and Technol-\nogy (TIST) , 12(5):1\u201332.\nGehrmann, S., F. Dernoncourt, Y. Li, E. T.\nCarlson, J. T. Wu, J. Welt, J. Foote Jr,\nE. T. Moseley, D. W. Grant, P. D. Tyler,\net al. 2018. Comparing deep learning and\nconcept extraction based methods for pa-\ntient phenotyping from clinical narratives.\nPloS one, 13(2):e0192360.\nGhadery, E., S. Movahedi, H. Faili, and\nA. Shakery. 2018. An unsupervised ap-\nproach for aspect category detection us-\ning soft cosine similarity measure. arXiv\npreprint arXiv:1812.03361.\nHe, R., W. S. Lee, H. T. Ng, and\nD. Dahlmeier. 2017. An unsupervised\nneural attention model for aspect extrac-\ntion. In Proceedings of the 55th Annual\nMeeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Pa-\npers), pages 388\u2013397, Vancouver, Canada,\nJuly. Association for Computational Lin-\nguistics.Hercig, T., T. Brychc\u00b4 \u0131n, L. Svoboda, and\nM. Konkol. 2016. Uwb at semeval-2016\ntask 5: Aspect based sentiment analy-\nsis. InProceedings of the 10th interna-\ntional workshop on semantic evaluation\n(SemEval-2016) , pages 342\u2013349.\nHetal, V. et al. 2021. Ensemble mod-\nels for aspect category related absa sub-\ntasks.Turkish Journal of Computer and\nMathematics Education (TURCOMAT) ,\n12(13):2348\u20132364.\nLiu, B. and L. Zhang. 2012. A survey of\nopinion mining and sentiment analysis. In\nMining text data .Springer, pages415\u2013463.\nL\u00b4 opez Ramos, D. and L. Arco Garc\u00b4 \u0131a. 2019.\nAprendizaje profundo para la extracci\u00b4 on\nde aspectos en opiniones textuales. Re-\nvista Cubana de Ciencias Inform\u00b4 aticas ,\n13(2):105\u2013145.\nMach\u00b4 a\u02c7 cek, J. 2016. BUTknot at SemEval-\n2016 task 5: Supervised machine learn-\ning with term substitution approach in as-\npect category detection. In Proceedings of\nthe 10th International Workshop on Se-\nmantic Evaluation (SemEval-2016) , pages\n301\u2013305, San Diego, California, June. As-\nsociation for Computational Linguistics.\nMovahedi, S., E. Ghadery, H. Faili, and\nA. Shakery. 2019. Aspect category detec-\ntion via topic-attention network. CoRR,\nabs/1901.01183.\nMowlaei, M. E., M. Saniee Abadeh, and\nH. Keshavarz. 2020. Aspect-based senti-\nment analysis using adaptive aspect-based\nlexicons. Expert Systems with Applica-\ntions, 148:113234.\nPennington, J., R. Socher, and C. D. Man-\nning. 2014. Glove: Global vectors for\nword representation. In Proceedings of\nthe 2014 conference on empirical methods\nin natural language processing (EMNLP) ,\npages 1532\u20131543.\nPontiki, M., D. Galanis, H. Papageorgiou,\nI.Androutsopoulos,S.Manandhar,M.Al-\nSmadi, M. Al-Ayyoub, Y. Zhao, B. Qin,\nO. De Clercq, et al. 2016. Semeval-2016\ntask 5: Aspect based sentiment analy-\nsis. In10th International Workshop on\nSemantic Evaluation (SemEval 2016) .\nToh, Z. and J. Su. 2016. NLANGP at\nSemEval-2016 task 5: Improving aspect\n126\nMonserrat V\u00e1zquez-Hern\u00e1ndez, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez based sentiment analysis using neural net-\nwor\nk features. In Proceedings of the\n10th International Workshop on Semantic\nEvaluation (SemEval-2016) , pages 282\u2013\n288, San Diego, California, June. Associ-\nation for Computational Linguistics.\nXenos, D., P.Theodorakakos, J.Pavlopoulos,\nP. Malakasiotis, and I. Androutsopoulos.\n2016. Aueb-absa at semeval-2016 task 5:\nEnsembles of classi\ufb01ers and embeddings\nfor aspect based sentiment analysis. In\n*SEMEVAL.\nXue, W., W. Zhou, T. Li, and Q. Wang.\n2017. Mtna: a neural multi-task model\nfor aspect category classi\ufb01cation and as-\npect term extraction on restaurant re-\nviews. In Proceedings of the Eighth In-\nternational Joint Conference on Natural\nLanguage Processing (Volume 2: Short\nPapers), pages 151\u2013156.\nZhang, X., X. Song, A. Feng, and Z. Gao.\n2021. Multi-self-attention for aspect cate-\ngory detection and biomedical multilabel\ntext classi\ufb01cation with bert. Mathemati-\ncal Problems in Engineering, 2021.\n127\nA Semantic-Proximity Term-Weighting Scheme for Aspect Category Detection 128\nDetecci\u00b4 on de Indicios de Autolesiones No Suicidas enInformes M\u00b4 edicos de Psiquiatr\u00b4 \u0131a Mediante el An\u00b4 alisis\ndel Lenguaje\nDetecting Signs of Non-suicidal Self-Injury in Psychiatric\nMedical Reports Using Language Analysis\nJuan Martinez-Romo1,2\nLourdes Araujo1,2\n1NLP & IR Group\nUniversidad Nacional de\nEducaci\u00b4 on a Distancia (UNED)\n2Instituto Mixto UNED-ISCIII\nIMIENS\njuaner,lurdes@lsi.uned.esBlanca Reneses1,2,3\nJ. Sevilla-Llewellyn-Jones1\n1IdISSC\nHospital Cl\u00b4 \u0131nico San Carlos\n2CIBERSAM\n3Universidad Complutense\nblanca.reneses@salud.madrid.org\njulia.sevilla@salud.madrid.orgIgnacio Mart\u00b4 \u0131nez-Capella\nGerm\u00b4 an Seara-Aguilar\nUnidad de Innovaci\u00b4 on\nIdISSC\nHospital Cl\u00b4 \u0131nico San Carlos\nMadrid\nimcapella@salud.madrid.org\ngseara@shealth.eu\nResumen: La autolesi\u00b4 on no suicida, a menudo denominada autolesi\u00b4 on, es el acto de\nda\u02dc narse deliberadamente el propio cuerpo, como cortarse o quemarse. Normalmente,\nno pretende ser un intento de suicidio. En este trabajo se presenta un sistema de\ndetecci\u00b4 on de indicios de autolesiones no suicidas, basado en el an\u00b4 alisis del lenguaje,\nsobre un conjunto anotado de informes m\u00b4 edicos obtenidos del servicio de psiquiatr\u00b4 \u0131a\nde un Hospital p\u00b4 ublico madrile\u02dc no. Tanto la explicabilidad como la precisi\u00b4 on a la hora\nde predecir los casos positivos, son los dos principales objetivos de este trabajo. Para\nlograr este \ufb01n se han desarrollado dos sistemas supervisados de diferente naturale-\nza. Por un lado se ha llevado a cabo un proceso de extracci\u00b4 on de diferentes rasgos\ncentrados en el propio mundo de las autolesiones mediante t\u00b4 ecnicas de procesamien-\nto del lenguaje natural para alimentar posteriormente un clasi\ufb01cador tradicional.\nPor otro lado, se ha implementado un sistema de aprendizaje profundo basado en\nvarias capas de redes neuronales convolucionales, debido a su gran desempe\u02dc no en\ntareas de clasi\ufb01caci\u00b4 on de textos. El resultado es el funcionamiento de dos sistemas\nsupervisados con un gran rendimiento, en donde destacamos el sistema basado en un\nclasi\ufb01cador tradicional debido a su mejor predicci\u00b4 on de clases positivas y la mayor\nfacilidad de cara a explicar sus resultados a los profesionales sanitarios.\nPalabras clave: Detecci\u00b4 on de autolesiones no suicidas, an\u00b4 alisis del lenguaje, apren-\ndizaje autom\u00b4 atico, redes neuronales.\nAbstract: Non-suicidal self-injury, often referred to as self-injury, is the act of deli-\nberately harming one\u2019s own body, such as cutting or burning oneself. It is not usually\nintended as a suicide attempt. This paper presents a system for detecting signs of\nnon-suicidal self-injury, based on language analysis, on an annotated set of medical\nreports obtained from the psychiatric service of a public hospital in Madrid. Both\nexplainability and accuracy in predicting positive cases are the two main objecti-\nves of this work. In order to achieve this goal, two supervised systems of di\ufb00erent\nnatures have been developed. On the one hand, a process of extraction of di\ufb00erent\nfeatures focused on the world of self-injury itself has been carried out using natural\nlanguage processing techniques to subsequently feed a traditional classi\ufb01er. On the\nother hand, a deep learning system based on several layers of convolutional neural\nnetworks, due to its high performance in text classi\ufb01cation tasks. The result are two\nsupervised systems with high performance, where we highlight the system based on\na traditional classi\ufb01er due to its better prediction of positive classes and the greater\nease to explain its results to health professionals.\nKeywords: Non-suicidal Self-injury detection, language analysis, machine learning,\nneural networks.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 129-140\nrecibido 21-03-2022 revisado 17-05-2022 aceptado 30-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-11\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introducci\u00b4 on\nLos\ntrastornos de salud mental, como las au-\ntolesiones, son problemas cuya incidencia en\nla poblaci\u00b4 on aumenta de manera alarmante\nen los \u00b4 ultimos a\u02dc nos. Estas afecciones pueden\npasardesapercibidasdurantemuchosa\u02dc nos,lo\nque hace que las personas que las padecen no\nreciban la asistencia m\u00b4 edica adecuada. Los\nproblemas de salud mental sin tratar pue-\nden acarrear graves consecuencias, como el\ndeterioro personal o incluso el suicidio. Las\nautolesiones, tambi\u00b4 en conocidas como auto-\nlesiones deliberadas o autoagresiones, son un\ntipo de problema de salud mental menos co-\nnocido que afecta principalmente a los j\u00b4 ove-\nnes (Young et al., 2007). La autolesi\u00b4 on se re-\n\ufb01ere al acto de causarse da\u02dc no corporal a s\u00b4 \u0131\nmismo sin intenci\u00b4 on suicida, como cortarse,\nquemarse o tirarse del pelo, y se ha relaciona-\ndo con problemas de salud mental subyacen-\ntes,comoladepresi\u00b4 onylaansiedad(Greaves,\n2018b). Entre las diferentes acciones que las\npersonas afectadas llevan a cabo dentro del\nconcepto general de autolesi\u00b4 on, existen gran-\ndes diferencias tanto en los motivos para lle-\nvarlas a cabo, como en el g\u00b4 enero (Rodham,\nHawton, y Evans, 2004). La naturaleza a me-\nnudo impulsiva de estos actos (especialmen-\nte el auto-corte) signi\ufb01ca que la prevenci\u00b4 on\ndebe centrarse en fomentar m\u00b4 etodos alterna-\ntivos de gesti\u00b4 on de la ansiedad, la resoluci\u00b4 on\nde problemas y la b\u00b4 usqueda de ayuda antes\nde que se desarrollen pensamientos de auto-\nlesi\u00b4 on. Dada la gravedad de los s\u00b4 \u0131ntomas y\nlos riesgos, es importante dedicar esfuerzos a\ndetectar mejor los problemas de salud men-\ntal en la sociedad para que puedan recibir\nla ayuda que necesitan. Tambi\u00b4 en se han en-\ncontrado diferencias en la forma de comuni-\ncarse y en el lenguaje empleado por las per-\nsonas que sufren problemas de salud mental\n(Pennebaker, Mehl, y Niederho\ufb00er, 2003). A\npesar de que los informes m\u00b4 edicos est\u00b4 an ge-\nneralmente escritos por m\u00b4 edicos, en ocasio-\nnes tratan de plasmar las ideas subyacentes\ndel paciente e incluso escriben literalmente\nfrases o expresiones utilizadas y que puedan\ndenotar de forma clara estados de \u00b4 animo o\npensamientos. De esta forma, el an\u00b4 alisis del\nlenguaje empleado en estos informes m\u00b4 edi-\ncos mediante t\u00b4 ecnicas de Procesamiento del\nLenguaje Natural (PLN) pueden ayudar a la\ndetecci\u00b4 on temprana de pacientes con otros\ntrastornos previos. Sin embargo, cabe se\u02dc nalar\nquelamayor\u00b4 \u0131adelosestudiossobredetecci\u00b4 ontemprana de peligros para la seguridad y la\nsalud se han centrado en el texto en ingl\u00b4 es.\nPor otra parte, hay que se\u02dc nalar que apenas\nexisten conjuntos de datos (datasets o corpo-\nra) para entrenar modelos de identi\ufb01caci\u00b4 on\nen las tareas mencionadas, y los existentes se\nlimitan al ingl\u00b4 es y son de tama\u02dc no reducido, lo\ncual es un claro indicador del camino que a\u00b4 un\nqueda por recorrer para que los profesionales\nsanitarios puedan disponer de herramientas\nmaduras de an\u00b4 alisis de textos. En este tra-\nbajo se presenta un sistema de detecci\u00b4 on de\nautolesiones en informes m\u00b4 edicos proceden-\ntes del servicio de psiquiatr\u00b4 \u0131a del Hospital\nCl\u00b4 \u0131nico San Carlos de Madrid. Este sistema\nde detecci\u00b4 on de autolesiones entrenado y eva-\nluado sobre un corpus anotado de informes\nm\u00b4 edicos, tiene el objetivo de aplicarse a cual-\nquier informe del servicio de psiquiatr\u00b4 \u0131a para\npermitir la detecci\u00b4 on temprana de este tras-\ntorno en pacientes que hayan sido tratados\npor dicho servicio. De esta forma, pacientes\ncon otro tipo de trastornos de car\u00b4 acter m\u00b4 as\nleve, podr\u00b4 \u0131an ser diagnosticados y recibir tra-\ntamiento antes de adquirir estos h\u00b4 abitos tan\nperjudiciales. Y es que la detecci\u00b4 on tempra-\nna es clave en el tratamiento de los proble-\nmas de salud mental, ya que una intervenci\u00b4 on\nr\u00b4 apida mejora las probabilidades de un buen\npron\u00b4 ostico.\nEl resto del art\u00b4 \u0131culo se organiza de la si-\nguiente forma: en la Secci\u00b4 on 2 se analiza el\nestado del arte y trabajos relacionados. en la\nSecci\u00b4 on 3 se describe el corpus y las t\u00b4 ecnicas\nutilizadaparaanotarlo.EnlaSecci\u00b4 on4sede-\ntallan las caracter\u00b4 \u0131sticas del sistema de detec-\nci\u00b4 on de autolesiones. La Secci\u00b4 on 5 se centra\nen la experimentaci\u00b4 on y el an\u00b4 alisis de resul-\ntados. Finalmente, en la Secci\u00b4 on 6 se extraen\nlas principales conclusiones y se exponen las\nl\u00b4 \u0131neas de trabajo futuro.\n2 Estado del Arte\nEl estudio de las autolesiones y m\u00b4 as concreta-\nmentedesuinvestigaci\u00b4 onatrav\u00b4 esdelan\u00b4 alisis\nde textos no es demasiado extenso. Existen\ntrabajos (Baetens et al., 2011) en los que se\ninvestigaronlaprevalenciadelasautolesiones\nno suicidas (NSSI) y las autolesiones suicidas\n(SSI)enunamuestradeadolescentesdeentre\n12 y 18 a\u02dc nos, as\u00b4 \u0131 como las diferencias psico-\nsociales entre los adolescentes que practican\nNSSI y los que practican SSI. Tambi\u00b4 en hay\ntrabajos (Nicolai, Wielgus, y Mezulis, 2016)\nque apoyan la teor\u00b4 \u0131a de la cascada emocio-\n130\nJuan Martinez-Romo, Lourdes Araujo, Blanca Reneses, J. Sevilla-Llewellyn-Jones, Ignacio Mart\u00ednez-Capella, Germ\u00e1n Seara-Aguilar nal, en la que la rumiaci\u00b4 on distingue entre\nlas\npersonas que se autolesionan y las que no\nlo hacen, y hacen especial hincapi\u00b4 e en la re-\nlaci\u00b4 on entre el afecto negativo y las NSSI.\nHay trabajos (Burke, Ammerman, y Jaco-\nbucci, 2019) que se han centrado en abordar\nlas limitaciones de los sistemas de detecci\u00b4 on\nde riesgo y el tiempo de c\u00b4 omputo utilizan-\ndo herramientas anal\u00b4 \u0131ticas avanzadas, como\nel procesamiento del lenguaje natural (PLN)\ny el aprendizaje autom\u00b4 atico. Existen estudios\ncentrados en la ideaci\u00b4 on de suicidio que usan\nenfoques de PLN y que han utilizado en gran\nmedida modelos basados en la historia cl\u00b4 \u0131ni-\nca electr\u00b4 onica (HCE) (Haerian, Salmasian, y\nFriedman, 2012; Kessler et al., 2017) y mo-\ndelos de predicci\u00b4 on basados en PLN y rasgos\nling\u00a8 u\u00b4 \u0131sticos(Fernandesetal.,2018;McCoyet\nal., 2016; Poulin et al., 2014).\nEn 2017, un trabajo (Walsh, Ribeiro, y\nFranklin, 2017) utiliz\u00b4 o aprendizaje autom\u00b4 ati-\nco para predecir el riesgo de suicidio en pa-\ncientes de autolesiones a lo largo del tiempo\nanalizando informes m\u00b4 edicos de una gran ba-\nse de datos m\u00b4 edica. Tambi\u00b4 en se han usado\ntests adaptativos informatizados (CAT) para\nentrenar un \u00b4 arbol de decisi\u00b4 on con el objeti-\nvo de predecir el riesgo de suicidio (Delgado-\nGomez et al., 2016). Otros trabajos (Metz-\nger et al., 2017) han empleado algoritmos de\nclasi\ufb01caci\u00b4 on como random forest and na\u00a8 \u0131ve\nBayes sobre informes m\u00b4 edicos para prede-\ncir suicidios, demostrando que los m\u00b4 etodos\nde aprendizaje autom\u00b4 atico pueden mejorar la\ncalidad de los indicadores epidemiol\u00b4 ogicos en\ncomparaci\u00b4 on con la actual vigilancia nacio-\nnal de los intentos de suicidio de un pa\u00b4 \u0131s co-\nmo Francia. Los \u00b4 arboles de decisi\u00b4 on tambi\u00b4 en\nse han usado en otros trabajos (Mann et al.,\n2008) para estudiar la correlaci\u00b4 on entre pa-\ncientes de psiquiatr\u00b4 \u0131a analizando su conducta\nsuicida pasada frente a la ideaci\u00b4 on de suicidio\nque mostraban en un momento dado.\nLa clasi\ufb01caci\u00b4 on de textos cl\u00b4 \u0131nicos median-\nte redes neuronales ha resultado una herra-\nmientadegranutilidadenproblemascomola\nidenti\ufb01caci\u00b4 on de fenotipos en informes m\u00b4 edi-\ncos para pacientes con un conjunto determi-\nnado de signos y s\u00b4 \u0131ntomas cl\u00b4 \u0131nicos (Obeid et\nal., 2019). En los \u00b4 ultimos a\u02dc nos se han produ-\ncido avances signi\ufb01cativos en los enfoques de\naprendizaje profundo, como las redes neuro-\nnales convolucionales (CNN), y su aplicaci\u00b4 on\nha sido un \u00b4 exito en problemas como el pro-\ncesamiento y la clasi\ufb01caci\u00b4 on de textos o elreconocimiento del habla (LeCun, Bengio, y\nHinton, 2015).\nRecientemente ha surgido un estudio\n(Obeid et al., 2020) que aprovecha la infor-\nmaci\u00b4 on de las notas cl\u00b4 \u0131nicas utilizando redes\nneuronales profundas (DNNs) para identi\ufb01-\ncar los pacientes tratados por autolesi\u00b4 on in-\ntencional y predecir futuros eventos de au-\ntolesi\u00b4 on. Los autores utilizaron dos modelos\nbasados en una CNN y en una LSTM con re-\nsultados prometedores. Tambi\u00b4 en se han usa-\ndo t\u00b4 ecnicas de clasi\ufb01caci\u00b4 on como el Gradient\nBoosting para la detecci\u00b4 on de autolesiones e\nideaci\u00b4 on suicida en las notas de triaje de los\nservicios de urgencias (Rozova et al., 2022).\nEn los \u00b4 ultimos a\u02dc nos han aparecido bastan-\ntes trabajos en el \u00b4 ambito de las redes sociales\ny la salud mental. Aunque el formato del tex-\nto de las redes sociales y en lenguaje escrito\nen primera persona hacen abordar este pro-\nblema desde un enfoque diferente, queremos\ndestacar algunos trabajos por su relevancia y\nsu cercan\u00b4 \u0131a al problema de las autolesiones.\nDesde 2017 y de forma anual se celebra\nla tarea competitiva eRisk (Losada, Cresta-\nni, y Parapar, 2019; Losada, Crestani, y Pa-\nrapar, 2020; Parapar et al., 2021), dentro del\ncongreso CLEF (Cross Language Evaluation\nForum). eRisk trata de avanzar en la predic-\nci\u00b4 on temprana en redes sociales de problemas\nrelacionados con la salud mental. Depresi\u00b4 on,\nanorexia, ludopat\u00b4 \u0131a y autolesiones desde el\na\u02dc no 2019 han sido los trastornos elegidos por\nlos organizadores. Dentro de esta competi-\nci\u00b4 on, un sistema con resultados prometedores\nfue el equipo iLab (Mart\u0131nez-Castano et al.,\n2020)enelquelosinvestigadorespropusieron\nun sistema de clasi\ufb01caci\u00b4 on basado en BERT\ny transformers. En contraposici\u00b4 on al uso pe-\nsado de las redes neuronales y los transfor-\nmers, tambi\u00b4 en resultan interesantes las par-\nticipaciones del grupo NLP-UNED (Ageitos,\nMart\u00b4 \u0131nez-Romo, y Araujo, 2020; Campillo-\nAgeitos et al., 2021), que emplearon t\u00b4 ecnicas\nde PLN y an\u00b4 alisis de sentimientos para ali-\nmentar un clasi\ufb01cador r\u00b4 apido y e\ufb01ciente. Fi-\nnalmente, un trabajo que obtuvo buenos re-\nsultados con un sistema innovador fue el del\ngrupo UNSL(Loyola et al., 2021), que emple\u00b4 o\npol\u00b4 \u0131ticas de alerta, un sistema basado en re-\nglas y un modelo de aprendizaje por refuerzo.\n3 Corpus\nEl corpus de evaluaci\u00b4 on procede de un con-\njunto de informes m\u00b4 edicos anonimizados pro-\n131\nDetecci\u00f3n de Indicios de Autolesiones No Suicidas en Informes M\u00e9dicos de Psiquiatr\u00eda Mediante el An\u00e1lisis del Lenguaje cedentes del servicio de psiquiatr\u00b4 \u0131a del Hospi-\ntal\nCl\u00b4 \u0131nico San Carlos de Madrid en Espa\u02dc na.\nLa preparaci\u00b4 on de los informes para su\nan\u00b4 alisis ha sido desarrollada por la Unidad de\nInnovaci\u00b4 on del Hospital Cl\u00b4 \u0131nico San Carlos, a\npartir de la descarga autorizada de informes\ninformatizados del Servicio de Psiquiatr\u00b4 \u0131a co-\nrrespondientes a un periodo de cuatro a\u02dc nos.\nDicha preparaci\u00b4 on ha consistido en tres fases:\nlimpieza de los informes, compleci\u00b4 on y anoni-\nmizaci\u00b4 on. Previamente, esta cesi\u00b4 on de datos\nfue evaluada y aprobada por el Comit\u00b4 e de\n\u00b4Etica de la Investigaci\u00b4 on (20/586-E).\nA partir de este conjunto de informes ano-\nnimizados, se llev\u00b4 o a cabo un proceso de ano-\ntaci\u00b4 on por parte de expertos dando lugar a\nun corpus de 1252 informes anotados. Los\ndiagn\u00b4 osticos de estos informes son diversos,\npero entre ellos no se incluye sufrir autolesio-\nnes. Por ello ha sido necesaria una anotaci\u00b4 on\nmanual supervisada por los m\u00b4 edicos exper-\ntos en base al contenido textual de los infor-\nmes. Tras la anotaci\u00b4 on manual en busca de\nindicios de autolesiones, 1138 han sido ano-\ntados como negativos y 114 como positivos.\nDurante el proceso de anotaci\u00b4 on, se buscaban\nindicios claros de que el profesional sanitario\nque hubiera atendido al paciente indicara que\nlas autolesiones se hab\u00b4 \u0131an producido. La me-\nra ideaci\u00b4 on o pensamiento de esta situaci\u00b4 on\nfue tratada como un caso negativo. En el ca-\nso de situaciones en las que las autolesiones\nten\u00b4 \u0131an un \ufb01n autol\u00b4 \u0131tico tambi\u00b4 en fueron eti-\nquetadas como casos negativos al buscar otro\n\ufb01n diferente al ansiol\u00b4 \u0131tico. Estos \u00b4 ultimos ca-\nsos deber\u00b4 \u0131an tratarse en un estudio diferente\ncomo parte de los pacientes con riesgo de sui-\ncidio. Los informes tienen una media de 1310\npalabras y 7566 caracteres por cada informe,\nteniendo el informe de mayor tama\u02dc no 1639\npalabras y 32767 caracteres y el de menor ta-\nma\u02dc no 84 palabras y 510 caracteres. El corpus\nse ha dividido en dos conjuntos de entrena-\nmiento (80%) y test (20%), resultando dos\nconjuntos de 1001 y 251 informes respectiva-\nmente.Ladivisi\u00b4 onsehallevadoacabodefor-\nma estrati\ufb01cada para respetar la proporci\u00b4 on\nde clases en los conjuntos de entrenamiento\ny test.\n4 Sistema de Detecci\u00b4 on de\nAutolesiones\nPara la tarea de detecci\u00b4 on de autolesiones se\nhan desarrollado dos sistemas supervisados,\nuno de ellos basado en la extracci\u00b4 on de ras-gos y la aplicaci\u00b4 on de algoritmos cl\u00b4 asicos de\nclasi\ufb01caci\u00b4 on y el otro basado en redes neuro-\nnales con la aplicaci\u00b4 on de un modelo BERT\npara el tokenizado. Los dos sistemas desarro-\nllados solo analizan el texto anonimizado del\ninformesintenerencuentaeldiagn\u00b4 osticoque\napareceenotrocampoyques\u00b4 olohasidoteni-\ndo en cuenta en el proceso de anotaci\u00b4 on ma-\nnual para ayudar a los expertos en caso de\nduda.\n4.1 Sistema de Aprendizaje\nAutom\u00b4 atico\nEl sistema supervisado est\u00b4 a compuesto de\ntres m\u00b4 odulos diferentes que se encargan de\nlas tareas de pre-procesamiento, extracci\u00b4 on\nde rasgos y aplicaci\u00b4 on de algoritmos de clasi-\n\ufb01caci\u00b4 on.\n4.1.1 Pre-procesamiento de los\nInformes M\u00b4 edicos\nEn cuanto al pre-procesamiento se han apli-\ncado las t\u00b4 ecnicas habituales, como son la con-\nversi\u00b4 on a min\u00b4 usculas del texto, la elimina-\nci\u00b4 on de caracteres especiales, la normaliza-\nci\u00b4 on de determinados conectores, el tokeniza-\ndo del texto y el borrado de palabras vac\u00b4 \u0131as.\nTambi\u00b4 en se ha llevado a cabo un proceso de\nstemming mediante el segundo algoritmo de\nPorter para extraer la ra\u00b4 \u0131z de las palabras.\n4.1.2 Extracci\u00b4 on de Rasgos y\nAlgoritmos de Clasi\ufb01caci\u00b4 on\nLa extracci\u00b4 on de rasgos se puede agrupar en\ncinco conjuntos de caracter\u00b4 \u0131sticas:\n\u2022CountVectorizer : En primer lugar se\nha utilizado la herramienta CountVec-\ntorizer de la biblioteca scikit-learn en\nPython para obtener vectores de pala-\nbras a partir de los informes m\u00b4 edicos.\nEsta herramienta se utiliza para trans-\nformar un texto dado en un vector sobre\nla base de la frecuencia de cada palabra\nque aparece en todo el texto. La funci\u00b4 on\ncrea una matriz en la que cada palabra\n\u00b4 unica est\u00b4 a representada por una colum-\nna de la matriz, y cada muestra de texto\ndel documento es una \ufb01la en dicha ma-\ntriz.Elvalordecadaceldanoesm\u00b4 asque\nla frecuencia de la palabra en esa mues-\ntra de texto en particular.\n\u2022Vocabulario de Autolesiones : Se ha\ncompilado un conjunto de 53 palabras\nrelacionadas con el contexto de las auto-\nlesiones. En este conjunto hay palabras\n132\nJuan Martinez-Romo, Lourdes Araujo, Blanca Reneses, J. Sevilla-Llewellyn-Jones, Ignacio Mart\u00ednez-Capella, Germ\u00e1n Seara-Aguilar como morder, cortar, pellizcar, etc. Es-\nte\nconjunto de palabras, se emplea co-\nmo entrada del CountVectorizer para no\nusar todo el vocabulario completo sino\nsolo estas 53 palabras, algo que propor-\nciona mayor rapidez y precisi\u00b4 on.\n\u2022Diccionario NSSI: Greaves (Greaves,\n2018a) desarroll\u00b4 o un trabajo en el que\nllev\u00b4 o a cabo la clasi\ufb01caci\u00b4 on de un con-\njunto de conceptos relacionados con las\nautolesiones.Elresultadodeestetrabajo\nes un diccionario de palabras relaciona-\ndas con la autolesi\u00b4 on llamado Dicciona-\nrioNon-SuicidalSelf-Injury(NSSI),don-\nde las palabras se dividen en cinco cate-\ngor\u00b4 \u0131as: 1) M\u00b4 etodos de NSSI; 2) T\u00b4 ermi-\nnos de NSSI; 3) Instrumentos utilizados;\n4) Razones de NSSI; y (5) T\u00b4 erminos es-\npec\u00b4 \u0131\ufb01cos de cortes. De esta forma, se han\ncreado cuatro rasgos de NSSI, uno pa-\nra cada categor\u00b4 \u0131a. Estas caracter\u00b4 \u0131sticas\ncuentan la frecuencia de las palabras de\nsu categor\u00b4 \u0131a en el texto.\n\u2022Distancia de T\u00b4 erminos de Autole-\nsiones: Existen numerosos trabajos que\nhan probado la relevancia de las prime-\nras palabras de un documento en rela-\nci\u00b4 on al texto completo. En este grupo de\nrasgos se ha tratado de medir por un la-\ndo la distancia entre el inicio del docu-\nmento y la primera palabra del vocabu-\nlario de autolesiones presente en el texto\ny por otro lado la distancia media entre\npalabrasdelvocabulariodeautolesiones.\nEstas medidas se han realizado en fun-\nci\u00b4 on del n\u00b4 umero de palabras y del n\u00b4 ume-\nro de caracteres, dando lugar a cuatro\nrasgos.\n\u2022Negaci\u00b4 on : Se ha llevado a cabo un pro-\nceso de detecci\u00b4 on de la negaci\u00b4 on median-\nte una arquitectura (Fabregat, Arau-\njo Serna, y Mart\u00b4 \u0131nez Romo, 2019; Fa-\nbregat et al., 2019) basada en aprendi-\nzaje profundo. La detecci\u00b4 on de la nega-\nci\u00b4 onsehaaplicadoalosgruposderasgos\nde\ufb01nidos anteriormente para eliminar la\npresencia de los t\u00b4 erminos de autolesio-\nnes que han sido negados y de esta for-\nma restar su incidencia. Es decir, si en el\ntexto aparece una a\ufb01rmaci\u00b4 on como \u201dNo\nse aprecian cortes\u201d, la detecci\u00b4 on de ne-\ngaci\u00b4 on evita que el t\u00b4 ermino \u201dcortes\u201d se\ncontabilice en ninguno de los rasgos cal-\nculados en este trabajo.Una vez extra\u00b4 \u0131dos los rasgos descritos an-\nteriormente y con la ayuda del corpus anota-\ndo, se ha llevado a cabo la aplicaci\u00b4 on de los\nalgoritmos m\u00b4 as efectivos seg\u00b4 un el estado del\narte en este tipo tareas.\n4.2 Sistema basado en\nAprendizaje Profundo\nEl segundo sistema usa redes neuronales con\nuna arquitectura en la que se disponen tres\ncapas de redes neuronales convolucionales y\npara la que se ha adaptado la tecnolog\u00b4 \u0131a de\nBERT (Devlin et al., 2018) para el proceso\nde tokenizado, que est\u00b4 a basado en la repre-\nsentaci\u00b4 on de codi\ufb01cadores binarios a partir\nde Transformers. En este caso, hemos adap-\ntado esta tecnolog\u00b4 \u0131a para la clasi\ufb01caci\u00b4 on de\ntextos.\nAparte de la preparaci\u00b4 on del texto, para el\ntokenizado de los textos m\u00b4 edicos, hemos usa-\ndo dos modelos pre-entrenados: Un modelo\nbase de BERT1que est\u00b4 a disponible en seis\nidiomas, incluido el espa\u02dc nol, y fue creado pa-\nra tareas de clasi\ufb01caci\u00b4 on de textos. Y el mo-\ndelo RoBERTa-base-bne2, que es un modelo\nde lenguaje enmascarado basado en transfor-\nmersparaelespa\u02dc nol.Est\u00b4 abasadoenelmode-\nlo base de RoBERTa y ha sido pre-entrenado\nutilizando el mayor corpus en espa\u02dc nol cono-\ncido hasta la fecha, con un total de 570GB de\ntexto limpio y procesado expresamente para\neste trabajo. El texto procede de una com-\npilaci\u00b4 on de p\u00b4 aginas web realizada por la Bi-\nblioteca Nacional Espa\u02dc nola desde 2009 hasta\n2019.\nDe esta forma, se ha adoptado una arqui-\ntecturaqueconstadetrescapasderedesneu-\nronales convolucionales concatenadas. La ar-\nquitectura del sistema de aprendizaje profun-\ndo usada para este trabajo puede apreciarse\nenlaFigura1,enlaquesemuestraanivelge-\nneral la arquitectura de la red neuronal, con\ntres capas de redes neuronales convoluciona-\nles (CNN) y dos capas de redes neuronales\ndensamente conectadas, la \u00b4 ultima empleada\ncomo capa de clasi\ufb01caci\u00b4 on.\n5 Resultados\nPara la evaluaci\u00b4 on de los dos sistemas desa-\nrrollados vamos a usar las medidas tradicio-\nnales de clasi\ufb01caci\u00b4 on precisi\u00b4 on, cobertura y\n1https://huggingface.co/nlptown/bert-base-\nmultilingual-uncased-sentiment\n2https://huggingface.co/PlanTL-GOB-\nES/roberta-base-bne\n133\nDetecci\u00f3n de Indicios de Autolesiones No Suicidas en Informes M\u00e9dicos de Psiquiatr\u00eda Mediante el An\u00e1lisis del Lenguaje Figura 1: Arquitectura de la red neuronal.\nme\ndida-F. Adem\u00b4 as, como la detecci\u00b4 on de ca-\nsos de autolesiones es una tarea cuyas impli-\ncaciones requieren una gran precisi\u00b4 on, uno de\nlos principales objetivos de este trabajo es la\nb\u00b4 usqueda de un buen desempe\u02dc no a la hora de\npredecir casos positivos. Tambi\u00b4 en el hecho de\nserunatarearelacionadaconlasalud,requie-\nre de un grado satisfactorio de explicabilidad\nde cara a los profesionales que en \u00b4 ultima ins-\ntancia deben de tomar las decisiones.\n5.1 Baselines\nEn primer lugar, hemos desarrollado cuatro\nbaselines para medir la calidad de los siste-\nmas supervisados.\n\u2022Most frequent Class (MFC): El sis-\ntema anota todos los casos con la clase\nm\u00b4 as frecuente, que en este caso es la cla-\nse negativa.\n\u2022Less frequent Class (LFC): El siste-\nmaanotatodosloscasosconlaclaseme-\nnos frecuente, que en este caso es la clase\npositiva.\n\u2022Random prediction: El sistema asig-\nna una predicci\u00b4 on aleatoria a cada ins-\ntancia.\n\u2022Random Ratio prediction: El siste-\nma asigna mediante una funci\u00b4 on de pro-babilidad una predicci\u00b4 on aleatoria a ca-\nda instancia, manteniendo el mismo ra-\ntio (positivas/negativas) de anotaciones\nque el conjunto de test.\nLa Tabla 1 muestra los resultados tras la\naplicaci\u00b4 on de los baselines. Como era de espe-\nrar, al tratarse de un corpus desbalanceado,\nel baseline que mejor rendimiento obtiene es\naquel que predice como negativos todos los\ncasos al ser la clase mayoritaria.\n5.2 Combinaci\u00b4 on de Rasgos\nEn la Tabla 2 se pueden apreciar los resulta-\ndos obtenidos tras diferentes combinaciones\nde los rasgos descritos en la secci\u00b4 on 4.1.2. Pa-\nra estos resultados se ha aplicado un algorit-\nmo de regresi\u00b4 on log\u00b4 \u0131stica. De forma evidente\nen cuanto a la hip\u00b4 otesis de partida, los peo-\nres resultados se obtienen con los vectores de\npalabras formados por el vocabulario com-\npleto del corpus (32K palabras) y los mejo-\nres se consiguen con la combinaci\u00b4 on de todos\nlos rasgos computados. En cuanto a la parte\nm\u00b4 as interesante de esta combinaci\u00b4 on, destaca\nla diferencia entre la mejora obtenida por la\nclase negativa y la positiva al introducir los\nrasgos. La clase negativa solo aumenta tres\npuntossumedida-F,mientrasquelaclasepo-\nsitiva aumenta 21 puntos al introducir todos\nlos rasgos. Esta diferencia demuestra la e\ufb01-\nciencia de los rasgos introducidos en cuanto\nal prop\u00b4 osito general de mejorar sobre todo la\npredicci\u00b4 on de los casos positivos. En cuanto a\nlos rasgos, analizados de manera individual,\ndestaca la aportaci\u00b4 on de los vectores de pala-\nbras obtenidos a partir del vocabulario com-\npilado manualmente de 53 palabras. La dife-\nrencia entre usar el vocabulario completo o\nsolo las 53 palabras, se re\ufb02eja en un aumento\nde 14 puntos en la medida-F de la clase posi-\ntiva, aumentando tanto la precisi\u00b4 on como la\ncobertura. Los rasgos que menos aportaci\u00b4 on\nparecen tener en el c\u00b4 omputo global son las\ndistancias entre t\u00b4 erminos de autolesiones y la\nnegaci\u00b4 on, quiz\u00b4 as debido a que no se produ-\ncen demasiadas en los informes m\u00b4 edicos o su\nrelevancia es menor de la esperada en cuanto\nal contexto global del informe. En cuanto a\nla clase positiva, la precisi\u00b4 on y cobertura au-\nmentandeformadesigual,teniendolosrasgos\ncomputados un impacto mayor en la preci-\nsi\u00b4 on que en la cobertura. Este hecho era de\nesperar dado que al menos el rasgo que usa\nlos vectores de palabras con un vocabulario\n134\nJuan Martinez-Romo, Lourdes Araujo, Blanca Reneses, J. Sevilla-Llewellyn-Jones, Ignacio Mart\u00ednez-Capella, Germ\u00e1n Seara-Aguilar BASELINES\nBaseline F1 Todas Clases Clase Positiva\nF1-NO F1-SI F1-weighted Avg P-SI R-SI\nMFC 0.95 0.00 0.86 0.00 0.00\nLFC 0.00 0.17 0.02 0.09 1.00\nRandom Prediction 0.65 0.14 0.60 0.08 0.43\nRandom Ratio Prediction 0.91 0.05 0.83 0.05 0.04\nTabla 1: F1-SI: F1-measure de los casos positivos, F1-NO: F1-me asure de los casos negativos,\nF1-weighted Avg: F1-measure media de todo el conjunto de test, P-SI: Precisi\u00b4 on de los casos\npositivos, R-SI: Recall de los casos positivos\nreducido implica profundizar en esa direcci\u00b4 on\nprecisamente.\n5.3 An\u00b4 alisis de diferentes\nalgoritmos de Clasi\ufb01caci\u00b4 on\nEn la secci\u00b4 on anterior se emple\u00b4 o un algoritmo\nde regresi\u00b4 on log\u00b4 \u0131stica para la tarea de clasi\ufb01-\ncaci\u00b4 on. En la Tabla 3 se muestran los resul-\ntados al aplicar los diferentes algoritmos de\nclasi\ufb01caci\u00b4 on que mejor rendimiento han ob-\ntenido en diferentes trabajos del estado del\narte consultados. Para esta comparativa se\nha usado la combinaci\u00b4 on de rasgos que me-\njor rendimiento obtuvo en la secci\u00b4 on anterior\ny cuyos resultados se pueden observar en la\nTabla 2. Como se puede ver, hay tres algorit-\nmos (Logistic Regression, Gradient Boosting\ny SVM) que obtienen los mejores resultados\nen cuanto a la medida-F global. Sin embar-\ngo, como uno de los objetivos de este trabajo\nconsiste en mejorar la detecci\u00b4 on de la clase\npositiva, se observa que el algoritmo \u201cGra-\ndient Boosting\u201d obtiene el mejor rendimien-\nto en la predicci\u00b4 on de casos positivos. Esto\nunido a que era uno de los tres algoritmos\nque de forma global obten\u00b4 \u0131an mejores resul-\ntados lo convierten en la mejor opci\u00b4 on para\nnuestro sistema. Profundizando en los resul-\ntados de \u201cGradient Boosting\u201d, aparte de ob-\ntener los mejores resultados en las clases po-\nsitivas, negativas, y de forma global, obtiene\nmejor cobertura que ning\u00b4 un otro algoritmo.\nEsta parece ser su mejor aportaci\u00b4 on, ya que\nsu precisi\u00b4 on en la clase positiva es superada\npor otros algoritmos.\n5.4 Sistema basado en\nAprendizaje Profundo\nEn la Tabla 4 se puede observar el rendi-\nmiento de los sistemas basados en redes neu-\nronales. Se ha optado por variar dos hiper-\npar\u00b4 ametros como son el n\u00b4 umero de \u00b4 epocasy el dropout. En todos los experimentos se\nhan usado embeddings de 200 dimensiones.\nDe los resultados obtenidos en cuanto a las\ndiferentes combinaciones no se pueden obte-\nnerdemasiadasconclusiones.Quiz\u00b4 assepuede\nobservar que un dropout bajo mejora el ren-\ndimientoglobalaunquenoesconcluyente.De\nformageneralpodr\u00b4 \u0131adecirsequecinco\u00b4 epocas\nhan funcionado mejor que diez, al igual que\nocurre para la clase positiva con la que lige-\nramente se observan mejores resultados. En\ncuanto a la precisi\u00b4 on de los casos positivos,\ncon una combinaci\u00b4 on se obtienen mucho me-\njores resultados que con el resto, sin embargo\nsu cobertura y medida-F se ven negativamen-\nte afectadas. La \u00b4 unica conclusi\u00b4 on evidente a\nnivel de diferentes combinaciones se produ-\nce en la cobertura de la clase positiva. En\neste caso un menor n\u00b4 umero de \u00b4 epocas y un\nbajo dropout implican un signi\ufb01cativo mejor\nrendimiento que los casos opuestos con una\ndiferencia de 50 puntos.\n5.5 An\u00b4 alisis Global de Resultados\nDe forma general y tal como se muestra en\nla Tabla 5, los sistemas desarrollados superan\nampliamentealosbaselinespropuestosalini-\ncio del trabajo. En cuanto a la comparativa\nentre el mejor sistema supervisado y el me-\njor sistema basado en redes neuronales, glo-\nbalmente el sistema de aprendizaje profundo\nobtiene mejores resultados si atendemos a la\nmedida-F. Sin embargo, la peque\u02dc na diferen-\ncia a favor de las redes neuronales en relaci\u00b4 on\na la medida-F global y de la clase negativa, se\nve ampliamente superada en cuanto a la clase\npositiva tanto en la medida-F como en la pre-\ncisi\u00b4 on y la cobertura. Destaca notablemente\nla diferencia en la precisi\u00b4 on, de forma muy\nsigni\ufb01cativa la medida-F y de forma relevan-\nte la cobertura, siendo esta \u00b4 ultima la medi-\nda donde la diferencia de rendimiento es algo\n135\nDetecci\u00f3n de Indicios de Autolesiones No Suicidas en Informes M\u00e9dicos de Psiquiatr\u00eda Mediante el An\u00e1lisis del Lenguaje COMBINACI \u00b4ON DE RASGOS\nFeatures F1-NO F1-SI F1-W Avg P-SIR-SI\nCV 0.94 0.47 0.90 0.610.33\nCV + NSSI 0.96 0.49 0.92 0.640.39\nCV + VAL 0.96 0.61 0.93 0.840.50\nCV + VAL + NSSI 0.97 0.63 0.94 0.860.52\nCV + VAL + NSSI + DIS + NEG 0.97 0.65 0.95 0.870.54\nTabla 2: CV: CountVectorizer, VAL: Vocabulario de Autolesione s, NSSI: Rasgos de NSSI, DIS:\nRasgos de distancia de terminos de autolesion, NEG: Negaci\u00b4 on\nALGORITMOS DE CLASIFICACI \u00b4ON\nAlgoritmo F1 Todas Clases Clase Positiva\nF1-NO F1-SI F1-weighted Avg P-SI R-SI\nLogistic Regression 0.97 0.65 0.94 0.86 0.52\nRandom Forest 0.95 0.53 0.91 0.50 0.57\nGradient Boosting 0.97 0.68 0.94 0.71 0.65\nK Neighbours 0.96 0.36 0.91 1.00 0.22\nSVM 0.97 0.59 0.94 0.91 0.43\nAdaboost 0.96 0.59 0.93 0.62 0.57\nTabla 3: F1-SI: F1-measure de los casos positivos, F1-NO: F1-me asure de los casos negativos,\nF1-weighted Avg: F1-measure media de todo el conjunto de test, P-SI: Precisi\u00b4 on de los casos\npositivos, R-SI: Recall de los casos positivos\nmenor. De esta forma, y teniendo en cuen-\nta las implicaciones en cuanto a mejor expli-\ncabilidad del sistema supervisado basado en\nel algoritmo \u201cGradient Boosting\u201d, considera-\nmos que la opci\u00b4 on m\u00b4 as \u00b4 optima para la tarea\nconcreta en la que se centra este trabajo es\ndicho sistema.\n5.6 An\u00b4 alisis de la Incidencia de las\nCategor\u00b4 \u0131as de Rasgos\nDado que el sistema supervisado ofrece un\nmejor rendimiento en la detecci\u00b4 on de casos\npositivos y adem\u00b4 as su grado de explicabili-\ndad es mayor, hemos decidido profundizar en\nlos rasgos extra\u00b4 \u0131dos y su incidencia en los re-\nsultados. En la \ufb01gura 2 se muestra un gr\u00b4 a\ufb01-\nco de barras que representa la frecuencia de\naparici\u00b4 on de los rasgos que componen las di-\nferentes categor\u00b4 \u0131as en funci\u00b4 on de la clase a la\nque pertenecen. Dicha frecuencia se ha nor-\nmalizado en funci\u00b4 on del n\u00b4 umero de documen-\ntos de cada clase y tama\u02dc no del vocabulario\nde cada categor\u00b4 \u0131a, dado que el corpus est\u00b4 a\nmuy desbalanceado. En esta \ufb01gura destacan\npositivamente categor\u00b4 \u0131as como el vocabula-\nrio de autolesiones, los t\u00b4 erminos de NSSI, los\nconceptos de autolesiones por cortes de NS-\nSI y la negaci\u00b4 on. En cuanto al vocabulariode autolesiones, se intu\u00b4 \u0131a esta diferencia de-\nbido a los resultados obtenidos. En cuanto\na la negaci\u00b4 on, tambi\u00b4 en se observa una gran\ndisparidad. Finalmente en cuanto a los con-\nceptos de NSSI, los que mejor parecen repre-\nsentar a la clase positiva son los \u201cT\u00b4 erminos\u201d\ny el \u201cCutting\u201d. Sin embargo, los conceptos\nde \u201cRazones\u201d, \u201cM\u00b4 etodos\u201d e \u201cInstrumentos\u201d\nofrecen una menor divergencia. Una posible\nexplicaci\u00b4 on del distinto funcionamiento de es-\ntos conceptos de NSSI reside en el hecho de\nquelosinformesm\u00b4 edicostratandere\ufb02ejarlos\nhechos m\u00b4 as relevantes representados segura-\nmente de una forma gen\u00b4 erica y sin profun-\ndizar en determinados aspectos. De esta for-\nma, los conceptos de \u201cRazones\u201d, \u201cM\u00b4 etodos\u201d\ne \u201cInstrumentos\u201d implican un mayor detalle\nen la descripci\u00b4 on del suceso del que se suele\nencontrar en un informe.\nEn la \ufb01gura 3 se observa un gr\u00b4 a\ufb01co de ba-\nrras en el que aparecen las ra\u00b4 \u0131ces de los t\u00b4 ermi-\nnos m\u00b4 as frecuentes del vocabulario de auto-\nlesiones ordenados por su frecuencia norma-\nlizada de aparici\u00b4 on y en funci\u00b4 on de la clase\na la que pertenecen. Como se puede obser-\nvar,ra\u00b4 \u0131cescomo\u201cautolesi\u00b4 on\u201d,\u201ccort\u201dy\u201crasg\u201d\npresentan una gran diferencia a favor de las\nclases positivas, mientras que otras ra\u00b4 \u0131ces co-\n136\nJuan Martinez-Romo, Lourdes Araujo, Blanca Reneses, J. Sevilla-Llewellyn-Jones, Ignacio Mart\u00ednez-Capella, Germ\u00e1n Seara-Aguilar Red Neuronal\nF1 Todas Clases Clase Positiva\nModelo F1-NO F1-SI F1-weighted Avg P-SI R-SI\nBert Multiling\u00a8 ue\nCVN + BERT (5ep,0.05do) 0.97 0.56 0.95 0.50 0.64\nCVN + BERT (5ep,0.1do) 0.96 0.51 0.94 0.43 0.64\nCVN + BERT (5ep,0.2do) 0.96 0.47 0.94 0.40 0.57\nCVN + BERT (5ep,0.3do) 0.98 0.57 0.95 0.57 0.57\nCVN + BERT (5ep,0.4do) 0.96 0.44 0.93 0.36 0.57\nCVN + BERT (10ep,0.05do) 0.97 0.55 0.95 0.53 0.57\nCVN + BERT (10ep,0.1do) 0.98 0.33 0.94 0.75 0.21\nCVN + BERT (10ep,0.2do) 0.97 0.22 0.93 0.50 0.14\nCVN + BERT (10ep,0.3do) 0.97 0.24 0.93 0.67 0.14\nCVN + BERT (10ep,0.4do) 0.97 0.20 0.93 0.33 0.14\nRoBERTa\nCVN + RoBERTa (5ep,0.05do) 0.96 0.39 0.92 0.43 0.35\nCVN + RoBERTa (5ep,0.1do) 0.96 0.17 0.91 0.33 0.12\nCVN + RoBERTa (5ep,0.2do) 0.96 0.37 0.93 0.50 0.29\nCVN + RoBERTa (5ep,0.3do) 0.96 0.48 0.93 0.50 0.29\nCVN + RoBERTa (5ep,0.4do) 0.96 0.54 0.94 0.50 0.59\nCVN + RoBERTa (10ep,0.05do) 0.96 0.48 0.93 0.50 0.47\nCVN + RoBERTa (10ep,0.1do) 0.96 0.41 0.93 0.50 0.35\nCVN + RoBERTa (10ep,0.2do) 0.95 0.28 0.90 0.26 0.29\nCVN + RoBERTa (10ep,0.3do) 0.97 0.55 0.95 0.67 0.47\nCVN + RoBERTa (10ep,0.4do) 0.96 0.41 0.93 0.50 0.35\nTabla 4: F1-SI: F1-measure de los casos positivos, F1-NO: F1-me asure de los casos negativos,\nF1-weighted Avg: F1-measure media de todo el conjunto de test, P-SI: Precisi\u00b4 on de los casos\npositivos, R-SI: Recall de los casos positivos. Los sistemas var\u00b4 \u0131an en funci\u00b4 on del n\u00b4 umero de\n\u00b4 epocas (5-10 ep) y el dropout (0.05-0.4 do).\nCOMPARATIVA DE RESULTADOS\nSistema F1 Todas Clases Clase Positiva\nF1-NO F1-SI F1-weighted Avg P-SI R-SI\nBaseline MFC 0.95 0.00 0.86 0.00 0.00\nBaseline LFC 0.00 0.17 0.02 0.09 1.00\nGradient Boosting 0.97 0.68 0.94 0.71 0.65\nCVN + BERT (5ep,0.3do) 0.98 0.57 0.95 0.57 0.57\nTabla 5: F1-SI: F1-measure de los casos positivos, F1-NO: F1-me asure de los casos negativos,\nF1-weighted Avg: F1-measure media de todo el conjunto de test, P-SI: Precisi\u00b4 on de los casos\npositivos, R-SI: Recall de los casos positivos\nmo \u201ctir\u201d, e \u201cinger\u201d muestran una aparici\u00b4 on\nm\u00b4 as equilibrada dado los conceptos m\u00b4 as neu-\ntrales que representan en cuanto a las autole-\nsiones. Destaca la ra\u00b4 \u0131z \u201csangr\u201d, teniendo m\u00b4 as\npeso en la clase negativa debido seguramente\na que las lesiones relacionadas con la sangre\nno son las m\u00b4 as frecuentes en el problema es-\ntudiado.6 Conclusiones y trabajo futuro\nEn este trabajo se presenta un sistema de de-\ntecci\u00b4 on de indicios de autolesiones no suici-\ndas, basado en el an\u00b4 alisis del lenguaje, sobre\nun conjunto anotado de informes m\u00b4 edicos ob-\ntenidos del servicio de psiquiatr\u00b4 \u0131a de un Hos-\npital p\u00b4 ublico madrile\u02dc no. Dada la naturaleza\ntan cr\u00b4 \u0131tica de la tarea y las implicaciones a\n137\nDetecci\u00f3n de Indicios de Autolesiones No Suicidas en Informes M\u00e9dicos de Psiquiatr\u00eda Mediante el An\u00e1lisis del Lenguaje Figura 2: Frecuencia normalizada de las dife-\nre\nntes categor\u00b4 \u0131as de rasgos en funci\u00b4 on de la\nclase.\nFigura 3: Frecuencia normalizada de t\u00b4 ermi-\nnos\nde autolesiones en funci\u00b4 on de la clase.\nla hora de predecir incorrectamente un caso,\nque realmente tiene detr\u00b4 as a un ser humano\nreal, es necesario tratar este tipo de trabajos\ndesde un punto de vista diferente al mero re-\nsultado obtenido por un conjunto de m\u00b4 etricas\nde evaluaci\u00b4 on. Al inicio del trabajo se \ufb01ja-\nron tres objetivos prioritarios: por un lado el\nsistema deber\u00b4 \u0131a obtener un alto rendimiento\npara impedir en la medida de lo posible las\npredicciones err\u00b4 oneas, por otro lado la clasi\ufb01-\ncaci\u00b4 on correcta de los casos positivos deber\u00b4 \u0131a\nser prioritaria, y \ufb01nalmente se deber\u00b4 \u0131a bus-\ncar la mayor explicabilidad del sistema para\nque el profesional sanitario pudiera tener la\nm\u00b4 axima informaci\u00b4 on de cara a tomar una de-\ncisi\u00b4 on \ufb01nal. Teniendo en cuenta estos requisi-\ntos, el trabajo ha cumplido con los objetivos.\nPor un lado el rendimiento global obtenidoalcanza unos valores de medida-F de 0.95, al-\ncanzando un 0.68 de medida-F para los casos\npositivos, lo cual es una prueba de su buen\nfuncionamiento. Con la extracci\u00b4 on de un con-\njunto de rasgos muy focalizados en alcanzar\nun mayor rendimiento en cuanto a la detec-\nci\u00b4 on de los casos positivos, se ha conseguido\nel segundo objetivo equilibrando y mejoran-\ndo tanto la precisi\u00b4 on como la cobertura de\nforma signi\ufb01cativa. Y \ufb01nalmente, gracias al\nesfuerzo realizado para equiparar un sistema\nsupervisado basado en algoritmos tradiciona-\nles de clasi\ufb01caci\u00b4 on a un sistema basado en re-\ndes neuronales, se ha hecho posible el poder\nelegir el primero de los sistemas ya que con\nun rendimiento global similar tiene dos ven-\ntajas como son el mejor rendimiento en cuan-\nto a la clasi\ufb01caci\u00b4 on de casos positivos y una\nmejor explicabilidad debido a que los rasgos\nobtenidos forman parte de la decisi\u00b4 on toma-\nda \ufb01nalmente por el sistema. En este \u00b4 ultimo\ncaso, el sistema basado en redes neuronales,\na pesar de tener un ligero mejor rendimiento\nglobal, obtiene peores resultados en la clase\npositiva y adem\u00b4 as sus decisiones a d\u00b4 \u0131a de hoy\nsondif\u00b4 \u0131cilesdeexplicardecaraaunpsic\u00b4 ologo\no psiquiatra.\nEn cuanto al trabajo futuro, consideramos\nvarias l\u00b4 \u0131neas de actuaci\u00b4 on. Por un lado el cor-\npus est\u00b4 a desbalanceado y adem\u00b4 as no dispone\ndeungrann\u00b4 umerodecasospositivos.Deesta\nforma trabajaremos para obtener un corpus\nde mayor tama\u02dc no con la esperanza de que un\nmayor n\u00b4 umero de casos positivos nos ayude a\nmejorar a\u00b4 un m\u00b4 as la detecci\u00b4 on de este tipo de\ncasos. Por otro lado, debido al gran potencial\nde tecnolog\u00b4 \u0131as como las redes neuronales, tra-\nbajaremos para optimizar el sistema e incluir\ntransformers con los objetivos de mejorar su\nrendimiento en cuanto a los casos positivos e\niniciar un trabajo de estudio para mejorar la\nexplicabilidad de este tipo de sistemas.\nAgradecimientos\nThis work has been partially supported by\nthe Spanish Ministry of Science and In-\nnovation within the DOTT-HEALTH Pro-\nject (MCI/AEI/FEDER, UE) under Grant\nPID2019-106942RB-C32 and the project\nRAICES (IMIENS 2022).\nBibliograf\u00b4 \u0131a\nAgeitos, E. C., J. Mart\u00b4 \u0131nez-Romo, y L. Arau-\njo. 2020. Nlp-uned at erisk 2020: Self-\nharm early risk detection with sentiment\n138\nJuan Martinez-Romo, Lourdes Araujo, Blanca Reneses, J. Sevilla-Llewellyn-Jones, Ignacio Mart\u00ednez-Capella, Germ\u00e1n Seara-Aguilar analysis and linguistic features. En CLEF\n(\nWorking Notes) .\nBaetens, I., L. Claes, J. Muehlenkamp,\nH. Grietens, y P. Onghena. 2011. Non-\nSuicidal and Suicidal Self-Injurious Beha-\nvior among Flemish Adolescents: A Web-\nSurvey. Archives of Suicide Research,\n15(1):56\u201367.\nBurke, T. A., B. A. Ammerman, y R. Jaco-\nbucci. 2019. The use of machine learning\nin the study of suicidal and non-suicidal\nself-injurious thoughts and behaviors: A\nsystematic review. Journal of a\ufb00ective di-\nsorders, 245:869\u2013884.\nCampillo-Ageitos, E., H. Fabregat, L. Arau-\njo, y J. Martinez-Romo. 2021. Nlp-uned\nat erisk 2021: self-harm early risk detec-\ntion with tf-idf and linguistic features.\nWorking Notes of CLEF , p\u00b4 aginas 21\u201324.\nDelgado-Gomez, D., E. Baca-Garcia,\nD. Aguado, P. Courtet, y J. Lopez-\nCastroman. 2016. Computerized adap-\ntive test vs. decision trees: development\nof a support decision system to identify\nsuicidal behavior. Journal of a\ufb00ective\ndisorders , 206:204\u2013209.\nDevlin, J., M.-W. Chang, K. Lee, y K. Tou-\ntanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for lan-\nguage understanding. arXiv preprint ar-\nXiv:1810.04805.\nFabregat, H., L. Araujo Serna, y\nJ. Mart\u00b4 \u0131nez Romo. 2019. Deep lear-\nning approach for negation trigger and\nscope recognition.\nFabregat, H., A. Duque, J. Mart\u00b4 \u0131nez-Romo,\ny L. Araujo. 2019. Extending a deep lear-\nning approach for negation cues detection\ninspanish. En IberLEF@ SEPLN,p\u00b4 aginas\n369\u2013377.\nFernandes, A. C., R. Dutta, S. Velupillai,\nJ. Sanyal, R. Stewart, y D. Chandran.\n2018. Identifying suicide ideation and sui-\ncidal attempts in a psychiatric clinical re-\nsearch database using natural language\nprocessing. Scienti\ufb01c reports , 8(1):1\u201310.\nGreaves, M. M. 2018a. A Corpus Linguis-\ntic Analysis of Public Reddit and Tumblr\nBlog Posts on Non-Suicidal Self-Injury,\nAn abstract. Ph.D. tesis, College of Edu-\ncation, Oregon State University.Greaves, M. M. 2018b. A corpus linguistic\nanalysis of public reddit and tumblr blog\nposts on non-suicidal self-injury.\nHaerian, K., H. Salmasian, y C. Friedman.\n2012. Methods for identifying suicide\nor suicidal ideation in ehrs. En AMIA\nannual symposium proceedings , volumen\n2012, p\u00b4 agina 1244. American Medical In-\nformatics Association.\nKessler, R. C., M. B. Stein, M. V. Petukhova,\nP. Bliese, R. M. Bossarte, E. J. Bromet,\nC. S. Fullerton, S. E. Gilman, C. Ivany,\nL. Lewandowski-Romps, y others. 2017.\nPredicting suicides after outpatient men-\ntal health visits in the army study to\nassess risk and resilience in servicemem-\nbers (army starrs). Molecular psychiatry ,\n22(4):544\u2013551.\nLeCun, Y., Y. Bengio, y G. Hinton. 2015.\nDeep learning. nature, 521(7553):436\u2013\n444.\nLosada, D. E., F. Crestani, y J. Parapar.\n2019. Overview of erisk 2019 early risk\nprediction on the internet. En Interna-\ntional Conference of the Cross-Language\nEvaluation Forum for European Langua-\nges, p\u00b4 aginas 340\u2013357. Springer.\nLosada, D. E., F. Crestani, y J. Parapar.\n2020. erisk 2020: Self-harm and depres-\nsion challenges. En European Conferen-\nce on Information Retrieval , p\u00b4 aginas 557\u2013\n563. Springer.\nLoyola, J. M., S. Burdisso, H. Thompson,\nL. Cagnina, y M. Errecalde. 2021. Unsl\nat erisk 2021: A comparison of three early\nalert policies for early risk detection. En\nWorking Notes of CLEF 2021-Conference\nand Labs of the Evaluation Forum, Buca-\nrest, Romania.\nMann, J. J., S. P. Ellis, C. M. Waternaux,\nX. Liu, M. A. Oquendo, K. M. Malone,\nB. S. Brodsky, G. L. Haas, y D. Currier.\n2008. Classi\ufb01cation trees distinguish sui-\ncide attempters in major psychiatric di-\nsorders: a model of clinical decision ma-\nking.The Journal of clinical psychiatry ,\n69(1):2693.\nMart\u0131nez-Castano, R., A. Htait, L. Azzopar-\ndi,yY.Moshfeghi. 2020. Earlyriskdetec-\ntion of self-harm and depression severity\nusing bert-based transformers. Working\nNotes of CLEF , p\u00b4 agina 16.\n139\nDetecci\u00f3n de Indicios de Autolesiones No Suicidas en Informes M\u00e9dicos de Psiquiatr\u00eda Mediante el An\u00e1lisis del Lenguaje McCoy,T.H.,V.M.Castro,A.M.Roberson,\nL.\nA. Snapper, y R. H. Perlis. 2016. Im-\nproving prediction of suicide and acciden-\ntal death after discharge from general hos-\npitals with natural language processing.\nJAMA psychiatry , 73(10):1064\u20131071.\nMetzger, M.-H., N. Tvardik, Q. Gicquel,\nC. Bouvry, E. Poulet, y V. Potinet-\nPagliaroli. 2017. Use of emergency de-\npartment electronic medical records for\nautomated epidemiological surveillance of\nsuicide attempts: a french pilot study. In-\nternational journal of methods in psychia-\ntric research, 26(2):e1522.\nNicolai, K. A., M. D. Wielgus, y A. Mezu-\nlis. 2016. Identifying Risk for Self-Harm:\nRumination and Negative A\ufb00ectivity in\nthe Prospective Prediction of Nonsuicidal\nSelf-Injury. Suicide and Life-Threatening\nBehavior , 46(2):223\u2013233.\nObeid, J. S., J. Dahne, S. Christensen, S. Ho-\nward, T. Crawford, L. J. Frey, T. Stecker,\ny B. E. Bunnell. 2020. Identifying and\npredicting intentional self-harm in electro-\nnic health record clinical notes: deep lear-\nning approach. JMIR medical informa-\ntics, 8(7):e17784.\nObeid,J.S.,E.R.Weeda,A.J.Matuskowitz,\nK. Gagnon, T. Crawford, C. M. Carr, y\nL. J. Frey. 2019. Automated detection\nof altered mental status in emergency de-\npartment clinical notes: a deep learning\napproach. BMC medical informatics and\ndecision making, 19(1):1\u20139.\nParapar, J., P. Mart\u00b4 \u0131n-Rodilla, D. E. Losada,\ny F. Crestani. 2021. Overview of erisk\n2021:Earlyriskpredictionontheinternet.\nEnInternational Conference of the Cross-\nLanguage Evaluation Forum for European\nLanguages , p\u00b4 aginas 324\u2013344. Springer.\nPennebaker, J. W., M. R. Mehl, y K. G.\nNiederho\ufb00er. 2003. Psychological as-\npects of natural language use: Our words,\nour selves. Annual review of psychology ,\n54(1):547\u2013577.\nPoulin, C., B. Shiner, P. Thompson, L. Veps-\ntas, Y. Young-Xu, B. Goertzel, B. Watts,\nL. Flashman, y T. McAllister. 2014.\nPredicting the risk of suicide by analy-\nzing the text of clinical notes. PloS one,\n9(1):e85733.Rodham, K., K. Hawton, y E. Evans. 2004.\nReasons for deliberate self-harm: Compa-\nrison of self-poisoners and self-cutters in a\ncommunity sample of adolescents. Jour-\nnal of the American Academy of Child &\nAdolescent Psychiatry , 43(1):80\u201387.\nRozova, V., K. Witt, J. Robinson, Y. Li, y\nK. Verspoor. 2022. Detection of self-\nharm and suicidal ideation in emergency\ndepartment triage notes. Journal of the\nAmerican Medical Informatics Associa-\ntion, 29(3):472\u2013480.\nWalsh, C. G., J. D. Ribeiro, y J. C. Franklin.\n2017. Predicting risk of suicide attempts\nover time through machine learning. Cli-\nnical Psychological Science, 5(3):457\u2013469.\nYoung, R., M. Van Beinum, H. Sweeting, y\nP. West. 2007. Young people who self-\nharm.The British Journal of Psychiatry ,\n191(1):44\u201349.\n140\nJuan Martinez-Romo, Lourdes Araujo, Blanca Reneses, J. Sevilla-Llewellyn-Jones, Ignacio Mart\u00ednez-Capella, Germ\u00e1n Seara-Aguilar Semantic Relations Pred ict the Bracketing of Three-Component \nMulti word Terms  \nLas Relaciones Sem\u00e1nticas  Predicen la Desamb iguaci\u00f3n Estructural de  las Unidades \nT erminol\u00f3gicas Polil\u00e9xicas con Tres Formantes  \nJuan Rojas -Garcia  \nUniversi ty of Granada , Granada,  Spain \njuanrojas@ugr.es  \n \nAbstract : For English multiword terms (MWTs)  of three or more constituents  (e.g., sea level rise ), a \nsemantic analysis, based on lin guistic and domain knowledge, is necessary to resolve the d ependency \nbetween components. This structu ral disambiguation, often known as bracketing , involves the \ngrouping of the dependent components so that the MWT is reduced to its basic form of \nmodifier+h ead, as in [ sea level ] [rise]. Knowledge  of these de pendencies facilitates the \ncomprehension of an MW T and  its accurate translation into other languages . Moreover , the \nresolution of MWT bracketing provides a higher overall accuracy in machine translation s ystems and \nsentence parsers . This paper  thus presents  a pilot study that explored whether  the bracket ing of a \nternary compound, when used as an argument in a sentence, can be predicted from the semantic \ninformation encoded in that sentence.  It is shown tha t, with a random  forest  model, the semantic \nrelation  of the MWT to  another argument in the same sente nce, the lexical domain  of the predicate , \nand the semantic role  of the MWT were able to  predict the bracketing of t he 190 ternary compounds  \nused as argumen ts in a sample of 188 semantically annotated sentences  from a Coastal Engineering \ncorpus  (100% F1-score). Furthermore, only the semantic relation of an MWT to another argument in \nthe same sentence  proved enormous capab ility to predict ternary compound bracketing with a  binary  \ndecision -tree model  (94.12% F1-score). \nKeywords : Semantic Relation, Multiword -Term Bracketing , Random Forest, Decision Tree.  \nResumen : En unidades term inol\u00f3gicas polil\u00e9xicas ( UTP) con tres o m\u00e1s formantes  en leng ua \ninglesa  (p.ej., sea level rise), establecer  la dependencia entre dichos for mantes requiere de un  an\u00e1lisis \nling\u00fc\u00edstico  y de conocimiento especializado  del \u00e1rea concreta en que se emplean  las UTP.  Esta \ndesambiguaci\u00f3n estructural , o bracketing , implica el agrupamiento de los form antes para reducir la \nUTP a su estructura  b\u00e1sica  de modificador+n\u00facleo, como en [sea level ] [rise]. Conoce r el bracketing  \nde una UTP no solo facilita su compre nsi\u00f3n  y traducci\u00f3n a otras lenguas , sino que tambi\u00e9n  mejora el \ndesempe\u00f1o de los sistemas de tradu cci\u00f3n autom\u00e1tica  y de los analizadores sint\u00e1cti cos. Por tanto, en \neste art\u00edculo presentamos un estudi o piloto  que explora  si el bracketin g de un a UTP con tres \nformantes , al emplearse como argumento en una oraci\u00f3n, puede predecirse a partir de la informaci\u00f3 n \nsem\u00e1ntica  codifi cada en dicha oraci\u00f3n.  Se muestra que, con un modelo random forest, la relaci\u00f3n \nsem\u00e1ntica de la UTP con otro argumento en la misma oraci\u00f3n, el dominio l\u00e9xico del verbo  y el rol \nsem\u00e1ntico de la UTP son capaces de predecir el bracketing  de las 190 UTP ternarias  que se usan \ncomo argumento  en una muestra de 188 oraciones , anotadas sem\u00e1ntica mente  y extra\u00eddas de un \ncorpus sobre in genier\u00eda de costas  (con un  valor de F1 del 100  %). Adem\u00e1s, \u00fanicamente la relaci\u00f3 n \nsem\u00e1ntica  que manti ene una UTP ternaria con otro argumento en la misma oraci\u00f3n  posee una \nenorme capacidad para predecir su bracketing  mediante  un \u00e1rbol de decisi\u00f3n  binario  (con un  valor de \nF1 del 94, 12 %). \nPalabras clave : Relaci\u00f3n Sem\u00e1ntica, Desambiguaci\u00f3n Estructural de Unidades T erminol\u00f3gi cas \nPolil\u00e9xicas, Random Forest, \u00c1 rbol de Decisi\u00f3n.  \n \n \nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 141-152\nrecibido 31-03-2022 revisado 11-05-2022 aceptado 19-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-12\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introduc tion \nA set of 1,694 sentences  from a Co astal Engineering \ncorpus , in which a named river (e.g., Salinas River ) \nwas an argument of the predicate of the sentences, \nwere semantically analyzed and an notated with the  \nsemantic relation between the arguments , the lexical \ndomain of the predicates, and the semantic role of the \narguments . \nThis paper  presents the statistical analysis of those \nsemantic annotations with a view to finding evidence \nthat the stru ctural disambiguation, or bracketing, of a \nthree-component multi word term (e.g., [ sand supply ] \n[decrease]) can be predicted from the semantic \ninformation encoded in the sentence where the \nternary compound is used as an argument. For this \nexperiment, we as sumed  that the context, which \nconstrains the factors that d rive understanding (Leech  \nG., 1981), also helps to resolve the structural \ndisambiguation of a ternary compound. This \nassumption comes from the daily experience of a \ntranslator who must deal with te rnary compounds in a \nspecialized text. Although the compounds are \nsomewhat familiar, it is useful to craft definitions for \nthem to facilitate their translation into another \nlanguage based on their context of use.  \nThe rest of this paper  is organized as foll ows. \nSection  2 presents a fundamental background of \nbracketing of multi word terms . Section  3 provide s a \nliterature review of predictive models  for bracketing , \nmostly from the perspective of variables and \nresources used for the task of compound bracketing \nprediction. Section  4 explain s the materials  used in \nthis study . Section  5 cover s our semantic approa ch to \npredict ing ternary compound bracketing  based on two \nsupervised models , namely decision tree, and random \nforest . Also d escribed are t he sample of tern ary \ncompounds , the training and testing phases for the \npredictive models , and the r esults, which  provide \nlinguistic insigh ts as to how semantic relations,  \npredicate lexical domain s, and semantic role s are \nintertwined with  the bracketing of ternary \ncompound s. Section 6 discusses th e results and \ncompares them to those outlined in the literature \nreview. Fina lly, Section  7 presents the conclusions  \nderived from this work along with plans for future  \nresearch.  \n2 Bracke ting of Multiword Terms \nWhen multi word express ions a re used in specialized \ndomains, they are known as multi word terms \n(MWTs). MWTs often have more  than two \ncomponents. For instance, in Co astal Engineering, \nbeach size sand supply  refers to the  supply of s and, usually provided by rivers, whose grain si ze is \nappropriate to mitigate beach erosion. The most \nfrequent MWTs in specialized texts are endocent ric \nbecause they specify a broader concept or hypernym. \nFor example, beach size sand supply  is a type of sand \nsupply  since the grain size of the sand is sp ecified . It is \nthus the dimension activated to form the hyponym.  \nFor MWTs of three or more constituen ts, a \nsemantic analysis, based on linguistic and domain \nknowledge, is necessary to resolve the d ependency \nbetween components. This structural disambiguatio n, \noften known as bracketing  or parsing , involves the \ngrouping of the dependent components so that th e \nMWT is reduced to its basic form of modifier+head, \nas in [ beach size ] [sand supply ]. Knowledge  of these \ndependencies facilitates the comprehension of an \nMWT and , consequently, its accurate translation into \nother languages.  \nTherefore, before including MWT s in \nterminological knowledge bases, it is often necessary \nto structurally disambiguate them to make their \nrelational structure explicit and thus favor kno wledge \nacquisition (Le\u00f3n -Ara\u00faz P. et al., 2021). Furthermore, \nthe resolution of MWT bracketing provid es a higher \noverall accuracy in machine translation systems \n(Green  N., 2011), sentence parsers (Vadas  D. and \nCurran  J.R., 2008), and in systems aimed at \ndetermini ng the implicit semantic rel ation holding \nbetween modifier and head in MWTs of three or \nmore c omponents (Kim  S.N. and Baldwin  T., 2013).  \n3 Review of Bracketing Prediction  Methods  \nPrevious work on compound parsing/bracketing \nexploits either unsuper vised m ethods (e.g., based on \nbigram corpus frequency) or supervised ones (i.e., \nbased on training da ta, containing manually \nparsed/bracketed compound s, which are used to train \nan algorithm for predicting compound bracketing).  \nThe two basic unsupervised ap proache s are the \nadjacency model (Marcus  M., 1980; Pustejovsky  J. et \nal., 1993) , and the dependency m odel (Lauer  M., \n1994). For a ternary compound  such as sea level rise  \n(i.e., increase in sea leve l), the adjacency model \nconcludes whether level is more closely associated \nwith sea (leading to a left -branched structure) or  to \nrise (leading to a right -branch ed s tructure). In \ncontrast,  the dependency model resolves whether sea \nis more strongly associated  with level (leading to a \nleft-branched structure) or with rise (leading to a \nright-branched structure). In this case, the correct \nbracketing of sea level rise  is left-branched. The way \nof measuring the association strength between two of \nthe words  (or con stituents)  in the compound is based  \non association measure s estim ated from corpus data, \n142\nJuan Rojas-Garcia   such as bigram frequency , point -wise mutual \ninformation,  or chi -square d, among others.  \nResnik  P.S.'s (1993) method for ternary \ncompound s, based on the adjacency model and the \nassociation measure  called  selectional association , \nestima ted from the parsed Wall Street Journal  corpus \n(30 million words), achieved an overall accurac y of \n72.6% (with a sample of 157 ternary compounds \nfrom the Penn Treebank corpus, 64.1% left -branched, \nand 35.9% right -branched). In contrast , Lauer  M. \n(1995) adopted the dependency model for his \nmethod, based on the ratio of left - to right -bracketing \nprobability for a ternary compound, estimated from \nGrolier\u2019s Encycloped ia (8 million words). The author \ncalculated probabilities o f conceptual categories in the \ntaxon omy underlying  Roget\u2019s Thesaurus  (Roget  \nP.M., 1852 )1, rather than for individual words, to \navoid data sparsity problems. His method reached an \noverall accuracy of 80 .7% (with a sample of 244 \nternary compounds from Grolier\u2019s Encyclopedia , \n66.8% left -branche d, and 33.2% right -branched).  \nNakov  P. and Hearst M. (2005) developed an \nunsupervised, knowled ge-rich method for parsing \nternary compounds. Their approach included:  \n(1) Ten types of surface variable , such as dashes \n(e.g., beach -sand transport  points  to a l eft-bracketed \ncompound), possessive markers (e.g., city's water \nsupply indicates a right -brack eted compound), and \nacronyms (e.g., pH quality control (QC)  reveals a \nright-bracketed compound) . \n(2) Three types of paraphrase variable, namely \npreposition al phra ses (e.g., distance from the  river \nmouth  means that river mouth distance  is \nleft-bracketed), c opula paraphrases (e.g., water \nproduct that/which is a  mixture  proves that mixture \nwater product  is right-bracketed), and verbal \nparaphrases (e.g., impacts  associ ated with  river \npollution  implies that river pollution impact  is \nleft-bracketed).  \nThe authors concluded that the adjacency and \ndependency models showed comp arable performance \nwhen using the chi -square d association measure and \nthe number of web searc h engin e page hits for \napproximating corpus frequencies, as suggested by \nLapata  M. and Keller  F. (2004). Although t heir \nmethod achieved an  overall accuracy of 95.35%, this  \nresult was probably biased toward the majority \nleft-bracketing class because of  the \nbracket ing-imbalanced sample of 430 ternary \ncompounds from a corpus of biomedical domain \nabstracts re trieved from MEDLINE  (84% \nleft-branched, and only 16% right -branched).  \n \n1 http://www.gutenber g.org/ebooks/10681 . Girju  R. et al. (2005) implemented a supervised \nmodel for bracketing ternary compoun ds with  the \nmachine -learning technique decision tree. They \nemployed a total of 15 semantic variables based on \nWordNet senses, five variables for each compound \nconstituent, na mely the top three WordNet semantic \ncategories for each constituent, derivationall y-related \nforms, and whether the constituent was a \nnominalization. The algorithm reached an overall \naccuracy of 83.10% , with a sample of 728 ternary \ncompounds from the Wall S treet Journal  component \nof the Penn Treebank  corpus  (Marcus  M. et al., \n1993) , 67.4 % left -branched, and 32.6% \nright-branched.  \nKim S.N. and Baldwin  T. (2013) devised a \nmethod that consi sted of automatically determining \nthe semantic relations between the pairs of words in a \nternary compound, and then predicting bracketing \nfrom the constitu ent pai r whose semantic relation \ncoincided with that of the ternary compound. When \nthis method was  combin ed with that of Nakov  P. and \nHearst M. (2005) , it achiev ed an overall accuracy of \n74.1% with a sample of 1 ,571 ternary compounds \nfrom the Wall Street J ournal  corpus . However,  no \ninformation was provided regarding  the percentage of \nleft- and right -brack eting within th e sample.  \nThe supervised method by Bergsma  S. et al. \n(2010) used both n -gram variables (the logarithm of \nthe frequency of all constituent su bsets a ppearing in \nthe Google V2 corpus), and Boolean lexical variables \nthat indicated the presence o r absence of a par ticular \nstring at a given position in the compound (the \nconstituents and their position, the entire ternary \ncompound, as well as a capita lizatio n pattern of the \nconstituent sequence). As a machine -learning \ntechnique, the authors applied t he support -vector \nmachine algorithm, which reached an overall \naccuracy of 91.6% , with a sample of 2 ,150 ternary \ncompounds from  the Wall Street Journal  corpus \n(70.5% left -branched, and 29.5% right -branched).  \nVadas  D. and Curran  J.R. (2007) developed a \nsuper vised method for parsing ternary compounds \nbased on the machine -learning technique of logistic \nregression. They used 88 ,568 variables, an extremely \nlarge n umber, which can be summarized as follows:  \n(1) Bigram frequencies wer e collected from two \nsources, na mely hit counts from the web search \nengine Google, and frequencies in the Google Web \n1T corpus (Brants  T. and Franz  A., 1993).  \n(2) The pairs of compound co nstituen ts, and the \nsurface variables by Nakov  P. and Hearst M. (2005), \nwere compared  according to bo th the adjacency and \ndependency models by means of the chi -square d, and \nbigram probability  association measures . \n143\nSemantic Relations Predict the Bracketing of Three-Component Multiword Terms (3) Lexical features for all unigrams and bigrams \nin a ternary compound, along with their position \nwithin the compound . \n(4) Contextual variable s, c onsisting of \nbag-of-word features for both the words in the \nsentence where the compound is used, and for a \ntwo-word window on each side of  the compound . \n(5) For every n -gram and context window feature, \ntheir part -of-speech tags and named entity tags we re \nadded. \n(6) For each sense of each constituent in the \nternary compound, a semantic feature for its synset, a s \nwell as  the synset of each of its hypernyms  up to t he \nroot, were extracted from WordNet, and incorporated \ninto the supervise d model as additiona l variables.  \nThis method achieved an F 1-score of 93.01% , \nwith a sample of 5 ,582 ternary compounds from  the \nPenn Treebank corpus  (58.99% left -branched, and \n41.01% r ight-branched).  \nThe supervised system by Pitler E. et al. (2010) \nwas able to  bracket compound s of three or more \nconstituents (including the conjunction and). \nApplying the support -vector machine algorithm, the \nsystem first calculated the probability  that a word \nsequence, within a compound, was a constituent, \ngiven the entire compou nd as context. Th en, using \nthese probabilities, the system predicted the \nbracketing of a compound with the CYK parser (i.e., \nCocke -Younger -Kasami algorithm) . As variables f or \nthe system, the authors employed:  \n(1) The position of the proposed bracketing within \nthe compound . \n(2) The association measure point -wise mutual \ninformation (PMI) between all word pairs in the \ncompound, derived from the Google V2 corpus  (Lin \nD. et al., 2010) . \n(3) Boolean lex ical variables to indicate the \npresence of a particular word at each position i n the \ncompound . \n(4) Boolean variables to inform about the shape of \nthe compound, namely the presence of capitalized \nletters and hyphenated words provided i nformati on \nconcerning  the possibility that the compound included \na named entity.  \nThe system reached a n overall accuracy of 95.4% , \nwith a sample of 64 ,844 compounds of three or more \nconstituents from the Penn Treebank  corpus, but \nbracketing -related informat ion in the form of \npercentages was not provided.  \nLazaridou A. et al. (2013) tackled the parsing of a \nternary compound using a semantic plausibility \nmeasure  derived from a distributional semantic model \ntrained on a corpus of 2.8 billion tokens, where the \nvector of a ternary compoun d was obtained from the \ncombination of the vectors of each of its constituen ts. This supervised method relied on the support -vector \nmachine algorithm with 14 variables, summarized as \nfollows: (1)  12 variables for representing the s emantic \nplausibility of e ither the left - or right -bracketing ; \n(2) two variables for the PMI values of  the word pairs \nin the compound, according to the adjacency model. \nThe method achieved an overall accuracy of 85.6% , \nwith a sample of 2 ,227 ternary compoun ds from the \nPenn Treebank  corpus  (34.4% left -branched, and \n65.6% right -branched).  \nFaruqui  M. and Dyer C. (2015) also addressed the \nternary compound bracketing with word vectors. \nHowever, their semantic model was \nnon-distributional because the vectors did n ot encod e \nany word co -occurrence information. Instead, the \nvector dimensions were Boolean variables  that \nrepresented linguistic knowledge derived from \nresources such as WordNet  (Fellbaum  C.A., 1998), \nFrameNet  (Ruppenhofer J. et al., 2010), and Penn \nTreeban k. As su ch, the vector length for a single word \nincluded a total  of 172 ,418 dimensions. The vector of  \na ternary compound was  then obtained by appending \nthe vector of each constituent , which result ed in a \nternary compound vector of 517 ,254 dimensions . \nThis combined  vector was the input of the \nmachine -learning technique of logistic regression, \nwhich achiev ed an overall accuracy of 83.3% in the \nsame sample of ternary compounds collected by \nLazaridou  A. et al. (2013).  \nFor the unsupervised method by M\u00e9nard P.A. and \nBarri\u00e8re C. (2014), the usage of different resources for \nthe bracketing of compounds of three and  more \nconstituents was compared, namely the English \nGoogle Web N -grams  (Lin D. et al., 2010 ), English \nGoogle Books Ngrams  (Michel J.B. et al., 2010 ), and \nopen link ed data DBpedia  (Hellmann  S. et al., 2009 ). \nThe association measures chi -square d, PMI, and \nDice, and the number of valid DB Pedia paths were \nalso analyzed. Their algorithm created an initial  list \ncontaining all of the  word pairs from a compound, \nwhich were then sorted  in de scend ing order of \nassociation scores. A second list of dependencies, \nwhich d efined the complete bracketing of the \ncompound, was constructed from the first list. For \nternary compounds, the method with the English \nGoogle Books N -grams and th e PMI achieved the \nhighest overall accuracy, with a  value of 81.47%  on a \nsample of 2 ,889 tern ary compounds from t he Penn \nTreebank corpus  (79.2% left -branched, and 20.8% \nright-branched).  \nSimilarly, f or the bracketing of compounds of \nthree and more c onstitue nts, Barri\u00e8re  C. and M\u00e9nard \nP.A. (2014) applied the unsupervised method of \nM\u00e9nard P.A. and Barri\u00e8re C. (2014), but relied on a \nword a ssociation model that combined the lexical, \n144\nJuan Rojas-Garcia   relational, and coordinate nature of the associations \nbetween all pairs  of word s within a compound. The \ninformation for their word association model was \ncollected from Wiki pedia. The system reached an \noverall acc uracy of 73.16% , with a sample of 4 ,749 \ncompounds of three and more constituents from the \nPenn Treebank corpus, but the spe cific accuracy for \nthe subset of ternary compounds was not provided.  \nLe\u00f3n-Ara\u00faz  P. et al. (20 21) developed an \nunsupervised, knowledge -rich method for bracketing \nspecialized ternary compounds in the  domain of wind \nenergy. The authors used 12 variabl es, mainly related \nto the surface and paraphrase variables proposed by \nNakov  P. and Hearst M. (2005),  which measured \nfrequency counts in a specialized corpus on win d \nenergy. The counts were collected by means of CQL \n(Corpus Query Language) queries in the S ketch \nEngine corpus manager. A total of 34 specific CQL \nqueries were designed for the extraction of \noccurrences of each of the linguistic structures \nunderlying the 12 variables. Based on the results, the \nauthors formulated 16 rules to decide on the \nbracket ing of a ternary com pound. Hence, the final \nbracketing structure was decided by applying the \nmajority  vote strategy to the votes of the individual \nrules. As such, t he CQL queries and rules permit ted \nthe implement ation of  a system to automate the \ncompound b racketing task for u sers such as \ntranslators and terminologists. The method achieved \nan overall accur acy of 86.4% , with a sample of 103 \nternary compounds from the wind energy domain  \n(67% left -branched, and 33% right -branched).  \nIn short , previous research  focused on semantic \ninformation provided by the components of an MWT. \nThe number of variables used for  prediction ranged \nfrom 12 to 517,254 features. These variables were \nmostly based on n -gram s tatistics, and semantic \ninformation of the MWT components stor ed in \nlinguistic res ources such as WordNet. The overall \naccuracy of the prediction models ranged from  \n72.60% to 95.40%.  \nOur approach, however, was based on semantic \ninformation that previous res earch has not as yet \nconsidered. Th is semantic information was  encoded \nin both the  co-text of a ternary compound (i.e., the \nsentence where the ternary compound was  used as an \nargument) and the ternary compound  seen as a unit \n(i.e., its semantic role ). The set of predictor variables \nconsisted of  only three (i.e., the semantic relation, \npredicate lexical do main, and semantic role of the \nMWT ), wherea s previous research  employed a \nminimum of 12 va riables (Le\u00f3n -Ara\u00faz P. et al., \n2021).  4 Materials  \nA set of 1,694 sentences , in which a named river (e.g., \nMississippi  River) was an argument of the predicate \nof the sen tences, were semantically analyzed and \nannotated . These senten ces were extracted from a \nsubcorpus of English texts on Coastal Engineering, \ncomprising roughly 7 million tokens and composed \nof specialized texts (scienti fic articles, technical \nreports, and PhD  dissertations), and semi -specialized \ntexts (textbooks and en cyclopedias on  Coastal \nEngineering). This subcorpus is part of the English \nEcoLexicon Corpus (23.1 million words) (see \nLe\u00f3n-Ara\u00faz P. et al. (2018) for a de tailed description).  \n5 Semantic Approach  for MWT Br acketing  \nSince the semantic  information in a sentence fi rmly \nguides its syntactic parsing (Fillmore  C.J., 1968; \nLazaridou A. et al., 2013), one could assume that the \ncorrect br acketing of a n MWT , when use d as an \nargument in a sentence, can be predicted from the \nsemantic information encoded in that sentence. In other \nwords, the context, which constrains the factors that \ndrive understanding (Leech  G., 1981), helps  to resolve \nthe structural disambiguation of the ternary compound.  \nAs semantic information in a sentence, this pilot \nstudy explored the contribution of three semantic \nvariables to the prediction of ternary compound \nbracketing. These variables were the lexic al domain \nof the verb, semantic role of the ternary compound, \nand semantic relation of the ternary compou nd to the \nnamed river . From the 1 ,694 sentence s semantically \nanalyzed and annotated , 188 sentences contained 190 \nternary compounds as arguments. This sampl e of 190 \nternary compounds, alon g with the values of the \nabovementioned three semantic variables anno tated in \ntheir corresponding sentence s, were em ployed for the \ntraining and testing of two supervised models to \npredict whether a ternary compound was \nright-branched or left -branched.  \n5.1 Annotation of t he Semantic Variables  \nA set of 1,694 sentences from the co rpus, where  294 \ndifferent rivers are mentio ned, were annotated by \nthree terminologists fro m the LexiCon research group  \nof the University of  Granada (Spain ). They performed \nthe semantic annotation of the predicate -argument \nstructure of a sentence by assigni ng a: (1) lexical \ndomain to the predicate ; (2) semantic role to the \narguments of the predicate ; (3) semantic relation to \nthe link between the named riv er and the other \narguments in the sentence ; and (4) bracketing  (left or \nright) to the ternary comp ounds  used as arguments  in \nthe sentence . The value s of these four semantic \nvariables are shown in T able 1. \n145\nSemantic Relations Predict the Bracketing of Three-Component Multiword Terms Semantic \nvariables  \nannotated  Values  \nLexical dom ain of \nthe predicates  (8 \nvalues ) CHANGE , MOVEM ENT, EXISTEN CE, \nPOSSESSION , POSITION , MANIPULATION , \nACTION , COGNITION  \nSemantic roles of \nthe arguments  (13 \nvalues ) AGENT , RESULT , PATIENT , THEME , \nLOCATION , RECI PIENT , INSTRUMENT , \nTIME, RATE , MANNER , DESCRIPTION , \nCONDITION , PURPOSE  \nSemantic rela tion \nbetween the ternar y \ncompound and the \nnamed r iver (30 \nvalues ) type_o f, part_of , made_of , \ndelimited_by , located_a t, \ntakes_place_in , phase_of , affects , \ncauses , result_of , attribute_of , \nhas_function , studies , measures , \neffected_by , improves , worsens , \ncreates , becomes , gives , gives_to , \nreceives , receives_from , drains , \nhas_path , transfers , discharges_into , \nplaces , contro ls, applied_to  \nBracketing of the \nternary co mpounds \nin the sentences  (2 \nvalues ) RIGHT , LEFT \nTable 1: Semantic variab les annotated in the set of \nsentences, and the ir values.  \n \nThe m ost frequent verbs in the corpus  are general \nlanguage verbs (e.g., accumulat e, pollute , increase , \ndischarge , supply , drain ), which are  also used in \nspecialized texts  and thus reflec t how environmental \nentities interact . In this sense, such  verbs are \nsusceptible to classification in  the lexical domains \nproposed by Faber  P. and Mair al R. (1999) , within the \nFunctional  Lexematic Model . These lexical domains \nwere used to annotate the pred icates of our set of \nsentences , and shown  in Table  1. \nSpecialized knowledge representation includes \nsemanti c properties that  help to describe the natur e of \nentitie s and processe s. These semantic properties  are \nreflected as the relation s between a predicate  and its \narguments, which are typical semantic roles.  The \nsemantic roles  used to annotate the arguments  in our \nset of sentences  largely coincide d with those specified \nby Kroeger  P.R. (2005 : 54-55), and Thompson  P. et \nal. (2009) , and summarized  in Table  1. \nConceptual descrip tion of speci alized concepts \nincludes  their relational behavior. These relations , \ndepict ed by Faber P. et al. (2009) for environmenta l \nconcepts , with additional  non-hierarchical relation s \nspecific to named rivers  (Rojas -Garcia J., \nforthcoming ), were all used to annotate the semantic \nrelation between the  arguments in our set of \nsentences , and collected in Table  1. \nThe inter -annotation ag reement  coefficient , \nCohen\u2019s kappa  (\u03ba), showed a very good agreement \nfor all the annotator pairs (\u03ba>90%, p-value<0.05) in \nthe annotation of the semantic role s, relation s, and \nbracketing  according to Krippendorff  K.\u2019s (2012) recommendations for text content  analysis.  \nNotwithstanding, the disagreements in the original \nannotations were resolved based on discussi on between \nthe annotators to reach a consensus on the definitive \nannotations of semantic roles , relations , and bracketing . \nFor the i nitial annotation o f predicates with lexical \ndomains, the inter -annotation agreement was lower \nfor all the annotator pairs ( 84%<\u03ba<88%, \np-value<0.05), indicating that this variable lent itself \nto alternative, though plausible, interpretations. A \nreview of t he differences between annotators showed \nthat the lexical domains of MOVEMENT  and \nPOSSESSION  were more prone to confusion. T he \nissues fun damentally arose from verbs that could \npotentially belong to more than one lexical domain  \n(e.g., drain  and discharge ), as Faber  P. and Mairal R. \n(1999) already proved.  To arrive at a consensus on the \ndefinitive annotations of lexical domains, the \nfactorization of meaning from the Functiona l \nLexematic Model framework was applied to verbs  to \nresolve disagreement s between the  annotators.  \n5.2 Description o f the Sample o f MWT s  \nA selection of 1 0 sentences from the sample, which \nincorporated  ternary c ompounds as arguments, is \nprovided in Table  2. For each of those 1 0 sentences, \nTable  3 shows the values of the following four \nannotated variables:  (1) lexical domain of the \npredicate ( LexDom ); (2) semantic role of the ternary \ncompound ( SemRol_mwt ); (3) semantic relati on \nbetween the ternary compound and the named r iver \n(SemRel ); and  (4) bracketing of the ternary compound \n(Brack eting), which was the variable to be predicted.2 \nThe distribution of bracketing struct ures w ithin the \nMWT sample was reasonably balan ced between \nleft-branching ( 110 MWTs, 58% of the sample), and \nright-branching ( 80 MWTs , 42% of the sample). \nTable  4 summarizes the counts for the sample data, \ndisaggregated  by lexical domain  and bracketing \nstructure of the MWTs , and describes the distribut ion \nof the 190 MWTs across these variables. Some \nconclusions could be drawn from the ch aracteristics \nof the sample: (1) sentences whose predicate \nbelonged to the lexical domains of MOVEMENT , \nACTION, POSITION , MANIPULATION , and COGNITION  \nincluded ternary co mpounds which were only \nright-branched ; and ( 2) sentences whose predicate \nbelonged to t he lexical domain of POSSESSION  \nincorporated ternary compounds which were only \nleft-branched.  \n \n2 The whole dataset  of MWT s, the values of the \nannotated variables , and the corpus will be available on the \nwebsite of the LexiCon research group of the University of \nGranada (Granada, Spain) (http://lexicon.ugr.es/).  \n146\nJuan Rojas-Garcia   Sentences from the Sample with Ternary Compounds as \nArguments  \n(1) Blackstone  River draining into Narragansett Bay has been \nextensively dammed, and although not well quantified, models show  \ndecreasing sedime nt load  in the Blackstone Riv er. \n(2) The dramatical sediment load variation in the Pearl River , with the \nalmost unchanged water di scharge level , represents  an example of \nsuch effect that human activities can have on river deltas.  \n(3) Muddy silt deposition  in the Clyne River  discharging into the \nSwansea Bay would increase . \n(4) Rising sea levels  change  Salinas River Estuary  and could thus \npotentially alter sediment supplies and process patterns.  \n(5) The Salinas River  no longer  contributes  substantial beach size \nsand  to the Littoral Cell because the river gradient has greatly \ndecreased with sea lev el rise, reducing the flow rate.  \n(6) The River Murray  flows across Tertiary formations to enter  coastal \nlagoons behind the dune calcarenite barriers  of Encounter Bay.  \n(7) Not all the sedimen ts drained by the Dee River  participate  to \ncoastal sediment transport . \n(8) The field site for this study is the Zuidgors salt mars h, located  in \nthe Western Scheldt estuary  in The Netherlands.  \n(9) Natural sediment supply  within this region is defined  by the \nVentura  River  that drain s large watersheds.  \n(10) The average discharge rate  of beach  size sand in the Salinas River  \nis estimated  at approx imately 65,000 cubic yards per year.  \nTable 2: Selection of 1 0 sentences (from the sample \nof 188 sentences), which included 1 0 ternary \ncompounds as arguments . \n \nMWT  LexDom  SemR ol\n_mwt  SemRel  Brack\neting \ndecreasing  \n[sediment \nload] EXISTEN CE DESCRIP T\nION attribute\n_of RIGHT  \n[water \ndisch arge] \nlevel EXISTEN CE THEME  attribute\n_of LEFT \n[muddy \nsilt] \ndeposition  CHANGE  PATI ENT takes_pl\nace_in LEFT \nrising [sea \nlevel]  CHANGE  AGENT  worsens  RIGHT  \n[beach \nsize] sand POSSESSION  THEME  gives LEFT \ndune \n[calcarenite \nbarrier]  MOVEMENT  AGENT  has_path  RIGHT  \ncoastal \n[sediment \ntransport]  ACTION  DESCRIPT\nION affects  RIGHT  \nZuidgors \n[salt \nmarsh]  POSITION  THEME  located_\nat RIGHT  \nnatural \n[sediment \nsupply]  MANIPULA TION PATIENT  controls  RIGHT \naverage \n[discharge \nrate] COGNITION  THEME  attribute\n_of RIGHT  \nTable 3: Semantic annotations and variables  for a set \nof 10 MWTs out of the 190 MWTs that comprised \nthe sample. The  semantic information in the rows \ncorresponds to the respective sentences in Table  2. \n Lexical \nDomain  LEFT-branched \nMWTs  RIGHT -branched  \nMWTs  Total  \nMOVEMENT    0 10 10 (  5.3% ) \nPOSSESSION  30   0 30 (15.8% ) \nCHANGE  20 10 30 (15.8% ) \nEXISTENCE  60 20 80 (42.0% ) \nACTION    0 10 10 (  5.3% ) \nPOSITION    0 10 10 (  5.3% ) \nMANIP ULAT.   0 10 10 (  5.3% ) \nCOGNITION    0 10 10 (  5.3%) \nTotal 110 (58%) 80 (42%) 190 (100% ) \nTable  4: Description of the sample of ternary \ncompounds . \n5.3 Supervised Models  \nRegarding the supervised models for classification, \nbinary decision tree  and random forest  were tested to \npredict ternary compound bracketi ng. Since variables \nin our dataset were categorical, both tree -based \nmodels were adopted because they can efficiently \nmanage qualitative variables (James G. et al., 2015 : \n315). \nA decision -tree model is simple and readily \ninterpretable because the set of pr ediction rules is \ngraphically summarized in a tree, typically drawn \nupside down, in the sense that the terminal nodes or \nleaves, which convey the predictions, are at the \nbottom of the tree . However, it is usually not \ncompetitive with other predictive model s. \nFor that reason, we also experimented with a \nrandom forest model, which produces a large number \nof decision trees, and  then combines them to reach a \nsingle consensus prediction. Namely,  each tree in the \nensemble (or forest) casts a vote for the bracketi ng of \nan MWT, which is finally classified into the \nbracketing structure that has the most votes. Random \nforest models thus  lead to remarkable improvements \nin prediction accuracy, at the exp ense of loss in \ninterpretation since it is difficult to obtain insig ht as to \nhow the model makes the predictions.  \n5.4 Data Splitting  \nFor the construction and evaluation of the models, the \ndataset with the 190 MWTs was divided into two: \n(1) the training d ataset to create the models (with 133 \nMWTs, 70% of the original dataset), and (2) the test \ndataset to qualify model performance ( 57 MWTs, \n30% of the original dataset).  \nFor both the training and tes t data sets to have the \nsame distribution in the outcome vari able (i.e., \nBracketing ) as the original dataset (i.e., 58% \nleft-branched MWTs, and 42% right -branched \nMWTs) , stratified random sampling  was conducted, \nwhich randomly sampled observations within th e \nclasses LEFT and RIGHT  of the Bracketing  variable  in \nthe original dataset.  \n147\nSemantic Relations Predict the Bracketing of Three-Component Multiword Terms 5.5 Model Performance Measures  \nThe quality of the two mod els (decision tree and \nrandom forest) was assessed by analyzing how well  \nthey perform ed on the test data set, which was hidden \nfrom the model -building process for evaluation \npurpos es. As such, the predictions of the model s were \ncompared to the true classes of the test data set (i.e., \nthe true bracketing structures LEFT and RIGHT , \nrecorded in the Bracketing  variable of the test data set), \nand performance measures were calculated . \nA widely used performance measure is overall \naccuracy , which provides the percenta ge of correctly \nclassified instances . However, this measure has some \ndrawbacks in imbalanced datasets , or datasets whose \noutco me variable exhibits a significant disproportion \namong the number of instances  of each class . \nAccording to Fern\u00e1ndez A. et al. (20 18: vii), the \nlearning process of most classification algorithms, \nincluding decision tree and random forest, is often \nbiased t oward the majority -class instances, and \nminority -class ones are thus not well modelled into \nthe final system. Consequently, in imb alanced \nscenarios, the accuracy measure may mask a poor \nclassification performance in the minority class. \nUnfortunately, as al ready seen in the literature review, \nthere is much research on bracketing prediction that \nstill uses overall accuracy with severel y \nbracketing -imbalanced datasets. Therefore, despite \nthe fact that our dataset was only slightly \nbracketing -imbalanced, we pre ferred to use, in \naddition to accuracy, other measures that were not \nsensitive to disparities in the class proportions to \nevaluate  classification performance. Such measures \nwere the area under the ROC curve , and the F1-score  \n(Fern\u00e1ndez A. et al., 2018 : 52-55). \nThe receiver operating characteristic (ROC) \ncurve  is a function of the sensitivity and specificity of \na two -class predictive model to evaluate its trade -off \nbetween both measures. Sensitivity  is the fraction of \nthe minority -class instances (in our cas e, the \nright-branched MWTs) that are correctly classified, \nwhereas specificity  refers to the proportion of the \nmajority -class inst ances (in our case, the left -branched \nMWTs) that are correctly classified. Hence, the area \nunder the ROC curve (henceforth ref erred to as AUC) \nis a method for combining sensitivity and specificity \ninto a single value. AUC ranges from 0 to 1. The \nhigher the  AUC, the better the performa nce of the \nmodel at distinguishing between the two classes.  \nThe F 1-score is the harmonic mean bet ween the \nprecision and recall of a predictive model. Precision  is \nthe fraction of correctly classified minority -class \ninstances am ong the instances classified as belonging \nto the minority class, whereas recall  is the same as sensitivity. Thus, the F 1-score evaluates the trade -off \nbetween correctness and coverage in classifying \nminority -class instances.  \n5.6 Construction of the Predict ive Mod els \nThe predictors SemRel , LexDom , and SemRol_mwt  \nwere used to construct  two predictive models  with the \ncaret package (Kuhn  M., 2021 ) for the R \nprogramming language . \nFor the random forest, 7-fold cross -validation in \nthe training dataset was used to evaluat e its \nperformance in training. Although 10 folds are \nconventionally employed, we chose  7 folds, a divisor \nof 133, so that the number of instances  in all folds \nwould be the same  (i.e., 19 instances ). During the \nprocess of tuning parameters, the  AUC  performa nce \nmeasure was chosen to be maxim ized. Accordingly, \nthe random forest model attain ed in training an AUC \nvalue equal to 1.0 when: (1)  the splits in the trees were \nallowe d to use one predictor of a subset of one \npredictor ; and (2)  the number of  trees in the  forest \nwas, surprisingly, only th ree trees. In the test dataset, \nthe random forest also achieved an AUC value equal \nto 1.0. Consequently, the three predictors were \ncapable of correctly predict ing bracketing in the test \ndataset with a random forest model.  \nSimilarly, for the decision tree, 7-fold \ncross-validation in the training dataset was employed \nto evaluate its performance in training. During the \nprocess of tuning par ameters, the  AUC  performance \nmeasure was also chosen to be maximized. Therefore, \nthe dec ision-tree model yielded in train ing the greatest \nAUC, equal to 0.9 545, when: (1)  the cost -complexity \nparameter  (cp) was equal to cp=0.8392857 ; and \n(2) the splitting cr iterion for predictors was the \ninformation gain , and not the Gini index . In the t est \ndataset, the decision -tree model ach ieved an AUC \nvalue also equal to 0.9545, which indicated a very \nsatisfactory perfor mance . \nTable  5 provides further performance measures, in \nthe training and test datasets, for the random forest \nand decision -tree mode ls. \n \nPredictors: SemRel , LexDom , and SemRol_m wt \n Decision Tree Model  \nDataset  AUC  Precis ion Recall  F1 Accur a. \nTrain  0.9545 0.8952 1.000  0.9430 0.9474 \nTest 0.9545 0.8889 1.000  0.941 2 0.9474 \n Random Fore st Model (3 ensembled decision trees ) \nTrain  1.000 0 1.000 0 1.000 0 1.000 0 1.000 0 \nTest 1.000 0 1.000 0 1.000 0 1.0000 1.000 0 \nTable 5: Performance measures  of the models for \nbracketing prediction with the predictors  semantic \nrelation,  lexical domain, and semantic role . \n \n148\nJuan Rojas-Garcia   Since the decision -tree model reached a \nsignific ant AUC in the  test dataset (AUC=0.95 45), its \nonly pre diction rule, graphically summarized in  \nFigure  1, is worth mentioning . \n \n \nFigure 1: Classification tree  for bracketing prediction , \ninferred by the decision -tree model trained with the \npredictors  semanti c relation, le xical domain, and \nsemantic role of the M WTs. \n \nIn our constrained context (i.e., specialized ternary \ncompounds from Coastal Engineering, used in \nsentences where a named river w as mentioned), the \nclassification tree of the model, displa yed in F igure 1, \ncan b e interpreted as follows . \nSemRel  was the  most important factor in \ndetermining Bracketing , and the only predictor \nselected by the decision -tree model. In our opinion, \nthe predi ctive power of the semantic relation between \nan MWT and ano ther argu ment in the sa me sentence \nis so high that the model was obliged  to reject the use \nof the predictors LexDom  and SemRol_mwt  to avoid \noverfitting to training data.  \nAs such, the ternary compoun ds whose semantic \nrelation to the other argument, filled w ith a nam ed \nriver in ou r case, belonged to the group formed by \ncauses , gives, improves , and takes_place_in  \n(right -hand branch in the classification tree) \naccounted for  53% of the sample; these MWTs were \nall left -branched and correctly classified. It thus \nseemed tha t these four s emantic relations forced the \nuse of only  left-branched MWTs.  \nIn contrast , the ternary compounds whose \nsemantic relation to the other argument fell into  the \ngroup formed by affects, attribute_of , controls , \nhas_path , located_at , and worsens (left-hand branch  \nin the classification tree) comprised 4 7% of the  \nsample, and could be right - or left -branched; under \nthese conditions, the model correctly classified all the \nright-branched M WTs (89%), but misclassified the \ntrue left -branched MWTs ( 11%) as r ight-branched.  \nAn analysis of the errors made  by the \ndecision -tree model revealed that, both in the training \nand test datasets, those left -branched MWTs with the \nvalues SemRel =attribute_of , LexDom =EXISTENCE , \nand SemRol_mwt =THEME  (e.g., water disch arge level, in row 2 of Table  3), were all misclassified as \nright-branched.  \n5.7 Baseline  Models  \nThe resu lts of our semantic approach were compa red \nto those of four baseline models , namely : \n(1) adjacency model with the  point-wise mutual \ninformation (PMI) association measure , as defined by \nMarcus M. (1980); (2) adjacency model with the \nchi-squared  association measure; (3) dependency \nmodel with PMI; and (4) dependency model with \nchi-squared. These non -supervised models, widely \nused in the literature  on bracketing prediction , were \napplied to the whole sample of 190 MWTs.  \nTable  6 shows that the two  predictive models , \nexplain ed in this paper , outperformed  the baseline \nmodels . Furthermore, the dependency model \nachieved better performance than the adjacency \nmodel , and the chi -squared association measure  \nyielded better results than PMI.  \n \nModels  Precision  Recall  F1 \nAdjacency model with PMI  0.6444 0.7250  0.6823  \nAdjacency model with chi-squared  0.6623 0.7375  0.6979  \nDependency model with PMI  0.6818  0.7500  0.7143  \nDependency model with chi -squared  0.7011  0.7625  0.7305  \nDecision tree  model  0.8889  1.0000  0.9412  \nRandom forest  model  1.0000  1.0000  1.0000 \nTable 6: Comparison of  the deci sion-tree and random  \nforest models to  four baseline models.  \n5.8 Comparison of the Models  \nDespite the promising results, it is obvious that further \ninvestigation is necessary to acquire a more in -depth \nunderstanding of the  influence of the semantic \nvariables in this study on ternary compound \nbracketing. Th erefore, the following statements \nshould be considered scope -bounded because they \nwere derived from a restricted framework in which \nthis research was c onducted, namely spe cialized \nternary compounds from Coastal Engin eering used in \nsentences mentioning name d rivers.  \nAs far as the selection of the best model is \nconcerned, there are convincing arguments in favor of \neither model. Since the ra ndom forest model had an \nerror-free performance, it could be used to  implement \na system for bracketing ternary compounds.  \nNeverthel ess, the performance of the decision -tree \nmodel was also fairly good. It also ha s the advantage \nof interpretability and visua lization, which afford s \nlinguistic i nsights into how ternary compoun d \nbracketing is governed by semantic information \nencoded in a s entence.  Since the binary decision -tree \nmodel only needed the SemRel  predictor to achieve a \nhighly satisfactory level of perf ormance, practical \napplications for automatic bracketing could emplo y \n149\nSemantic Relations Predict the Bracketing of Three-Component Multiword Terms solely the semantic relation between a ternary \ncomp ound and another argument in the same \nsentence.  \n6 Discussion  \nAlthough the  comparison of  our study  with previous \nresearch  in the literatur e review  is far from ideal , it \nstill serves as an indication of the performance o f our \nsemantic approach . \nFor bracketin g prediction, p revious research \nfocused on semantic information  provided by the \ncomponents  of an MWT . The  number of  variables  \nthat they u sed for prediction  ranged from 12 to  \n517,254 features. These variabl es were mostl y based \non n-gram statistics , which c ould arguably capture \nsome semantic information encoded in  frequent \nco-occurrences of MWT components ( Lazaridou  A. \net al., 2013 : 1909). Other research studies  relied on \nseman tic information of the MWT compo nents stored \nin linguistic resources such as WordNet . The overall \naccuracy of the prediction models ranged from \n72.60% to 95.40%.  \nOur semantic approach , however,  was based  on \nsemantic info rmation that previous research  has not as \nyet consider ed. The semant ic informatio n was \nencoded  in both the co -text of a t ernary compound  \n(i.e., the sentence where the ternary compound was \nused as an argument)  and the ternary compound seen \nas a unit (i.e., its semantic role ). The set of varia bles \nconsisted of only  three (semantic relati on, lexical \ndomain , and semantic role of  the MWT ), whereas \nprevious research employed a minimum of 12 \nvariables  (Le\u00f3n -Ara\u00faz  P. et al.,  2021) . This set of \nthree variables  yielded, in the test dataset, an error -free \nperformance with a random for est model, wh ereas the \nhighest overall accuracy achie ved in p revious \nresearch was 95.4 0% with support vector machine \n(Pitler  E. et al., 2010) , a less interpretable predictive \nmodel . \n7 Conclu sions  \nA set of 1 ,694 sentences , in w hich a named river was \nan argume nt of the pre dicate of the sentences, were \nsemantical ly analyzed  and annotated with the lexical \ndomain of the predicates, the semantic role of the \narguments, and the semantic relation betw een the \narguments . Those semantic an notations were \nanalyzed  to see w hether  the br acketing of a ternary \ncompound, when use d as an argument in a sentence, \ncan be predicted from the semantic information \nencoded in that sentence.  \nThe semantic relation  of the M WT to  another \nargument in the same s entence , the lexical domain  of \nthe predicate , and the semantic role  of the MWT were able to  predict the bracketing of t he 190 ternary \ncompounds  used as arguments in a sample of 188 \nsemantically annotated sentences  (out of the 1,694 \nannotated sentences) . A random forest model, with \nthree ensembled decis ion trees, achieved in the test \ndataset an AUC equal to 100% (overall accuracy of \n100%).  When a decision tree was trained , the model \nonly needed the semantic relation  to yield , in the test \ndataset , an AUC equa l to 95.45% (overall accuracy of  \n94.74% ). Hence , the semantic relation of an MWT to \nanother argument in the same sentence  proved \nenormous capab ility to predict ternary compound \nbracketing.  \nTherefore , this pilot study showed  that the \nsemantic information in  a sentence , encod ed in the \nsemantic relation  of the MWT to  another argument in \nthe same  sentence , the lexical domain  of the predicate , \nand the semantic role  of the MWT , contributed \nsubstantially  to compound parsing. Given the \nbeneficial effects of multiword-term bracketing on \noverall  accuracy of  senten ce parsers (Vadas D. and \nCurran  J.R., 2008), and machine  translation systems \n(Green  N., 2011 ), this result potentially suggests a \nnovel research direction in  the integration o f such \nsemantic variables into sy ntactic parsers and machine  \ntranslation  applicat ions, in line with Agirre E. et al. \n(2008), Girju R. et al. (2005), and Kim S.N. and \nBaldwin  T. (2013).  \nEvidently, it is not as yet clear whether  such \nsemantic variables are also able to predict the \nbracketin g of MWT s of four or more consti tuents . \nThis issue is thus deferred for further investig ation.  \nFinally, notwithstanding the promising results, \nthey should be considered scope -bounded because of \nthe small size of the MWT sam ple and the restricted \nframework in which the analysis has bee n conducted, \nnamely  specialized ternary compounds from Coas tal \nEngineering used in sentences that mention ed named \nrivers. In future research, a wider framework shall be \nestablished to acquire a more profound understanding \nof the influence of the semantic v ariables focused in  \nthis study on multiword -term bracketing . \nAcknowledgements  \nThis research was carried out as part of project s \nPID2020 -118369GB -I00, \"Transversal Integration of \nCulture in a Ter minological Knowledge Base on \nEnvironment\" (TRANSCULTURE), funded  by the \nSpanish Ministry of Science and Innovation ; and \nA-HUM -600-UGR20, \"Culture as Transversal \nModule in a Terminological Knowledge Base on the \nEnvironment \" (CULTURAMA ), funded by the \nAndal usian Ministry of Economy, Knowle dge, \nBusiness, a nd University . \n150\nJuan Rojas-Garcia   References  \nAgirre, E., T. Baldwin,  and D. Martinez  (2008). \nImproving parsing and PP attachment  \nperformance with sense information. In \nProceedings of th e 46th Annu al Meeting of the \nAssociation for Computational Linguistics: \nHuman Language Technologies  (pp. 317-325). \nACL.  \nBarri\u00e8re, C. , and P.A.  M\u00e9nard  (2014). Multiword \nnoun compound bracketing using Wikipedia . In \nProceedings of the First Workshop on \nComputational App roaches to Compound \nAnalysis ( ComAComA 2014)  (pp. 72 -80). ACL.  \nBergsma, S., E. Pitler, and D.  Lin (2010). Creat ing \nrobust supervised classifiers via web-scale n -gram \ndata. In Proceedings of the 48th Annual Meeting \nof the Association for Computational Lingu istics \n(ACL)  (pp. 865 -874). ACL. \nBrants, T., and A. Franz  (2006 ). Web 1T 5 -gram \nVersion 1. Linguistic Data Cons ortium . \nFaber, P., and R.  Mairal  (1999). Constructing a \nLexicon of English Verbs . Mouton de Gruyter.  \nFaber, P., P. Le\u00f3n-Ara\u00faz, and J. A. Prieto  (2009). \nSemantic relations, dyna micity, and \nterminological knowle dge bases. Current Issues \nin Lan guage Studies , 1, 1-23. \nFaruqui, M., and C.  Dyer (2015). Non-distributional \nword vector representations . In Proceedings of the \n53rd Annual Meeting of the Associa tion for \nComputational Linguis tics (pp. 464-469). ACL.  \nFellbaum , C.A. (1998). Semantic network of English: \nThe mother of all WordNets. Computers and the \nHumanities , 32, 209-220. \nFern\u00e1ndez, A., S. Garc\u00eda , M. Galar,  R.C. Prati, B. \nKrawczyk, and F.  Herrera  (2018). Learning from \nImbalanced  Data Sets . Springer.  \nFillmore, C .J. (1968). The case for case . In E. Bach , \nand R. Harms ( Eds.), Universals in Linguistic \nTheory  (pp. 1 -89). Holt, Rinehart, and Winston.  \nGirju, R., D.I. Moldovan , M. Tatu, and D.  Antohe  \n(2005).  On the semantics of noun comp ounds. \nComputer Speech and Langua ge, 19(4), 479 -496. \nGreen, N. (2 011). Effects of  noun phrase bracketing \nin dependency parsing and machine translation . In \n49th Annual Meeting of the Association for \nComputational Linguistics: H uman Language \nTechnologies. Proceedings of Student Session  \n(pp. 69-74). ACL.  Hellmann, S., C. Stadler , J. Lehma nn, and S. Auer \n(2009). DBpedia live extractio n. In R. Meersman, \nT. Dillon, and P. Herrero (Eds.) , On the Move to \nMeaningful Internet Systems (OT M 2009)  \n(Vol. 5871, pp.  1209-1223). Springer.  Lecture \nNotes in Computer Science . \nJames, G., D. Witten,  T. Hastie, and R.  Tibshirani  \n(2015). An Introduction to Statistical Learning . \nSpringer.  \nKim, S.N., and T.  Baldwin  (2013). A lexical semantic \napproach to interpreting and bracketing En glish \nnoun compounds. Natural Lan guage \nEngineering , 19(3), 385 -407. \nKrippendorff,  K. (2012). Content Analysis: An \nIntroduction to its Methodology . Sage.  \nKroeger, P.R. (2005). Analyzing Grammar: An \nIntroduction . Cambridge Unive rsity Press.  \nKuhn, M. (2021). caret: Classification and \nRegress ion Training . R package version 6.0-90. \nLapata, M., and F.  Keller  (2004). The web as a \nbaseline: Evaluating the performance of \nunsupervised web-based models for a range of \nNLP tasks . In Proceed ings of the Human \nLanguage Tec hnology Conference of the North \nAmerican Chapter of the ACL \n(HLT -NAACL  2004)  (pp. 121-128). ACL.  \nLauer, M. (1994). Conceptual Association for \nCompound Noun Analysis . CoRR.  \nLauer, M. (1995 ). Corpus statistics meet the noun \ncomp ound: Some empirical results . In \nProceedings of the 33rd Annual  Meeting of the \nACL (pp. 47-54). ACL.  \nLazari dou, A., E.M. Vecchi, and M.  Baroni  (2013). \nFish transporters and miracle homes: How \ncompositional distributional semantics can help \nNP parsing . In Proceedings of the  2013 \nConfere nce on Empirical Methods in Natur al \nLanguage Processing (EMNLP 20 13) \n(pp. 1908-1913). ACL.  \nLeech,  G. (1981). Semantics: The Study of Meaning . \nPenguin.  \nLe\u00f3n-Ara\u00faz, P., A. San Mart\u00edn, and A.  Reimerink  \n(2018). The EcoLexicon Engl ish corpus as an \nopen corpus i n Sketch Engine . In Proceedings o f \nthe 18th EURALEX International  Congress  \n(pp. 893-901). Euralex.  \nLe\u00f3n-Ara\u00faz, P., M. Cabezas -Garc\u00eda,  and P.  Faber  \n(2021). Multiword -term bracketing and \nrepresentation in terminological knowledg e bases . \n151\nSemantic Relations Predict the Bracketing of Three-Component Multiword Terms In Electronic Lexicog raphy in the 21st Century. \nProcee dings of the eLex 2021 Conferenc e \n(pp. 139-163). Lexical Computing CZ.  \nLin, D., K.W Church , H. Ji, S. Sekine , D. Yarowsky , \nS. Bergsma , K. Patil, E. Pitler , R. Lathbury , V. \nRao, K. Dalwani , and S. Narsale  (2010). New \ntools f or web-scale n-grams. In Proceedi ngs of the \nSeventh International  Conference on L anguage \nResources and Evaluation  (pp. 2221-2227). \nELRA.  \nMarcus, M. (1980). A Theory of Syntactic \nRecognition for Natural Language . MIT Press.  \nMarcus, M.P., M.A. Marcinkiewicz,  and B.  Santorini  \n(1993). Buildin g a large annotated  corpus of \nEnglish: The Penn Treebank. Computational \nLinguistics , 19(2), 313 -330. \nM\u00e9nard, P.A., and C.  Barri\u00e8re  (2014). Linke d open \ndata and web corpus data for noun compound  \nbracketing . In Proceedings of  the 9th \nInternational Conference  on Language Resources \nand Evalu ation (LREC\u201914)  (pp. 702-709). ELRA.  \nMichel, J.B., Y.K. Shen, A.P. Aiden , A. Veres , M.K. \nGray, T.G.B. Team , J.P. Pickett , D. Holberg,  D. \nClancy , P. Norvig , J. Orwant, S. Pinker , M.A. \nNowak , and E.L. Aiden  (2010). Quantitativ e \nanalysis of culture using millions of digitize d \nbooks. Science , 331(6014), 176-182. \nNakov, P. , and M.  Hearst  (2005). Search engine \nstatistics beyond the n -gram: Application to noun \ncompound b racketing . In Proceedings of t he 9th \nConference on Computationa l Natural Language \nLearning (CoN LL) (pp. 17-24). ACL.  \nPitler, E., S. Bergsma , D. Lin, and K. W. Church  \n(2010). Using web -scale n -grams to improve base \nNP parsing performance . In Proceedings of t he \n23rd International Conferen ce on Computational \nLinguistics ( Coling 2010)  (pp. 886-894). ACL.  \nPustejovsky, J. , P. Anick, and S.  Bergler  (1993). \nLexical semantic techniques for corpus analysis. \nComputation al Linguistics , 19(2), 331 -358. \nResnik, P.S. (1993 ). Selection and Information: A \nClass -based Approach to Lexical  Relationships . \nPh.D. Thesis.  University of Penn sylvania.  \nRoget, P.M. (1852). Roget\u2019s Thesaurus  of English \nWords and Phrases . Available in Project \nGutemberg.  \nhttps://ww w.gutenberg.org/ebooks/10681 . \nRojas-Garcia, J. (forthcoming). Se mantic \nrepresent ation of context for the inclusion of named rivers in a terminological knowledge base. \nFrontiers in  Psychology . \nRuppenhofer, J., M. Ellsworth , M.R.L. Petruck , C.R. \nJohnson, and J. Scheffczyk  (2010). FrameNet II: \nExtended Theory and Practice . International \nComputer Science Institute.  \nThompson, P., S.A. Iqbal, J. McNaught, and S. \nAnaniadou  (2009). Construction of an annotated \ncorpus to support biomedical information \nextraction. BMC Bioinformatics , 10, 349.  \nVadas , D., and J.R  Curran  (2007). Large-scale \nsupervi sed models for noun phrase bracketing . In \nProceedin gs of the 10th Conference of the Pacific \nAssociation for Computational Linguistics \n(PACLING -2007)  (pp. 104-112). PACLING.  \nVadas, D., and J.R. Curran  (2008). Parsing noun \nphrase structure with CCG . In Proceedings of the \n46th Annual Meeting of the Association for \nComputational Linguistics: Human Language \nTechnologies  (pp. 335 -343). ACL.  \n \n152\nJuan Rojas-Garcia Evaluating Contextualized Vectors from both Large\nLanguage Models and Compositional Strategies\nEvaluando vectores contextualizados generados a partir de\ngrandes modelos de lenguaje y de estrategias composicionales\nPablo Gamallo, Marcos Garcia, Iria de-Dios-Flores\nCentro de Investigaci\u00b4 on en Tecnolox\u00b4 \u0131as Intelixentes (CITIUS)\nUniversidade de Santiago de Compostela, Galiza\n{pablo.gamallo, marcos.garcia.gonzalez, iria.dedios }@usc.gal\nAbstract: In this article, we compare contextualized vectors derived from large\nlanguage models with those generated by means of dependency-based compositional\ntechniques. For this purpose, we make use of a word-in-context similarity task. As\nall experiments are conducted for the Galician language, we created a new Galician\nevaluation dataset for this specific semantic task. The results show that compo-\nsitional vectors derived from syntactic approaches based on selectional preferences\nare competitive with the contextual embeddings derived from neural-based large\nlanguage models.\nKeywords: Large Language Models, Contextualized Vectors, Comositionality, Se-\nmantic Similarity, Selection Preferences, Syntactic Dependencies.\nResumen: En este art\u00b4 \u0131culo, comparamos los vectores contextualizados derivados\nde grandes modelos de lenguaje con los generados mediante t\u00b4 ecnicas de composici\u00b4 on\nbasadas en dependencias sint\u00b4 acticas. Para ello, nos servimos de una tarea de simil-\nitud de palabras en contextos controldados. Como se trata de una experimentaci\u00b4 on\norientada a la lengua gallega, creamos un nuevo conjunto de datos de evaluaci\u00b4 on en\ngallego para esta tarea sem\u00b4 antica espec\u00b4 \u0131fica. Los resultados muestran que los vec-\ntores composicionales derivados de enfoques sint\u00b4 acticos basados en restricciones de\nselecci\u00b4 on son competitivos con los embeddings contextuales derivados de los modelos\nde lenguaje de gran tama\u02dc no basados en arquitecturas neuronales.\nPalabras clave: Grandes Modelos de Lenguaje, Vectores Contextualizados,\nComposicionalidad, Similitud Sem\u00b4 antica, Restricciones de Seleccion, Dependencias\nSint\u00b4 acticas.\n1 Introduction\nLarge Language Models (LLMs) are a disrup-\ntive breakthrough in Artificial Intelligence\nthat have received an increasing amount of\nattention in many Natural Language Process-\ning (NLP) tasks. As in the case of classical\nmodels, it is possible to use two different ap-\nproaches to evaluate LLMs: intrinsic and ex-\ntrinsic evaluations. Intrinsic evaluation con-\nsists of using a metric to evaluate the lan-\nguage model itself, without considering any\ntask in which it may be involved. Extrin-\nsic evaluation consists of evaluating the mod-\nels by employing them in a downstream NLP\ntask. This strategy allows us to compare how\ntheir final representation affects the accom-\nplishment of the target task.\nPerplexity is one of the most popular\nmetrics for intrinsically evaluating languagemodels. It measures how good a language\nmodel is at predicting real sentences. Al-\nthough perplexity measurements allow re-\nsearchers to assess the quality of a model in a\nfast and inexpensive way, it is not considered\na fair metric to compare models because the\nfinal value is highly dependent on the models\u2019\nsize and vocabulary. In addition, while this\nmetric can be easily applied to classical lan-\nguage models, it is not well-defined for auto-\nencoding LLM (Salazar et al., 2020), such as\nmasked language models such as BERT (De-\nvlin et al., 2019).\nMost commonly, LLMs are evaluated on\nseveral NLP tasks by making use of extensive\nand comprehensive benchmarks (e.g., GLUE\n(Wang et al., 2019)). However, extrinsic eval-\nuation also has some drawbacks. First, it is\na costly and computationally slow process,\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 153-164\nrecibido 30-03-2022 revisado 19-05-2022 aceptado 23-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-13\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalsince it requires supervised fine-tuning (i.e.,\ntraining a new model with annotated exam-\nples to adapt it to the task). Second, hyper-\nparameters for fine-tuning are likely to have\nan important influence on the results of the\nevaluations (Shibayama et al., 2020). And\nthird, the most comprehensive datasets to\nevaluate LLMs are only available for either\nEnglish or a dozen of mid-resource languages\n(Lin et al., 2021), but not for low-resource\nlanguages such as Galician.\nAs an alternative to fine-tuning, which ad-\njusts the vector weights with new annotated\ndata, it is possible to optimize a pre-trained\nlanguage model for many different tasks by\nmaking use of prompt tuning, which is a sort\nof zero-shot learning approach based on the\noptimization of the model by embedding the\ndescription of the task in the input. So LLMs\ncan also be externally evaluated through the\nevaluation of their prompted-based tasks.\nAnother way to evaluate LLMs is to do\nso on the basis of some of their components,\nfor instance word embeddings. LLMs trans-\nform input sentences into contextual vectors\nof each token constituent. These sensitive\ncontext word embeddings are seen as compo-\nnents that are dynamically derived from the\nLLM. Contextual embeddings can be eval-\nuated in a manner analogous to the way\nnon-contextual and static word embeddings\nare evaluated. While the latter are evalu-\nated intrinsically on subtasks searching for\nword similarity and analogy completion out\nof context, the former can be evaluated by\nmeans of tasks that measure both in-context\nword similarity and sentence similarity. For\nLLMs, these tasks are simpler and faster than\nextrinsic evaluations, since they do not re-\nquire supervision or fine-tuning, and allow\nus to directly check the quality of the model\nthat generated the contextual embeddings. It\nshould be noted that, even though this type\nof evaluation is known as intrinsic evaluation\nof embeddings, it is not actually intrinsic for\nthe LLM from which the embeddings are de-\nrived. To avoid terminological confusion, we\nwill call it vector-based evaluation of LLMs.\nImportantly, contextual embeddings can\nbe generated not only from LLMs, but also\nby compositional techniques that combine\nstatic embeddings, as described in numer-\nous works on compositional distributional se-\nmantics (Baroni, 2013; Weir et al., 2016;\nGamallo et al., 2019; Wijnholds, Sadrzadeh,and Clark, 2020). In some of these ap-\nproaches, static embeddings representing the\nmeaning of words in a sentence are com-\nbined by syntactic dependencies in an en-\ntirely compositional manner, resulting in con-\ntextualized vectors of each constituent word\n(Gamallo et al., 2019; Weir et al., 2016).\nThe aim of this work is to compare contex-\ntual embeddings generated from LLMs with\nthose generated by using syntax-based com-\npositional techniques. All embeddings will\nbe evaluated in a word-in-context similar-\nity task. To do so a very specific dataset\nis needed because the compositional tech-\nniques, due to their linguistic complexity, can\nonly be evaluated on controlled and simple\nsyntactic constructions (e.g. adjective-noun,\nnoun-verb, noun-verb-noun, etc). For this\npurpose, we created a syntactically controlled\ndataset in Galician language. In sum, the\nmain contributions of the paper are the fol-\nlowing:\n\u2022Creation of a new Galician dataset\nto perform word-in-context similarity\ntasks.\n\u2022Vector-based evaluation (via contextual-\nized word embeddings) of four different\nLLMs, namely three BERT monolingual\nmodels for Galician, and the official mul-\ntilingual one (mBERT).\n\u2022Evaluation of dependency-based compo-\nsitional vectors generated from Galician\nWikipedia.\n\u2022Comparison of the performance of all\nthese dynamic and contextually sensitive\nembeddings against the same dataset.\nThe rest of the article is organized as\nfollows. The next section introduces some\nrelated work (2). Then, the different\ntypes of language models, both LLMs and\ndependency-based, are defined in Section 3.\nThe results and the dataset used in the evalu-\nation are described and analyzed in Section 4.\nFinally, the conclusions are presented in Sec-\ntion 5.\n2 Related work\nThe first well-known datasets to evaluate\ncontextualized vectors in controlled syntactic\nconstructions are those described in Mitchell\nand Lapata (2008; 2010). The authors did\n154\nPablo Gamallo, Marcos Garcia, Iria de-Dios-Flores not actually use the term contextualized vec-\ntors for what they called the representation\nof the meaning of sentences in vector space by\nmeans of vector composition. In their work,\nthe meaning of phrases or sentences is rep-\nresented as the combination of constituent\nword vectors together with arithmetic oper-\nations such as addition and component-wise\nmultiplication. The main drawback of this\napproach is that it is not fully compositional\nbecause word order and syntactic functions\nare not taken into account. The dataset\ncreated by Mitchell and Lapata (2008) in\norder to evaluate vector composition con-\ntains pairs of intransitive English sentences\n(subject-verb constructions) differing only in\nthe verb. In Mitchell and Lapata (2010)\nthe dataset contains pairs of verb-object con-\nstructions differing also in the verb.\nLater, Grefenstette et al. (2011a) and\nKartsaklis and Sadrzadeh (2013) built very\nsimilar evaluation datasets, always for En-\nglish. These datasets also consist of pairs\nof sentences, all of which are subject-verb-\nobject transitive constructions that differ\nonly in the verb. Yet, unlike the previ-\nous work by Mitchell and Lapata, the se-\nmantic approaches that were evaluated on\nthese datasets were full compositional mod-\nels based on functional words represented as\nhigh-dimensional tensors (Baroni, Bernardi,\nand Zamparelli, 2014). The main con-\ncern with these approaches is that they re-\nquire several high-order tensor representa-\ntions of verbs with several arguments, some-\nthing which is computationally inefficient.\nTo facilitate linguistic preprocessing, all\nsentences in those datasets are presented\nas sequences of lemmas, for instance\nballricochet instead of the ball ricocheted.\nThus, they are not true sentence but n-grams\nof lemmas representing controlled syntactic\nconstructions.\nThe increasing development of context-\nsensitive word embeddings derived from neu-\nral language models has marginalized syntax-\nbased and compositional semantic models.\nOne of the main reasons for the low inter-\nest in these models is the difficulty to adapt\nthem to open phrases and sentences with\nany type of syntactic construction. Purely\ncompositional models, due to their linguis-\ntic complexity, are so far only successfully\napplied to datasets with controlled syntac-\ntic expressions. In contrast, as context-sensitive embeddings derived from LLMs are\nbuilt in open syntactic environments, most\ndatasets available and used in shared tasks\nare composed of open text without syntactic\nconstraints (Pilehvar and Camacho-Collados,\n2019; Armendariz et al., 2020). However, it is\nworth mentioning that the syntactically con-\ntrolled datasets cited above have been used\nto compare the two contextual approaches,\nthat is, both compositional embeddings built\nby means of dependencies and contextual em-\nbeddings derived from LLMs, e.g., Wijnholds\net al. (2020) and Gamallo et al. (2021) for\nEnglish, and Gamallo et al. (2021) for Por-\ntuguese and Spanish.\nThere are recent studies which also take\nadvantage of syntactically controlled datasets\n(such as BiRD (Asaadi, Mohammad, and\nKiritchenko, 2019)) to probe the composi-\ntional abilities of LLMs. In this respect, Yu\nand Ettinger (2020) found that Transformer-\nbased models mostly rely on word content,\nand therefore miss additional information\nprovided by compositional operations.\nFinally, recent approaches (Nguyen et al.,\n2020; Bai et al., 2021) use syntactic infor-\nmation to improve self-attention mechanism,\nresulting in interesting attempts to include\ncompositional semantic strategies to build\nthe contextualized meaning of words from\nLLMs.\n3 Contextualized Word Vectors\nfrom Galician Language Models\nContextualized word vectors can be derived\nfrom different types of language models fol-\nlowing distributional-based strategies. In our\nwork we explore contextualized word vectors\nfrom two types of models for the Galician\nlanguage, described below: (a) BERT-based\nLLMs, and (b) transparent models with syn-\ntactic dependencies.\n3.1 Contextualized Word Vectors\nfrom BERT-based Models\nBesides the official multilingual model\n(mBERT, with 12 hidden layers) provided by\nDevlin et al. (2019), we evaluate the fol-\nlowing monolingual models: Bertinho-base,\nwith 12 layers (Vilares, Garcia, and G\u00b4 omez-\nRodr\u00b4 \u0131guez, 2021), and two models of Bert-\nGalician (\u2018base\u2019 and \u2018small\u2019) released by Gar-\ncia (2021), with 12 and 6 layers, respectively.\nConcerning the size of the training corpus of\neach model, mBERT and Bertinho-base were\n155\nEvaluating Contextualized Vectors from both Large Language Models and Compositional Strategies trained on the Wikipedia, which contains\nabout 42M tokens, while the two versions of\nBert-Galician were trained on a larger corpus\nwith about 500M tokens.\nTo obtain the contextualized vector of a\nword in the input sentence, we use the stan-\ndard approach of adding the last four lay-\ners, as they have been found to provide more\ncontext-specific representations (Ethayarajh,\n2019; Vuli\u00b4 c et al., 2020). When the tokenizer\ndivides a word into several sub-words (or af-\nfixes), only the first subword is considered\nsince it represents the lexical stem of the full\ntoken.\nWe also generate sentence embeddings\nfrom LLMs by making use a pooling strategy.\nThis is the same strategy used by Sentence-\nBERT for English (Reimers and Gurevych,\n2019). The main difference with regard\nto Sentence-BERT is that the Galician pre-\ntrained models of our experiments are not\nfine-tuned with annotated collections of se-\nmantically similar pairs of sentences. The\nbasic pooling strategy used to generate our\nsentence embeddings consists of computing\nthe mean of all output vectors.\n3.2 Contextualized Word Vectors\nfrom a Galician\nDependency-Based Model\n3.2.1 Selectional Preferences\nDependency-based distributional models,\nalso known as structured vector spaces,\nallow us to directly deal with issues related\nto semantic compositionality and selectional\npreferences between syntactically related\nwords. To build such a syntax-based model\nin a transparent way, we opt for a count-\nbased strategy with explicit and sparse\ndimensions representing lexical-syntactic\ncontexts of words. For instance, given the\ndependency ( obj, catch, ball ), representing\nthe heading verb catch occurring with\ndependent noun ballin the direct object re-\nlation (obj ), we extract two lexical-syntactic\ncontexts: either being a dependent noun\noccurring with catch inobjrelation, or being\na verbal head occurring with ball inobj\nrelation.\nThe high number of dimensions (lexical-\nsyntactic contexts) of the vector space is re-\nduced by selecting the Nmost relevant con-\ntexts per word (Biemann and Riedl, 2013;\nPadr\u00b4 o et al., 2014; Gamallo, 2017), where N\nis a global, arbitrarily defined constant whoseusual values range from 100 to 1000 (Padr\u00b4 o\net al., 2014). The relevance value of a con-\ntext with regard to a word is computed by\nmeans of a lexical association measure (e.g.,\npointwise mutual information, loglikelihood,\netc.). This is an explicit, transparent, and\nstatic representation of word meaning, very\nsimilar to the predictive-based and also static\n(i.e. out of context) representation known as\nword embeddings (Mikolov, Yih, and Zweig,\n2013).\nIn order to build contextualized word\nvectors from these dependency-based rep-\nresentations, we follow the concept of se-\nlectional preference formalized in Erk and\nPad\u00b4 o (2008), which states that the two\nwords related by a dependency relation\nimpose restrictions on each other. Let\nA(obj, catch, ball ) denote the lexical asso-\nciation Abetween verbal head catch and\ndependent noun ball via relation objin a\nparsed corpus, then the selectional prefer-\nences, noted handd, imposed by the two\nlemmas on each other in relation objare com-\nputed in equations 1 and 2.\n\u20d7hball(obj) =X\nh:A(obj,h,ball )>\u03b8\u20d7h (1)\n\u20d7dcatch(obj) =X\nd:A(obj,catch,d )>\u03b8\u20d7d (2)\nWhere h:A(obj, h, ball )> \u03b8 is the set\nof heading verbs (e.g., catch, throw, orga-\nnize...) that have ballasobjwith a lexi-\ncal association value higher than threshold\n\u03b8, and d:A(obj, catch, d )> \u03b8 is the set\nof dependent nouns (e.g., ball, baseball, cold,\ndrift...) occurring with catch viaobjwith an\nassociation value higher than \u03b8. Note that\nthe former represents the paradigmatic class\nof those relevant verbs having ballas direct\nobject, while the latter is the paradigmatic\nclass of relevant nouns appearing as direct ob-\njects of catch. In both cases, the selectional\npreferences imposed by the two related lem-\nmas result in two new compositional vectors,\n\u20d7hball(obj) and \u20d7dcatch(obj), created by the it-\nerative sum of the static vectors, respectively\nnoted \u20d7hand\u20d7d, of the paradigmatic classes\n(see equations 1 and 2).\nOnce the selectional preferences are built,\nthey are combined by component-wise multi-\nplication with the static vectors of both the\nhead and dependent lemmas, giving rise to\n156\nPablo Gamallo, Marcos Garcia, Iria de-Dios-Flores two new contextualized vectors: the vector\nof the head lemma, \u20d7catch (obj,h,ball ), contex-\ntualized with the selectional preferences of\nball(equation 3), and the vector of the de-\npendent lemma, \u20d7ball (obj,catch,d ), contextual-\nized with the selectional preferences of catch\n(equation 4).\n\u20d7catch (obj,h,ball )=\u20d7catch\u2299\u20d7hball(obj) (3)\n\u20d7ball (obj,catch,d )=\u20d7ball\u2299\u20d7dcatch(obj) (4)\nAt the end of this compositional pro-\ncess, the two contextualized vectors repre-\nsent the in-context meaning of the two re-\nlated words, which are more precise than the\nout-of-context meaning of the initial static\nvectors: catch means in the context of ball\nsome event similar to grab, and not to con-\ntract as in catch a disease, while ballmeans\nin the context of catch a spherical object and\nnot and dancing event as in attend a ball . In\nsum, these two contextualized vectors repre-\nsent discriminated and disambiguated word\nsenses.\n3.2.2 Incremental Contextualization\nSo far we have defined the process of com-\npositional semantics between two dependent\nwords, but this process can be extended to\nthe sentence level. Given the dependency\nparse tree of an input sentence, the contextu-\nalization of all constituent words in the sen-\ntence is the result of applying the composi-\ntional operations carried out by all dependen-\ncies identified in the parse tree in an iterative\nand incremental way. Thus, at the end of the\nprocess, each word of the sentence, includ-\ning the root one, is assigned a contextualized\nvector. The order in which compositional op-\nerations are applied is not predetermined and\nthe incremental and iterative process can go\neither from left-to-right or from right-to-left.\n3.2.3 Galician Model\nTo build the language model and their\ncorresponding vector space, the Galician\nWikipedia (dump file of November 2019)\nwas parsed with LinguaKit (Gamallo et al.,\n2018).1The Linguakit module used for this\npurpose is dep(endencies), which in turn\nmakes use of the PoS tagger and lemmatizer\nmodules. Since it is a small corpus con-\ntaining about 42.7 million tokens, we used\n1https://github.com/citiususc/Linguakitlemmas as the main lexical unit. Lemmas\nappearing less than 100 times were filtered\nout, and lexico-syntactic contexts with fre-\nquency less than 50 were removed. Then,\nfor each lemma, we selected the 500 most\nrelevant lexico-syntactic contexts by means\nof loglikelihood as lexical association mea-\nsure. The final model resulted in a non-\nzero matrix of about 50k different lemmas\nand over 33k different contexts. In total,\nthe vector space consists of a non-zero matrix\nwith about 4.251 million word-context pairs.\nAll static vectors of out-of-context Galician\nwords are derived from this language space.\nSee Gamallo (2017) for more details on how\ndependency-based vectors are built.\nThe software used to dynamically build\ncontextualized word vectors from the Gali-\ncian static vectors is freely available.2This\nis an improved upgrade we implemented on\nthe basis of an older version that was fully de-\nscribed in Gamallo (2019). For the Galician\ncorpus, the \u03b8parameter was set to 0. Previ-\nous experiments did not show any improve-\nment by assigning positive values to this pa-\nrameter. It means that the second selection\nof relevant contexts made by this parameter\nis not justified with small corpus sizes such\nas the one used here.\nAlthough the compositional strategy is de-\nsigned to work with any type of sentence,\ndue to the difficulty of the task, the imple-\nmented version only applies to linguistic ex-\npressions with a fixed and predefined syntac-\ntic structure (e.g., adjective-noun, subject-\nverb-object, and son on).\n4 Evaluation\nIn order to compare contextual embed-\ndings generated from BERT-based models\nwith those generated with the compositional\ndependency-based strategy, we use a word-\nin-context similarity task in Galician. For\nthis purpose, we created a syntactically con-\ntrolled Galician dataset with subject-verb-\nobject sentences.\n4.1 Test Dataset\nFollowing the structure of the English dataset\ndescribed in Grefenstette and Sadrzadeh\n(2011a), a new Galician dataset with 192 sen-\ntence pairs of subject-verb-object sentences\n2https://github.com/gamallo/DepFunc\n157\nEvaluating Contextualized Vectors from both Large Language Models and Compositional Strategies was built.3\nAs it is not possible to make a direct trans-\nlation of the original English sentences, since\nthe selection preferences are very different\nfrom one language to another, we chose anal-\nogous examples with 68 different polysemous\nverbs and 149 different nouns in subject and\nobject position. All sentences consist of just\none basic nominal phrase as subject, a verb\nas predicate, and a basic nominal phrase as\ndirect object.\nIn each pair, one transitive sentence con-\nsisting of a verb with its subject and direct\nobject is compared to another transitive sen-\ntence combining the same subject and object\nwith a semantically related verb that is cho-\nsen to be either appropriate or inappropri-\nate in the same context. For instance, a em-\npresa compra un pol\u00b4 \u0131tico (\u2018the company buys\na politician\u2019) is semantically appropriate and\nvery close to a empresa suborna un pol\u00b4 \u0131tico\n(\u2018the company bribes a politician\u2019) as com-\nprar (\u2018buy\u2019) is a very close synonym of sub-\nornar (\u2018bribe\u2019) in this context, where the sub-\nject is a person or organization and the object\nis also a person or organization. However, the\nsame pair of verbs have a very dissimilar be-\nhavior in a different context, e.g., o director\ncompra unha acci\u00b4 on /??o director suborna\nunha acci\u00b4 on (\u2018the director buys a share\u2019 /\n\u2018??the director bribes a stock\u2019), as the verb\nsubornar (\u2018bribe\u2019) cannot be applied on ob-\njects that are not provided with the human\nfeature. The selectional preferences imposed\nby that verb are not fulfilled by the direct\nobject.\nUnlike the original English dataset from\nwhich it is inspired, we created complete sen-\ntences, and not just triples of lemmas. How-\never, all sentences were also lemmatized to\nenable to be evaluated with the dependency-\nbased approach. Most verbs and their argu-\nments were adapted (and not literally trans-\nlated) to Galician from the English original\ndataset.\nThree native speakers of Galician (and ex-\npert linguists) were asked to rate the degree\nof semantic correctness and similarity of each\nsentence pair using a 1 to 7 Likert scale. The\naverage scores per annotator are 4.22, 3.45\nand 3.94, with the following standard devi-\nations: 1.96, 2.02 and 1.97, respectively. In\norder to measure the reliability of the ratings\n3The dataset is available with the software (Cf.\nfootnote 2).provided by the annotators, we calculated an\nintraclass correlation coefficient (ICC) using\ntheirrpackage in R (Gamer et al., 2019).\nThe agreement ICC was 0.71, indicating a\nhigh reliability among raters. Then, the av-\nerage of the three scores per pair was com-\nputed.\nAs in many intrinsic evaluations of word\nembeddings, we compute Spearman correla-\ntion between human scores (the average of\nthe three evaluators) and the predictions re-\nturned by the systems. Both human evalua-\ntors and systems should provide high scores\nto semantically similar sentence pairs with a\nhigh degree of semantic correctness.\n4.2 Types of Sentence Similarity\nContextualized word embeddings are power-\nful semantic artifacts that can be used to\nmeasure the similarity between two sentences\nfrom different points of view. In the following\nsubsections, we define different types of sen-\ntence similarity depending on which is the\nmost representative constituent of the sen-\ntence.\n4.2.1 BERT Sentence Similarity\nAs all constituent words are fully contextu-\nalized, we assume that any of them can rep-\nresent the whole sentence semantically from\na specific point of view. For example, in the\nsentence the president signed the decree , in\naddition to the verb, the contextualized sub-\nject refers to the president who signed the\ndecree, while the contextualized direct ob-\nject designates the decree that is signed by\nthe president. So, in a transitive sentence,\neach of the three contextualized word vectors\n(subject, verb, or object) might be used to\ncompute similarity at the sentence level (and\nnot just at the word level). Moreover, it is\nalso possible to build a new vector represent-\ning the whole sentence by combining the em-\nbeddings of its constituent words. In total,\nwe can build the following four vectors:\nBERT - verb : Contextualized vector of\nthe verb head, resulting from adding the\n4 last layers.\nBERT - subj : Contextualized vector of\nthe subject word, resulting from adding\nthe 4 last layers.\nBERT - obj : Contextualized vector of the\ndirect object, resulting from adding the\n4 last layers.\n158\nPablo Gamallo, Marcos Garcia, Iria de-Dios-Flores BERT - sentence : Mean of all output vec-\ntors.\nNote that for English, Sentence-BERT\n(Reimers and Gurevych, 2019) generates\nfixed sized vectors of sentences in a way\nthat is similar to our BERT-sentence strat-\negy. There are, however, two significant dif-\nferences: Sentence-BERT was derived from\nBERT-large (with 24 layers) and was fine-\ntuned with two very large dataset collections:\nSNLI (Bowman et al., 2015) and MultiNLI\n(Williams, Nangia, and Bowman, 2018) con-\ntaining 1 million sentence pairs which were\nannotated for semantic tasks such as infer-\nence, contradiction, and entailment. So,\nwhile Sentence-BERT is a fine-tuned model\ntrained with a supervised technique on anno-\ntated corpora, BERT-sentence is a fully un-\nsupervised model.\n4.2.2 Dependency-Based Sentence\nSimilarity\nThis strategy builds compositional vectors in\nan incremental way. Thus, it is sensitive to\nthe order of application of the identified syn-\ntactic dependencies. The semantic meaning\nofThe company buys the politician can be in-\nterpreted either from left to right (see 5 be-\nlow) or from right to left (see 6), according\nto the order in which the two dependencies\nof the sentence are applied:\n(nsubj, buy, company ),(obj, buy, politician ) (5)\n(obj, buy, politician), (nsubj, buy, company ) (6)\nThen, considering the direction of the\ncompositional process, several compositional\nvectors representing the meaning of the tran-\nsitive sentence are built:\nleft-to-right - verb : This builds the com-\npositional vector of the verb head buy. It\nresults from being contextualized first by\nthe selectional preferences imposed by\nthe nominal subject company and then\nby the selectional preferences of the di-\nrect object politician.\nleft-to-right - obj : This builds the com-\npositional vector of the direct object\npolitician. It results from being contex-\ntualized by the preferences imposed by\nbuypreviously combined with the sub-\njectcompany.left-to-right - sentence : The addition of\nthe two previous left-to-right values\n(head and dep).\nright-to-left - verb : This builds the com-\npositional vector of the verb head buy. It\nresults from being contextualized first by\nthe selectional preferences imposed by\nthe direct object politician and then by\nthe selectional preferences of the subject\ncompany.\nright-to-left - subj : This builds the com-\npositional vector of the subject company.\nIt results from being contextualized by\nthe preferences imposed by buyprevi-\nously combined with the direct object\npolitician.\nright-to-left - sentence : The addition of\nthe two previous right-to-left values\n(head and dep).\nNote that, in the left-to-right direction,\nthe object is fully contextualized by the verb\nand the subject. By contrast, the subject\nis not contextualized by the object, so that\nthis partially contextualized sense of the sub-\nject is not used to represent the sentence.\nThe same occurs for the direct object in the\nright-to-left compositional processes. The in-\ncremental direction is not relevant in BERT\nLLMs because the BERT-like strategy relies\non bidirectional scanning by jointly condi-\ntioning on both left and right context in all\nlayers. Hence, any constituent word is con-\ntextualized by the other words in both left-\nto-right and right-to-left direction.\n4.3 Results\nAll the models and their different vector con-\nfigurations were evaluated using the Galician\ndataset described above.\nTable 1 shows the results of the four BERT\nmodels for the four types of contextualized\nvectors introduced in subsection 4.2.1, by us-\ning Spearman correlation between the sys-\ntem scores and the human evaluators. We\nobserve that the most significant element of\nthe meaning of the sentence is the contextu-\nalized sense of the verb -something expected\nbecause the verb is the syntactic root. The\nverb provides even better results than the\nsentence method, as a representative of the\nwhole sentence, in three of the four models.\nBy contrast, the subject tends to be the least\nsignificant constituent. Perhaps this might\n159\nEvaluating Contextualized Vectors from both Large Language Models and Compositional Strategies be explained by the fact that in transitive\nconstructions the object is mostly determined\nby the verb (through more restrictive selec-\ntion preferences) than by the subject.\nIf we focus on the comparison of the four\nLLMs, we observe that the best one is clearly\nBERT-base (57 for the verb), followed by\nBERT-small (46). The big differences be-\ntween these two LLMs and Bertinho-base\n(21) and mBERT (25) are quite remarkable.\nTable 2 shows the results obtained with\ndifferent contextualized vectors derived from\nthe dependency-based model (see subsec-\ntion 4.2.2). In the first row, the table also\nshows a non-compositional baseline strategy\njust comparing the similarity of verb vectors\nout of context.\nAs it was the case with BERT models,\nthe verb is also the most representative con-\nstituent of the meaning of the sentence: it\nachieves 47 and 41 correlation in the two di-\nrections, compared to only 18 and 15 for sub-\nject and object respectively. It follows that\nthe verbal root, once contextualized by the\nsense of the arguments, can be taken as the\nmeaning of the whole sentence.\nAlthough they are not totally comparable,\nwe also show in the last rows of the table the\nbest values obtained by compositional sys-\ntems applied to the English dataset described\nin (Grefenstette and Sadrzadeh, 2011b) and\nfrom which we have created the Galician one.\nThe values obtained on the Galician dataset\nby using BERT-Base-Galician outperform all\nthe compositional methods for English, in-\ncluding the highest score, 54, obtained by the\nsystem described in (Wijnholds, Sadrzadeh,\nand Clark, 2020).\n4.4 Discussion\nThe analysis of the results presented in the\nprevious section leads us to draw some con-\nclusions about the strategies compared in the\nexperiments.\nBERT-base-Galician and BERT-small-\nGalician models clearly outperform both\nBertinho-base and mBERT in the proposed\nsemantic task. It is also important to point\nout that the best scores of both Bertinho-\nbase and mBERT are still below the baseline\n(28).\nConcerning the dependency-based strat-\negy, its results are comparable to those\nof BERT-small-Galician, even if they are\nfar from the higher correlation given byModels \u03c1\nBERT-base-Galician - sentence 54\nBERT-base-Galician- verb 57\nBERT-base-Galician - subj 33\nBERT-base-Galician - obj 49\nBERT-small-Galician- sentence 46\nBERT-small-Galician - verb 43\nBERT-small-Galician - subj 28\nBERT-small-Galician - obj 36\nmBERT - sentence 21\nmBERT - verb 25\nmBERT - subj 23\nmBERT - obj 24\nBertinho-base - sentence 9\nBertinho-base - verb 21\nBertinho-base - subj 6\nBertinho-base - obj 9\nTable 1: Spearman correlation between dif-\nferent configurations of BERT and human\njudgments on 192 subject-verb-object sen-\ntence pairs.\nModels \u03c1\nbaseline - verb 28\nleft-to-right - sentence 37\nleft-to-right - verb 47\nleft-to-right - obj 15\nright-to-left - sentence 32\nright-to-left - verb 41\nright-to-left - subj 18\nHashimoto and Tsuruoka (2014) 43 (en)\nPolajnar et al. (2015) 35 (en)\nWijnholds et al. (2020) 54 (en)\nTable 2: Spearman correlation between dif-\nferent configurations of the compositional\ndependency-based method and human judg-\nments on 192 subject-verb-object sentence\npairs (in lemmas). The table also shows a\nbaseline based on just comparing verbs out-\nof-context (first row) and some related eval-\nuations on a quite similar dataset for English\n(three last rows).\nBERT-base-Galician. Let us note that the\ntraining corpus of the dependency-based\nmethod, as well as that of Bertinho-base\nand mBERT (Galician part) is just the Gali-\ncian Wikipedia, and this is much smaller\nthan the training corpus used for the two\nBERT-Galician models: \u224842 million tokens\nvs.\u2248500 million.\nAs it was already reported, the contextu-\nalized sense of the verbal root is the most\n160\nPablo Gamallo, Marcos Garcia, Iria de-Dios-Flores representative meaning of the sentence for\nmost strategies. It behaves better than the\nother constituents (subject and object), and\neven than computing a global meaning for\nthe whole sentence. This is a very relevant\nobservation as most systems computing the\nsentence meaning do not know which is the\nroot word because they do not rely on a de-\npendency tree, and so they make use of the\nvectors of all constituent words.\nAnd finally, we must point out that the\ncorrelation values obtained here and in other\nrelated experiments for other languages are in\nlow to medium ranges. This shows that this\nis a semantic task of great complexity that\nstill requires improved language models, per-\nhaps not larger or computationally deeper,\nbut of higher quality and with deeper linguis-\ntic knowledge.\n5 Conclusions\nIn this article, we evaluated and compared\nthe performance of contextualized vectors\nbuilt with LLMs and a fully compositional\nstrategy based on syntactic dependencies and\nselectional preferences. The use of selectional\npreferences to build contextualized vectors is\na linguistically motivated attention strategy\nfocused on selecting only syntactically rele-\nvant contextual elements. It can be seen,\ntherefore, as a mechanism of attention driven\nby syntactic information.\nAccording to the results obtained, the\ncompositional strategy turned out to be com-\npetitive when compared to several configura-\ntions of BERT in a specific task focused on\nsentence similarity in Galician. It should be\nnoted that the computational cost of train-\ning compositional models is much lower than\nthat of neural-based LLMs. In addition, the\nsyntax-based vectors we have used for the\ncompositional approach are more transpar-\nent and interpretable than those derived from\nthe Transformer architecture. Transparent\nmodels make it easier to explain the errors\nand successes committed in a particular task,\nsince it is possible to explicitly list the syntac-\ntic contexts involved in vector composition.\nHowever, the dependency-based strategy\nhas important weaknesses. First, as it mainly\nrelies on syntactic parsing, it has a vulnerable\nexposure to parser errors. Second, this strat-\negy cannot be easily adapted to syntactically\nopen sentences.\nIn order to overcome these drawbacks, infuture work we will define and implement a\nsyntax-based model allowing us to build fully\ncontextualized vectors for open sentences.\nThis will enable to apply the compostional\nmethod to any sentence in as similar way as\nTransformers do.\nAcknowledgements\nThis research was funded by the project\n\u201dN\u00b4 os: Galician in the society and economy\nof artificial intelligence\u201d, agreement between\nXunta de Galicia and University of Santiago\nde Compostela, and grant ED431G2019/04\nby the Galician Ministry of Education,\nUniversity and Professional Training, and\nthe European Regional Development Fund\n(ERDF/FEDER program), and Groups of\nReference: ED431C 2020/21. In addition:\nRam\u00b4 on y Cajal grant (RYC2019-028473-I)\nand Grant ED431F 2021/01 (Galician Gov-\nernment).\nReferences\nArmendariz, C. S., M. Purver, S. Pollak,\nN. Ljube\u02c7 si\u00b4 c, M. Ul\u02c7 car, M. Robnik- \u02c7Sikonja,\nI. Vuli\u00b4 c, and M. T. Pilehvar. 2020.\nSemEval-2020 task 3: Graded word simi-\nlarity in context (GWSC). In Proceedings\nof the 14th International Workshop on Se-\nmantic Evaluation.\nAsaadi, S., S. Mohammad, and S. Kir-\nitchenko. 2019. Big BiRD: A large, fine-\ngrained, bigram relatedness dataset for ex-\namining semantic composition. In Pro-\nceedings of the 2019 Conference of the\nNorth American Chapter of the Associa-\ntion for Computational Linguistics: Hu-\nman Language Technologies, Volume 1\n(Long and Short Papers), pages 505\u2013516,\nMinneapolis, Minnesota, June. Associa-\ntion for Computational Linguistics.\nBai, J., Y. Wang, Y. Chen, Y. Yang, J. Bai,\nJ. Yu, and Y. Tong. 2021. Syntax-BERT:\nImproving pre-trained transformers with\nsyntax trees. In Proceedings of the 16th\nConference of the European Chapter of the\nAssociation for Computational Linguis-\ntics: Main Volume , pages 3011\u20133020, On-\nline. Association for Computational Lin-\nguistics.\nBaroni, M. 2013. Composition in distribu-\ntional semantics. Language and Linguis-\ntics Compass , 7:511\u2013522.\n161\nEvaluating Contextualized Vectors from both Large Language Models and Compositional Strategies Baroni, M., R. Bernardi, and R. Zampar-\nelli. 2014. Frege in space: A program\nfor compositional distributional seman-\ntics. Linguistic Issues in Language Tech-\nnology (LiLT) , 9:241\u2013346.\nBiemann, C. and M. Riedl. 2013. Text:\nNow in 2d! a framework for lexical expan-\nsion with contextual similarity. Journal of\nLanguage Modelling , 1(1):55\u201395.\nBowman, S. R., G. Angeli, C. Potts, and\nC. D. Manning. 2015. A large annotated\ncorpus for learning natural language in-\nference. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural\nLanguage Processing, pages 632\u2013642, Lis-\nbon, Portugal. Association for Computa-\ntional Linguistics.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. In Proceedings of\nNAACL-2019, Volume 1 , pages 4171\u2013\n4186, Minneapolis, Minnesota. Associa-\ntion for Computational Linguistics.\nErk, K. and S. Pad\u00b4 o. 2008. A structured\nvector space model for word meaning in\ncontext. In 2008 Conference on Empirical\nMethods in Natural Language Processing\n(EMNLP-2008, pages 897\u2013906, Honolulu,\nHI.\nEthayarajh, K. 2019. How contextual\nare contextualized word representations?\ncomparing the geometry of bert, elmo,\nand gpt-2 embeddings. In K. Inui,\nJ. Jiang, V. Ng, and X. Wan, editors,\nEMNLP/IJCNLP (1), pages 55\u201365. Asso-\nciation for Computational Linguistics.\nGamallo, P., M. Garcia, C. Pi\u02dc neiro,\nR. Martinez-Casta\u02dc no, and J. C. Pichel.\n2018. LinguaKit: A Big Data-Based Mul-\ntilingual Tool for Linguistic Analysis and\nInformation Extraction. In 2018 Fifth\nInternational Conference on Social Net-\nworks Analysis, Management and Security\n(SNAMS), pages 239\u2013244.\nGamallo, P. 2017. Comparing ex-\nplicit and predictive distributional seman-\ntic models endowed with syntactic con-\ntexts. Language Resources and Evalua-\ntion, 51(3):727\u2013743.\nGamallo, P. 2019. A dependency-based\napproach to word contextualization us-ing compositional distributional seman-\ntics. Language Modelling , 7(1):53\u201392.\nGamallo, P. 2021. Compositional distribu-\ntional semantics with syntactic dependen-\ncies and selectional preferences. Applied\nSciences, 11(12).\nGamallo, P., M. P. Corral, and M. Gar-\ncia. 2021. Comparing dependency-based\ncompositional models with contextualized\nword embedding. In 13th International\nConference on Agents and Artificial Intel-\nligence (ICAART-2021) .\nGamallo, P., S. Sotelo, J. R. Pichel, and\nM. Artetxe. 2019. Contextualized trans-\nlations of phrasal verbs with distribu-\ntional compositional semantics and mono-\nlingual corpora. Computational Linguis-\ntics, 45(3):395\u2013421.\nGamer, M., J. Lemon, I. Fellows, and\nP. Singh, 2019. irr: Various Coefficients\nof Interrater Reliability and Agreement. R\npackage version 0.84.1.\nGarcia, M. 2021. Exploring the repre-\nsentation of word meanings in context:\nA case study on homonymy and syn-\nonymy. In Proceedings of the 59th Annual\nMeeting of the Association for Computa-\ntional Linguistics and the 11th Interna-\ntional Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Pa-\npers) , pages 3625\u20133640, Online, August.\nAssociation for Computational Linguis-\ntics.\nGrefenstette, E. and M. Sadrzadeh. 2011a.\nExperimental support for a categori-\ncal compositional distributional model of\nmeaning. In Conference on Empirical\nMethods in Natural Language Processing\n(EMNLP 2011), pages 1394\u20131404.\nGrefenstette, E. and M. Sadrzadeh. 2011b.\nExperimenting with transitive verbs in\na discocat. In Workshop on Geometri-\ncal Models of Natural Language Semantics\n(EMNLP 2011).\nKartsaklis, D. and M. Sadrzadeh. 2013.\nPrior disambiguation of word tensors for\nconstructing sentence vectors. In Con-\nference on Empirical Methods in Natu-\nral Language Processing (EMNLP 2013) ,\npages 1590\u20131601.\n162\nPablo Gamallo, Marcos Garcia, Iria de-Dios-Flores Lin, B. Y., S. Lee, X. Qiao, and X. Ren. 2021.\nCommon sense beyond english: Eval-\nuating and improving multilingual lan-\nguage models for commonsense reasoning.\nCoRR, abs/2106.06937.\nMikolov, T., W.-t. Yih, and G. Zweig.\n2013. Linguistic regularities in continuous\nspace word representations. In Proceed-\nings of the 2013 Conference of the North\nAmerican Chapter of the Association for\nComputational Linguistics: Human Lan-\nguage Technologies , pages 746\u2013751, At-\nlanta, Georgia.\nMitchell, J. and M. Lapata. 2008. Vector-\nbased models of semantic composition. In\nProceedings of the Association for Com-\nputational Linguistics: Human Language\nTechnologies (ACL-08: HLT) , pages 236\u2013\n244, Columbus, Ohio.\nMitchell, J. and M. Lapata. 2010. Composi-\ntion in distributional models of semantics.\nCognitive Science, 34(8):1388\u20131439.\nNguyen, X.-P., S. Joty, S. C. H. Hoi, and\nR. Socher. 2020. Tree-structured atten-\ntion with hierarchical accumulation.\nPadr\u00b4 o, M., M. Idiart, A. Villavicencio, and\nC. Ramisch. 2014. Nothing like good old\nfrequency: Studying context filters for dis-\ntributional thesauri. In Proceedings of the\n2014 Conference on Empirical Methods\nin Natural Language Processing, EMNLP\n2014, October 25-29, 2014, Doha, Qatar,\nA meeting of SIGDAT, a Special Interest\nGroup of the ACL, pages 419\u2013424.\nPilehvar, M. T. and J. Camacho-Collados.\n2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning\nrepresentations. In Proceedings of the\n2019 Conference of the North American\nChapter of the Association for Compu-\ntational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers) , pages 1267\u20131273, Minneapo-\nlis, Minnesota. Association for Computa-\ntional Linguistics.\nReimers, N. and I. Gurevych. 2019.\nSentence-BERT: Sentence embeddings us-\ning Siamese BERT-networks. In Proceed-\nings of the 2019 Conference on Empir-\nical Methods in Natural Language Pro-\ncessing and the 9th International JointConference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 3982\u20133992,\nHong Kong, China. Association for Com-\nputational Linguistics.\nSalazar, J., D. Liang, T. Q. Nguyen, and\nK. Kirchhoff. 2020. Masked language\nmodel scoring. Proceedings of the 58th\nAnnual Meeting of the Association for\nComputational Linguistics .\nShibayama, N., R. Cao, J. Bai, W. Ma, and\nH. Shinnou. 2020. Evaluation of pre-\ntrained BERT model by using sentence\nclustering. In Proceedings of the 34th Pa-\ncific Asia Conference on Language, Infor-\nmation and Computation, pages 279\u2013285,\nHanoi, Vietnam, October. Association for\nComputational Linguistics.\nVilares, D., M. Garcia, and C. G\u00b4 omez-\nRodr\u00b4 \u0131guez. 2021. Bertinho: Galician\nBERT Representations. Procesamiento\ndel Lenguaje Natural, 66:13\u201326.\nVuli\u00b4 c, I., E. M. Ponti, R. Litschko, G. Glava\u02c7 s,\nand A. Korhonen. 2020. Probing pre-\ntrained language models for lexical seman-\ntics. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natu-\nral Language Processing (EMNLP) , pages\n7222\u20137240, Online, November. Associa-\ntion for Computational Linguistics.\nWang, A., A. Singh, J. Michael, F. Hill,\nO. Levy, and S. R. Bowman. 2019.\nGLUE: A multi-task benchmark and anal-\nysis platform for natural language under-\nstanding. In ICLR 2019.\nWeir, D. J., J. Weeds, J. Reffin, and\nT. Kober. 2016. Aligning packed depen-\ndency trees: A theory of composition for\ndistributional semantics. Computational\nLinguistics, 42(4):727\u2013761.\nWijnholds, G., M. Sadrzadeh, and S. Clark.\n2020. Representation learning for type-\ndriven composition. In Proceedings of\nthe 24th Conference on Computational\nNatural Language Learning , pages 313\u2013\n324, Online. Association for Computa-\ntional Linguistics.\nWilliams, A., N. Nangia, and S. Bowman.\n2018. A broad-coverage challenge corpus\nfor sentence understanding through infer-\nence. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the\n163\nEvaluating Contextualized Vectors from both Large Language Models and Compositional Strategies Association for Computational Linguis-\ntics: Human Language Technologies, Vol-\nume 1 (Long Papers) , pages 1112\u20131122,\nNew Orleans, Louisiana. Association for\nComputational Linguistics.\nYu, L. and A. Ettinger. 2020. Assess-\ning phrasal representation and composi-\ntion in transformers. In Proceedings of the\n2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) ,\npages 4896\u20134907, Online, November. As-\nsociation for Computational Linguistics.\n164\nPablo Gamallo, Marcos Garcia, Iria de-Dios-Flores An Overview of Drugs, Diseases, Genes and Proteins\nin the CORD-19 Corpus\nUna visi\u00b4 on general de los F\u00b4 armacos, Enfermedades, Genes y\nProte\u00b4 \u0131nas en el corpus CORD-19\nCarlos Badenes-Olmedo, \u00b4Alvaro Alonso, Oscar Corcho\nOntology Engineering Group, Universidad Polit\u00b4 ecnica de Madrid, Spain\n{carlos.badenes, oscar.corcho }@upm.es\n{alvaro.alonsoc}@alumnos.upm.es\nAbstract: Several initiatives have emerged during the COVID-19 pandemic to\ngather scientific publications related to coronaviruses. Among them, the COVID-\n19 Open Research Dataset (CORD-19) has proven to be a valuable resource that\nprovides full-text articles from the PubMed Central, bioRxiv and medRxiv reposito-\nries. Such a large amount of biomedical literature needs to be properly managed to\nfacilitate and promote its use by health professionals, for example by tagging docu-\nments with the biomedical entities that appear on them. We created a biomedical\nnamed entity recognizer (NER) that normalizes (NEN) the drugs, diseases, genes\nand proteins mentioned in texts with the codes of the main standardization systems\nsuch as MeSH, ICD-10, ATC, SNOMED, ChEBI, GARD and NCBI. It is based on\nfine-tuning the BioBERT language model independently for each entity type using\ndomain-specific datasets and an inverse index search to normalize the references. We\nhave used the resultant BioNER+BioNEN system to process the CORD-19 corpus\nand offer an overview of the drugs, diseases, genes and proteins related to coron-\naviruses in the last fifty years.\nKeywords: ner, normalization, bioentities, document retrieval.\nResumen: Durante la pandemia del COVID-19 han surgido varias iniciativas para\nrecopilar publicaciones cient\u00b4 \u0131ficas relacionadas con el coronavirus. Entre ellos, el con-\njunto de datos de investigaci\u00b4 on abierta sobre COVID-19 (CORD-19) ha demostrado\nser un recurso valioso que proporciona el texto completo de art\u00b4 \u0131culos extra\u00b4 \u0131dos de los\nrepositorios PubMed Central, bioRxiv y medRxiv. Una cantidad tan grande de lit-\neratura biom\u00b4 edica debe gestionarse adecuadamente para facilitar y promover su uso\npor parte de los profesionales de la salud, por ejemplo, etiquetando documentos con\nlas entidades biom\u00b4 edicas que aparecen mencionadas. Hemos creado un reconocedor\nbiom\u00b4 edico de entidades nombradas (NER) que normaliza (NEN) los f\u00b4 armacos, enfer-\nmedades, genes y prote\u00b4 \u0131nas mencionados en textos con los c\u00b4 odigos de los principales\nsistemas de estandarizaci\u00b4 on como MeSH, ICD-10, ATC, SNOMED, ChEBI, GARD\ny NCBI. Se basa en afinar el modelo de lenguaje BioBERT de forma independiente\npara cada tipo de entidad utilizando conjuntos de datos espec\u00b4 \u0131ficos de dominio y\nuna b\u00b4 usqueda de \u00b4 \u0131ndice inverso para normalizar las referencias. Hemos utilizado el\nsistema BioNER+BioNEN resultante para procesar el corpus CORD-19 y ofrecer\nuna visi\u00b4 on general de los f\u00b4 armacos, enfermedades, genes y prote\u00b4 \u0131nas relacionados\ncon el coronavirus en los \u00b4 ultimos cincuenta a\u02dc nos.\nPalabras clave: identificaci\u00b4 on de entidades, normalizaci\u00b4 on, bio-entidades, recu-\nperaci\u00b4 on de documentos.\n1 Introduction\nSeveral initiatives have emerged during the\nCOVID-19 pandemic to gather scientific pub-\nlications related to coronaviruses. TheCOVID-19 Data Portal1, maintained by the\nEU, or the Humandata2, focused on COVID-\n1https://www.covid19dataportal.org\n2https://data.humdata.org/event/covid-19\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 165-176\nrecibido 31-03-2022 revisado 11-05-2022 aceptado 13-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-14\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural19 cases around the world, are some exam-\nples. The Allen Institute for Artificial Intelli-\ngence created the COVID-19 Open Research\nDataset (CORD-19)(Wang et al., 2020). It is\na continuously growing corpus with all pub-\nlicly available COVID-19 and coronavirus-\nrelated research (e.g. SARS, MERS, etc.)\npublished during the last fifty years, with\na huge increase in the last two years. This\ndataset provides full-text research papers in\nPDF and JSON format, which can be used as\na source of information to extract knowledge\nrelated to the infection and disease. At the\ntime of this study (January 2022), it is com-\nposed of 334,572 scientific articles retrieved\nfrom PubMed Central, a corpus maintained\nby the World Health Organization (WHO),\nbioRxiv and medRxiv pre-prints.\nSuch a large amount of biomedical litera-\nture needs to be properly managed to facil-\nitate and promote its use by health profes-\nsionals. Natural Language Processing (NLP)\nfacilitates document analysis through the ex-\ntraction of key information from the under-\nlying texts and turning them into structured\nknowledge that can be understood by hu-\nmans (Pyysalo et al., 2007). One of the main\nNLP tasks is the recognition of relevant enti-\nties found in texts, what is commonly known\nas Named Entity Recognition (NER)(Nadeau\nand Sekine, 2007). This task enables the ex-\nploration of texts guided by key terms and\nthe discovery of relationships between them.\nThe NER task identifies meaningful terms in\na domain, called named entities, and classi-\nfies them into predefined entity classes (Li et\nal., 2020). In the biomedical domain, these\nentities are medical concepts such as drugs,\ndiseases, or gene mutations, and the task is\nmore specifically known as BioNER. Enti-\nties can also be classified according to exist-\ning taxonomies to avoid ambiguity, in a task\nthat is commonly known as Entity Linking\nor Named Entity Normalization (NEN) and,\nwhen applied in the biomedical domain, Bio-\nNEN (Campos, Matos, and Oliveira, 2012).\nThe main objective in BioNEN is to use\ncontrolled and curated biomedical vocab-\nularies such as Medical Subject Headings\n(MeSH)3codes or the Anatomical Therapeu-\ntic Chemical (ATC)4classification system, to\nreduce ambiguities and to extend the infor-\nmation about the entities. Once the entity\n3https://www.nlm.nih.gov/mesh\n4https://www.whocc.norecognition and normalization tasks are ap-\nplied in biomedical literature, a set of nor-\nmalized concepts can be used by Informa-\ntion Retrieval processes, such as the creation\nof efficient search algorithms, content clas-\nsification, or Knowledge-Graph construction\namong others (Chatterjee et al., 2021). These\nprocesses play a key role in subsequent NLP\ntasks such as Question-Answering, Relation\nExtraction, Knowledge-base population, or\nSemantic search (Nadeau and Sekine, 2007).\nHowever, the biomedical language entails\nsome challenges in identifying entities (Zhou\net al., 2004): (1) highly specialized terms\n(i.e. most of the terms are exclusive of\nthese kinds of texts, making it difficult to\nreuse general domain knowledge to iden-\ntify and classify specific domain concepts),\n(2)sharing of nouns (e.g. \u201d5kb and 17kb\nviruses\u201d refers to \u201d5kb viruses and 17kb\nviruses\u201d), and (3) non-standardized naming\nconvention (e.g. \u201dN \u2212acetyl \u2212\u03b2\u2212D\u2212\nglucosamine\u201d, \u201dN \u2212Acetylglucosamine\u201d,\nand \u201dC 18H15NO 6\u201d refers to the same con-\ncept).\nThis article describes how we performed\nBioNER and BioNEN tasks on the CORD-\n19 corpus, and our analysis of the presence\nof diseases, drugs, genes and proteins in their\ntexts. Our main contributions are:\n\u2022A BioNER+BioNEN system based on\nindependently fine-tuned BioBERT\nmodels to identify diseases, drugs and\ngenes/proteins from technical texts.5\n\u2022A collection of scientific texts tagged\nwith normalized terms and codes of\ndiseases, drugs, and genes/proteins6.\n(Badenes-Olmedo, Alonso, and Corcho,\n2022)\n\u2022A statistical analysis of the presence of\nbiomedical entities in the January 2022\nedition of the CORD-19 corpus.\nThe paper is structured as follows: Sec-\ntion 2 review the state-of-the-art methods to\nidentify biomedical entities and present our\napproach. The normalization process that we\nhave followed is described in section 3. Sec-\ntion 4 details how the CORD-19 corpus has\nbeen processed, and show and discuss the re-\nsults. Final remarks and future work are pre-\nsented in section 5.\n5https://github.com/drugs4covid/bio-ner\n6https://doi.org/10.5281/zenodo.6532473\n166\nCarlos Badenes-Olmedo, \u00c1lvaro Alonso, Oscar Corcho 2 Biomedical Named Entity\nRecognition\nNER tasks usually follow the pipeline showed\nin Fig. 1. The text is firstly pre-processed de-\npending on the requirements of subsequent\nprocesses (e.g. word cleaning, stemming,\nverb tense normalization, etc). Afterwards,\na representation of the words which com-\npose a text span is made, what serves as\nan input to a NER model which performs\nthe classification of these features to assign\ntags to the words. Sometimes, in order to\nrefine the results, a post-processing step is\nalso required to extend or group the entities.\nBiomedical-NER (BioNER) specializes the\nNER classification task for the medical do-\nmain and, sometimes, particularizes the tech-\nniques used to characterize texts. A BioNER\nmethod, depending on the type of technique\nused to classify terms, can be organized into:\nRule/Dictionary-based (i.e requires domain\nknowledge to define patterns of the different\nsorts of named entities to characterize them),\nMachine Learning-based (i.e. discovers rules\nthrough automatic patterns and reduces the\nneed for domain knowledge) and Hybrid ap-\nproaches (i.e. combines methods to leverage\nbenefits from different approaches) (Li et al.,\n2020) (Perera, Dehmer, and Emmert-Streib,\n2020)(Yadav and Bethard, 2019).\n2.1 Our Approach\nWe have created a hybrid system for the\nrecognition and normalization of biomedical\nentities based on state-of-the-art methods.\nOur model specializes the BioBERT (Lee et\nal., 2020) model pretrained with millions of\nscientific and biomedical articles, with addi-\ntional training corpora to extend the BioNER\ntask and to cover the BioNEN task from mul-\ntiple external standardization databases.\nThe entity classes considered for our sys-\ntem were the most widely used classes in\nBioNER modelling and the ones with a\nhigher number of corpora available for a fine-\ntuning process (see Table 1). It is important\nto note that these biomedical entities can be\nmentioned in different ways and this further\nmakes it more difficult to achieve a correct\nrecognition and normalization. Variations\ncan be trivial names (e.g. water ), technical\nnames (.e.g lung infection with Mycoplasma\npneumoniae to refer to Bacterial Pneumo-\nnia), brands (e.g. Veklury\u00ae), systematic IU-\nPAC names (e.g. 2,5,5-trimethyl-2-hexene ),\nFigure 1: NER pipeline.\ngeneric names (e.g. Benzenes ), molecular\nformulas (e.g. CH3), abbreviated forms (e.g.\nDMA fordimethylacetamide ) and identifiers\nof curated databases such as ChEBI7(e.g.\n145994 ).\nFor each entity class (i.e. disease, drugs\nand gene/proteins), a different BioNER\nmodel was created (see Fig. 2). One model\nwas fine-tuned to recognize disease entities,\nanother for chemical (i.e. drugs) entities and\nanother for genes/proteins. We adopted this\nstrategy because it has proven to behave bet-\nter for fine-tuned tasks than combining sev-\neral entity classes in the same task in only one\nmodel. The more specific the model is, the\nbetter results will be usually obtained for a\nspecific task (Gururangan et al., 2020). Sep-\narate models capture better patterns within\neach of the entity classes allowing to max-\nimize its tagging performance, resulting in\na system with the better model possible for\neach of the entities. Our system offers slightly\nlower performance than BioBERT model be-\ncause we jointly use several datasets for fine-\ntuning. The aim is to increase the ability\nto identify as many entities as possible, even\nat the cost of penalizing the accuracy of the\nmodel, since our pipeline incorporates an ad-\nditional normalization step where the enti-\nties will be filtered out. The post-processing\ntasks are based on an inverse index search.\nThe architecture of the system is described\nin Fig. 2 and further details about each of\nthe components are revised along the follow-\ning sections.\n2.2 Datasets\nWe have added an untrained fully-connected\nlayer on top of a BioBERT model to per-\nform the fine-tuning. At least three fine-\ntuning processes have been done to cover the\n7https://www.ebi.ac.uk/chebi\n167\nAn Overview of Drugs, Diseases, Genes and Proteins in the CORD-19 Corpus Figure 2: Overview of our BioNER+BioNEN architecture.\nthree different entities (i.e diseases, drugs and\ngenes/proteins). The corpora used for each\ntraining process were selected from existing\ndatasets (see Table 1) according to criteria\nbased on data volume and quality. Further-\nmore, since the techniques used to identify\nentities slightly differ between the existing\ndatasets despite being the same target en-\ntity, we use two corpora for each entity class\nto supply the model with a better generaliza-\ntion capacity in situations where a never-seen\ntext is used (Lee et al., 2020).\n2.2.1 Diseases Dataset\nThe BC5CDR-Diseases dataset (Li et al.,\n2016), with around 13,000 annotations, and\nthe NCBI-Diseases dataset (Do\u02d8 gan, Leaman,\nand Lu, 2014), with almost 7,000, were the\ncorpora used to recognize diseases. These\ndatasets are the most widely used in BioNER\ntasks for disease entities and most models\nprovide results for each of them, includ-\ning BioBERT which obtained an F1-score\nof 89.71 for NCBI-Diseases and 87.15 for\nB5CDR-Diseases. Our model, created by\ncombining both datasets during the fine-\ntuning process, offers a slightly lower per-\nformance with a F1-score of 87.4 and 85.8,\nrespectively. This is likely because of the\nhyperparameter search intensity and because\nthe number of epochs done is lower.\n2.2.2 Drugs Dataset\nFor chemical entities, the two selected\ndatasets were BC4CHEMD (Krallinger et al.,2015) and BC5CDR-Chemicals (Li et al.,\n2016) with around 80,000 and 15,000 entities\nrespectively. The largest annotated corpus,\nBioSemantics (Akhondi et al., 2014), was\nnot considered since it is based on patents\nwhich could slightly differ from biomedical\narticles that are the kind of texts in which\nour system is focused on. The selected\ndatasets were also the most widely adopted\ncorpora for NER tasks in chemical entities\nand most models provide performance results\nfor them. BioBERT obtained state-of-the-art\nresults in BC4CHEMD with an F1-score of\n92.36 and the second best result for BC5CDR\nwith 93.47, which is almost the same than\nthe state-of-the-art result obtained by Blue-\nBERT (Peng, Yan, and Lu, 2019) which was\n93.5 . Our system obtained F1 results of 91.7\nfor BC4CHEMD and 92.99 for BC5CDR-\nChemicals.\n2.2.3 Gene and Proteins Dataset\nGene and protein entities were jointly con-\nsidered since they belong to similar semantic\ntypes. This consideration is widely adopted\nin most existent corpus, which consider them\ntogether (Goyal, Gupta, and Kumar, 2018).\nThe pair of selected datasets were JNLPBA\n(Kim et al., 2004) and BC2GM (Smith\net al., 2008), which offer around 35,000\nand 25,000 annotations respectively. The\nCRAFT corpus (Bada et al., 2012), which is\nthe largest Gene/Protein NER corpus, was\ndiscarded since most models report results\n168\nCarlos Badenes-Olmedo, \u00c1lvaro Alonso, Oscar Corcho Year Reference Corpus Name Entities #Annotations #T\nokens\n2004 (Kim et\nal., 2004) JNLPBAGenes/Proteins 35460597333\nCellLines 4332\n2008 (Smith et\nal., 2008) BC2GM Genes/Proteins 24583 508257\n2012 (Bada et\nal., 2012) CRAFTChemicals 8137\n560000Genes/Proteins 49961\nSpecies 7449\nCellLines 5760\n2013 (Pafilis\net al., 2013) Species-800 Species 3646 195197\n2013 (Ohta\net al., 2013) BioNLP13CGSpecies\n21683 129878 Anatom y\nGenes/Proteins\n2013 (Segura Bedmar,\nMart\u00b4 \u0131nez, and Herrero Zazo, 2013) SemEv al2013\n- DrugBank Chemicals 15745 \u224865000\n2013 (Segura Bedmar,\nMart\u00b4 \u0131nez, and Herrero Zazo, 2013) SemEv al2013\n- Medline Chemicals 2746 \u224820000\n2013 (Pyysalo et\nal., 2015) BioNLP13PCGenes/Proteins15901 108356\nChemicals\n2014 (Bagew adi\net al., 2014) mi-RNAGenes/Proteins 1006\n65998 Species 726\nDiseases 2123\n2014 (Akhondi et\nal., 2014) BioSeman tics Chemicals 386110 5690518\n2014 (Pyysalo and\nAnaniadou, 2014) AnatEM Anatom y 13000 250000\n2014 (Do\u02d8gan,\nLeaman, and Lu, 2014) NCBI Diseas\ne Diseases 6881 174487\n2015 (Goldb erg\net al., 2015) LocT\next Species 276 22550\n2015 (Krallinger et\nal., 2015) BC4CHEMD Chemicals 79842 2235435\n2016 (Liet\nal., 2016) BC5CDRDiseases 12694323281\nChemicals 15411\n2016 (Kaewphan et\nal., 2016)CLLCellLines341 6547\nGellus 640 278910\n2020 (Legrand et\nal., 2020) PGxCorpusDiseases 635\n\u224835000 Chemicals 1718\nGenes/Proteins 1708\nTable 1: Corpora with biomedical entities.\nbased on those corpus and a comparison be-\ntween them can be established. BioBERT\nreported state-of-the-art results on BC2GM\nresults with a F1 of 84.72 and in JNLPBA\nresults (77.59) were slightly worse than state-\nof-the-art which were reported by PubMed-\nBERT with a F1 of 80.06. Results from our\nfine-tuning model were a bit worse with 83.0\nand 76.0 for BC2GM and JNLPBA respec-\ntively. Results on this joint entity class are\nsignificantly worse than other entity classes,\nperhaps due to the broad range of suben-\ntity classes which take part within this class.\nThis makes the amount of linguistic variabil-\nity larger, and hence harder to capture than\nthe former entity classes.\nOnce the models have been fine-tuned, we\nrequire some additional steps before having\na homogeneous representation of the enti-\nties (see Fig. 2). The following section de-\ntails the entity normalization process and the\nadditional tasks required in our NER+NEN\npipeline ( Fig. 1).3 Entity Normalization\nThe normalization process has been ad-\ndressed through an inverted index search.\nEach entity is associated with a set of related\nterms extracted from external coding sys-\ntems. Once the medical term is recognized,\nwe search for entities that contain that term\nin any of their related fields, and we sort that\nset of candidates based on the BM25 rank-\ning function (Robertson et al., 1994). Those\nwith fewer related terms will have greater\nrelevance. Each type of entity has its own\ndatabase (i.e index). This way, indexes can\nbe built separately with curated and related\nterms that helps to map concepts with terms\nand codes (see Table 2). Multiple sources\nwere taken into account in each of the en-\ntity classes, mainly from BioPortal ontolo-\ngies8, but also from the Comparative Toxi-\ncogenomics Database9and PubChem10.\n8http://bioportal.bioontology.org\n9http://ctdbase.org\n10https://pubchem.ncbi.nlm.nih.gov\n169\nAn Overview of Drugs, Diseases, Genes and Proteins in the CORD-19 Corpus Type Entities Codes Sources\nDiseases 126573 5 4\nDrugs 344238 7 5\nGenes 946584 3 4\nTable 2: Resources used for normalization.\nFor each entity, regardless of whether it\nis a drug, disease or gene/protein, the fol-\nlowing information was collected: (1) a term\nordescription of the underlying concept (e.g.\n\u201dHydroxychloroquine \u201d) ; (2) a list of syn-\nonyms that holds all possible related words\npresent for a given term (e.g. \u2019Oxichloro-\nquine \u2019, \u2019Polirreumin \u2019); (3) a semantic type\n(e.g. \u2019 Pharmacologic Substance \u2019 ) and (4) a\nlist of identifiers based on MeSH, CUI, ATC,\nor any other more specific database cross\nreferences (e.g. mesh id:D006886, cid:3652,\natc:P01BA02 ). The range of possibilities to\nrefer to the same element (i.e by code, term\nor synonym) allow choosing the one with the\nhigher score between different search crite-\nria (e.g. terms or synonyms; strict or similar\nmatches) and filtering criteria (e.g. based on\nword order, or single terms). The result with\nthe higher score is considered.\n3.1 Diseases\nFour different sources were merged in the\nsame index to normalize disease terms based\non the mappings between their codes and the\nmedical terms used to represent them.\nMeSH - Diseases : Medical Subject\nHeadings11is a thesaurus with hierarchi-\ncal and controlled vocabulary produced by\nthe National Library of Medicine (NLM).\nThis thesaurus includes thousands of terms\nregarding to several semantic types with\ndisease-related terms among them. BioPortal\nincludes an ontology version of this thesaurus\nfrom which we have extracted disease-related\nterms attending to the UMLS Semantic Type\neach term belongs to.\nCTD - Diseases: CTD\u2019s MEDIC dis-\nease vocabulary is a modified subset of the\n\u201cDiseases\u201d branch of the NLM\u2019s MeSH, com-\nbined with genetic disorders from the Online\nMendelian Inheritance in Man12(OMIM)\ndatabase. These terms have been merged\nwith the previous ones through an outer join\non MeSH IDs.\nDOID: The Human Disease Ontology\n11https://www.nlm.nih.gov/mesh\n12https://www.omim.org(Schriml et al., 2012) is a comprehensive\nknowledge base of inherited, developmental\nand acquired human diseases. It integrates\nterms from a wide range of medical vocab-\nularies such as MeSH, SNOMED, NCI, or\nOMIM, and has been used to extend terms\nwhich were not previously captured by the\nother sources. The way this was done is\nthrough an outer join on MeSH IDs.\nICD-10-CM: The International Classifi-\ncation of Diseases is a hierarchical classifica-\ntion listed by the World Health Organization\n(WHO), in which are encoded a wide range of\nsigns, symptoms, abnormal findings, causes\nof damage, diseases, and/or other disease-\nrelated terms. The ICD-10-CM is the 10th\nversion of this classification with a Clinical\nModification of the source. Since this clas-\nsification is used in its proper BioPortal on-\ntology, further mapping concepts are added,\nwhich is the case of Unified Medical Lan-\nguage System identifiers (CUIs). The way\nthis source extends the previous sources is\nthrough this CUI since not MeSH IDs are in-\ncluded. For that purpose, an outer join on\nthis id was done.\n3.2 Drugs\nFive sources were considered to merge chemi-\ncal terms in a shared index. The main objec-\ntive was to capture the wide range of possible\nchemical mentions that this entity class can\nsupport\nPubChem: PubChem is the world\nlargest chemistry open database maintained\nby the National Institute of Health (NIH).\nAmong the classification systems offered to\norganize the chemical entities, we used the\nMeSH hierarchy for our database. Approxi-\nmately 130000 terms were considered which\nis expected to have the most widely adopted\nchemical terms within all the collection.\nChEBI: ChEBI is a chemical database\nmainly focused on small chemical compo-\nnents of molecular entities and therefore it\ncomplements other types of terms considered\nin the rest of sources. Any biological or\nsynthetical component present in biological\norganisms is aimed to be captured on this\ndatabase. An outer join on InChIKey was\nused for connecting these terms with the ones\npresent in the previous source. InChIKey is a\nhashed key of InChI, an International Iden-\ntifier for chemicals, which offers an IUPAC\nidentifier for an standardized codification of\n170\nCarlos Badenes-Olmedo, \u00c1lvaro Alonso, Oscar Corcho chemicals.\nMeSH - Chemicals: MeSH also includes\nthousands of terms regarding to chemical-\nrelated terms. The ontology version in Bio-\nPortal has been used to extract chemical-\nrelated terms attending to the UMLS Se-\nmantic Type each term belongs to. Since\nPubChem already includes MeSH terms, this\nsource has been just used to add MeSH IDs\nand extend information from the previous\nterms. This source was combined with the\nprevious ones through checking if the term is\nfound either on term field or on the synonyms\nlist. If it is not found, it has been appended\nto chemical terms.\nCTD - Chemicals: Database that incor-\nporates terms from multiple chemical sources\nand therefore it has been used for comple-\nmenting previously existent processed terms.\nIt also helps to extend the retrieved informa-\ntion about previously considered terms. Non\npreviously found terms have been appended\nfrom this source.\nATC: Classification of pharmacological\nsubstances organized in therapeutic levels.\nThe ontology version of BioPortal has been\nthe source considered for ATC since it in-\ncorporates further information and relations\nwith other terms. Information regarding\nATC level and ATC code was added to the\npreviously considered terms. If the term is\nnot present, it has been appended.\n3.3 Genetics\nThis entity class is composed of a broad se-\nmantic type since it includes both gene and\nproteins-related terms. They are close se-\nmantic types and even in some occasions the\nuse of the same expressions is diffuse. This\nhas led to a wide range of terms within this\nentity class in which four large and comple-\nmentary sources were merged in the same\nindex to cover the biggest amount of entity\nvariability possible.\nGO: The knowledgebase underlying the\nGene Ontology (Ashburner et al., 2000) is the\nlargest source for the functions of genes and\ntherefore it has been used aiming to capture\nterms related to genetic mechanisms.\nOGG: The Ontology of Genes and\nGenomes (He, Liu, and Zhao, 2014) collects\ngenes and genomes of certain organisms such\nas humans, virus and bacteria. Mappings to\nmultiple sources are found in the BioPortal\nontology.EntitiesCov\nerage Normalization\n(%) (%)\nDiseases 18,355 49.4 4.2\nDrugs 55,120 15.1 22.3\nGenes/Proteins 79,063 16.6 9.1\nTable 3: CORD-19 statistics (January-2022). To-\ntal number of appearances ( Entities ), diversity\n(Coverage ) and standardization (Normalization )\nratio.\nPR: The Protein Ontology (Natale et al.,\n2017) contains a wide range of protein-related\nentities along with relations between them.\nThis source contains a large amount of terms\nthat covers the protein part.\nCTD - Genes: It contains a vocabu-\nlary retrieved from multiple sources with a\ngreat variety of genes in multiple species.\nIt has been used to extend the gene terms\nwhich were not previously captured, append-\ning non-retrieved genes.\n4 CORD-19 Entities\nThe BioNER+BioNEN system described in\nthis paper was used to identify and normalize\nthe drugs, diseases and genetic-related terms\nmentioned in the CORD-19 corpus (January\n2022 Edition). The recognition process was\ntime consuming (approximately 48 days) in\na server composed by a 32 CPU-cores In-\ntel Xeon with 256GB RAM. The lack of\nGPUs made the process considerably slower\n(i.e. 1173hours at a rate of 0,4s/task )\nsince it requires matrix computation for the\ntransformer-based language models, one for\neach biomedical concept. The source code is\npublicly available13.\nEntity recognition and normalization was\ndone for each paragraph of the scientific\narticle. A first group of labels is created to\nidentify the medical terms as they appear\nin the text (i.e. diseases ss,chemicals ss,\ngenetics ss), and in a standardized way\n(i.e. disease terms ss,chemical terms ss,\ngenetic terms ss). In the case of diseases and\ngenes/proteins, a predefined category is also\nestablished during the normalization process\n(i.e. disease types ss, genetic types ss).\nThe following group of labels contains the\ncodes for each of the classification systems\ndescribed in Section 3 (i.e. mesh codes ss,\natccodes ss, cidcodes ss, doid codes ss,\ncuicodes ss,icd10 codes ss,icd9 codes ss,\n13https://github.com/drugs4covid/cord-19\n171\nAn Overview of Drugs, Diseases, Genes and Proteins in the CORD-19 Corpus gard codes ss,snomed codes ss,ncicodes ss,\nncbi codes ss,uniprot codes ss). The suffix\nssin all tags indicates that the format is a\ntextual list (i.e. string sequence).\nTable 3 shows some statistics about entity\nclasses once the corpus was processed. As ex-\npected, almost half of the paragraphs contain\nat least one mention of a disease or symptom\n(see column Coverage ), while drugs, genes\nor proteins appear less frequently. This is\nstrongly influenced by the criteria used by\nAllen AI to create the CORD-19 corpus, as\nthey filter articles that contain coronavirus-\nrelated terms in their title or abstract. This\nguide the content of the article and also ex-\nplains why the variety of disease and symp-\ntom entities (see column Entities ) is far infe-\nrior to drugs and genetic information. How-\never, what is striking is the high rate of stan-\ndard terms (according to our model) used to\nrefer to drugs, with respect to the rest of\nbiomedical entities. Column Normalization\nshows the ratio of entities mentioned in the\ntext using any of the terms extracted from\nthe classification systems described in section\n3. We think that there is more flexibility in\nscientific texts to refer to symptoms or dis-\neases than to drugs or active ingredients, with\nrespect to the standards (e.g. ATC, MeSH,\nICD-10 or SNOMED mainly). Regarding ge-\nnetic information, perhaps the cause lies in\nthe precision in the recognition of the bound-\nary that defines the entity, being sometimes\neliminated part of the chemical expression of\nthe entity itself.\nTable 4 shows the most widely captured\nentities according to the following classi-\nfication systems: ICD-10, MeSH, ChEBI,\nATC, MedGen (CUID), GARD, NCBI and\nSNOMED. Jointly with the code and descrip-\ntion of the entity, the occurrences of these\nwords are given (column Ratio ). This allows\nus to have an idea about the relevance of the\nconcept in the corpus with respect to the rest\nof the concepts of the same classification sys-\ntem. In top positions we can find general con-\ncepts related to respiratory difficulties. As we\ngo down in the top, more specific terms begin\nto appear. In the systems that cover diseases\nsuch as MeSH or ICD-10, we can find as the\nmost relevant concept the COVID-19 disease,\nas expected, and the related symptoms (e.g.\nU07.1 in ICD-10, D000086382 in MeSH or\nC5203670 in MedGen). The systems more\noriented to chemicals identify substances re-lated to respiratory disorders (e.g. Dioxyegn\nin ChEBI or Oxygen in ATC). And the sys-\ntems focused on genetic and protein informa-\ntion show, with similar relevance, the path-\nways of the coronavirus (e.g. Angiotensin\nconverting enzyme 2, Interleukin-6 or Inter-\nferon in NCBI).\nThanks to the normalization process that\nwe incorporate in our entity recognition sys-\ntem, we can use the hierarchies defined in\nthe underlying classification system to estab-\nlish more or less general labels. For exam-\nple, the Anatomical Therapeutic Chemical\n(ATC) classification system, which is sup-\nported by the World Health Organization\n(WHO) and widely used in hospital pharma-\ncies to identify drug components, organizes\nactive substances according to the organ or\nsystem on which they act and their thera-\npeutic, pharmacological, and chemical prop-\nerties. Drugs are classified into groups at five\ndifferent levels. The first one corresponds to\nmain groups, the second one to pharmacolog-\nical or therapeutic subgroups, the third and\nthe fourth one are chemical-pharmacological-\ntherapeutic subgroups and the last one is the\nchemical substance. Once the code of a drug\nhas been identified in this classification sys-\ntem, we can extend the labels of the text with\nthose groups of the hierarchy, enabling addi-\ntional ways of exploiting the results of the\nannotation process.\nIn the following experiment we want to\ntake advantage of the labels generated by our\nsystem to find evidence about the anatom-\nical behavior of drugs used to treat coron-\navirus. We do not know a priori which groups\nof drugs are related in this domain, and we\nassume that an evidence implies the joint\npresence of several groups in the same para-\ngraph. Since the ATC classification system\nis hierarchical and establishes 14 anatomic\ngroups at the first level of drug organiza-\ntion, we can create a matrix with the para-\ngraphs where drugs are mentioned and the\nanatomic groups to which they belong. Fig-\nure 3 shows the correlation between each\nof these anatomical groups based on anal-\nysis of mentions of drugs in texts. It can\nbe seen how the highest correlation exists\nbetween drugs associated with Sensory or-\ngans and Anti-infectives for systemic use.\nThis may be due to the fact that many of\nthe anti-infective active substances used sys-\ntemically (i.e. orally or intravenously) are\n172\nCarlos Badenes-Olmedo, \u00c1lvaro Alonso, Oscar Corcho Entit\ny\nRatio Code Description\nICD-1051.1 U07.1 COVID-19\n8.0 J12.81 Pneumonia due\nto SARS-associated coronavirus\n3.7 J11.1 Influenza due\nto unidentified influenza virus with other respiratory manifestations\n3.3 A79.0 Trenc\nh fever\n3.2 F53.0 Postpartum\ndepression\nMeSH34.0 D000086382 COVID-19\n11.1 D000085343 Laten t\nInfection\n4.5 D018352 Corona virus\nInfections\n3.9 D003643 Death\n3.7 D045169 Severe\nAcute Respiratory Syndrome\nChEBI8.8 15379 Dioxygen\n5.2 33708 Amino-acid Residue\n3.8 172234 TG(14:1(9Z)/22:5(7Z,10Z,13Z,16Z,19Z)/24:1(15Z))\n3.2 30879 Alcohol\n2.9 5801 Hydro xyc\nhloroquine\nATC14.6 V03AN01 Oxygen\n6.4 V04CA02 Glucose\n4.9 P01BA02 Hydro xyc\nhloroquine\n3.0 A12AA Calcium\n2.8 V03AN04 Nitrogen\nMedGen (CUI)40.6 C5203670 COVID-19\n13.2 C0872054 Laten t\nInfection\n6.4 C1175175 Severe\nacute respiratory syndrome\n4.3 C3714514 Infection\n3.0 C0003467 Anxiet y\nGARD26.9 9237 SARS\n11.7 5698 Acute respiratory\ndistress syndrome\n5.9 6427 Farmer\u2019s\nlung\n4.1 2035 Lymphatic filariasis\n2.9 6254 Dengue fev\ner\nNCBI6.6 59272 Angiotensin con\nverting enzyme 2\n5.5 100628202 Interleukin-6\n4.5 100304604 Interferon\n4.3 101180090 Immunoglobulin\nG level\n4.0 7124 tumor necrosis\nfactor\nSNOMED57.4 840539006 Disease c\naused by 2019 novel coronavirus (disorder)\n6.3 398447004 Severe\nacute respiratory syndrome (disorder)\n4.2 155559006 Influenza (disorder)\n4.1 266391003 Pneumonia and\ninfluenza or pneumonia (disorder)\n3.8 82214002 Trenc\nh fever (disorder)\nTable 4: Presence (Ratio ) of the most frequent entities (Code ) organized by coding system.\nalso categorized within the sensory organs\ngroup, for example the Ciprofloxacin, since\nthey can also be administered by the otic or\nophthalmic route. Thanks to the tags cre-\nated by our system, it is sufficient to fil-\nter the paragraphs labeled with the ATC\ncodes \u2019S\u2019 (i.e Sensory organs ) and \u2019J\u2019 (i.e.\nAnti-infectives ) to find the candidates for ev-\nidence. The other most notable correlation is\nbetween Anti-infectives for systemic use and\nAnti-parasitic products. It could be explained\nbecause the anti-infective drugs used for par-asites are classified as anti-parasitic products,\nand the active substances most used exper-\nimentally for the treatment of coronavirus\nwere found within these categories, such as\nLopinavir/Ritonavir (anti-infective) and Hy-\ndroxychloroquine (anti-parasitic). Again, we\ncan take advantage of the tags in our system\nto find texts in the articles that help us vali-\ndate this assumption.\n173\nAn Overview of Drugs, Diseases, Genes and Proteins in the CORD-19 Corpus Figure 3: Correlation matrix of ATC data at Anatomical group level.\n5 Conclusions\nWe have created a corpus with the diseases,\ndrugs, genes, and proteins mentioned in the\nparagraphs of the articles in the January edi-\ntion of the CORD-19 corpus. It contains not\nonly the biomedical entities, but also their\nnormalized references based on several cu-\nrated databases such as MeSH, ICD-10, ATC,\nChEBI or SNOMED. The generated corpus is\npublicly available and is updated periodically\nto take up changes in the CORD-19 dataset.\nAn analysis has been carried out on this\ncorpus to measure the presence and degree of\nnormalization of each type of biomedical en-\ntity. As expected, practically half of the para-\ngraphs contain some reference to a disease or\nsymptom. However, only 4% of them were\nmentioned using any of the standard codes\nor alias. The behavior in genes and proteins\nis similar although much lower in terms of\npresence. Drugs are the least present and\nmost varied type of entity in the corpus. The\ncorrelation between the anatomical groups of\nthe drugs has also been measured to value the\nusefulness of the tags created. The procedureto easily extract the evidence, i.e. paragraphs\nwhere the groups are mentioned, is also de-\nscribed.\nOur biomedical named entity recognizer\ncreated to produce the tags is also described.\nIt is based on the pre-trained BioBERT lan-\nguage model and combines three different\nmodels each of them specialized in the recog-\nnition of a different biomedical entity: dis-\nease, drug and gene/protein. In the future\nwe want to explore the ability of the tags\nto produce knowledge, either to organize en-\ntities or to discover relationships that may\narise between them, and to take advantage\nof the knowledge acquired to create a Span-\nish BioNER+BioNEN model.\nAcknowledgments\nWork supported by the DRUGS4COVID++\nproject , financed by Ayudas Fundaci\u00b4 on\nBBVA a equipos de investigaci\u00b4 on cient\u00b4 \u0131fica\nSARS-CoV-2 y COVID-19 .\n174\nCarlos Badenes-Olmedo, \u00c1lvaro Alonso, Oscar Corcho References\nAkhondi, S. A., A. G. Klenner, C. Tyr-\nchan, A. K. Manchala, K. Boppana,\nD. Lowe, M. Zimmermann, S. A. Ja-\ngarlapudi, R. Sayle, J. A. Kors, et al.\n2014. Annotated chemical patent corpus:\na gold standard for text mining. PloS one ,\n9(9):e107477.\nAshburner, M., C. A. Ball, J. A. Blake,\nD. Botstein, H. Butler, J. M. Cherry, A. P.\nDavis, K. Dolinski, S. S. Dwight, J. T. Ep-\npig, et al. 2000. Gene ontology: tool for\nthe unification of biology. Nature genetics,\n25(1):25\u201329.\nBada, M., M. Eckert, D. Evans, K. Gar-\ncia, K. Shipley, D. Sitnikov, W. A. Baum-\ngartner, K. B. Cohen, K. Verspoor, J. A.\nBlake, et al. 2012. Concept annotation\nin the craft corpus. BMC bioinformatics,\n13(1):1\u201320.\nBadenes-Olmedo, C., A. Alonso, and O. Cor-\ncho. 2022. Drugs, Diseases, Genes and\nProteins in the CORD-19 Corpus, March.\nBagewadi, S., T. Bobi\u00b4 c, M. Hofmann-\nApitius, J. Fluck, and R. Klinger. 2014.\nDetecting mirna mentions and relations in\nbiomedical literature. F1000Research , 3.\nCampos, D., S. Matos, and J. L. Oliveira.\n2012. Biomedical named entity recogni-\ntion: a survey of machine-learning tools.\nTheory and Applications for Advanced\nText Mining , 11:175\u2013195.\nChatterjee, A., C. Nardi, C. Oberije, and\nP. Lambin. 2021. Knowledge graphs for\ncovid-19: An exploratory review of the\ncurrent landscape. Journal of Personal-\nized Medicine, 11(4).\nDo\u02d8 gan, R. I., R. Leaman, and Z. Lu. 2014.\nNcbi disease corpus: a resource for disease\nname recognition and concept normaliza-\ntion. Journal of biomedical informatics ,\n47:1\u201310.\nGoldberg, T., S. Vinchurkar, J. M. Cejuela,\nL. J. Jensen, and B. Rost. 2015. Linked\nannotations: a middle ground for manual\ncuration of biomedical databases and text\ncorpora. In BMC proceedings, volume 9,\npages 1\u20133. BioMed Central.\nGoyal, A., V. Gupta, and M. Kumar. 2018.\nRecent named entity recognition and clas-\nsification techniques: a systematic review.\nComputer Science Review , 29:21\u201343.Gururangan, S., A. Marasovi\u00b4 c,\nS. Swayamdipta, K. Lo, I. Beltagy,\nD. Downey, and N. A. Smith. 2020.\nDon\u2019t stop pretraining: Adapt language\nmodels to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nHe, Y., Y. Liu, and B. Zhao. 2014. Ogg: a\nbiological ontology for representing genes\nand genomes in specific organisms. In\nICBO, pages 13\u201320. Citeseer.\nKaewphan, S., S. Van Landeghem, T. Ohta,\nY. Van de Peer, F. Ginter, and S. Pyysalo.\n2016. Cell line name recognition in sup-\nport of the identification of synthetic\nlethality in cancer from text. Bioinfor-\nmatics, 32(2):276\u2013282.\nKim, J.-D., T. Ohta, Y. Tsuruoka, Y. Tateisi,\nand N. Collier. 2004. Introduction to\nthe bio-entity recognition task at jnlpba.\nInProceedings of the international joint\nworkshop on natural language processing\nin biomedicine and its applications, pages\n70\u201375. Citeseer.\nKrallinger, M., O. Rabal, F. Leitner,\nM. Vazquez, D. Salgado, Z. Lu, R. Lea-\nman, Y. Lu, D. Ji, D. M. Lowe, et al.\n2015. The chemdner corpus of chemicals\nand drugs and its annotation principles.\nJournal of cheminformatics, 7(1):1\u201317.\nLee, J., W. Yoon, S. Kim, D. Kim, S. Kim,\nC. H. So, and J. Kang. 2020. Biobert: a\npre-trained biomedical language represen-\ntation model for biomedical text mining.\nBioinformatics, 36(4):1234\u20131240.\nLegrand, J., R. Gogdemir, C. Bousquet,\nK. Dalleau, M.-D. Devignes, W. Digan,\nC.-J. Lee, N.-C. Ndiaye, N. Petitpain,\nP. Ringot, et al. 2020. Pgxcorpus, a man-\nually annotated corpus for pharmacoge-\nnomics. Scientific data , 7(1):1\u201313.\nLi, J., Y. Sun, R. J. Johnson, D. Sciaky, C.-H.\nWei, R. Leaman, A. P. Davis, C. J. Mat-\ntingly, T. C. Wiegers, and Z. Lu. 2016.\nBiocreative v cdr task corpus: a resource\nfor chemical disease relation extraction.\nDatabase, 2016.\nLi, J., A. Sun, J. Han, and C. Li. 2020.\nA survey on deep learning for named en-\ntity recognition. IEEE Transactions on\nKnowledge and Data Engineering.\n175\nAn Overview of Drugs, Diseases, Genes and Proteins in the CORD-19 Corpus Nadeau, D. and S. Sekine. 2007. A sur-\nvey of named entity recognition and clas-\nsification. Lingvisticae Investigationes ,\n30(1):3\u201326.\nNatale, D. A., C. N. Arighi, J. A. Blake,\nJ. Bona, C. Chen, S.-C. Chen, K. R.\nChristie, J. Cowart, P. D\u2019Eustachio, A. D.\nDiehl, et al. 2017. Protein ontology (pro):\nenhancing and scaling up the representa-\ntion of protein entities. Nucleic acids re-\nsearch, 45(D1):D339\u2013D346.\nOhta, T., S. Pyysalo, R. Rak, A. Rowley, H.-\nW. Chun, S.-J. Jung, S.-P. Choi, S. Ana-\nniadou, and J. Tsujii. 2013. Overview\nof the pathway curation (pc) task of\nbionlp shared task 2013. In Proceedings of\nthe BioNLP Shared Task 2013 Workshop,\npages 67\u201375.\nPafilis, E., S. P. Frankild, L. Fanini,\nS. Faulwetter, C. Pavloudi,\nA. Vasileiadou, C. Arvanitidis, and\nL. J. Jensen. 2013. The species and\norganisms resources for fast and accurate\nidentification of taxonomic names in text.\nPloS one, 8(6):e65390.\nPeng, Y., S. Yan, and Z. Lu. 2019. Trans-\nfer learning in biomedical natural lan-\nguage processing: an evaluation of bert\nand elmo on ten benchmarking datasets.\narXiv preprint arXiv:1906.05474.\nPerera, N., M. Dehmer, and F. Emmert-\nStreib. 2020. Named entity recognition\nand relation detection for biomedical in-\nformation extraction. Frontiers in Cell\nand Developmental Biology , 8:673.\nPyysalo, S. and S. Ananiadou. 2014.\nAnatomical entity mention recognition\nat literature scale. Bioinformatics,\n30(6):868\u2013875.\nPyysalo, S., F. Ginter, J. Heimonen,\nJ. Bjorne, J. Boberg, J. Jarvinen, and\nT. Salakoski. 2007. Bioinfer: a corpus for\ninformation extraction in the biomedical\ndomain. BMC bioinformatics, 8(1):1\u201324.\nPyysalo, S., T. Ohta, R. Rak, A. Rowley, H.-\nW. Chun, S.-J. Jung, S.-P. Choi, J. Tsu-\njii, and S. Ananiadou. 2015. Overview of\nthe cancer genetics and pathway curation\ntasks of bionlp shared task 2013. BMC\nbioinformatics, 16(10):1\u201319.Robertson, S. E., S. Walker, S. Jones,\nM. Hancock-Beaulieu, and M. Gatford.\n1994. Okapi at trec-3. In TREC.\nSchriml, L. M., C. Arze, S. Nadendla, Y.-\nW. W. Chang, M. Mazaitis, V. Felix,\nG. Feng, and W. A. Kibbe. 2012. Disease\nontology: a backbone for disease seman-\ntic integration. Nucleic acids research ,\n40(D1):D940\u2013D946.\nSegura Bedmar, I., P. Mart\u00b4 \u0131nez, and M. Her-\nrero Zazo. 2013. Semeval-2013 task 9:\nExtraction of drug-drug interactions from\nbiomedical texts (ddiextraction 2013).\nAssociation for Computational Linguis-\ntics.\nSmith, L., L. K. Tanabe, R. J. nee Ando, C.-\nJ. Kuo, I.-F. Chung, C.-N. Hsu, Y.-S. Lin,\nR. Klinger, C. M. Friedrich, K. Ganchev,\net al. 2008. Overview of biocreative ii\ngene mention recognition. Genome biol-\nogy, 9(2):1\u201319.\nWang, L. L., K. Lo, Y. Chandrasekhar,\nR. Reas, J. Yang, D. Eide, K. Funk,\nR. Kinney, Z. Liu, W. Merrill, et al.\n2020. CORD-19: The Covid-19 Open\nResearch Dataset. arXiv preprint\narXiv:2004.10706.\nYadav, V. and S. Bethard. 2019. A survey\non recent advances in named entity recog-\nnition from deep learning models. arXiv\npreprint arXiv:1910.11470.\nZhou, G., J. Zhang, J. Su, D. Shen, and\nC. Tan. 2004. Recognizing names in\nbiomedical texts: a machine learning ap-\nproach. Bioinformatics, 20(7):1178\u20131190,\nMay.\n176\nCarlos Badenes-Olmedo, \u00c1lvaro Alonso, Oscar Corcho Transformers for Lexical Complexity Prediction in\nSpanish Language\nTransformers para la Predicci\u00b4 on de la Complejidad L\u00b4 exica en\nLengua Espa\u02dc nola\nJenny Ortiz-Zambrano1, C\u00b4 esar Espin-Riofrio1, Arturo Montejo-R\u00b4 aez2\n1Universidad de Guayaquil\n2Universidad de Ja\u00b4 en\n{jenny.ortizz,cesar.espinr}@ug.edu.ec\namontejo@red.ujaen.es\nAbstract: In this article we have presented a contribution to the prediction of the\ncomplexity of simple words in the Spanish language whose foundation is based on\nthe combination of a large number of features of different types. We obtained the\nresults after run the fined models based on Transformers and executed on the pre-\ntrained models BERT, XLM-RoBERTa, and RoBERTa-large-BNE in the different\ndatasets in Spanish and executed on several regression algorithms. The evaluation of\nthe results determined that a good performance was achieved with a Mean Absolute\nError (MAE) = 0.1598 and Pearson = 0.9883 achieved with the training and evalu-\nation of the Random Forest Regressor algorithm for the refined BERT model. As a\npossible alternative proposal to achieve a better prediction of lexical complexity, we\nare very interested in continuing to carry out experimentations with data sets for\nSpanish, testing state-of-the-art Transformer models.\nKeywords: Lexical Complexity, Prediction, Encodings, Transformers.\nResumen: En este art\u00b4 \u0131culo hemos presentado una contribuci\u00b4 on a la predicci\u00b4 on de\nla complejidad de palabras simples en lengua espa\u02dc nola cuyo fundamento se basa en\nla combinaci\u00b4 on de un gran n\u00b4 umero de caracter\u00b4 \u0131sticas de distinta naturaleza. Obtuvi-\nmos los resultados despu\u00b4 es de ejecutar los modelos afinados basados en Transformers\ny ejecutados sobre los modelos pre-entrenados BERT, XLM-RoBERTa y RoBERTa-\nlarge-BNE en los diferentes conjuntos de datos en espa\u02dc nol y corridos con varios\nalgoritmos de regresi\u00b4 on. La evaluaci\u00b4 on de los resultados determin\u00b4 o que se logr\u00b4 o un\nbuen desempe\u02dc no con un Error Absoluto Medio (MAE) = 0.1598 y Pearson = 0.9883\nlogrado con el entrenamiento y evaluaci\u00b4 on del algoritmo Random Forest Regressor\npara el modelo BERT afinado. Como posible propuesta alternativa para lograr una\nmejor predicci\u00b4 on de la complejidad l\u00b4 exica, estamos muy interesados en seguir real-\nizando experimentaciones con conjuntos de datos para espa\u02dc nol probando modelos\nde Transformer de \u00b4 ultima generaci\u00b4 on.\nPalabras clave: Complejidad L\u00b4 exica, Predicci\u00b4 on, Incrustaciones de Palabra, Trans-\nformadores.\n1 Introduction\nA common assumption is that people who are\nfamiliar with the vocabulary of a text can\noften understand the meaning of the words,\neven if they have difficulty with grammati-\ncal structures (Uluslu, 2022). The task of\ndetecting in the content of the documents\nthe words that are difficult or complex to\nunderstand by the people of a given groupis known as Complex Word Identification -\nCWI (Rico-Sulayes, 2020) and it is a task\nthat constitutes a fundamental step in many\napplications related to natural language, such\nas Text Simplification. Automatic lexical\nsimplification can then become an effective\nmethod of making the text accessible to dif-\nferent audiences (Uluslu, 2022).\nDeep learning and its revolutionary tech-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 177-188\nrecibido 31-03-2022 revisado 03-06-2022 aceptado 06-06-2022\nISSN 1135-5948. DOI 10.26342/2022-69-15\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalnology constitute the new state of the art in\nvarious Natural Language Processing (NLP)\ntasks (Singh and Mahmood, 2021), in which\nlexical complexity prediction (LCP) is no ex-\nception (Nandy et al., 2021). It is impor-\ntant to point out that, after the comparison\nand analysis of other approaches versus deep\nlearning approaches, a viable path of pos-\nsible solutions is generated for low-resource\nlanguages where deep models are not always\navailable or work as well as those of deep\nlearning in English language. Likewise, it\nshould be taken into account that the com-\nputational requirements for the application\nof deep learning models turn out to be sig-\nnificantly higher compared to those used in\ntraditional approaches (Bender et al., 2021).\nThe field of NLP has shown incredible\nprogress in the last two years, this is par-\nticularly due to the Transformer architecture\n(Vaswani et al., 2017) that takes advantage\nof large amounts of unlabeled text corpus\n(Canete et al., 2020). Deep learning mod-\nels show significant improvement over shal-\nlow machine learning models with the rise\nof transfer learning and pretrained language\nmodels. The deep learning pretrained lan-\nguage models, BERT and XLM-RoBERTa,\nare considered state-of-the-art in many NLP\ntasks (Yaseen et al., 2021).\nWe present our approach aimed at pre-\ndicting the complexity score for single words\nin the Spanish language, since resources are\nscarce and are not as numerous as those avail-\nable for the English language.\nOur model leverages the combination of\nadvanced NLP techniques of deep learning\nmodels based on Transformers: BERT (Liu\net al., 2019), XLM-RoBERTa (Conneau et\nal., 2019), RoBERTa-large-BNE (Guti\u00b4 errez-\nFandi\u02dc no et al., 2021) and pre-trained word\nembeddings together with a set of textual\ncomplexity features made by hand (Hand-\nCrafted Features). For this, we use the cor-\npus in Spanish CLexIS2corpus proposed by\n(Zambrano and Montejo-R\u00b4 aez, 2021). Our\nchallenge is achieving to improve the lexical\ncomplexity prediction implementing a fine-\ntuned model on a previously trained model,\nfor which, we follow the research done by\n(Rojas and Alva-Manchego, 2021).\nThe models used achieve a good perfor-\nmance shown in the results with a MAE =\n0.1592 and a Person correlation 0.9883 for the\nidentification of simple complex words.2 Related Work\nIn past decades, the application of very sim-\nple metrics such as calculating the number of\nsyllables in words (Mc Laughlin, 1969) or ver-\nifying whether the word was part of a specific\nlist to classify it as easy or complex (Dale and\nChall, 1948) were the techniques that were\napplied in text legibility tasks.\nAfter, the systems based on the charac-\nterization of words (using contextual, lex-\nical and semantic characteristics) and the\napplication of a Random Forest classifier\n(Breiman, 2001) to determine whether a\nword is complex or not are presented. A\ntotal of 45 handwritten features were com-\nputed in these systems, and each word was\nmodeled as a feature vector. Surface func-\ntions (three functions), dependency tree func-\ntions (eight functions), Corpus-based func-\ntions (fifteen functions), WordNet functions\n(eleven functions), and WordNet and corpus\nfrequency functions (four functions) were ap-\nplied. The best result obtained was a Preci-\nsion value of 0.186, a Recall of 0.673, a G-\nscore of 0.750 99 and an F-score of 0.292.\nThe investigations in the last years are\ndirected to the Identification of Complex\nWords - CWI. The objective of these applica-\ntions is to be able to predict the complexity\nof words based on the construction of their\nfeatures, as exposed in the work carried out\nby (Shardlow, Cooper, and Zampieri, 2020)\npresenting their approach on a set of features\nof word embeddings from Glove, InferSent,\nand various linguistic features obtained as\npredictive sources of lexical complexity, such\nas word frequency, word length, or number\nof syllables. Then, they trained a linear re-\ngression model using different subsets of func-\ntions, obtaining as a result an MAE = of\n0.0853.\n(Shardlow et al., 2021) developed a system\nfor predicting word complexity for the shared\nLCP task hosted on SemEval 2021 where\ntask organizers distributed to participants\nthe CompLex corpus (Shardlow, Cooper, and\nZampieri, 2020) but in its augmented version.\nThe task was located on the Lexical Seman-\ntics track, which consisted of predicting the\ncomplexity value of words in context.\n(Ortiz-Zambrano and Montejo-R\u00b4 aez,\n2021) Carried out a machine learning\napproach that was based on 15 linguistic\nfeatures obtained at the word level and\ntheir environment. Trained a supervised\n178\nJenny Ortiz-Zambrano, C\u00e9sar Espin-Riofrio, Arturo Montejo-R\u00e1ez random forest regression algorithm on the\nset of features. Several runs were made with\ndifferent values to observe the performance\nof the algorithm. The best results achieved\nwere a MAE = 0.07347, MSE = 0.00938 and\nRMSE = 0.096871.\nIn our approach, we review the use of\nword embeddings from the pre-trained and\nfine-tuned models, and compare them to a\nbroader list of linguistic features at the lexi-\ncal level. Our objective is to provide an ex-\nhaustive evaluation that shows more clearly,\nthe executions carried out on several differ-\nent data sets in the Spanish language, how\nthe lexical features together with the deep\nencodings contribute to the prediction of lex-\nical complexity.\n3 Materials and Method\nThis section briefly details about the pre-\ntrained models and their application for the\ngeneration of encodings at both the sentence\nand word levels. Likewise, the data sets that\nhave been used in the different experiments\nare presented. Finally, the different classi-\nfication algorithms and the applied features\nare shown.\n3.1 Dataset\nThe CLexIS2corpus was elaborated with\nthe transcripts of the recorded classes of\nthe professors of the degrees of Computer\nSystems Engineering and Software Engineer-\ning, two degrees that belong to the Fac-\nulty of Mathematical Sciences of the Univer-\nsity of Guayaquil (Ecuador) (Zambrano and\nMontejo-R\u00b4 aez, 2021).\nCLexIS2has become a resource of great\ninterest and importance, due to the fact that\nthere are few resources in Spanish available\nfor NLP researchers1, and some of them do\nnot usually contain annotations that facili-\ntate the development of NLP models (David-\nson et al., 2020). For its construction, the\ncollection presented in the ALexS 2020 com-\npetition at IberLEF 2020 (Ortiz-Zambranoa\nand Montejo-R\u00b4 aezb, 2020) was taken as a ref-\nerence.\nAnnotated words as complex have an as-\nsociated level of complexity, computed based\non the number of annotators that agreed to\nconsider it as a complex word. Therefore,\nthe task we are facing here can be faced as\n1CLexIS2- https://osf.io/kfpc9/?view only =\n18ae61a 2049a 48cb91c6773d53fb8ac2a regression problem, so error metrics will be\nused to evaluate different systems.\nTable 1 shows some statistics on different\ntype of words present in the CLexIS2dataset.\n3.2 Transformer based language\nmodels\nThe models were taken from the Transform-\ners2library.\n\u2022The pre-trained BERT model that we\nchose was the one that the Spanish com-\nmunity uses mostly in research work\nto date, which is bert-base-uncased\n(BETO) (Canete et al., 2020).\nBERT-base model has the number of\nlayers L=12, the hidden size H=768,\nthe number of self-attention heads\nA=12,and Total Parameters=110M.\nBERT-large model has the number of\nlayers=24, the hidden size=1024, the\nnumber of self-attention heads=16, and\nTotal Parameters=335M (Conneau et\nal., 2019).\n\u2022The -RoBERTa model applied was xlm-\nroberta-base (Conneau et al., 2019).\nThe XLM-RoBERTa-base model has\nthe number of layers L=12, the hid-\nden size H=768, the number of self-\nattention heads A=12,and Total Param-\neters=270M.\nXLM-RoBERTa-large model has the\nnumber of layers L=24, the hidden\nsize H=1024, the number of self-\nattention heads A=16, and Total Pa-\nrameters=550M (Conneau et al., 2019).\n\u2022The RoBERTa-large-BNE model used\nwas PlanTL-GOB-ES/roberta-large-bne\nbeing the largest Spanish-specific model\nto date (Guti\u00b4 errez-Fandi\u02dc no et al., 2021).\nXLM-RoBERTa-large is a transformer-\nbased masked language model for the\nSpanish language. It is based on the\nRoBERTa large model3.\nThe RoBERTa-large-BNE model has\nthe number of layers L=24, the hid-\nden size H=1024, the number of self-\nattention heads A=16,and Total Param-\neters=335M(Conneau et al., 2019).\n2https://huggingface.co/\n3https://huggingface.co/PlanTL-GOB-\nES/roberta-large-bne\n179\nTransformers for Lexical Complexity Prediction in Spanish Language Number of Count\nSum. of content words (verbs, adjectives and nouns) 153,885\nDifferent content words 200,785\nRare words (low frequency in CREA corpus (Saggion et al., 2015)) 143,464\nSentences 9,756\nComplex sentences 4,101\nTotal words 300,420\nTable 1: Volumetrics for CLexIS2.\n3.3 Experiments design\nOur purpose is to demonstrate how the com-\nbination of different types of features con-\ntribute to a better performance in predicting\nlexical complexity. We base our proposal on\nseveral of the works presented at the Inter-\nnational Workshop on Semantic Evaluation -\nSemEval-2021 (Shardlow et al., 2021) where\na total of 198 teams were presented, of which\n54 teams officially sent their executions4; but\nthe work that most attracted us due to its\nmethodology was the experimentation car-\nried out by (Zaharia, Cercel, and Dascalu,\n2021) about Combining Deep Learning and\nHand-Crafted Features for Lexical Complex-\nity Prediction.\nThe figure 1 presents the workflow of\nthe process executed to obtain of the Lexi-\ncal Complexity Prediction. First, we chose\nthe data sets for training were: the first\ndata set was made up of the linguistic fea-\ntures made by hand - Hand-Crafted Fea-\ntures (HCF) and the second data set was\nmade up of the Transformers encodings from\nthe models: BERT in Spanish, multilin-\ngual XLM-RoBERTa and RoBERTa-grande-\nBNE. Next, we applied a fitted model on top\nof the previously trained model to demon-\nstrate how running the fitted model on the\npreviously trained model contributed to more\naccurate LCP results as see figure 1.\nFinally, the different supervised learning\nalgorithms were executed on the training\ndata set to evaluate which of them achieved\nthe best prediction score. Triple cross-\nvalidation was performed to ensure that the\npartitions contained independent data for\ntraining and testing. We have used some\nmetrics that were applied to the results of\nthe experiments presented in Sem-Eval 2021\n(Shardlow et al., 2021), which are appropri-\nate for evaluating continuous and classified\ndata, such as: MAE, MSE, RMSE and Pear-\n4https:// semeval.github.io/SemEval2021/tasksson\u2019s correlation.\n3.3.1 Features\n\u2022Hand-Crafted Features - HCF\nTo obtain the morphological aspects of the\ntext, we perform several experiments apply-\ning a total of 23 linguistic features and com-\nbine them with the word and sentence em-\nbeddings of previously trained deep learning\nmodels.\nWe have considered the 15 HCF pro-\nposed by (Ortiz-Zambrano and Montejo-\nR\u00b4 aez, 2021) and added a sets of features\ncomputed from POS categories counts (Vet-\ntigli and Sorgente, 2021), (Liebeskind, Elka-\nyam, and Liebeskind, 2021), giving a to-\ntal of 23 Hand-Crafted Features. We used\nthe Spacy library together with the model\nescore news smto extract these features.\nAll these features were normalized with a z-\nscore transformation before passing them to\nthe learning algorithm.\n1.Absolute frequency : the absolute fre-\nquency.\nThe frequency of words is a measure that\nserves as an indicator of lexical com-\nplexity. If in common parlance a word\noccurs frequently, it is more likely to\nbe recognized (Rayner and Duffy, 1986)\nand (Shardlow, Cooper, and Zampieri,\n2020).\n2.Relative frequency : the relative fre-\nquency of the target word.\n3.Word length : the number of characters\nof the token. The length of the word\nwas calculated in number of its charac-\nters. It is often the case that longer\nlength words are more difficult to pro-\ncess and can therefore be considered\ncomplex . (Shardlow, 2013) (Shardlow,\nCooper, and Zampieri, 2020) (Paetzold\nand Specia, 2016).\n180\nJenny Ortiz-Zambrano, C\u00e9sar Espin-Riofrio, Arturo Montejo-R\u00e1ez Figure 1: Representation of the workflow to obtain the Lexical Complexity Prediction.\n4.Number of syllables : the number of syl-\nlables. A good estimate of complexity\nis the number of syllables contained in a\nword (Shardlow, 2013) (Ronzano et al.,\n2016) (Shardlow, Cooper, and Zampieri,\n2020) (Paetzold and Specia, 2016).\n5.Target word position (token-position):\nthe position of the target word in the\nsentence. Position of the word (Word-\nPosition) (Shardlow, 2013) (Ronzano et\nal., 2016).\n6.Number of words in the sentence : num-\nber of words in the sentence. Words in\nsentence (NumSentenceWords) (Shard-\nlow, 2013) (Ronzano et al., 2016).\nBased on the work proposed by (Ron-\nzano et al., 2016) in the exploring lin-\nguistic features for lexical complexity\nprediction.\n7.Part Of Speech (POS) : the Part Of\nSpeech category.\n8.Relative frequency of the previous token :\nthe relative frequency of the word before\nthe token.\n9.Relative frequency of the word after the\ntoken : the relative frequency of the word\nafter the token.\n10.Length of previous word : the number of\ncharacters in the word before the token.\n11.Length of the after word : the number of\ncharacters in the word after the token.12.Lexical diversity - MTDL : the lexical di-\nversity of the target word in the sen-\ntence.\nAdditionally, the following WordNet fea-\ntures were also considered for each tar-\nget word, as in the works carried out by\n(Gooding and Kochmar, 2018):\n13.Number of synonyms.\n14.Number of hyponyms .\n15.Number of hyperonyms .\nWe follow the recommendations of\n(Paetzold and Specia, 2016), (Ronzano et\nal., 2016), (Gooding and Kochmar, 2018),\n(Liebeskind, Elkayam, and Liebeskind,\n2021), (Desai et al., 2021) with the aim of\nimproving results, generating 8 new features\noriginating from the POS, which were:\n1. PROPN - Number of pronouns within\nthe sentence.\n2. AUX - Number of auxiliaries within the\nsentence.\n3. VERB - Number of verbs within the sen-\ntence.\n4. ADP - Number of adverbs within the\nsentence.\n5. NOUN - Number of nouns within the\nsentence.\n6. NN - Number of Nouns, singular or mas-\nsive.\n181\nTransformers for Lexical Complexity Prediction in Spanish Language 7. SYM - Number of symbols within the\nsentence.\n8. NUM - Number of numbers within the\nsentence.\n\u2022BERT vector: The bert-base-uncased\nmodel from the Hugging Face trans-\nformer library (Wolf et al., 2020) was ap-\nplied. We took all the 768-dimensional\nnumerical representation produced by\nthe pre-trained and fine-tuned BERT\nmodel (Devlin et al., 2018) and added\nthe twenty-three Hand-Crafted Features\nobtaining a dataset with a total of 1559\nlinguistic features of different nature.\n\u2022XLM-RoBERTa vector: As in the\ncase of the BERT model, we take all\nthe 768-dimensional numerical repre-\nsentation produced by the pre-trained\nRoBERTa model (Conneau et al., 2019)\nin the different combinations of sentence\nand target word encodings, for both the\npre-trained model and the model fine-\ntuned, reaching a total of 1559 linguistic\ncharacteristics of different nature.\n\u2022RoBERTa-large-BNE vector: Re-\ngarding this model, we take all the 1024-\ndimensional numerical representation\nproduced by the pre-trained RoBERTa-\nlarge-model model (Guti\u00b4 errez-Fandi\u02dc no\net al., 2021), in the same way that they\nwere applied in the previous models, the\ndata sets were made up of for the differ-\nent combinations of sentence and target\nword encodings, for both the pre-trained\nmodel and the fine-tuned model, reach-\ning a total of 2071 linguistic characteris-\ntics of different nature.\n3.3.2  Machine  Learning  Algorithms\nSimilar  to the work  done  by (Zaharia,  Cer-\ncel, and Dascalu,  2021)  in the case  of the al-\ngorithms,  the training  and evaluation  of the \ndifferent  c ombinations  o f t he  s ets  w as car-\nried out with  a total  of eight  supervised  al-\ngorithms  for the regression,  these  are:\n1. AdaBoost - AB (Paetzold, 2021).\n2. Desicion Tree - DT (Shardlow, Evans,\nand Zampieri, 2021).\n3. Gradient Boosting - GB (Vettigli and\nSorgente, 2021).4. Stochastic Gradient - SG (Bottou,\n2010).\n5. Nearest Neighbors - KNN (Liebeskind,\nElkayam, and Liebeskind, 2021).\n6. Support Vector Machines - SVM (Liebe-\nskind, Elkayam, and Liebeskind, 2021).\n7. Passive Aggressive - PA (Crammer et al.,\n2006).\n8. Random Forest - RF) (Zaharia, Cercel,\nand Dascalu, 2021), (Desai et al., 2021).\nSeveral experiments were carried out for\neach of the datasets where different config-\nurations were explored for each of the algo-\nrithms. We apply the default values for the\nalgorithms except for the case for tree-based\nalgorithms, achieving to determine the best\nhyper-parameters with the following number\nof nodes:\n\u2022AdaBoost with 100 nodes.\n\u2022Random forest with 241 nodes.\n\u2022Gradient Boosting algorithm with 350\nnodes.\n4 Results\n4.1 Features Sets\nWe build several datasets composed of the\ncombination of the features described above\nto run them on the pre-trained models. The\ntable 2 table presents the description of the\nabbreviations that will be used for a better\nunderstanding of the features applied to the\ndata sets. The detail below:\n\u2022TheHand-Crafted Features with the fea-\ntures coming from the 768-dimensional\nvector of the initial [CLS] token as sen-\ntence embeddings ( BERT sent).\n\u2022The Hand-Crafted Features with the\n768-dimensional vector corresponding to\nthe target token as word embeddings\n(BERT word).\n\u2022The Hand-Crafted Features with encod-\nings of the [CLS] token and encodings of\nthe target token.\n\u2022The encodings of the [CLS] token.\n\u2022The encodings of the target token.\n\u2022The encodings of the [CLS] token with\nthe encodings of the target token.\n182\nJenny Ortiz-Zambrano, C\u00e9sar Espin-Riofrio, Arturo Montejo-R\u00e1ez Features identifier Description\nHCF Hand-Crafted (linguistic) Features.\nBERT sent Sentence encodings from BERT models.\nBERT word Token encodings from BERT models.\nXLMR sent Sentence encodings from XLM-RoBERTa model.\nXLMR word Token encodings from RoBERTa model.\nRBNE sent Sentence encodings from RoBERTa-large-BNE model.\nRBNE word Token encodings from RoBERTa-large-BNE model.\nTable 2: Description of the Feature sets.\nFor the evaluation of the trained and\nfine-tuned models, those that were widely\napplied to LCP for the shared LCP task\nhosted in SemEval 2021: Mean Absolute Er-\nror (MAE), Mean Square Error (MSE), Root\nMean Square Error (RMSE) and Pearson cor-\nrelation (Shardlow et al., 2021).\n4.2 BERT model pre-trained\nThe table 3 shows the eight best perfor-\nmances corresponding to different combina-\ntions of features described in section 4.1 exe-\ncuted with BERT pre-trained.\nAs we can see in the three best results in\npredicting lexical complexity were achieved\nby the ABR and SVR algorithms. The best\nperformance was achieved by the ABR - Ad-\naBoost algorithm presenting the best predic-\ntion for the Spanish language with a MAE =\n0.1632 and a Pearson = 0.999 in the execu-\ntion with the data set made up of the com-\nbination of the features generated at the sen-\ntence level and at the word level - BERT sent\u2295\nBERT word.\n4.3 BERT model fine-tuned\nWe have applied the fine-tuned BERT model\non top of the pre-trained BERT model for\nthe purpose of the results. The table 4 shows\nthe eight best executions, positioning RFR\n- Random Forest Regressor algorithm and\nthe GBR - Gradient Boosting Regressor al-\ngorithm in the first places.\nThe best performance was obtained with\nthe dataset composed of the combination of\nthe features with target word encodings to-\ngether with sentence encodings from BERT\nfine-tuned. The same combination of fea-\ntures achieved the best performance in the\npre-trained model, but with lower results.\nIt should be noted that the RFR algorithm\ndoes not appear within the top eight places in\nthe execution of the pre-trained model, but itachieves its best result when the model is re-\nfined, placing first and third within the three\nbest executions tuned. RFR presented the\nbest prediction for the Spanish language with\na MAE = 0.1592 and a Pearson = 0.988 com-\nbining BERT sent\u2295BERT word.\n4.4 XLM-RoBERTa model\npre-trained\nSimilar to the BERT model, the top eight\nsites were taken from all the runs that were\ndone on the different data sets. The results\nof the best place for the pre-trained XLM-\nRoBERTa model were achieved by the ABR\n- AdaBoots algorithm with a MAE = 0.1623\nand a Pearson = 0.9973 result of the combi-\nnation of the features with target word and\nsentence encodings together with the HCF\n- XLMR sent\u2295XLMR word\u2295HCF, as can be\nseen in the table 5. It can be clearly shown\nthat the pre-trained XLM-RoBERTa model\nhas a better performance compared to the\npre-trained BERT model, achieving a better\nprediction of Lexical Complexity.\n4.5 XLM-RoBERTa model\nfine-tuned\nWe also highlight that in the execution of\nthe XLM-RoBERTa tuned model, it achieved\na significant improvement compared to the\nresults of the pre-trained model, reaching\na MAE = 0.1601 and a Pearson = 0.998\nas result of the combination of the features\nwith target word encodings together HCF -\nXLMR word\u2295HCF. See table 6.\nComparing the results of the BERT and\nXLM-RoBERTa both tuned models, BERT\ntuned is so far the one that has an important\nperformance achieved by a MAE = 0.1592\nwith the execution of the RFR algorithm\ncombining BERT sent\u2295BERT word.\n183\nTransformers for Lexical Complexity Prediction in Spanish Language BERT model pre-trained with CLexIS2\nFeatures Alg MAE MSE RMSE Pearson\nBERT sent\u2295BERT word ABR 0,1632 0,0502 0,2343 0,9999\nBERT word SVR 0,1634 0,0432 0,2074 0,9023\nBERT word ABR 0,1643 0,0512 0,2332 0,9947\nBERT word GBR 0,1653 0,0494 0,0447 0,6977\nBERT word\u2295HCF GBR 0,1655 0,0454 0,2088 0,7040\nBERT sent\u2295BERT word\u2295HCF GBR 0,1659 0,0418 0,2074 0,7147\nBERT sent\u2295BERT word GBR 0,1694 0,0444 0,2039 0,7167\nBERT sent\u2295BERT word\u2295HCF ABR 0,1699 0,0554 0,2334 0,9939\nTable 3: Results of the model BERT pre-trained with features of different nature.\nBERT model fine-tuned with CLexIS2\nFeatures Alg MAE MSE RMSE Pearson\nBERT sent\u2295BERT word RFR 0,1592 0,0379 0,1982 0,9883\nBERT sent\u2295BERT word\u2295HCF GBR 0,1600 0,0367 0,1979 0,8202\nBERT sent\u2295BERT word\u2295HCF RFR 0,1610 0,0401 0,1988 0,9987\nBERT sent\u2295BERT word\u2295HCF ABR 0,1610 0,0506 0,2242 0,9998\nBERT sent\u2295BERT word\u2295HCF ABR 0,1621 0,0487 0,2300 0,9999\nBERT word\u2295HCF GBR 0,1622 0,0430 0,1984 0,8983\nBERT word SVR 0,1622 0,0429 0,2018 0,9183\nBERT word GBR 0,1632 0,0472 0,0429 0,7083\nTable 4: Results of the model BERT tuned with features of different nature.\n4.6 RoBERTa-large-BNE model\npre-trained\nThe novelty of this research is to have incor-\nporated the executions with the pre-trained\nmodel RoBERTa-large-BNE and its adjusted\nmodel. The eight best results are displayed\nin the table 7. The best position were\nachieved by the ABR-AdaBoost algorithm\nwith a MAE = 0.1609 and a Person =\n0.6754 combining the sentence and word en-\ncodings together with the HCF - RBNE sent\u2295\nRBNE word\u2295HCF.\nIt should be noted that the pre-trained\nmodel RoBERTa-large-BNE is the one that\nachieves a better prediction for lexical com-\nplexity in the Spanish language compared\nto the pre-trained models BERT and XLM-\nRoBERTa. See table 9.\n4.7 RoBERTa-large-BNE model\nfine-tuned\nExecuting the RoBERTa-large-BNE tuned\nmodel, the results are encouraging, there is\nan improvement compared to the results of\nthe pre-trained model. The table 8 displays\nthe first places reached by the GBR-Gradient\nBoosting Regressor and SVR-Super Vector\nRegressor algorithms. It presents a low im-\nprovement, achieving in its performance a\nMAE = 0.1609 and a Pearson = 0.6754\ncombining the sentence and word encod-\nings together with the HCF - RLBNE sent\u2295RLBNE word\u2295HCF, and the second and third\nplaces prove it in comparison with the pre-\ntrained model.\nIt should be noted that the tuned model\nBERT is the one that achieves a better pre-\ndiction for lexical complexity in the Span-\nish language compared to the tuned models\nXLM-RoBERTa and RBNE. See table 10.\nIt can be seen that the fined models based\non Transformers make an important contri-\nbution to the Prediction of Lexical Complex-\nity in the Spanish language. The table 11\npresents the best five best results of all the ex-\nperiments carried out with the models, both\npre-trained and fined. It is important to men-\ntion that the Hand-Crafted Features, being\nsuch simple features because they are only\nbased on the frequency of the words and sev-\neral manual calculations, have been shown to\ncontribute to improving the level of predic-\ntion of the complexity of the words.\n5 Discussion\nWe have applied the BERT, RoBERTa, and\nRoBERTa-large-BNE models for our research\nin predicting lexical complexity in Spanish.\nWe have closely followed the methodology ap-\nplied in several of the works presented in the\nLCP task of the SemEval 2021 International\nConference (Shardlow et al., 2021) which has\nallowed us to achieve very important results\nthat demonstrate a relevant contribution in\n184\nJenny Ortiz-Zambrano, C\u00e9sar Espin-Riofrio, Arturo Montejo-R\u00e1ez XLM-RoBERTA model pre-trained with CLexIS2\nFeatures Alg MAE MSE RMSE Pearson\nXLMR sent\u2295XMLR word\u2295HCF ABR 0,1623 0,0527 0,2270 0,9973\nXLMR word\u2295HCF ABR 0,1623 0,0513 0,2273 0,9973\nXLMR sent\u2295HCF ABR 0,1630 0,0524 0,2293 0,9973\nXLMR word\u2295HCF GBR 0,1653 0,0433 0,2073 0,4848\nXLMR sent\u2295XMLR word\u2295HCF GBR 0,1658 0,0434 0,2074 0,4874\nXLMR sent\u2295HCF GBR 0,1663 0,0433 0,2082 0,4807\nXLMR word\u2295HCF SVR 0,1680 0,0483 0,2194 0,3095\nXLMR word\u2295HCF RFR 0,1690 0,0445 0,2093 0,9803\nTable 5: Results of the model XLMR pre-trained with features of different nature.\nXLM-RoBERTA model fine-tuned with CLexIS2\nFeatures Alg MAE MSE RMSE Pearson\nXLMR word\u2295HCF ABR 0,1601 0,0501 0,2251 0,9987\nXLMR sent\u2295XMLR word\u2295HCF ABR 0,1620 0,0526 0,2268 0,9987\nXLMR sent\u2295HCF ABR 0,1620 0,0519 0,2287 0,9979\nXLMR sent\u2295HCF GBR 0,1630 0,0420 0,2062 0,4790\nXLMR word\u2295HCF GBR 0,1638 0,0429 0,2034 0,4800\nXLMR sent\u2295XMLR word\u2295HCF GBR 0,1652 0,0430 0,2069 0,4930\nXLMR word\u2295HCF SVR 0,1660 0,0482 0,2172 0,3083\nXLMR word\u2295HCF RFR 0,1669 0,0427 0,2013 0,9849\nTable 6: Results of the model XLMR tuned with features of different nature.\nRoBERTa-large-BNE model pre-trained with CLexIS2\nFeatures Alg MAE MSE RMSE Pearson\nRBNE sent\u2295RBNE word\u2295HCF ABR 0,1609 0,0421 0,2047 0,6754\nRBNE sent\u2295RBNE word ABR 0,1675 0,0556 0,2347 0,9952\nRBNE sent\u2295RBNE word\u2295HCF GBR 0,1691 0,0434 0,2073 0,6607\nRBNE word ABR 0,1693 0,0563 0,2360 0,9948\nRBNE word GBR 0,1696 0,0447 0,2101 0,6400\nRBNE sent\u2295RBNE word GBR 0,1698 0,0447 0,2102 0,6450\nRBNE sent\u2295RBNE word\u2295HCF SVR 0,1708 0,0507 0,2224 0,2363\nRBNE sent\u2295HCF SVR 0,1708 0,0507 0,2224 0,0857\nTable 7: Results of the model RBNE pre-trained with features of different nature.\nRoBERTa-large-BNE model fine-tuned with CLexIS2\nFeatures Alg MAE MSE RMSE Pearson\nRBNE sent\u2295RBNE word\u2295HCF GBR 0,1609 0.0421 0.2047 0.6754\nRBNE sent\u2295RBNE word\u2295HCF SVR 0,1630 0,0435 0,2070 0,4883\nRBNE word\u2295HCF SVR 0,1666 0,0466 0,2136 0,4220\nRBNE word ABR 0,1677 0.0551 0,2336 0,9952\nRBNE sent\u2295RBNE word SVR 0,1684 0.0472 0.2152 0.4425\nRBNE sent\u2295RBNE word\u2295HCF GBR 0,1686 0,0432 0,2067 0,6854\nRBNE word SVR 0,1686 0,0468 0,2146 0,5021\nRBNE sent\u2295RBNE word\u2295HCF ABR 0,1689 0,0558 0,2351 0,9951\nTable 8: Results of the model RBNE tuned with features of different nature.\nTheSpanish Language Models pre-trained\nBest Result\nModel Features Alg MAE\nRBNE RBNE sent\u2295RLBNE word\u2295HCF ABR 0.1609\nXLMR XLMR sent\u2295XLMR word\u2295HCF ABR 0.1623\nBERT BERT sent\u2295BERT word ABR 0.1632\nTable 9: Best results models pre-trained.\n185\nTransformers for Lexical Complexity Prediction in Spanish Language TheSpanish Language Models fine-tuned\nBest Result\nModel Features Alg MAE\nBERT BERT sent\u2295BERT word RFR 0,1592\nXLMR XLMR word\u2295HCF ABR 0,1601\nRBNE RBNE sent\u2295RBNE word\u2295HCF GBR 0,1609\nTable 10: Best results models fine-tuned.\nSummary of best results on the CLexIS2corpus\nModel Features Alg MAE MSE RMSE Pearson\nBERT fine\u2212tuned BERT sent\u2295BERT word RFR 0,1592 0,0379 0,1982 0,9883\nBERT fine\u2212tuned BERT sent\u2295BERT word\u2295HCF GBR 0,1600 0,0367 0,1979 0,8202\nXLMR fine\u2212tuned XLMR word\u2295HCF ABR 0,1601 0,0501 0,2251 0,9987\nRBNE fine\u2212tuned RBNE sent\u2295RBNE word GBR 0,1609 0,0421 0,2047 0,6754\nBERT fine\u2212tuned BERT sent\u2295BERT word\u2295HCF RFR 0,1610 0,0401 0,1988 0,9987\nTable 11: Summary of best results on the CLexIS2dataset.\nthe area of Lexical Simplification for Span-\nish.\nWe observe that according to the results\nof the final evaluation, especially in terms of\nfine-tuning, the Spanish language fined mod-\nels made an important contribution to the\nprediction of lexical complexity by outper-\nforming the proposal presented after the ex-\necution of the manual features-HCF. In the\ncase of the RoBERTa-large-BNE model, we\nhave found a performance that exceeds the\nrest of the models after the execution of the\npre-trained model and even remains within\nthe three best executions in the results of\nthe tuned models, such as the proposals pre-\nsented by (Guti\u00b4 errez-Fandi\u02dc no et al., 2021)\n6 Conclusions and Further Work\nIn this article, we have presented a contri-\nbution to predict the complexity of simple\nwords in the Spanish language, combining a\nlarge number of features of different types.\nWe consider that, after the multiple experi-\nmentations that we carried out, it allowed us\nto know the maximum performance for the\ndifferent combinations of the data sets by ap-\nplying the regression algorithms.\nIn our experiments, we obtained the re-\nsults after the execution of several previ-\nously trained transformer-based models on\nseveral datasets in Spanish, combining fea-\ntures of different nature. The application\nof the fine-tuned models to generate fea-\ntures (embeddings) achieved a better per-\nformance of explored machine learning algo-\nrithms, which led to a MAE = 0.1598 and aPearson of 0.9883 achieved with the evalua-\ntion and training of the Random Forest Re-\ngressor algorithm for the tuned model BERT.\nAdditional features can boost pre-trained\nmodels to levels of performance close to those\nof fine-tuned models alone, so it could be a\nfeasible approach when there are not enough\ncomputational resources for such a down-\nstream training.\nAs a possible alternative proposal to\nachieve a better prediction of lexical complex-\nity, we are very interested in continuing to\ncarry out experimentations on data sets for\nSpanish, testing state-of-the-art Transformer\nmodels. To this end, extrinsic evaluation will\nbe overcome, comparing the best systems on\nthis specific task with the possibilities of in-\ntegrating external features like the ones pro-\nposed in this work.\nAcknowledgements\nWe appreciate Jostin Daniel Escobar Suarez,\nJoel Stalin Sorroza Contreras, Diego Gabriel\nBernal Yucailla y Diana Geovanna Aroca\nPincay graduate of the Computer Systems\nEngineering degree from the University of\nGuayaquil, for their valuable contribution to\nthe development of our work.\nThis work is partially funded by grants\nP2000956 (PAIDI 2020) and 1380939\n(FEDER Andaluc\u00b4 \u0131a 2014-2020) from the An-\ndalusian Regional Government.\nReferences\nBender, E. M., T. Gebru, A. McMillan-\nMajor, and S. Shmitchell. 2021. On the\n186\nJenny Ortiz-Zambrano, C\u00e9sar Espin-Riofrio, Arturo Montejo-R\u00e1ez dangers of stochastic parrots: Can lan-\nguage models be too big? . In Proceedings\nof the 2021 ACM Conference on Fairness,\nAccountability, and Transparency, FAccT\n\u201921, page 610\u2013623, New York, NY, USA.\nAssociation for Computing Machinery.\nBottou, L. 2010. Large-scale ma-\nchine learning with stochastic gradient\ndescent. In Proceedings of COMP-\nSTAT\u20192010 . Springer, pages 177\u2013186.\nBreiman, L. 2001. Random forests. Machine\nlearning , 45(1):5\u201332.\nCanete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u00b4 erez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. Pml4dc at iclr , 2020:2020.\nConneau, A., K. Khandelwal, N. Goyal,\nV. Chaudhary, G. Wenzek, F. Guzm\u00b4 an,\nE. Grave, M. Ott, L. Zettlemoyer, and\nV. Stoyanov. 2019. Unsupervised cross-\nlingual representation learning at scale.\narXiv preprint arXiv:1911.02116.\nCrammer, K., O. Dekel, J. Keshet, S. Shalev-\nShwartz, and Y. Singer. 2006. Online\npassive-aggressive algorithms. Journal of\nMachine Learning Research, 7:551\u2013585.\nDale, E. and J. S. Chall. 1948. A formula for\npredicting readability: Instructions. Edu-\ncational research bulletin, pages 37\u201354.\nDavidson, S., A. Yamada, P. F. Mira,\nA. Carando, C. H. S. Gutierrez, and\nK. Sagae. 2020. Developing nlp tools with\na new corpus of learner spanish. In Pro-\nceedings of the 12th Language Resources\nand Evaluation Conference , pages 7238\u2013\n7243.\nDesai, A., K. North, M. Zampieri, and\nC. M. Homan. 2021. Lcp-rit at semeval-\n2021 task 1: Exploring linguistic features\nfor lexical complexity prediction. arXiv\npreprint arXiv:2105.08780.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding.\nGooding, S. and E. Kochmar. 2018. Camb\nat cwi shared task 2018: Complex word\nidentification with ensemble-based voting.\nInProceedings of the Thirteenth Workshop\non Innovative Use of NLP for Building\nEducational Applications, pages 184\u2013194.Guti\u00b4 errez-Fandi\u02dc no, A., J. Armengol-\nEstap\u00b4 e, M. P` amies, J. Llop-Palao,\nJ. Silveira-Ocampo, C. P. Carrino,\nA. Gonzalez-Agirre, C. Armentano-Oller,\nC. Rodriguez-Penagos, and M. Villegas.\n2021. Spanish language models. arXiv\npreprint arXiv:2107.07253.\nLiebeskind, C., O. Elkayam, and S. Liebe-\nskind. 2021. Jct at semeval-2021 task\n1: Context-aware representation for lex-\nical complexity prediction. In Proceedings\nof the 15th International Workshop on Se-\nmantic Evaluation (SemEval-2021), pages\n138\u2013143.\nLiu, X., P. He, W. Chen, and J. Gao.\n2019. Improving multi-task deep neu-\nral networks via knowledge distillation for\nnatural language understanding. arXiv\npreprint arXiv:1904.09482.\nMc Laughlin, G. H. 1969. Smog grading-a\nnew readability formula. Journal of read-\ning, 12(8):639\u2013646.\nNandy, A., S. Adak, T. Halder, and S. M.\nPokala. 2021. cs60075 team2 at semeval-\n2021 task 1: Lexical complexity pre-\ndiction using transformer-based language\nmodels pre-trained on various text cor-\npora. In Proceedings of the 15th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval-2021), pages 678\u2013682.\nOrtiz-Zambrano, J. A. and A. Montejo-R\u00b4 aez.\n2021. Complex words identification us-\ning word-level features for semeval-2020\ntask 1. In Proceedings of the 15th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval-2021), pages 126\u2013129.\nOrtiz-Zambranoa, J. A. and A. Montejo-\nR\u00b4 aezb. 2020. Overview of alexs 2020:\nFirst workshop on lexical analysis at se-\npln. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2020) .\nPaetzold, G. 2021. Utfpr at semeval-\n2021 task 1: Complexity prediction by\ncombining bert vectors and classic fea-\ntures. In Proceedings of the 15th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval-2021), pages 617\u2013622.\nPaetzold, G. and L. Specia. 2016. Sv000gg at\nsemeval-2016 task 11: Heavy gauge com-\nplex word identification with system vot-\ning. In Proceedings of the 10th Interna-\n187\nTransformers for Lexical Complexity Prediction in Spanish Language tional Workshop on Semantic Evaluation\n(SemEval-2016), pages 969\u2013974.\nRayner, K. and S. A. Duffy. 1986. Lexical\ncomplexity and fixation times in reading:\nEffects of word frequency, verb complex-\nity, and lexical ambiguity. Memory & cog-\nnition, 14(3):191\u2013201.\nRico-Sulayes, A. 2020. General lexicon-\nbased complex word identification ex-\ntended with stem n-grams and morpholog-\nical engines. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2020), CEUR-WS, Malaga, Spain .\nRojas, K. R. and F. Alva-Manchego. 2021.\nIapucp at semeval-2021 task 1: Stack-\ning fine-tuned transformers is almost all\nyou need for lexical complexity predic-\ntion. In Proceedings of the 15th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval-2021), pages 144\u2013149.\nRonzano, F., L. E. Anke, H. Saggion, et al.\n2016. Taln at semeval-2016 task 11: Mod-\nelling complex words by contextual, lexi-\ncal and semantic features. In Proceedings\nof the 10th International Workshop on Se-\nmantic Evaluation (SemEval-2016), pages\n1011\u20131016.\nSaggion, H., S. \u02c7Stajner, S. Bott, S. Mille,\nL. Rello, and B. Drndarevic. 2015. Mak-\ning it simplext: Implementation and eval-\nuation of a text simplification system for\nspanish. ACM Transactions on Accessible\nComputing (TACCESS), 6(4):1\u201336.\nShardlow, M. 2013. A comparison of tech-\nniques to automatically identify complex\nwords. In 51st Annual Meeting of the\nAssociation for Computational Linguis-\ntics Proceedings of the Student Research\nWorkshop, pages 103\u2013109.\nShardlow, M., M. Cooper, and M. Zampieri.\n2020. Complex: A new corpus for lexi-\ncal complexity prediction from likert scale\ndata. arXiv preprint arXiv:2003.07008.\nShardlow, M., R. Evans, G. H. Paetzold, and\nM. Zampieri. 2021. Semeval-2021 task\n1: Lexical complexity prediction. arXiv\npreprint arXiv:2106.00473.\nShardlow, M., R. Evans, and M. Zampieri.\n2021. Predicting lexical complex-\nity in english texts. arXiv preprint\narXiv:2102.08773.Singh, S. and A. Mahmood. 2021. The nlp\ncookbook: Modern recipes for transformer\nbased deep learning architectures. IEEE\nAccess, 9:68675\u201368702.\nUluslu, A. Y. 2022. Automatic lexical\nsimplification for turkish. arXiv preprint\narXiv:2201.05878.\nVaswani, A., N. Shazeer, N. Parmar,\nJ. Uszkoreit, L. Jones, A. N. Gomez,\n L. Kaiser, and I. Polosukhin. 2017. Atten-\ntion is all you need. In Advances in neu-\nral information processing systems , pages\n5998\u20136008.\nVettigli, G. and A. Sorgente. 2021. Compna\nat semeval-2021 task 1: Prediction of lex-\nical complexity analyzing heterogeneous\nfeatures. In Proceedings of the 15th In-\nternational Workshop on Semantic Eval-\nuation (SemEval-2021), pages 560\u2013564.\nWolf, T., L. Debut, V. Sanh, J. Chaumond,\nC. Delangue, A. Moi, P. Cistac, T. Rault,\nR. Louf, M. Funtowicz, et al. 2020. Trans-\nformers: State-of-the-art natural language\nprocessing. In Proceedings of the 2020\nconference on empirical methods in nat-\nural language processing: system demon-\nstrations, pages 38\u201345.\nYaseen, T. B., Q. Ismail, S. Al-Omari, E. Al-\nSobh, and M. Abdullah. 2021. Just-blue\nat semeval-2021 task 1: Predicting lexi-\ncal complexity using bert and roberta pre-\ntrained language models. In Proceedings\nof the 15th International Workshop on Se-\nmantic Evaluation (SemEval-2021), pages\n661\u2013666.\nZaharia, G.-E., D.-C. Cercel, and M. Das-\ncalu. 2021. Upb at semeval-2021 task\n1: Combining deep learning and hand-\ncrafted features for lexical complexity pre-\ndiction. arXiv preprint arXiv:2104.06983.\nZambrano, J. A. O. and A. Montejo-R\u00b4 aez.\n2021. Clexis2: A new corpus for complex\nword identification research in comput-\ning studies. In Proceedings of the Inter-\nnational Conference on Recent Advances\nin Natural Language Processing (RANLP\n2021), pages 1075\u20131083.\n188\nJenny Ortiz-Zambrano, C\u00e9sar Espin-Riofrio, Arturo Montejo-R\u00e1ez Building a comparable corpus and a benchmark\nfor Spanish medical text simpli\fcation\nConstrucci\u0013 on de un corpus comparable y un recurso de\nreferencia para la simpli\fcaci\u0013 on de textos m\u0013 edicos en espa~ nol\nLeonardo Campillos-Llanos,1Ana R. Terroba Reinares,2\nSof\u0013 \u0010a Zakhir Puig,1Ana Valverde-Mateos3,Adri\u0013 an Capllonch-Carri\u0013 on4\n1ILLA - Consejo Superior de Investigaciones Cient\u0013 \u0010\fcas (CSIC)\n2Fundaci\u0013 on Rioja Salud\n3Unidad de Terminolog\u0013 \u0010a M\u0013 edica, Real Academia Nacional de Medicina de Espa~ na\n4Centro de Salud Retiro, Hospital General Universitario Gregorio Mara~ n\u0013 on\nfleonardo.campillos,sofia.zakhir g@csic.es, arterroba@riojasalud.es ,\navalverde@ranm.es, adrian.capllonch@salud.madrid.org\nAbstract: We report the collection of the CLARA-MeD comparable corpus, which\nis made up of 24 298 pairs of professional and simpli\fed texts in the medical domain\nfor the Spanish language (>96M tokens). Texts types range from drug lea\rets and\nsummaries of product characteristics (10 211 pairs of texts, >82M words), abstracts\nof systematic reviews (8138 pairs of texts, >9M words), cancer-related information\nsummaries (201 pairs of texts, >3M tokens) and clinical trials announcements (5748\npairs of texts, 451 690 words). We also report the alignment of professional and\nsimpli\fed sentences, conducted manually by pairs of annotators. A subset of 3800\nsentence pairs (149 862 tokens) has been aligned each by 2 experts, with an average\ninter-annotator agreement kappa score of 0.839 (\u00060.076). The data are available\nin the community and contributes with a new benchmark to develop and evaluate\nautomatic medical text simpli\fcation systems.\nKeywords: Comparable corpus. Medical text simpli\fcation. Biomedical natural\nlanguage processing.\nResumen: Se describe la recogida del corpus comparable CLARA-MeD, formado\npor 24 298 pares de textos profesionales y simpli\fcados de dominio m\u0013 edico en lengua\nespa~ nola (>96M palabras). Los tipos de textos var\u0013 \u0010an desde prospectos m\u0013 edicos y\n\fchas t\u0013 ecnicas de medicamentos (10 211 pares de textos, >82M palabras), res\u0013 umenes\nde revisiones sistem\u0013 aticas (8138 pares de textos, >9M palabras), res\u0013 umenes de infor-\nmaci\u0013 on sobre el c\u0013 ancer (201 pares de textos, >3M palabras) y anuncios de ensayos\ncl\u0013 \u0010nicos (5748 pares de textos, 451 690 palabras). Tambi\u0013 en presentamos el alin-\neamiento de frases t\u0013 ecnicas y simpli\fcadas, realizado a mano por pares de anota-\ndores. Un subconjunto de 3800 pares de frases (149 862 tokens) se han emparejado,\ncon un acuerdo medio entre anotadores con valor kappa = 0.839 (\u00060.076). Los datos\nest\u0013 an disponibles en la comunidad y este nuevo recurso permite desarrollar y evaluar\nsistemas de simpli\fcaci\u0013 on autom\u0013 atica de textos m\u0013 edicos.\nPalabras clave: Corpus comparable. Simpli\fcaci\u0013 on de textos m\u0013 edicos. Proce-\nsamiento de lenguaje natural biom\u0013 edico.\n1 Introduction\nText simpli\fcation is the task of transforming\na text into an equivalent which is more under-\nstandable (Saggion et al., 2011). The appli-\ncation of natural language processing (NLP)\ntechniques makes it possible to automate\nthe simplication of texts across domains andtasks, ranging from legal and administrative\ntexts (Scarton et al., 2018), language learn-\ning (Petersen and Ostendorf, 2007), users\nwith special reading needs (Barbu et al.,\n2015) or health literacy (Kindig et al., 2004).\nCorpus data are required for analysing\ntext simpli\fcation strategies, developing and\ntesting NLP systems. This work introduces\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 189-196\nrecibido 29-03-2022 revisado 20-05-2022 aceptado 27-05-2022\nISSN 1135-5948. DOI 10.26342/2022-69-16\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturala new resource made up of documents from\nthe medical domain, which is available at:\nhttps://digital.csic.es/handle/10261/269887.\n2 Background\nText simpli\fcation approaches are commonly\nconceived as a translation task|from the\ntechnical to laymen's register. Simpli\f-\ncation involves operations at multiple lin-\nguistic levels: grammar (e.g. simpler\nword order, passive to active voice), dis-\ncourse (e.g. split long sentences) or lexis\n(e.g. replace complex words with clearer\nsynonyms). Some automatic simpli\fcation\napproaches rely on rule-based or lexicon-\nbased modules to address each linguistic\nlevel. In contrast, data-driven approaches\nmay use machine translation of mono-\nlingual (professional/simpli\fed) corpora|\nat present, mostly via deep-learning-based\nmethods (Van den Bercken et al., 2019;\nSakakini et al., 2020; Devaraj et al., 2021;\nMartin et al., 2021).\nRegardless of the approach, dedicated cor-\npora are need: comparable resources (profes-\nsional and simpli\fed versions of a text) or,\nideally, parallel corpora (texts with almost\nidentical content in di\u000berent registers).\nIn addition to corpora for the English\nlanguage (Van den Bercken et al., 2019;\nSakakini et al., 2020), simpli\fcation text col-\nlections exist for Brazilian Portuguese (Caseli\net al., 2009), German (Klaper et al.,\n2013), Italian (Tonelli et al., 2016) or\nFrench (Grabar and Cardon, 2018; Gala\net al., 2020). Other multilingual resources\nhave been released in challenges for complex\nword identi\fcation (Yimam et al., 2017)\nFor Spanish, there is the EASIER corpus,1\nwith di\u000berent characteristics compared to\nthe CLARA-MeD text collection. First, the\nEASIER corpus is a general domain resource;\neven though some sentences are related to\nhealth topics, the CLARA-MeD resource fo-\ncuses only on the medical domain. Sec-\nond, the EASIER corpus gathers 3977 sen-\ntences annotated with 8155 complex words,\nand 3396 sentences labeled with 7892 sug-\ngested synonyms. The CLARA-MeD corpus\nis not annotated with these data, but fea-\ntures a sentence-level alignment of 3800 pairs\nof technical and simpli\fed sentences, follow-\ning speci\fc criteria ( x3.6). Lastly, besides in-\n1shorturl.at/lvCJUcluding parallel data, the CLARA-MeD cor-\npus is larger in size. These data can be used\nto enlarge the number of aligned sentences\nor can be annotated in more detail in future\nversions.\nSeveral methods have been applied to col-\nlect such type of corpora. Wikipedia and\nSimple Wikipedia have been aligned to ob-\ntain a parallel corpus in the general do-\nmain (Zhu et al., 2010). However, the\ncorrespondence of content between techni-\ncal/simpli\fed versions are often de\fcient (Xu\net al., 2015). Moreover, this method can\nnot be directly extended to languages with-\nout a simpli\fed Wikipedia. In these cases,\nsome teams (Palmero Aprosio et al., 2019;\nRauf et al., 2020) have created synthetic cor-\npora via translations from SimpleWikipedia\nor similar sources such as WikiLarge (Grabar\nand Cardon, 2018). Another method is man-\nual simpli\fcation by domain or task special-\nists (Gala et al., 2020), which assures lin-\nguistic quality but requires an adequate team\nand is more time-consuming. Hybrid meth-\nods have also been conducted for medical En-\nglish (Van den Bercken et al., 2019; Mora-\nmarco et al., 2021).\n3 Methods and Sources\nFigure 1 summarizes the work\row applied to\ncreate the corpus. The \frst stage involved\ncollecting a comparable resource. We gath-\nered medical texts with two versions (for pro-\nfessionals and laymen readers) from sources\nrecently reported (Moreno-Sandoval et al.,\n2019). At the current stage, we have not\nused articles from the Medicine category in\nWikipedia, given that there is not a Spanish\nversion from Simple Wikipedia.\nThe second stage implied matching pro-\nfessional and simpli\fed sentences from each\ncomparable subcorpus. In the following, we\ndetail the data sources to create this resource\n(x3.1-x 3.4), and Section 3.6 explains the cri-\nteria and methods to align sentences.\n3.1 Drug lea\rets and summaries\nof product characteristics\nThe Medicine Online Information Center\n(Centro de Informaci\u0013 on de Medicamentos,\nCIMA)2is a drug-related service and knowl-\nedge database maintained by the Spanish\nAgency of Medicines and Medical Devices\n2https://cima.aemps.es\n190\nLeonardo Campillos-Llanos, Ana R. Terroba Reinares, Sof\u00eda Zakhir Puig, Ana Valverde-Mateos, Adri\u00e1n Capllonch-Carri\u00f3n Figure 1: Methods and stages to compile the corpus.\n(AEMPS). CIMA provides all the informa-\ntion related to drugs prescribed in Spain\nthrough a search engine, the Nomenclator re-\nsource (a rich database of medical drugs),\nand pharmaceutical/pharmacovigilance re-\nports. In addition, the information about\neach medical drug (indication, medical\nbrand, dosage, unit of presentation, etc.) is\nprovided in two types of documents: sum-\nmaries of product characteristics (written in\na professional register and aimed at health-\ncare professionals) and drug lea\rets (with\nsimpler structures or terms, and aimed at pa-\ntients). We downloaded both types of data,\nand release only cleaned, noise-free texts with\nboth versions available (10 211 pairs of texts,\n82 907 317 words).3\n3.2 Systematic reviews\nThe Cochrane Library4is an updated\ndatabase of systematic reviews and metanal-\nyses. This is the main collection of medi-\ncal evidence (Sackett et al., 1996), mostly\nfrom publications reporting results of clini-\ncal trials. Healthcare professionals use this\ndatabase to keep up to date with the latest\nevidence to apply in their clinical practice.\nThe Cochrane Library is a multilingual re-\nsource, although not all reviews are available\nin all languages. Each review presents an ab-\nstract of the full text and also a summary\nin plain language, which is aimed at a non-\nspecialist readership. We collected a total of\n8138 pairs of documents (9 618 698 words).\n3.3 Cancer-related information\nsummaries\nThe National Cancer Institute (NCI) website\npresents a large volume of bilingual (English\nand Spanish) information.5Contents revolve\naround cancer types, disorders and symp-\ntoms, oncological therapies, pharmacological\nsubstances, genetics, screening, prevention,\n3The data from the Cochrane Library cannot be\nreleased without permission.\n4https://www.cochranelibrary.com/\n5https://www.cancer.govpalliative care or patient-oriented counseling.\nNoteworthy, Physician Data Queries (PDQ )\narticles gather updated and evidence-based\ninformation about essential aspects of can-\ncer in two versions: for professionals and\npatients. We collected 201 pairs of PDQ\ntexts, and removed noisy information from\nthe web pages (e.g. URLs, content menus,\ntables, etc). The cleaned texts amount up to\n3 044 461 words.\n3.4 Clinical trials announcements\nThe European Clinical Trials register (Eu-\ndraCT)6gathers public data about all clinical\ntrials conducted in the European Union, ei-\nther at national or multinational level. Clin-\nical trials announcements are published both\nin English and the European language cor-\nresponding to the countries involved in the\ntrial. Trial announcements describe the trial\nprotocols, patients and participants, inter-\nventions, indications and expected outcomes\nof the trial, among others. Two sections\nare written with equivalent contents between\nscienti\fc (aimed at healthcare professionals)\nand popularization levels (aimed at patients\nor laymen users): the public and scienti\fc ti-\ntle of the trial, and the public and scienti\fc\nindication. To gather these data, we reused\n700 texts from the Clinical Trials for Evi-\ndence Based Medicine in Spanish (CT-EBM-\nSP) corpus (Campillos-Llanos et al., 2021).\nWe also downloaded more than 7500 an-\nnouncements from the website, and selected\nonly those with both a public and scienti\fc\nversion of the title and/or indication. Af-\nter \fltering out noisy or redundant data, we\ngathered 5784 pairs of texts (451 690 words).\n3.5 Descriptive statistics\nTable 1 includes excerpts of professional and\nsimpli\fed versions of each data source. Ta-\nble 2 shows the word count of the compara-\nble corpus, and Table 3, the average of sen-\ntences per text and average words per sen-\ntence (with standard deviation, SD). Texts\n6https://www.clinicaltrialsregister.eu\n191\nBuilding a comparable corpus and a benchmark for Spanish medical text simplification Source Professional version Simpli\fed version\nCIMA La administraci\u0013 on concomitante de metamizol con\nmetotrexato u otros antineopl\u0013 asicos puede aumentar\nla toxicidad sangu\u0013 \u0010nea de los antineopl\u0013 asicos particu-\nlarmente en pacientes de edad avanzada.\n`Concomitant administration of metamizole with\nmethotrexate or other antineoplastics may increase the\nblood toxicity of antineoplastics particularly in elderly\npatients.'Si se administra conjuntamente con metotrexato\nu otros medicamentos para el tratamiento de los\ntumores (antineopl\u0013 asicos), puede potenciar los efectos\nt\u0013 oxicos en sangre de los antineopl\u0013 asicos, sobre todo\nen pacientes de edad avanzada.\n`If co-administered with methotrexate or other drugs\nfor the treatment of tumors (antineoplastics), it may\npotentiate the toxic e\u000bects of antineoplastics in the\nblood, especially in elderly patients.'\nCochrane La administraci\u0013 on de suplementos de vitamina D\npodr\u0013 \u0010a disminuir la necesidad de ventilaci\u0013 on mec\u0013 anica\ninvasiva, pero la evidencia es incierta (evidencia de\ncerteza baja).\n`Vitamin D supplementation may decrease need for\ninvasive mechanical ventilation, but the evidence is\nuncertain (low-certainty evidence).'La vitamina D podr\u0013 \u0010a reducir la necesidad de conec-\ntar a los pacientes a un respirador para ayudarles a\nrespirar, pero se desconoce la evidencia.\n`Vitamin D may reduce the need for patients to be\nput on a ventilator to help them breathe, but the\nevidence is uncertain.'\nEudraCT Ensayo cl\u0013 \u0010nico aleatorizado, doble ciego, controlado\ncon placebo, para evaluar la e\fcacia y seguridad\nde la vacuna COMIRNATY (vacuna COVID-19\nARNm, P\fzer-BioNTech) en personas con COVID\npersistente\n`Randomized, double-blind, placebo-controlled clin-\nical trial to evaluate the e\u000ecacy and safety of the\nCOMIRNATY vaccine (COVID-19 mRNA vaccine,\nP\fzer-BioNTech) in people with long COVID'El objetivo del estudio es analizar si la administraci\u0013 on\nde una vacuna contra la infecci\u0013 on COVID19 puede\nhacer disminuir los s\u0013 \u0010ntomas de COVID persistente.\n`The aim of the study is to analyze whether the ad-\nministration of a vaccine against COVID19 infection\ncan reduce the symptoms of long COVID.'\nNCI El LH que se diagnostica durante el primer trimestre\ndel embarazo no constituye un indicador absoluto de\nla necesidad de un aborto terap\u0013 eutico.\n`HL that is diagnosed in the \frst trimester of preg-\nnancy does not constitute an absolute indication for\ntherapeutic abortion.'Cuando el linfoma de Hodgkin se diagnostica durante\nel primer trimestre del embarazo, no siempre signi\fca\nque se aconsejar\u0013 a a la mujer que interrumpa el\nembarazo.\n`When Hodgkin lymphoma is diagnosed in the \frst\ntrimester of pregnancy, it does not necessarily mean\nthat the woman will be advised to end the pregnancy.'\nTable 1: Samples of professional and simpli\fed versions of texts from di\u000berent data sources.\nfrom CIMA (drug lea\rets and summaries\nof product characteristics) and NCI (cancer-\nrelated summaries) are longer. Professional\ntexts from all sources tend to be longer than\nsimpli\fed texts. Likewise, the average sen-\ntence length (number of words per sentence)\nis generally longer in the professional version\nof almost all sources (except for CIMA).\n3.6 Parallel text alignment\nWe aligned 3800 professional-laymen sen-\ntences extracted from the CLARA-MeD cor-\npus. Pairs of annotators with varied back-\ngrounds (a computational linguist, a medical\ndoctor and medical terminologists) matched\nscienti\fc and simpli\fed versions of a subset\nof sentences.\nGathering parallel sentences from the Eu-\ndraCT announcements was straightforward.\nEach clinical trial announcement contains\ntwo versions (for patients/laymen users and\nhealthcare professionals) of speci\fc sections:\nthe public and scienti\fc title of the trial,\nand a public and scienti\fc indication. We\ngathered the data from both versions, whichyielded a preliminary noisy alignment of 5784\nsentence pairs. Similar sentences were re-\njected, and two annotators per data batch\nconducted a manual revision. We followed a\nset of linguistic criteria to accept a sentence\npair as adequate equivalences (x 3.6.2). Prob-\nlematic pairs of sentences were discussed to\nachieve a consensus.\nFor the other sources, we automated the\nalignment by extracting, for each professional\nsentence, the most similar simpli\fed version\n(x3.6.1). After the semi-automatic align-\nment, we followed the same methodology for\nthe manual revision, and two annotators per\ndata batch checked the sentence pairs. We\nthereby \fltered out the most reliable sentence\npairs and assessed the inter-annotator agree-\nment.\n3.6.1 Semi-automatic alignment\nTo gather aligned pairs of sentences, we\nshould combine, for each pair of texts, all pro-\nfessional sentences with all simpli\fed ones.\nNonetheless, the amount of candidate pairs\ncollected in this way is una\u000bordable to be re-\n192\nLeonardo Campillos-Llanos, Ana R. Terroba Reinares, Sof\u00eda Zakhir Puig, Ana Valverde-Mateos, Adri\u00e1n Capllonch-Carri\u00f3n Source Text pairs Professional Simpli\fed Total\nCIMA 10 211 55 463 410 27 443 907 82 907 317\nCochrane abstracts 8138 6 235 454 3 383 244 9 618 698\nEudraCT 5748 255 902 195 788 451 690\nNCI 201 2 093 569 955 480 3 049 049\nTotal 24 298 64 048 335 31 978 419 96 022 166\nTable 2: Word count per source data of the comparable corpus.\nSource Professional Simpli\fed\nCIMA Avg sentences per text (\u0006SD) 431.72 (\u0006234.47) 210.03 (\u000671.96)\nAvg words per sentence ( \u0006SD) 12.36 (\u00062.48) 12.76 (\u00061.61)\nCochrane abstracts Avg sentences per text ( \u0006SD) 34.06 (\u00069.05) 19.57 (\u000611.70)\nAvg words per sentence ( \u0006SD) 22.08 (\u00063.94) 22.02 (\u00064.39)\nEudraCT Avg sentences per text (\u0006SD) 1.77 (\u00060.65) 1.75 (\u00060.62)\nAvg words per sentence ( \u0006SD) 26.47 (\u000611.20) 19.73 (\u00068.74)\nNCI Avg sentences per text (\u0006SD) 505.53 (\u0006492.57) 309.39 (\u0006177.61)\nAvg words per sentence ( \u0006SD) 20.53 (\u00063.75) 15.47 (\u00061.49)\nTable 3: Average (avg) sentences per text and average words per sentence ( \u0006standard deviation).\nvised, and only very few pairs will be ade-\nquate alignments.\nPrevious work (Cardon and Grabar, 2020)\nhas reduced this search space by lever-\naging parsing information and developing\na machine-learning classi\fer using features\nsuch as sentence length, the Levenshtein dis-\ntance or number of shared words between\neach version, among others. The alignment\nmay also be automated by means of tools\nsuch as CATS ( \u0014Stajner et al., 2018), which\nextracts similar sentences according to char-\nacter n-grams, the average of word embed-\ndings (WAVG) in the sentence or the continu-\nous word alignment-based similarity analysis\nmodel. Another tool is MASSAlign (Paet-\nzold et al., 2017), which aligns paragraphs\nor sentences by computing a TF-IDF-based\nsimilarity matrix of items between each pair\nof texts, and a vicinity procedure to complete\nthe alignment iteratively.\nNonetheless, we experimented with a\nstate-of-the-art procedure, BERT-based Sen-\ntence Embeddings (Reimers and Gurevych,\n2019). With this method, we obtained an\nembedding representation of each sentence,\nand compute the cosine similarity between\nthe professional and simpli\fed version. We\napplied a threshold set empirically on the\ncosine similarity score ( cosine > 0.6). We\ndiscarded sentences that were not similarenough, to obtain a subset to be revised man-\nually later. We provide a companion python\njupyter notebook to reproduce our method.7\n3.6.2 Alignment criteria\nWe adopted the guidelines from Grabar and\nCardon (2018) to align pairs of sentences (e.g.\nidentical pairs are not used), and we added\nnew rules. Table 4 summarizes our criteria.\nWe followed these criteria and rejected\nthose sentences that each pair of annota-\ntors per data batch judged as bad align-\nments. Disagreements were discussed to\nachieve a consensus, and the medical practi-\ntioner solved speci\fc questions about medical\naspects of the contents. We aligned a total of\n3800 sentence pairs (149 862 words). The av-\nerage inter-annotator agreement between ex-\nperts was of kappa = 0.839 (\u00060.076), which\nrepresents an almost perfect agreement.\n4 Conclusions and future work\nThis work has presented the CLARA-MeD\ncorpus of comparable (professional/laymen)\nmedical texts in Spanish, a new contribu-\ntion for analysing text simpli\fcation strate-\ngies and conduct experiments on text simpli-\n\fcation tasks. A limitation of our work is the\nscarce data obtained to train and test data-\nintensive methods (e.g. deep-learning). More\n7https://github.com/lcampillos/CLARA-MeD/\n193\nBuilding a comparable corpus and a benchmark for Spanish medical text simplification 1. We prioritize aligning one-to-one sentences; however, in some cases, one simpli\fed\nsentence needs to be aligned with two professional ones, and vice versa:\nP:Linfoma folicular recidivante/resistente (`Relapsed/refractory follicular lymphoma')\nS:El linfoma folicular es un c\u0013 ancer que afecta a los gl\u0013 obulos blancos llamados linfocitos.\nEl t\u0013 ermino reca\u0013 \u0010da o refractaria indica una enfermedad que vuelve a crecer o no\nresponde al tratamiento (`Follicular lymphoma is a cancer that a\u000bects white blood cells\ncalled lymphocytes. The term relapsed or refractory indicates disease that grows again\nor does not respond to the treatment.')\n2. Sentence pairs that only di\u000ber in punctuation or functional words (e.g. preposi-\ntions or adverbs) are not aligned if the simpli\fed version does not have a simpler structure.\n3. Simpli\fed sentences that have unintelligible acronyms without their explanation or\nexpansion are not used (we except widely-used acronyms: e.g. SIDA , `AIDS'):\nP:Cancer colorectal (CCR) (`Colorectal cancer (CRC)')\nS:El CCR es el desarrollo del c\u0013 ancer desde el colon o el recto (Not aligned)\n(`CRC is the development of cancer from the colon or rectum')\n4. Professional and simpli\fed sentences are not aligned if the simpli\fed version presents\na large loss of essential information that is present in the professional version:\nP:Tratamiento del s\u0013 \u0010ndrome de Hunter y deterioro cognitivo\n(`Treatment of Hunter syndrome and cognitive impairment')\nS:S\u0013 \u0010ndrome de Hunter: de\fciencia de la enzima iduronato 2-sulfatasa (Not aligned)\n(`Hunter syndrome-Iduronate-2-Sulfatase enzyme de\fciency')\n5. We do not align sentence pairs with incoherent data, imprecise information or contra-\ndictions between the professional and the simpli\fed version:\nP:Diabetes mellitus tipo 1 (`Type 1 Diabetes Mellitus')\nS:Altos niveles de azucar (glucosa) en sangre (Not aligned)\n(`High levels of sugar (glucose) in the blood')\n6. Sentences consisting of paraphrases, de\fnitions or explanations of technical terms are\nconsidered adequate simpli\fed versions and can be aligned with the professional version:\nP:Colitis ulcerosa (`Ulcerative Colitis')\nS:La colitis ulcerosa es una enfermedad in\ramatoria intestinal que provoca in\ramaci\u0013 on\nen el revestimiento del intestino grueso con irritaci\u0013 on e hinchaz\u0013 on (`Ulcerative Colitis\nis a type of in\rammatory bowel disease that causes the lining of the large intestine\n(colon) to become in\ramed (irritated and swollen)') (Aligned)\n7. Aligned sentences may have redundant information or elliptic words (e.g. prepositions),\nminor spelling, grammar or typographic errors, provided that the meaning is not distorted:\nP:Neumon\u0013 \u0010a por SARS-CoV-2 (`SARS-CoV-2 infected patients with pneumonia')\nS:Neumon\u0013 \u0010a COVID (`COVID-Pneumonia') (Aligned)\nTable 4: Alignment criteria with examples ( P: professional; S: simpli\fed).\ndrug-related documents from CIMA need to\nbe cleaned and revised to be used. More-\nover, more types of comparable data could\nbe obtained from medical websites with two\nversions (professional-oriented and patient-oriented contents). In addition to this, the\nmethodology applied by van der Bercken et\nal. (2019) could be useful to widen the cover-\nage of Spanish medical texts from Wikipedia,\nand thus include this source in the cor-\n194\nLeonardo Campillos-Llanos, Ana R. Terroba Reinares, Sof\u00eda Zakhir Puig, Ana Valverde-Mateos, Adri\u00e1n Capllonch-Carri\u00f3n pus. Sometimes, the sentence-level alignment\nmight not be satisfactory because there is\nnot always a one-to-one correspondence. In\nsuch cases, a paragraph-level alignment is a\nmore suitable option (Devaraj et al., 2021).\nLastly, the corpus is not annotated with com-\nplex words, which would be useful in complex\nword identi\fcation (CWI) tasks.\nEven so, this is the \frst version of a bench-\nmark for medical text simpli\fcation in the\nSpanish language. The high inter-annotator\nagreement values show a \fne sentence-level\nalignment, which was assessed by linguists, a\nlexicographer and with the advice of a health\nprofessional. This ensures that these data are\na quality benchmark for developing and test-\ning medical text simpli\fcation systems.\nAcknowledgements\nWe thank the reviewers for their valu-\nable comments to improve this work.\nProject CLARA-MED (PID2020-\n116001RA-C33) funded by MCIN/AEI/\n10.13039/501100011033/, in project call:\n\\Proyectos I+D+i Retos Investigaci\u0013 on\".\nReferences\nBarbu, E., Mart\u0013 \u0010n-Valdivia, M. T., Martinez-\nCamara, E., and Urena-L\u0013 opez, L. A.\n(2015). Language technologies applied to\ndocument simpli\fcation for helping autis-\ntic people. Expert Systems with Applica-\ntions, 42(12):5076{5086.\nCampillos-Llanos, L., Valverde-Mateos, A.,\nCapllonch-Carri\u0013 on, A., and Moreno-\nSandoval, A. (2021). A clinical tri-\nals corpus annotated with UMLS entities\nto enhance the access to evidence-based\nmedicine. BMC medical informatics and\ndecision making , 21(1):1{19.\nCardon, R. and Grabar, N. (2020). Construc-\ntion d'un corpus parall\u0012 ele \u0012 a partir de cor-\npus comparables pour la simpli\fcation de\ntextes m\u0013 edicaux en fran\u0018 cais. Traitement\nAutomatique des Langues, 61(2):15{39.\nCaseli, H. M., Pereira, T. F., Specia, L.,\nPardo, T. A., Gasperin, C., and Alu\u0013 \u0010sio,\nS. M. (2009). Building a Brazilian Por-\ntuguese parallel corpus of original and\nsimpli\fed texts. Proc. of 10th CICLing ,\n41:59{70.\nDevaraj, A., Marshall, I., Wallace, B., and\nLi, J. J. (2021). Paragraph-level simpli-\fcation of medical texts. In Proc. of the\nNAACL 2021, pages 4972{4984.\nGala, N., Tack, A., Javourey-Drevet, L.,\nFran\u0018 cois, T., and Ziegler, J. C. (2020).\nAlector: A parallel corpus of simpli\fed\nFrench texts with alignments of misread-\nings by poor and dyslexic readers. In Proc.\nof LREC 2020 , page 1353{1361.\nGrabar, N. and Cardon, R. (2018). CLEAR\n- Simple corpus for medical French. In\nProc. of the 1st Workshop on Automatic\nText Adaptation (ATA), pages 3{9.\nKindig, D. A., Panzer, A. M., Nielsen-\nBohlman, L., et al. (2004). Health literacy:\na prescription to end confusion. Washing-\nton (DC): National Academies Press.\nKlaper, D., Ebling, S., and Volk, M. (2013).\nBuilding a German/simple German par-\nallel corpus for automatic text simpli\fca-\ntion. In Proc. of the 2nd Workshop on\nPredicting and Improving Text Readabil-\nity for Target Reader Populations (PITR\n2013), So\fa, Bulgaria.\nMartin, L., Fan, A., de la Clergerie, \u0013E., Bor-\ndes, A., and Sagot, B. (2021). Muss:\nMultilingual unsupervised sentence sim-\npli\fcation by mining paraphrases. arXiv\npreprint arXiv:2005.00352.\nMoramarco, F., Juric, D., Savkov, A., Flann,\nJ., Lehl, M., Boda, K., Grafen, T., Zhelez-\nniak, V., Gohil, S., Kor\fatis, A. P., et al.\n(2021). Towards more patient friendly\nclinical notes through language models\nand ontologies. In Proc. of the AMIA An-\nnual Symposium, pages 881{890.\nMoreno-Sandoval, A., Torre-Toledano, D.,\nValverde-Mateos, A., and Campillos-\nLlanos, L. (2019). Estudio sobre\ndocumentos reutilizables como recursos\nling\u007f u\u0013 \u0010sticos en el marco del desarrollo\ndel plan de impulso de las tecnolog\u0013 \u0010as\ndel lenguaje. Procesamiento del Lenguaje\nNatural, 63:167{170.\nPaetzold, G., Alva-Manchego, F., and Spe-\ncia, L. (2017). Massalign: Alignment and\nannotation of comparable documents. In\nProceedings of the IJCNLP 2017, System\nDemonstrations, pages 1{4.\nPalmero Aprosio, A., Tonelli, S., Turchi,\nM., Negri, M., and Di Gangi Mattia, A.\n195\nBuilding a comparable corpus and a benchmark for Spanish medical text simplification (2019). Neural text simpli\fcation in low-\nresource conditions using weak supervi-\nsion. In Workshop on Methods for Op-\ntimizing and Evaluating Neural Language\nGeneration (NeuralGen) , pages 37{44.\nPetersen, S. E. and Ostendorf, M. (2007).\nText simpli\fcation for language learn-\ners: a corpus analysis. In Workshop on\nspeech and language technology in educa-\ntion. Citeseer.\nRauf, S. A., Ligozat, A.-L., Yvon, F., Illouz,\nG., and Hamon, T. (2020). Simpli\fcation\nautomatique de texte dans un contexte de\nfaibles ressources. In Actes 6e conf\u0013 erence\nTraitement Automatique des Langues Na-\nturelles (TALN), vol. 2, pages 332{341.\nReimers, N. and Gurevych, I. (2019).\nSentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proc.\nof the 2019 Conference on Empirical\nMethods in Natural Language Processing,\npages 3982{3992.\nSackett, D. L., Rosenberg, W. M., Gray,\nJ. M., Haynes, R. B., and Richardson,\nW. S. (1996). Evidence based medicine:\nwhat it is and what it isn't. British Med-\nical Journal , 312(7023):71{72.\nSaggion, H., G\u0013 omez-Mart\u0013 \u0010nez, E., Etayo,\nE., Anula, A., and Bourg, L. (2011).\nText simpli\fcation in simplext: Making\ntexts more accessible. Procesamiento del\nlenguaje natural , (47):341{342.\nSakakini, T., Lee, J. Y., Duri, A., Azevedo,\nR. F., Sadauskas, V., Gu, K., Bhat, S.,\nMorrow, D., Graumlich, J., Walayat, S.,\net al. (2020). Context-aware automatic\ntext simpli\fcation of health materials in\nlow-resource domains. In Proc. of the 11th\nLOUHI Workshop, pages 115{126.\nScarton, C., Paetzold, G., and Specia, L.\n(2018). Simpa: A sentence-level simpli-\n\fcation corpus for the public administra-\ntion domain. In Proc. of LREC 2018,\npages 4333{4338.\n\u0014Stajner, S., Franco-Salvador, M., Rosso, P.,\nand Ponzetto, S. P. (2018). CATS: A tool\nfor customized alignment of text simpli\f-\ncation corpora. In Proc. of LREC 2018,\npages 3895{3903.\nTonelli, S., Aprosio, A. P., and Saltori, F.\n(2016). SIMPITIKI: a Simpli\fcation cor-pus for Italian. In CLiC-it/EVALITA,\npages 4333{4338.\nVan den Bercken, L., Sips, R.-J., and Lo\f,\nC. (2019). Evaluating neural text simpli-\n\fcation in the medical domain. In Proc.\nof the World Wide Web Conference, pages\n3286{3292.\nXu, W., Callison-Burch, C., and Napoles,\nC. (2015). Problems in current text sim-\npli\fcation research: New data can help.\nTransactions of the Association for Com-\nputational Linguistics, 3:283{297.\nYimam, S. M., \u0014Stajner, S., Riedl, M., and\nBiemann, C. (2017). Multilingual and\ncross-lingual complex word identi\fcation.\nInProc. of the Int. Conference Recent Ad-\nvances in Natural Language Processing,\nRANLP 2017, pages 813{822.\nZhu, Z., Bernhard, D., and Gurevych, I.\n(2010). A monolingual tree-based trans-\nlation model for sentence simpli\fcation.\nInProc. of the 23rd Intern. Conference\non Computational Linguistics (COLING\n2010), pages 1353{1361, Beijing, China.\n196\nLeonardo Campillos-Llanos, Ana R. Terroba Reinares, Sof\u00eda Zakhir Puig, Ana Valverde-Mateos, Adri\u00e1n Capllonch-Carri\u00f3n Ibe\nrLEF 2022: Res\u00famenes \nde las tareas de evaluaci\u00f3nABSAPT 2022 at IberLEF: Overview of the Task onAspect-Based Sentiment Analysis in Portuguese\nResumen de la Tarea de An\u0013 alisis de Sentimientos Basado en\nAspectos en Portugu\u0013 es (ABSAPT) en IberLEF 2022\nFelix L. V. da Silva1, Guilherme da S. Xavier1, Heliks M. Mensenburg1,\nRodrigo F. Rodrigues1, Leonardo P. dos Santos3, Ricardo M. Ara\u0013 ujo1 2,\nUlisses B. Corr^ ea1 2, Larissa A. de Freitas1 2\n1Center for Technological Development (CDTec),\nFederal University of Pelotas (UFPel), Pelotas, RS, Brazil\n2Arti\fcial Intelligence Innovation Hub (H2IA),\nFederal University of Pelotas, Pelotas, RS, Brazil\n3University of S~ ao Paulo, S~ ao Paulo, SP, Brazil\nf\rvdsilva, gdsxavier, hmmersenburg, rfrodrigues, ricardo, ub.correa, larissag@inf.ufpel.edu.br,\nleonardosantsper@gmail.com\nResumen: Este art\u0013 \u0010culo presenta la Tarea sobre An\u0013 alisis de Sentimientos basado\nen Aspectos en Portugu\u0013 es (ABSAPT), realizada en el IberLEF 2022. Les pedimos\na los participantes que desarrollaran sistemas capaces de identi\fcar aspectos (AE)\ny extraer la polaridad (ASC) en textos escritos en portugu\u0013 es. Doce equipos se\ninscribieron en la tarea, entre los cuales cinco presentaron predicciones e informes\nt\u0013 ecnicos. El sistema con mejor rendimiento logr\u0013 o un valor de precisi\u0013 on (Acc) de\n0,67 para la subtarea de AE (Equipo Deep Learning Brasil) y un valor de precisi\u0013 on\nequilibrada (Bacc) de 0,82 para la subtarea de ASC (Equipo Deep Learning Brasil).\nPalabras clave: An\u0013 alisis de Sentimiento basado en Aspectos, Portugu\u0013 es, Rese~ nas\nde Hoteles.\nAbstract: This paper presents the task on Aspect-Based Sentiment Analysis in\nPortuguese (ABSAPT), held within Iberian Languages Evaluation Forum (IberLEF\n2022). We asked the participants to develop systems capable of extracting aspects\n(AE) and classifying sentiment of aspects (ASC) in texts. We created one cor-\npora containing reviews about hotels. Twelve teams registered to the task, among\nwhich \fve submitted predictions and technical reports. The best performing sys-\ntem achieved an Accuracy (Acc) value of 0.67 in AE sub-task (Team Deep Learning\nBrasil) and a Balanced Accuracy (Bacc) value of 0.82 in ASC sub-task (Team Deep\nLearning Brasil).\nKeywords: Aspect-Based Sentiment Analysis, Portuguese, Hotel Reviews.\n1 Introduction\nSentiment Analysis (SA) is the \feld of Nat-\nural Language Processing (NLP) that au-\ntomatically analyzes people's sentiments or\nopinions towards some entity. These senti-\nments can be valuable sources of information\nabout the consumer's feelings about a par-\nticular product or idea, which can help in\ndecisions by companies or governments (Liu,\n2015).SA can be done on di\u000berent levels, focus-\ning mainly on three possible granularity lev-\nels: document level, sentence level, and as-\npect level (de Freitas, 2015). At the aspect\nlevel, it is possible to analyze di\u000berent opin-\nions held towards di\u000berent aspects of some\nentity or di\u000berent entities in the same docu-\nment or sentence.\nThe ABSAPT 2022 task aims to challenge\ndi\u000berent teams to propose techniques capa-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 199-205\nrecibido 05-07-2022 revisado 22-07-2022 aceptado 25-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-17\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalble of extracting aspects and classifying sen-\ntiment of aspects in the hotel reviews. The\npresent paper presents an overview of the\ntask. First, we brie\ry present some theoreti-\ncal re\rections Aspect-Based Sentiment Anal-\nysis (ABSA) (Section 2) and describe the\nproposal of our task (Section 3). Section 4\npresents the corpora description and the an-\nnotation process. In Section 5, we describe\nthe evaluation measures. Participant systems\nand the results are discussed in Section 6. Fi-\nnally, the \fnal remarks are done in Section 7.\n2 Aspect-Based Sentiment\nAnalysis\nOn this granularity level, all opinions ex-\npressed towards any aspect of the entity are\nanalyzed individually. This level allows a bet-\nter understanding of the opinions and entities\nin the text. To accomplish the analysis on\nthis level, the task used to be broken into\ntwo sub-tasks: Aspect Extraction (AE) and\nAspect Sentiment Classi\fcation (ASC).\n2.1 Aspect Extraction\nThis sub-task determines which aspects of a\ngiven entity are considered in a text.\nFor example, in the sentence \\Hotel com\nboalocaliza\u0018 c~ ao \" [\\Hotel with good local-\nization\"], the goal of AE would be to iden-\ntify the aspect ` localiza\u0018 c~ ao '.\n2.2 Aspect Sentiment\nClassi\fcation\nThis sub-task consists of the classi\fcation of\nthe polarity for each aspect that has been\nidenti\fed in the text.\nFor example, in the sentence \\Hotel com\nboalocaliza\u0018 c~ ao \" [\\Hotel with good local-\nization\"], the goal of ASC would be to clas-\nsify the aspect ` localiza\u0018 c~ ao ' as positive.\n3 Task Description\nPeople's opinions are a great source of infor-\nmation for other people and organizations,\npublic or private. Typically works focused\non Portuguese perform document level SA. It\nis hard to \fnd ABSA approaches or datasets\navailable for Portuguese.\nWe propose to create an ABSA for Tri-\npAdvisor reviews written in Portuguese. Two\nsub-tasks will be available: AE and ASC. The\n\frst sub-task comprehends the identi\fcation\nof aspects in the reviews, and the second sub-\ntask proposes to extract the sentiment orien-tation (polarity) of the review about a single\naspect mentioned in it.\nThe availability of corpora written in Por-\ntuguese is scarce, which limits the amount of\nresearch done for this language.\nThis task will contribute to the progress\nof Portuguese NLP, as there is a demand for\ndeveloping new methods and tools.\nPrevious ABSA competitions, such as Se-\nmEval 2014 (Pontiki et al., 2014), SemEval\n2015 (Pontiki et al., 2015), SemEval 2016\n(Pontiki et al., 2016) and EVALITA (Mattei\net al., 2020) inspired us to develop a speci\fc\ntask for Portuguese.\n4 Corpora Description and\nAnnotation Process\nThis section describes the corpora proposed\nfor evaluation and the annotation process\n(annotation guidelines and inter-annotator\nagreement).\n4.1 Corpora Description\nThe corpora contain travellers\u0013 reviews about\naccommodation services companies, written\nin Portuguese. In ABSAPT 2022, we used\ncorpora developed previously by Freitas (de\nFreitas, 2015) and Corr^ ea (Corr^ ea, 2021).\nFreitas\u0013 corpus is publicly available, so it will\nbe used only in the training dataset (3111\nsamples from 847 reviews). Corr^ ea's cor-\npus is private and will be split into train-\ning and test dataset (257 samples to AE and\n686 samples to ASC). The full dataset will\nbe available after the event on the website\nhttp://absapt2022.tk/.\nBoth datasets were annotated following\nthe same annotation guidelines (de Freitas,\n2015). The concepts on the Accommoda-\ntion Services Domain Ontology, HOntology\n(Chaves, de Freitas, and Vieira, 2012) were\naspects annotated. HOntology contains 282\nconcepts categorized into 16 top-level con-\ncepts. The concept hierarchy has a maximum\ndepth of 5.\nIn Figure 1, one can see an overview of the\ntraining dataset. The full training dataset\ncontains 77 aspects. Due to space limitation,\nwe present the 40 most frequent aspects and\nthe polarities distribution. In Figure 2, we\npresent an overview of the test dataset.\n4.2 Annotation Process\nThe manual annotation of Freitas' corpus\n(training dataset) was conducted by two an-\nFelix L. V. da Silva, Guilherme da S. Xavier, Heliks M. Mensenburg, Rodrigo F. Rodrigues, Leonardo P. dos Santos, \nRicardo M. Ara\u00fajo, Ulisses B. Corr \u00eaa, Larissa A. de Freitas\n200Figure 1: Training dataset: polarity frequency for aspects with at least 10 samples.\nnotators, both native speakers of Portuguese,\none linguist, and one computer scientist. And\nthe manual annotation of Corr^ ea's corpus\n(training and test dataset) was conducted by\ntwelve students and professors of computer\nscience and engineering annotators. Bothused the tool developed in the context of (de\nFreitas, 2015).\nThe agreement between annotators (Fre-\nitas' corpus) was measured with Kappa\nStatistics (Landis and Koch, 1977). The an-\nnotators agreement about ABSA from do-\nABSAPT 2022 at IberLEF: Overview of the Task on Aspect-Based Sentiment Analysis in Portuguese\n201Figure 2: Test dataset: polarity frequency for aspects with at least 10 samples.\nmain ontology using Kappa was 0.58 for ex-\nplicit aspects, which is considered a moderate\nagreement. We believe that the annotation\nhas an acceptable Kappa value. It is also im-\nportant to note that only in a few cases the\nannotators disagreed between negative and\npositive polarities, the majority of disagree-\nments was about positive and neutral polar-\nities, or negative and neutral polarities.\nThe agreement between annotators\n(Corr^ ea's corpus) was measured with Fleiss\nKappa (Fleiss, 1971) suitable for more than\ntwo annotators. The majority annotator\ngroup share k >0.4 (moderate agreement).\n5 Evaluation Measures\nThe training set was released on April 08th,\nand participants had sixteen days to train\ntheir systems. The test set was released on\nApril 24th, and each participant had twelve\ndays to submit one run.\nParticipating teams will receive training\nand test datasets. The latter was sent with-\nout the label of the samples.\nWe evaluated the predictions sent by the\nparticipants using several metrics: Acc (Eq.\n1), Precision (Eq. 2), Recall (Eq. 3), F1\n(Eq. 4), and Bacc (Eq. 6). Bacc to rank\ncompetitors. The submissions will be ranked\naccording to Acc in AE sub-task and Bacc in\nASC sub-task.Acc=True Positives +True Negatives\nTotal Number of Instances\n(1)\nPrecision =True Positives\nTrue Positives +True Negatives\n(2)\nRecall =True Positives\nTrue Positives +False Negatives\n(3)\nF1 = 2\u0002Precision\u0001Recall\nPrecision +Recall(4)\nSpecificity =True Negatives\nTrue Negatives +False Positives\n(5)\nBacc =(Recall +Specificity )\n2(6)\n6 Participants Systems and\nDiscussion of the Results\nTwelve teams registered for the task, among\nwhich \fve submitted predictions and techni-\ncal reports. Participants are from universities\nFelix L. V. da Silva, Guilherme da S. Xavier, Heliks M. Mensenburg, Rodrigo F. Rodrigues, Leonardo P. dos Santos, \nRicardo M. Ara\u00fajo, Ulisses B. Corr \u00eaa, Larissa A. de Freitas\n202and institutes in Brazil (UFG, UFPI, IFPI,\nUFSCAR, USP, UFPR and UESC).\nParticipants used rules and lexicon (Team\nUFSCAR), traditional machine learning ap-\nproaches as CRF (Team NILC and Team\nUFPR), and deep learning methods as Trans-\nformers (Team Deep Learning Brasil, Team\nPiLN, and Team UFPR).\nTables 1 and 2 present participants' re-\nsults for each sub-task submitted run. The\nresults are ranked according to the Acc in\nAE and Bacc in ASC.\nAccuracy Team\n0.67 TeamDeepLearningBrasil\n0.65 TeamPiLN\n0.59 TeamUFSCAR\n0.22 TeamNILC\n0.17 TeamUFPR\nTable 1: Participants results ranked in terms\nof Acc in AE sub-task.\nFor each system, best run is highlighted\nin bold. Team Deep Learning Brasil, used\ntransformers to achieve an Acc of 0.67 in AE\nand a Bacc of 0.82 in ASC.\nBelow we summarize the proposed ap-\nproach of each team:\n\u000fTeam Deep Leaning Brasil: The au-\nthors proposed di\u000berent methodologies\nfor both sub-tasks of ABSA. The AE\nused a single sentence tagging approach,\nand the ASC tested with two di\u000ber-\nent strategies, one as a Sentence Pair\nClassi\fcation and the other as a Con-\nditional Text Generation. In addition,\nalso were used other ABSA datasets such\nas Evalita, MAMs, Semeval 2014, 2015,\nand 2016 competition, and Bidirectional\nEncoder Representations from Trans-\nformers (BERT) pre-trained models on\nthe Portuguese language and multilin-\ngual. The proposed approach reached\nthe best-performing systems, achieving\nnew state-of-the-art results on both sub-\ntasks (Gomes et al., 2022).\n\u000fTeam PiLN: The authors proposed\nsimple and well-known approaches to\nthe sub-tasks of ABSA. The AE used\na string-match strategy using a mul-\ntilingual ontology for the accommoda-\ntion sector named HOntology. The ASC\nused a BERTimbau (BERT pre-trainedmodel on the Portuguese language), ap-\nproaching the reviews as a Sentence Pair\nClassi\fcation. The proposed approach\nreached the second best-performing sys-\ntem, achieving the following results: Acc\nof 0.65 in AE, Bacc of 0.78, F1 of 0.77,\nPrecision of 0.76, Recall of 0.78 in ASC\n(Neto et al., 2022).\n\u000fTeam UFSCAR: The AE sub-task in-\ncludes preprocessing, tokenization, fea-\nture extraction, a lexicon, and rule-\nbased aspect identi\fcation. The ASC\nsub-task has two main steps: meaningful\nsurroundings extraction and sentiment\nextraction using GoEmotions (Dem-\nszky et al., 2020), followed by polar-\nity extraction. The proposed approach\nreached the third best-performing sys-\ntem, achieving the following results: Acc\nof 0.59 in AE, Bacc of 0.62, F1 of 0.61,\nPrecision of 0.65, Recall of 0.62 in ASC\n(Assi et al., 2022).\n\u000fTeam NILC: The authors participated\nonly in AE sub-task. Its approach is\nbased on the Conditional Random Fields\n(CRF) machine learning algorithm com-\nbined with a post-processing step. Af-\nter applying the method, the authors\nperformed an error analysis of detected\nand non-detected aspects (Machado and\nPardo, 2022).\n\u000fTeam UFPR: In the AE used CRF,\nthe dataset was adapted with tokeniza-\ntion and the POS Tagging technique,\nand a pre-trained model for Portuguese\nwas used for the POS Tagging process.\nIf the POS Tagging process has a bet-\nter adaptation for Portuguese, there will\nbe a gain in results for the CRF perfor-\nmance. In the ASC used BERTimbau\n(Heinrich and Marchi, 2022). The pro-\nposed approach reached the worst run,\nachieving the Acc of 0.17 in AE.\n7 Final Remarks\nMotivated by the necessity of improvements\nin the ABSA task focused on Portuguese, we\nproposed a task within the IberLEF 2022.\nThis paper overviews the \frst task on ABSA\nin Portuguese to identify aspects and extract\nthe polarity in hotel reviews.\nThe datasets (training and test) have been\nmanually annotated. The inter-annotator\nABSAPT 2022 at IberLEF: Overview of the Task on Aspect-Based Sentiment Analysis in Portuguese\n203Bacc F1 Precision Recall Team\n0.82 0.81 0.81 0.82 TeamDeepLearningBrasil\n0.78 0.77 0.76 0.78 TeamPiLN\n0.62 0.61 0.65 0.62 TeamUFSCAR\n0.62 0.61 0.65 0.62 TeamUFPR\nTable 2: Participants results ranked in terms of Bacc in ASC sub-task.\nagreement for training and test dataset is\nconsidered moderate.\nThe deep learning methods based on\nTransformers performed better than ap-\nproaches based on rules and lexicons. The\nDeep Learning Brasil Team achieved a Bacc\nof 0.67 for the AE and Acc of 0.82 for the\nASC, while the UFSCAR Team achieved a\nBacc of 0.59 for the AE and Acc of 0.62 for\nthe ASC.\nAcknowledgments\nThank you to all annotators for their essen-\ntial work. We gratefully acknowledge the\nsupport of NVIDIA Corporation with the do-\nnation of the Titan X Pascal GPU used for\nthis research. This work was \fnanced in part\nby the following Brazilian research agencies:\nCAPES and CNPq.\nReferences\nAssi, F. M., G. B. Candido, L. N.\ndos Santos Silva, D. F. Silva, and\nH. de Medeiros Caseli. 2022. Ufs-\ncar's team at absapt 2022: Using syn-\ntax, semantics and context for solving\nthe tasks. In Proceedings of the Iberian\nLanguages Evaluation F\u0013 orum (IberLEF\n2022), co-located with the 38th Conference\nof the Spanish Society for Natural Lan-\nguage Processing (SEPLN 2022), Online.\nCEUR.org.\nChaves, M. S., L. A. de Freitas, and R. Vieira.\n2012. Hontology: A multilingual on-\ntology for the accommodation sector in\nthe tourism industry. In J. Filipe and\nJ. L. G. Dietz, editors, KEOD, pages 149{\n154. SciTePress.\nCorr^ ea, U. B. 2021. An\u0013 alise de sentimento\nbaseada em aspectos usando aprendizado\nprofundo: uma proposta aplicada \u0012 a l\u0013 \u0010ngua\nportuguesa. Ph.D. thesis, Universidade\nFederal de Pelotas, Pelotas.\nde Freitas, L. A. 2015. Feature-level sen-\ntiment analysis applied to Brazilian Por-tuguese reviews . Ph.D. thesis, Pontif\u0013 \u0010cia\nUniversidade Cat\u0013 olica do Rio Grande do\nSul, Porto Alegre.\nDemszky, D., D. Movshovitz-Attias, J. Ko,\nA. Cowen, G. Nemade, and S. Ravi. 2020.\nGoEmotions: A dataset of \fne-grained\nemotions. In Proceedings of the 58th An-\nnual Meeting of the Association for Com-\nputational Linguistics, pages 4040{4054,\nOnline, July. Association for Computa-\ntional Linguistics.\nFleiss, J. L. 1971. Measuring nominal scale\nagreement among many raters. Psycho-\nlogical Bulletin , 76(5):378{382.\nGomes, J. R. S., R. C. Rodrigues, E. A. S.\nGarcia, A. F. B. Junior, D. F. C. Silva,\nand D. F. Maia. 2022. Deep learning\nbrasil at absapt 2022: Portuguese trans-\nformer ensemble approaches. In Proceed-\nings of the Iberian Languages Evaluation\nF\u0013 orum (IberLEF 2022), co-located with\nthe 38th Conference of the Spanish Society\nfor Natural Language Processing (SEPLN\n2022), Online. CEUR.org.\nHeinrich, T. and F. Marchi. 2022. Tea-\nmufpr at absapt 2022: Aspect extrac-\ntion with crf and bert. In Proceedings of\nthe Iberian Languages Evaluation F\u0013 orum\n(IberLEF 2022), co-located with the 38th\nConference of the Spanish Society for Nat-\nural Language Processing (SEPLN 2022),\nOnline. CEUR.org.\nLandis, J. R. and G. G. Koch. 1977. The\nmeasurement of observer agreement for\ncategorical data. Biometrics, 33(1).\nLiu, B. 2015. Sentiment Analysis: Min-\ning Opinions, Sentiments, and Emotions .\nCambridge University Press.\nMachado, M. T. and T. A. S. Pardo. 2022.\nNilc at absapt 2022: Aspect extraction for\nportuguese. In Proceedings of the Iberian\nLanguages Evaluation F\u0013 orum (IberLEF\n2022), co-located with the 38th Conference\nFelix L. V. da Silva, Guilherme da S. Xavier, Heliks M. Mensenburg, Rodrigo F. Rodrigues, Leonardo P. dos Santos, \nRicardo M. Ara\u00fajo, Ulisses B. Cor r\u00eaa, Larissa A. de Freitas\n204of the Spanish Society for Natural Lan-\nguage Processing (SEPLN 2022), Online.\nCEUR.org.\nMattei, L. D., G. D. Martino, A. Iovine,\nA. Miaschi, M. Polignano, and G. Ram-\nbelli. 2020. Ate absita@ evalita2020:\nOverview of the aspect term extraction\nand aspect-based sentiment analysis task.\nProceedings of the 7th Evaluation Cam-\npaign of Natural Language Processing and\nSpeech tools for Italian (EVALITA 2020),\nOnline. CEUR.org.\nNeto, F. A. R., R. F. de Sousa, R. L.\nde S. Santos, R. T. Anchi^ eta, and R. S.\nMoura. 2022. Piln at absapt 2022: Lex-\nical and bert strategies for aspect-based\nsentiment analysis in portuguese. In Pro-\nceedings of the Iberian Languages Evalu-\nation F\u0013 orum (IberLEF 2022), co-located\nwith the 38th Conference of the Spanish\nSociety for Natural Language Processing\n(SEPLN 2022), Online. CEUR.org .\nPontiki, M., D. Galanis, H. Papageor-\ngiou, I. Androutsopoulos, S. Manandhar,\nM. AL-Smadi, M. Al-Ayyoub, Y. Zhao,\nB. Qin, O. D. Clercq, V. Hoste, M. Apid-\nianaki, X. Tannier, N. Loukachevitch,\nE. Kotelnikov, N. Bel, S. M. Jim\u0013 enez-\nZafra, and G. Eryi\u0015 git. 2016. SemEval-\n2016 task 5: Aspect based sentiment anal-\nysis. In Proceedings of the 10th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval-2016), pages 19{30, San Diego,\nCalifornia, June. Association for Compu-\ntational Linguistics.\nPontiki, M., D. Galanis, H. Papageorgiou,\nS. Manandhar, and I. Androutsopoulos.\n2015. SemEval-2015 task 12: Aspect\nbased sentiment analysis. In Proceedings\nof the 9th International Workshop on Se-\nmantic Evaluation (SemEval 2015) , pages\n486{495, Denver, Colorado, June. Associ-\nation for Computational Linguistics.\nPontiki, M., D. Galanis, J. Pavlopoulos,\nH. Papageorgiou, I. Androutsopoulos, and\nS. Manandhar. 2014. SemEval-2014\ntask 4: Aspect based sentiment analy-\nsis. In Proceedings of the 8th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval 2014) , pages 27{35, Dublin, Ire-\nland, August. Association for Computa-\ntional Linguistics.\nABSAPT 2022 at IberLEF: Overview of the Task on Aspect-Based Sentiment Analysis in Portuguese\n205206Overview of DA-VINCIS at IberLEF 2022:\nDetection of Aggressive and Violent Incidents\nfrom Social Media in Spanish\nResumen de la Tarea DA-VINCIS en IberLEF 2022:\nDetecci\u00b4 on de Incidentes Violentos en\nRedes Sociales en Espa\u02dc nol\nLuis Joaqu\u00b4 \u0131n Arellano1, Hugo Jair Escalante1, Luis Villase\u02dc nor-Pineda1,2,\nManuel Montes-y-G\u00b4 omez1, Fernando Sanchez-Vega3,4,5\n1Laboratorio de Tecnolog\u00b4 \u0131as del Lenguaje (INAOE), Mexico.\n2Centre de Recherche GRAMMATICA (EA 4521), Universit\u00b4 e d\u2019Artois, France\n3Mathematics Research Center (CIMAT), Guanajuato, Mexico.\n4El Colegio de M\u00b4 exico (COLMEX), Mexico\n5Consejo Nacional de Ciencia y Tecnolog\u00b4 \u0131a (CONACYT), Mexico.\n{arellano.luis, hugojair, villasen, mmontesg}@inaoep.mx\nfernando.sanchez@cimat.mx\nAbstract: This paper presents the overview of the DA-VINCIS 2022 task, orga-\nnized at IberLEF 2023 and co-located with the 38th International Conference of the\nSpanish Society for Natural Language Processing (SEPLN 2022). DA-VINCIS chal-\nlenged participants to develop automated solutions for the detection of violent events\nmentioned in social networks. We released a novel corpus collected from Twitter and\nmanually labeled with 4 categories of violent incidents (plus the no-incident label).\nThe shared task focused on the Mexican variant of Spanish and it was divided into\ntwo tracks: (1) a binary classification task in which users had to determine whether\ntweets were associated to a violent incident or not; and (2) a multi-label classification\ntask in which the category of the violent incident should be spotted. More than 40\nteams registered for the task and 12 participants submitted predictions for the final\nphase. Very competitive results were reported in both sub tasks, where transformer-\nbased solutions obtained the best results. Corpora and results are available at the\nshared task website at https://codalab.lisn.upsaclay.fr/competitions/2638.\nKeywords: DA-VINCIS, violent event detection, text classification.\nResumen: Se presenta el resumen de la tarea DA-VINCIS 2022, organizada en\nIberLEF 2022 junto a la 38 \u00aaConferencia Internacional de la Sociedad Espa\u02dc nola\npara el Procesamiento del Lenguaje Natural (SEPLN 2022). DA-VINCIS plantea el\nreto de detectar autom\u00b4 aticamente piezas de informaci\u00b4 on en redes sociales que est\u00b4 en\nasociadas a eventos violentos. Se liber\u00b4 o un nuevo corpus para el Espa\u02dc nol Mexicano\nque fue etiquetado manualmente con 4 categor\u00b4 \u0131as de eventos violentos (adem\u00b4 as de la\ncategor\u00b4 \u0131a no-violento). Se propusieron dos subtareas: (1) una tarea de clasificaci\u00b4 on\nbinaria donde se buscaba distinguir tuits asociados a eventos violentos del resto; y\notra (2) donde se buscaba identificar la categoria del evento violento. M\u00b4 as de 40\nparticipantes se registraron en el portal y 12 enviaron resultados para la fase final.\nLos resultados obtenidos fueron muy competitivos para ambas tareas; las soluciones\nque obtuvieron los mejores resultados se basaron en modelos tipo transformer para\nel espa\u02dc nol. El corpus y los resultados detallados pueden consultarse en el sitio web\nde la tarea: https://codalab.lisn.upsaclay.fr/competitions/2638.\nPalabras clave: DA-VINCIS, Detecci\u00b4 on de eventos violentos, Clasificaci\u00b4 on de tex-\ntos.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 207-215\nrecibido 05-07-2022 revisado 22-07-2022 aceptado 25-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-18\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introduction\nViolence has obvious negative effects on those\nwho witness or experience it, including a\nhigher incidence of depression, anxiety, post-\ntraumatic stress disorder, among others. In\naddition, violence events have a high im-\npact for governments, as they are in charge\nof guaranteeing security to their population.\nTherefore, the detection and tracking of vi-\nolence related events is critical. In this con-\ntext, social networks comprise a valuable in-\nformation source for the detection and moni-\ntoring of violent events, as people very often\npost publications notifying the occurrence of\nviolent events in real time. This represents\nan important opportunity for IT researchers\nthat can provide solutions based on natural\nlanguage processing for the timely detection\nof violent incidents in social networks. Solu-\ntions of this kind could be used by authorities\nto respond more efficiently to events happen-\ning in real time, and to develop crime preven-\ntion policies according to geographical zones\nand types of events. Likewise, such solutions\nwould be very helpful to the population, as\none could know what violent events are hap-\npening in which zones in real time.\nWe organized a shared task collocated\nwith IberLEF2022 called DA-VINCIS. This\ntask focused on the detection of violent inci-\ndents on Twitter. It challenged participants\nto develop methods able to classify tweets as\nreporting a violent event or not. For this first\nedition, the shared task targeted Spanish in\nits Mexican variant. This is motivated by the\nlack of resources in Spanish for approaching\nthe task, and the fact that Mexican Span-\nish is the most spoken variant of this lan-\nguage1. We released a novel corpus carefully\nlabeled according to violent event categories.\nThe shared task comprised two tracks: vi-\nolent event identification and violent event\ncategory recognition. Labeled data was pro-\nvided to participants for both tracks for the\ndevelopment of their solutions, and unlabeled\ndata was used for the final evaluation of the\ncorresponding tracks.\nAs far as we know, this was the\nfirst shared-task aiming at detecting violent\nevents from social media. This is an issue\nthat has received little attention from the\ncommunity, despite its enormous potential\nimpact. Therefore, the aim of the challenge\n1In terms of the number of native speakers.was to motivate research on a topic little ex-\nplored in Spanish, but with great potential\nimpact for the whole population and authori-\nties. In addition, an implicit goal was to raise\nawareness of the relevance of this problem.\nThe task posed several challenges to the\ncommunity, including: dealing with Mexi-\ncan Spanish, the ambiguous language inher-\nent to Twitter, the high class imbalance ra-\ntios present in our datasets, among others.\nWe are confident that the shared task will\ngive rise to novel solutions that could be used\nin the near future for applications of societal\nimpact, for example, generating real-time oc-\ncurrence crime maps. Last but not least, we\nplan to release the associated corpus in the\nnear future so that the community can keep\nworking on it even at the end of the shared\ntask.\nThe remainder of this paper is organized\nas follows. Section 2 describes the shared\ntask in detail. Then, Section 3 introduces the\nDA-VINCIS corpus. Section 4 presents the\nresults obtained and a summary of partici-\npants\u2019 solutions. Finally, Section 5 outlines\nconclusions and future work directions.\n2 Task description\nAs previously mentioned, the DA-VINCIS\nshared task comprised two tracks: a binary\nclassification subtask that aimed at distin-\nguishing tweets associated to violent inci-\ndents from those that are not; and (2) a\ntask that challenged participants to identify\nthe type of violent incident (if any) being re-\nported in tweets. The categories considered\nfor the latter task are described in Table 2.\nThe DA-VINCIS corpus, described in de-\ntail in the next section, was used for the eval-\nuation of both subtasks. The challenge was\nrun in the CodaLab platform (Pavao et al.,\n2022). The shared task was divided into two\nstages as follows:\n\u2022Development phase. Participants\nwere provided with labeled training data\nand unlabeled validation data. Dur-\ning this phase, which lasted about two\nmonths, participants were able to sub-\nmit predictions for the validation set and\nreceive immediate feedback in the Co-\ndaLab site.\n\u2022Final phase. Participants were pro-\nvided with unlabeled test data. They\nwere able to upload up to five submis-\nLuis Joaqu\u00edn Arellano, Hugo Jair Escalante, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez, Fernando Sanchez-Vega\n208Reference Considered categories\n(Mata Riv\nera et al.,\n2016)Theft, Crime,\nTheft with violence,\nTheft walking, Theftin car, Theft\nwithout violence\n(Sandagiri, Kumara,\nand\nKuhaneswaran,\n2020b)Assault, Burglary\n, Drugs Viola-\ntions, Homicide, Sex Offences, Sui-\ncide\n(Sandagiri, Kumara,\nand\nKuhaneswaran,\n2020a)Assault, Burglary\n, Drugs Viola-\ntions, Homicide, Sex Offences\n(Pi\u02dc\nna-Garc\u00b4 \u0131a and\nRam\u00b4 \u0131rez-Ram\u00b4 \u0131rez,\n2019)Robb ery\npasserby, Theft of motor\nvehicle, Robbery of business prop-\nerty, Card fraud, Homicide, Do-\nmestic burglary, Robbery on public\ntransportation, Rape, Firearm in-\njuries, Robbery in subway, Robbery\non taxi, Robbery to Carrier, Rob-\nbery to deliver personDA-VINCIS Acciden t,\nHomicide, Theft, Kidnap-\nping, Non-incident\nTable 1: Violent incidents considered in pre-\nvious work.\nsions during the competition. Perfor-\nmance on the test set was used to rank\nparticipants.\nFor subtask 1, recall, precision and f1\nscore with respect to the violent-incident\nclass were considered as evaluation measures.\nFor subtask 2, macro average recall, precision\nand f1score were considred. In both cases,\nthe leading evaluation measure was that of f1\nscore.\n3 DA-VINCIS corpus\nThe DA-VINCIS corpus is a collection of\ntweets2associated to reports of violent inci-\ndents in Mexican Spanish. The aim of this\nnovel corpus is to boost research in the au-\ntomated detection and monitoring of violent\nincidents in social networks. Summarizing, a\nlarge number of tweets was retrieved using\nqueries associated to predefined categories.\nThen, the tweets were filtered, and a subset\nof these was manually labeled. In the remain-\nder of this section we provide details on the\nconstruction of this corpus.\nA set of categories of violent incidents was\ndefined after a careful analysis of relevant\nliterature, see Table 1. The categories con-\nsidered in each study differ according to the\nlegal, psycho-social or geographical context,\nand commonly they are finally filtered by the\ncriteria of the research group involved.\n2Please note that the corpus is formed by both,\nthe text in tweets and their associated images, if any,\nfor this shared task only textual information was con-\nsidered.The categories considered in the DA-\nVINCIS corpus are shown in the last row of\nTable 1. The criteria for selecting such cat-\negories involved: categories that appear in\nmost of previous studies (e.g., Homicide and\nTheft ), generic categories (e.g., we considered\na single Theft category) and categories that\nappeared most frequently among in Twitter\naccounts associated to local news in Mex-\nico (e.g., Kidnapping ). Finally, our choice\nfor these categories relied on the number of\ntweets that we retrieved per each category.\nIt is important to mention that since the\nlong term goal of this project is the real-\ntime monitoring of violent incidents, the Ac-\ncident category was taken into account. As\non a daily basis authorities use the same com-\nmunication channels to deal with this kind\nof problem. Categories such as Sexual of-\nfences andDrug violations were initially con-\nsidered because of their relevance and the\nurgent need to prevent them. However the\nstudy of these categories is particularly com-\nplicated, because although there are reports\nor complaints on the internet, these are not\nfrequent, in addition to the fact that these are\ncommon topics of conversation and discus-\nsion, therefore it makes it extremely difficult\nto find the reports among the large number of\nopinions. Definitions for the categories con-\nsidered in the DA-VINCIS corpus are shown\nin Table 2.\nTo obtain keywords for the retrieval of vi-\nolent incidents tweets, a research work was\ncarried out where 30,000 tweets published\nin news accounts in Spanish were recovered,\n5,000 tweets were manually tagged to iden-\ntify if the news was violent (i.e. binary la-\nbeling) once established, an ML model was\napplied to label the rest of the corpus, the\npseudo labels obtained were used to study\nthe tweets and the unigrams, bigrams and tri-\ngrams that provided the most information for\nthe classification, the most significant words\nfrom the top-100 were filtered, these were the\nkeywords used to search for tweets of violent\nincidents.\nOnce the keywords were obtained, a tweet\nretrieval was performed using each of the se-\nlected keywords, where it was required that\n(1) tweets had an associated image, (2) lan-\nguage was Spanish and, (2) the tweet was ge-\nolocated in approximation to Latin America.\nThe result of this process were 8000 tweets\nthat were further filtered by eliminating those\nOverview of DA-VINCIS at IberLEF 2022: Detection of Aggressive and Violent Incidents from Social Media in Spanish\n209Category Definition\nAc\ncident Even\ntual event or action that results\nin involuntary damage to people or\nthings.\nHomicide Depriv ation\nof life.\nTheft Seizure or\nwillful destruction of\nsomeone else\u2019s property without the\nright and without the consent of the\nperson who can legally dispose of\nthem.\nKidnapping Depriv ation\nof liberty.\nNon-incident Selected when\nthere is no crime re-\nported.\nTable 2: Definition of the categories consid-\nered in the DA-VINCIS corpus.\nthat could no longer recover any of their ele-\nments, that were written in a language other\nthan Spanish but that were filtered in the\nsearch and the empty elements or that only\nconsisted of a series of hashtags.\nThe filtered dataset was formed by 5000\ntweets. Each tweet in the dataset was labeled\nby at least two annotators. Labels assigned\nby annotators took into account the context\nprovided by the text of the tweet and the as-\nsociated images, if any. However, even when\nhaving all the context available, the label-\ning process was not straightforward in some\ncases. Sometimes the images were conducive\nto confusion or vice versa. For example, con-\nfusing a traffic accident with a homicide with\na car (cyclist hit by a car).\nRandomly selected tweets from each cat-\negory are shown in Table 3. Despite this\nsamples were assigned the correct label, there\nwere some samples that could be considered\nnoisy, see Section 4. Table 4 shows the pro-\nportion of samples per class in the dataset.\nPlease note that since categories are not nec-\nessarily disjoint (except the no-incident one).\nMore than one label can be assigned to a sin-\ngle tweet, that is why the total number of\nlabels is different from 5000.\nTo analyze the difficulty of the task, the\nagreement between the annotators was cal-\nculated. Table 5 presents the results of the\nKappa coefficient, by the number of judg-\nments collected. The coefficient values in-\ndicate moderate agreement, see (McHugh,\n2012). However, we found some samples with\nnoisy annotations, see Section 4, evidencing\nthe need for a detailed curation for the DA-\nVINCIS corpus.4 Participants approaches and\nresults\nIn the following subsections we describe the\nmain ideas addressed by the different partici-\npants, and present a general analysis of their\nresults.\n4.1 Systems\u2019 descriptions\nA total of 12 teams participated in the DA-\nVINCIS shared-task; the majority tackled\nboth subtasks, however, four teams only ad-\ndressed the subtask 1 and one team only pre-\nsented a solution for subtask 2. Something\ninteresting to highlight is that the teams\nwith the best performance in each of the two\nsubtasks presented a proposal exclusively fo-\ncused on the particular subtask. This indi-\ncates that both subtasks have their own spe-\ncific challenges and, therefore, that it is not\nalways convenient to approach them using\nthe same strategy.\nFrom the different solutions presented at\nthe DA-VINCIS shared task, we found sev-\neral coincidences, which indeed align with\nsome general trends in Natural Language\nProcessing. The main shared aspects corre-\nspond to the use of:\n\u2022Pretrained Transformers: All partic-\nipant approaches used some pretrained\ntransformer to take advantage of all\nthe knowledge encoded in their pre-\ntraining. Some applied the traditional\nfine-tuning (Ta et al., 2022b), while oth-\ners proposed some interesting modifi-\ncations (Vallejo-Aldana, L\u00b4 opez-Monroy,\nand Villatoro-Tello, 2022; Tur\u00b4 on et al.,\n2022; Monta\u02dc n\u00b4 es-Salas, del Hoyo-Alonso,\nand Pe\u02dc na-Larena, 2022; Garc\u00b4 \u0131a-D\u00b4 \u0131az et\nal., 2022; Ta et al., 2022b). On the\nother hand, some approaches used the\npretrained transformer as a frozen source\nof knowledge, only using the contex-\ntual embedding encodings (Garc\u00b4 \u0131a-D\u00b4 \u0131az\net al., 2022), or extracting relationships\nfrom the instances and task description\nusing a Prompt-based framework (Qin et\nal., 2022).\n\u2022Ensembles: Multiple approaches em-\nployed ensembles to take advantage of\nvariations of their base solution mod-\nels. For example, the majority voting\nscheme was successfully used for subtask\n1 (Vallejo-Aldana, L\u00b4 opez-Monroy, and\nVillatoro-Tello, 2022; Tur\u00b4 on et al., 2022;\nLuis Joaqu\u00edn Arellano, Hugo Jair Escalante, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez, Fernando Sanchez-Vega\n210Categories Original text Translation\nAccident #Ahora Reportan accidente de tr\u00b4 ansito\nen el ingreso al municipio de Salcaj\u00b4 a.\nDos veh\u00b4 \u0131culos tipo picop involucrados en\nel percances. Precauci\u00b4 on al conducir por\nel sector. Ampliaremos la informaci\u00b4 on.\n#Stereo100Noticias#Now Car accident is being reported at\nthe entrance of the Salcaj\u00b4 a municipality.\nTwo pickup vehicles involved. Caution\nwhen driving nearby. We will extend the\ninformation #Stereo100Noticias\nHomicide La violencia y las ejecuciones contin\u00b4 uan\ncada d\u00b4 \u0131a en la CDMX un hombre fue\nejecutado a 2 calles de la alcald\u00b4 \u0131a de\nCuahut\u00b4 emoc en la calle de Pedro MorenoViolence and killings continue everyday in\nCDMX a mean was killed two blocks from\nCuahut\u00b4 emoc town hall in Pedro Moreno\nstreet\nTheft Im\u00b4 agenes en las que un sujeto que ingres\u00b4 o\na robar a un local ubicado en Av.Tonal\u00b4 a y\nMadero en la Cabecera Municipal. El hom-\nbre iba armado y despu\u00b4 es del robo huy\u00b4 o\nen un auto Kia color gris que lo esperaba\nafuera del local.footage in which a subject that entered to\nsteal a facility in Av. Tonal\u00b4 a and Madero\nin the municipality. The man was armed\nand after the robbery escaped in a grey Kia\nthat was waiting outside the facility.\nKidnapping Secuestraron a sujeto frente al palacio mu-\nnicipal de Coatzacoalcos A plena luz del\nd\u00b4 \u0131a realizan acto delictivo; los detienen\ny desarticula UECS banda de plagiarios\nreci\u00b4 en formada; se quedan en el Cereso Du-\nport Osti\u00b4 onA man was kidnapped in front of Coatza-\ncoalcos\u2019 town all. The criminal act was\nperfored during daylight; they were ar-\nrested and the UECS dismantled a band\nof kidnappers just formed; they are stay-\ning in the Duport Osti\u00b4 on prision.\nTable 3: Samples from the DA-VINCIS corpus for the violent incident categories.\nMonta\u02dc n\u00b4 es-Salas, del Hoyo-Alonso, and\nPe\u02dc na-Larena, 2022). More sophisticated\nensemble techniques were also applied,\nsuch as intermediate fusion of NNs using\nKnowledge Integration (KI), ensemble\nlearning (Garc\u00b4 \u0131a-D\u00b4 \u0131az et al., 2022), and\na kind of multilevel fusion that incorpo-\nrates information from multiple sources\n(Qin et al., 2022).\n\u2022Multi-Task Learning (MTL) : Sev-\neral proposals took advantage of the\npairing of subtasks 1 and 2 to carry\nout some kind of multi-task learning.\nFor example, (Vallejo-Aldana, L\u00b4 opez-\nMonroy, and Villatoro-Tello, 2022) per-\nformed MTL for subtask 1 through a bi-\nnary transformation of subtask 2 that is\njointly learned in order to incorporate\nadditional information for subtask 1. In\ncontrast, in (Ta et al., 2022b) the predic-\ntion of each class of subtask 2 was trans-\nformed into a binary problem that is per-\nCategories #Examples %T\notal\nAcciden t 1800 33.45\nHomicide 417 7.75\nTheft 286 5.31\nKidnapping 72 1.33\nNon-violen t 2878 53.49\nTable 4: Proportion of samples from each\nclassJudgmen tsTweets Coefficien\nt\n2 1349 0.5758\n3 1643 0.5767\n4 1024 0.5979\n5 350 0.5829\nTable 5: Kappa coefficients by number of\njudgments (only results for 2 to 5 judgments\nare shown).\nformed with MTL on the complete set\nof binary problems. Finally, (Ta et al.,\n2022a) proposed an interesting MTL ap-\nproach where subtask 2 was carried out\nwhile jointly learning to distinguish real\ninstances from instances generated by a\nGAN.\n\u2022Data Augmentation (DA) : This\ntechnique was also widely used by the\nparticipant teams. The most used\nmethod was back-translation (Tur\u00b4 on et\nal., 2022; Monta\u02dc n\u00b4 es-Salas, del Hoyo-\nAlonso, and Pe\u02dc na-Larena, 2022; Ta et\nal., 2022b; Ta et al., 2022a), however,\nsome approaches also integrated the ex-\namples in the intermediate languages to\nthe augmented data, and in consequence\nused multilingual models in their train-\ning phase (Tonja et al., 2022).\n\u2022Preprocessing : Most teams performed\nstandard preprocessing operations to\nallow the transformers-based language\nOverview of DA-VINCIS at IberLEF 2022: Detection of Aggressive and Violent Incidents from Social Media in Spanish\n211models to handle the input texts. For\nexample, they removed URLs, hash-\ntag symbols, non-alphanumeric sym-\nbols, and adjusted user mentions (strings\nwith @). Additionally, in (Vallejo-\nAldana, L\u00b4 opez-Monroy, and Villatoro-\nTello, 2022; Tur\u00b4 on et al., 2022;\nMonta\u02dc n\u00b4 es-Salas, del Hoyo-Alonso, and\nPe\u02dc na-Larena, 2022) emojis were replaced\nby their descriptive words, and acronyms\nand abbreviations were expanded in\n(Garc\u00b4 \u0131a-D\u00b4 \u0131az et al., 2022).\nThree approaches show some interesting\nfeatures that do not fit the generalities de-\nscribed above; these are:\n\u2022Noise Reduction: VICOMTECH\n(Tur\u00b4 on et al., 2022) carried out a rela-\nbelling process of the training data con-\nsidering the votes of 5 systems learned\nfrom the original noisy data set. They\n\u201ccorrected\u201d the instance labels if at least\n4 of the 5 systems agreed to do so; using\nthis approach they modified around 5%\nof the training set labels.\n\u2022Use of Advanced Linguistic Fea-\ntures: UM-UJ-URJC (Garc\u00b4 \u0131a-D\u00b4 \u0131az et\nal., 2022) considered the use of a variety\nof features with the purpose of taking\ninto account multiple aspects of the writ-\ning and communication style of tweets.\n\u2022Use of Prompt Learning: GDUT\n(Qin et al., 2022) employed a prompt\nlearning module to inject information\nfrom a pre-trained language model into\nthe violent event category recognition\ntask. This approach incorporates the\ntext provided by the prompt module into\nthe tweet representation.\n4.2 Evaluation campaign results\nTable 6 presents the results obtained by\nthe participant teams in subtask 1, the bi-\nnary identification of violent incidentes. The\nteams are sorted by their F1-score over the\npositive class (i.e., the violent incident class);\nPrecision and Recall are also reported to al-\nlow a better interpretation of these results.\nAt the bottom it is included our baseline3\n3Please note that during the final phase of the\nshared task we uploaded a single run of the baseline\nthat obtained better results. However, in this paper\nwe report the average over 10 runs of the performance\nof the baseline, which is a more reliable estimate of\nits performance.Subtask 1:\nBinary violent event identification\nTeam Precision Recall F1-Score\nCIMA T-UG-UAM-IDIAP 0.803 0.750 0.775\nVICOMTECH 0.812 0.737 0.773\nITAINNO\nVA 0.779 0.751 0.765\nUM-UJ-URJC 0.774 0.753 0.764\nSdamian 0.761 0.750 0.756\nBernardo 0.780 0.730 0.754\nIPN-DLU-UNOMAHA-1 0.755 0.740 0.748\nCIC-IPN 0.761 0.730 0.745\nIPN-DLU-UNOMAHA-2 0.740 0.747 0.744\nJuanCalderon 0.723 0.763 0.742\nSustaitangel 0.710 0.742 0.726\nBaseline 0.763 0.780 0.750\nTable 6: Results of the participant teams in\nSubtasks 1. They correspond to the Preci-\nsion, Recall and F1 score in the positive class.\nresult, which corresponds to the direct use\nof a traditional fine-tuning (i.e., using a sin-\ngle linear layer and the use of softmax for\nclassification) of BETO (Ca\u02dc nete et al., 2020),\na well-known pre-trained language model in\nSpanish.\nThe best performance in Subtask 1 was\nobtained by the CIMAT team (Vallejo-\nAldana, L\u00b4 opez-Monroy, and Villatoro-Tello,\n2022) followed by VICOMTECH (Tur\u00b4 on et\nal., 2022) These two approaches have in com-\nmon that they took advantage of the paral-\nlelism of both subtasks, particularly, they in-\ncluded in their model for subtask 1 some in-\nformation from subtask 2. The third best\napproach is ITAINNOVA (Monta\u02dc n\u00b4 es-Salas,\ndel Hoyo-Alonso, and Pe\u02dc na-Larena, 2022),\nwhich, similarly to the CIMAT team, used an\nensemble of multiple transformer-based mod-\nels. On the one hand, the CIMAT approach\ncombined the output of three BERT-based\nmodels fine-tuned to perform MTL. In this\ncase, MTL is used to simultaneously learn the\nsubtask 1 and a binary version of a violent\nevent subcategory classification (i.e., each\nBERT model is different in the specific event\nsubcategory chosen). On the other hand,\nthe ITAINNOVA approach uses different pre-\ntrained models (such as BETO, Twitter-\nXLM-Roberta and BSC-Roberta), thus ob-\ntaining its diversity from the models and not\nfrom the data.\nIt should be noted that the different ap-\nproaches obtained very close results; the best\nperformance is only 6.8% greater than the\nlowest, and the standard deviation of the set\nof F1-scores is only 0.015.\nThe results obtained by the teams in sub-\ntask 2, the violent event category recognition,\nare shown in Table 7. The best performance\nin this subtask corresponds to the GDUT\nLuis Joaqu\u00edn Arellano, Hugo Jair Escalante, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez, Fernando Sanchez-Vega\n212Subtask 2:\nViolent event category recognition\nTeam Precision Recall F1-Score\nGDUT 0.550 0.564 0.554\nVICOMTECH 0.517 0.545 0.528\nITAINNO\nVA 0.509 0.503 0.504\nCIC-IPN 0.467 0.520 0.490\nCIMA T-UG-UAM-IDIAP 0.655 0.421 0.473\nUM-UJ-URJC 0.442 0.549 0.469\nSustaitangel 0.459 0.424 0.433\nCIC-IPN-DLU-UNOMAHA-1 0.377 0.438 0.392\nBaseline 0.498 0.460 0.570\nTable 7: Results of the participant teams in\nthe Subtask 2. They correspond to the macro\naverage values of Precision, Recall and F1\nscore.\nteam. Their solution is mainly characterized\nby the incorporation of semantic relations be-\ntween the text instances and the name of\nthe categories through the application of a\nPrompt learning module. The second and\nthird best performances were obtained, as\nin subtask 1, by the VICOMTECH (Tur\u00b4 on\net al., 2022) and ITAINNOVA (Monta\u02dc n\u00b4 es-\nSalas, del Hoyo-Alonso, and Pe\u02dc na-Larena,\n2022) teams, respectively. Their adequate\nperformance in both tracks suggests the rele-\nvance and robustness of these two approaches\nfor the task addressed.\nFrom the results, it is notorious that sub-\ntask 2 is much more challenging than sub-\ntask 1; something expected due to the high\nimbalance in some of the categories. This\nis reflected in a greater standard deviation\n(0.051) in the reported F1-scores, and also\nin the larger difference between the best and\nworst reported results (in this case, the for-\nmer is 41% greater than the later).\n4.3 Analysis\nTo provide insights on the complimen-\ntary and redundancy of solutions, the\nIntra-ensemble Coincident\u2013Failure Diversity\n(CFD) was calculated for the 11 submissions\nranked in Table 6. This index indicates how\ndiverse the errors of each model are with re-\nspect to each other if one would build an en-\nsemble with them. The resulting CFD was\n0.5590, indicating regular diversity (the range\nof values of CFD is [0,1]), that could be ex-\nploited for building a more robust model. On\nthe other hand the maximum possible accu-\nracy (a tweet is counted as well classified, if\nany of the models classified it correctly) was\n0.9181. Further evidencing the potential ben-\nefits of building an ensemble with the 11 eval-\nuate solutions.\nIn order to illustrate the inherent difficul-ties of the shared task, Table 8 shows ex-\namples of tweets that were missclassified by\nmost participants when approaching subtask\n1. Several interesting aspects can be dis-\ncussed around these examples. First, there\nare tweets that were wrongly labeled by the\nannotators, for instance sample 3. This was\na problem highlighted by participants during\nthe challenge (Tur\u00b4 on et al., 2022). Secondly,\nthere are tweets for which the assigned cate-\ngory is debatable. For instance, tweet 4 refers\nto an accident happening in the context of\nan F1 race, it is an accident, but not really\nrelevant for the purpose of the project. Also,\ntweet 1 refers to a report associated to several\nviolent events happening in different places\n(we hypothesize this is why it was labeled as\nNon-violent). Summarizing, a large portion\nof samples missclassified by the systems could\nbe due to subjective labeling. Therefore, we\nconclude the dataset needs of further man-\nual curation. Still, we think the DA-VINCIS\ncorpus is a valuable resource that will boost\nresearch in this relevant task.\n5 Conclusions\nThe DA-VINCIS shared task at IberLEF pro-\nmotes research into the identification of vi-\nolent incidents on social networks, a task\nwith a high social impact. A new dataset\nfor the task of identification of violent in-\ncidents as well as their subcategorization is\npresented. This evaluation campaign made\nit possible to evaluate an important diversity\nof approaches and contrast their effective-\nness. Different models, characteristics and\ntechniques of the proposed approaches were\npresented, contributing to the progress of the\nidentification of violent incidents in Spanish\nlanguage.\nThe results indicate, as might be ex-\npected, that the fine-grained subtask 2 was\nmore challenging. A strong presence of ap-\nproaches based on transformers was found,\nbut also there was a vitalizing variety of pro-\nposals with important novelties such as the\napplication of GANs, the automatic correc-\ntion of instances, and the use of non-learning\ntools to act as a kind of oracle, all of them\nintroduced to improve the methods\u2019 perfor-\nmance as well as to to deal the specific chal-\nlenges of the task at hand.\nIt was found that having some informa-\ntion on the subcategory of the general class of\ninterest seems to help to make a better iden-\nOverview of DA-VINCIS at IberLEF 2022: Detection of Aggressive and Violent Incidents from Social Media in Spanish\n213ID Translation Text Category\n1 Intense\npolice activity in Coacoatzintla\u2019s mu-\nnicipality, in response to the supposed kidnap\nof a young male. A family member of the\nkidnapped person was killed when trying to\nimpede this crime. In Jilotepec was found\nthe vehicle where the person was abducted\nSPVeracruzUna fuerte\nmovilizaci\u00b4 on policiaca se registr\u00b4 o\nen el municipio de Coacoatzintla ante el pre-\nsunto secuestro de un joven. Al tratar de im-\npedir el hecho, un familiar fue asesinado. En\nJilotepec fue hallado el veh\u00b4 \u0131culo en el que se\ncometi\u00b4 o el il\u00b4 \u0131cito SP Ver\nacruzNon-violen t\n2 30y\nears now from the Cimitarra massacre, a\nviolence act that left more than 250 thousand\ndeaths turning Colombia into a a huge com-\nmon grave.A30\na\u02dc nos de la masacre de Cimitarra, una vi-\nolencia que dejo m\u00b4 as de 250 mil muertos con-\nvirtiendo a Colombia en una gran fosa com\u00b4 un.Violen t\n3 Homicide -\nIn a clinic at #Cartago Bibiana\nLiseth Guzm\u00b4 an Ord\u00b4 o\u02dc nez, 31 years old and of-\nficial of the @ipscartago, died, after she was\nshoot with a firearm. In the same incident a\n26 years old man was hurt. The women left a\ndaughter.Homicidio -\nEn una cl\u00b4 \u0131nica de #Cartago fal-\nleci\u00b4 o Bibiana Liseth Guzm\u00b4 an Ord\u00b4 o\u02dc nez de 31\na\u02dc nos de edad, funcionaria de la @ipscartago\nluego de que le propinaran varios impactos con\narma de fuego. En este mismo hecho result\u00b4 o\nlesionado un hombre de 26 a\u02dc nos. La mujer\ndeja una hija.Non-violen t\n4 \u201cThe acciden\nt could have been avoided if they\nwould leave me enough space to take the curve.\nYou need of two persons for this to work, and\nI felt they throw me away. When we challenge\nto each other in a race this things can happen,\nunfortunately.\u201d\u201cElacciden\nte se pudo haber evitado si me hu-\nbieran dejado espacio suficiente para tomar la\ncurva. Necesitas 2 personas para que esto fun-\ncione y yo sent\u00b4 \u0131 que sacaban. Cuando nos re-\ntamos mutuamente en una carrera estas cosas\npueden pasar, desafortunadamente.\u201dViolen t\nTable 8: Examples of tweets incorrectly classified by all of the participant teams.\ntification. Multitask Learning is strongly po-\nsitioned as a good alternative that improves\nperformance and takes advantage of the par-\nallelism between subtasks 1 and 2. These\nfindings open the possibility that other fu-\nture approaches could use virtual subtasks\nwith different fine grain levels in order to take\nadvantage of this type of scheme.\nLikewise, an in depth analysis of the cor-\npus revealed that there is room for improve-\nment in terms of the quality of annotations.\nOn the one hand, a curation process trying\nto identify noisy annotations should be per-\nformed. On the other hand, the definition of\ncategories should be further tuned, so that\nannotation guidelines result in objective an-\nnotations. This is work in progress.\nAs previously mentioned, the DA-VINCIS\ncorpus also comprises visual information,\ntherefore another venue of current work is\nstudying the potential added value of using\nimages associated to tweets when detecting\nviolent incidents. The corpus will allow us to\nstudy the performance of solutions that con-\nsider multimodal information.\nAcknowledgements\nThis work was supported by CONACyT\nunder grant CB-S-26314, Integraci\u00b4 on de\nLenguaje y Visi\u00b4 on mediante Representa-\nciones Multimodales Aprendidas para Clasi-\nficaci\u00b4 on y Recuperaci\u00b4 on de Im\u00b4 agenes. Wealso would like to thank CONACyT for par-\ntially supporting this work under grant CB-\n2015-01-257383. Additionally, the authors\nthank CONACYT for the computer resources\nprovided through the INAOE Supercomput-\ning Laboratory\u2019s Deep Learning Platform for\nLanguage Technologies.\nReferences\nCa\u02dc nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u00b4 erez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. In PML4DC at ICLR 2020 .\nGarc\u00b4 \u0131a-D\u00b4 \u0131az, J. A., S. M. Jim\u00b4 enez-Zafra,\nM. Rodr\u00b4 \u0131guez-Garc\u00b4 \u0131a, and R. Valencia-\nGarc\u00b4 \u0131a. 2022. UMUTeam at DA-VINCIS\n2022: Aggressive and Violent classifi-\ncation using Knowledge Integration and\nEnsemble Learning. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR Workshop Pro-\nceedings. CEUR-WS.org.\nMata Rivera, M., M. Torres-Ruiz,\nG. Guzm\u00b4 an, R. Quintero, R. Zagal-\nFlores, M. Moreno, and E. Loza. 2016.\nA Mobile Information System Based on\nCrowd-Sensed and Official Crime Data\nfor Finding Safe Routes: A Case Study\nof Mexico City. Mobile Information\nSystems, 2016:1\u201311, 03.\nLuis Joaqu\u00edn Arellano, Hugo Jair Escalante, Luis Villase\u00f1or-Pineda, Manuel Montes-y-G\u00f3mez, Fernando Sanchez-Vega\n214McHugh, M. L. 2012. Interrater reliability:\nthe kappa statistic.\nMonta\u02dc n\u00b4 es-Salas, R. M., R. del Hoyo-\nAlonso, and P. Pe\u02dc na-Larena. 2022.\nITAINNOVA@DA-VINCIS: A Tale of\nTransformers and Simple Optimization\nTechniques. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2022), CEUR Workshop Proceedings.\nCEUR-WS.org.\nPavao, A., I. Guyon, A.-C. Letournel,\nX. Bar\u00b4 o, H. Escalante, S. Escalera,\nT. Thomas, and Z. Xu. 2022. CodaLab\nCompetitions: An open source platform\nto organize scientific challenges. Techni-\ncal report, Universit\u00b4 e Paris-Saclay, FRA.,\nApril.\nPi\u02dc na-Garc\u00b4 \u0131a, C. and L. Ram\u00b4 \u0131rez-Ram\u00b4 \u0131rez.\n2019. Exploring crime patterns in Mex-\nico City. Journal of Big Data, 6, 07.\nQin, G., J. He, Q. Bai, N. Lin, J. Wang,\nK. Zhou, D. Zhou, and A. Yang. 2022.\nPrompt Based Framework for Violent\nEvent Recognition in Spanish. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2022), CEUR Workshop\nProceedings. CEUR-WS.org.\nSandagiri, C., B. Kumara, and\nB. Kuhaneswaran. 2020a. Detecting\nCrime Related Twitter Posts using Arti-\nficial Neural Networks based Approach.\npages 5\u201310, 11.\nSandagiri, S., B. Kumara, and\nB. Kuhaneswaran. 2020b. Deep Neural\nNetwork-Based Approach to Identify\nthe Crime Related Twitter Posts. 2020\nInternational Conference on Decision Aid\nSciences and Application (DASA), pages\n1000\u20131004.\nTa, H. T., A. B. S. Rahman, L. Naj-\njar, and A. Gelbukh. 2022a. GAN-\nBERT: Adversarial Learning for Detec-\ntion of Aggressive and Violent Incidents\nfrom Social Media. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR Workshop Pro-\nceedings. CEUR-WS.org.\nTa, H. T., A. B. S. Rahman, L. Najjar, and\nA. Gelbukh. 2022b. Multi-Task Learning\nfor Detection of Aggressive and Violent\nIncidents from Social Media. In Proceed-\nings of the Iberian Languages EvaluationForum (IberLEF 2022), CEUR Workshop\nProceedings. CEUR-WS.org.\nTonja, A. L., M. Arif, O. Kolesnikova, A. Gel-\nbukh, and G. Sidorov. 2022. Detec-\ntion of Aggressive and Violent Incidents\nfrom Social Media in Spanish using Pre-\ntrained Language Model. In Proceedings\nof the Iberian Languages Evaluation Fo-\nrum (IberLEF 2022), CEUR Workshop\nProceedings. CEUR-WS.org.\nTur\u00b4 on, P., N. Perez, A. Garc\u00b4 \u0131a-Pablos, E. Zo-\ntova, and M. Cuadros. 2022. Vi-\ncomtech at DA-VINCIS: Detection of Ag-\ngressive and Violent Incidents from So-\ncial Media in Spanish. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR Workshop Pro-\nceedings. CEUR-WS.org.\nVallejo-Aldana, D., A. P. L\u00b4 opez-Monroy, and\nE. Villatoro-Tello. 2022. Leveraging\nEvents Sub-Categories for Violent-Events\nDetection in Social Media. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2022), CEUR Workshop\nProceedings. CEUR-WS.org.\nOverview of DA-VINCIS at IberLEF 2022: Detection of Aggressive and Violent Incidents from Social Media in Spanish\n215216Overview of DETESTS at IberLEF 2022:DETEction and classification of racial STereotypes in\nSpanish\nResumen de la tarea de DETESTS en IberLEF 2022:\nDETEcci\u00b4 on y clasificaci\u00b4 on de eSTereotipos raciales en eSpa\u02dc nol\nAlejandro Ariza-Casabona1,\u2217, Wolfgang S. Schmeisser-Nieto1,\u2217, Montserrat Nofre1,\nMariona Taul\u00b4 e1, Enrique Amig\u00b4 o2, Berta Chulvi3,4, Paolo Rosso3\n1CLiC, UBICS, Universitat de Barcelona, Spain\n2Research Group in NLP and IR, Universidad Nacional de Educaci\u00b4 on a Distancia, Spain\n3PRHLT Research Center, Universitat Polit` ecnica de Val` encia, Spain\n4Universitat de Val` encia, Spain\n{alejandro.ariza14, wolfgang.schmeisser, montsenofre, mtaule}@ub.edu,\nenrique@lsi.uned.es, berta.chulvi@upv.es, prosso@dsic.upv.es\nAbstract: This paper presents an overview of the DETESTS shared task as part\nof the IberLEF 2022 Workshop on Iberian Languages Evaluation Forum, within the\nframework of the SEPLN 2022 conference. We proposed two hierarchical subtasks:\nFor subtask 1, participants had to determine the presence of stereotypes in sentences.\nFor subtask 2, participants had to classify the sentences labeled with stereotypes\ninto ten categories. The DETESTS dataset contains 5,629 sentences in comments in\nresponse to newspaper articles related to immigration in Spanish. 51 teams signed\nup to participate, of which 39 sent runs, and 5 of them sent their working notes. In\nthis paper, we provide information about the training and test datasets, the systems\nused by the participants, the evaluation metrics of the systems and their results.\nKeywords: Stereotype detection and classification, DETESTS dataset, evaluation\nmetrics.\nResumen: Este art\u00b4 \u0131culo presenta un resumen de la tarea DETESTS como parte\ndel workshop IberLEF 2022, dentro de la conferencia SEPLN 2022. Proponemos dos\nsubtareas jer\u00b4 arquicas: En la subtarea 1, los participantes tuvieron que determinar\nla presencia de estereotipos raciales en oraciones. En la subtarea 2, de las oraciones\netiquetadas con estereotipo, los participantes tuvieron que clasificarlas en una o m\u00b4 as\nde diez categor\u00b4 \u0131as. El dataset DETESTS contiene 5.629 oraciones de comentarios\nque responden a art\u00b4 \u0131culos de peri\u00b4 odicos sobre inmigraci\u00b4 on en espa\u02dc nol. 51 equipos\nse registraron para participar, de los cuales 39 enviaron predicciones de sistemas y\n5 de ellos enviaron art\u00b4 \u0131culos. En este art\u00b4 \u0131culo presentamos informaci\u00b4 on sobre los\ndatasets de entrenamiento y de prueba, los sistemas utilizados por los participantes,\nlas m\u00b4 etricas de evaluaci\u00b4 on y sus resultados.\nPalabras clave: Detecci\u00b4 on y clasificaci\u00b4 on de estereotipos, dataset DETESTS,\nm\u00b4 etricas de evaluaci\u00b4 on.\n1 Introduction\nThe DETESTS (DETEction and classifica-\ntion of racial STereotypes in Spanish) task,\nheld at IberLEF 2022, focuses on the de-\ntection and classification of stereotypes re-\nlated to immigration in sentences taken from\ncomments posted in Spanish in response to\ndifferent online news articles. The present\n\u2217These authors contributed equally to this work.task is proposed to participants interested in\nracial, national, or ethnic stereotype detec-\ntion and classification tasks, which is a rel-\nevant and relatively novel area of research\ndue to its impact on modern society. Fur-\nthermore, the annotated dataset is a valu-\nable resource for exploratory linguistic anal-\nysis, as well as for comparing the application\nof deep learning and classical machine learn-\ning models to Spanish stereotyped expres-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 217-228\nrecibido 05-07-2022 revisado 28-07-2022 aceptado 31-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-19\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalsions under the recently introduced learning\nwith disagreements paradigm (Basile et al.,\n2021; Uma et al., 2021).\nThe following sections of this paper de-\nscribe the key aspects of this task. Section\n2 offers a background on what is understood\nas stereotypes and the related work on Nat-\nural Language Processing (NLP). Section 3\npresents both proposed subtasks. Section 4\ndescribes the DETESTS corpus, its training\nand test datasets and the annotation process.\nSection 5 presents the systems used by the\nparticipants, the evaluation metrics and the\nresults. Finally, Section 6 corresponds to con-\nclusions and draws some lines for future work.\n2 Background\nOne of the components that reinforces toxic\nand hateful speech is stereotypes. Under-\nstanding how they emerge and spread is cru-\ncial to tackling this issue, since stereotypes\nare not always expressed explicitly. The pres-\nence of stereotypes on social media and the\nneed to identify and mitigate them is driv-\ning the development of systems for their au-\ntomatic detection, especially in news com-\nments. Therefore, this is a new task that\nis attracting growing interest from the NLP\ncommunity.\nA stereotype is defined in social psychol-\nogy as a set of beliefs about others who are\nperceived as belonging to a different social\ncategory. The stereotype oversimplifies the\ngroup and generalizes a characteristic, apply-\ning it to all its members (Allport, Clark, and\nPettigrew, 1954). The common assumption\nin social psychology literature is that some\nof the behavior toward others is driven by\nstereotypes (cognitive component) and prej-\nudices (emotional component). One way of\nmanifesting stereotypes is through language\nin different degrees ranging from explicit to\nimplicit, thereby becoming a complex con-\ncept when they must be operationalized for\nnatural language processing. In order to nar-\nrow down this concept, we considered some\ncriteria for deciding whether a message con-\ntains a stereotype. Since not every linguistic\nexpression about immigration carries a racial,\nnational or ethnic stereotype, the first cri-\nterion to observe is whether there is a ho-\nmogenization of the target group in the com-\nment. Homogenization involves a process of\nthe generalization of a feature to the status\nof a social category, which negates individualdiversity (Tajfel, Sheikh, and Gardner, 1964;\nTajfel, 1984). In a second criterion, stereo-\ntypes are expressed in language through sev-\neral communication acts, which can be ex-\nplicit, that is, transparent and manifest, or\nimplicit, which means that a process of infer-\nence is necessary for the stereotype to be per-\nceived (Schmeisser-Nieto, Nofre, and Taul\u00b4 e,\n2022).\nSeveral works on stereotype detection and\nclassification have been carried out, in which\nspecific social groups, e.g., women and immi-\ngrants, have been the focus of research, since\nthey are usually the target of such messages.\nFor instance, Automatic Misogyny Identifi-\ncation (Fersini, Rosso, and Anzovino, 2018)\npresents a classification subtask in which one\nof the categories of misogyny is Stereotype\nand Objectification understood as a fixed\nand oversimplified image or idea of a woman.\nLast year\u2019s IberLEF 2021 edition task EX-\nIST (Rodr\u00b4 \u0131guez-S\u00b4 anchez et al., 2021) tack-\nled the topic of sexism in social networks.\nMoreover, studies on the detection of gen-\nder stereotypes have also been addressed in\n(Cryan et al., 2020; Chiril, Benamara, and\nMoriceau, 2021). Among the perspectives\non identifying stereotypes within narratives,\nthere are studies of microportraits in Muslim\nstereotyping in which a description of the tar-\nget group is provided in a single text (Fokkens\net al., 2019). Sap et al. (2020) approach\nthe problem of stereotypes for several tar-\nget groups in the Social Bias Frame, a new\nconceptual formalism that aims to model the\npragmatic frames in which people project so-\ncial bias and stereotypes onto others. Evalita\n2020\u2019s HaSpeeDe 2 task includes a subtask on\nthe identification of immigrants, Muslims and\nRoma (Sanguinetti et al., 2020). Narrowing\ndown on the topic of immigration, S\u00b4 anchez-\nJunquera et al. (2021) put forward a clas-\nsification of such stereotypes as manifested\nin political debates. The stereotype classi-\nfication applied in this task is based on the\nlatter work but uses a corpus extracted from\ncomments authored by web users on Spanish\nnews articles related to immigration. In gen-\neral, in these comments, a racial stereotype\nbased on origin, ethnicity, race and religion\nis associated with a target group.\n3 Task Description\nThe aim of the DETESTS task is to detect\nand classify stereotypes in sentences from\nAlejandro Ariza-Casabona, Wolfgang S. Schmeisser-Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique Amig\u00f3, Berta Chulvi, Paolo Rosso\n218comments posted in Spanish in response to\ndifferent online news articles related to im-\nmigration. A sentence can contain one or\nmore stereotypes belonging to different cat-\negories and, therefore, it may have multiple\nlabels that need to be accurately detected.\nThis scenario is known in the literature as a\nmulti-label classification problem. However,\nto adapt the problem to a variety of partici-\npants\u2019 interests, the task is designed in a hi-\nerarchical fashion by chaining two subtasks\nand allowing participants to either model the\nsimple binary scenario or complete the entire\npipeline by modeling the complex multi-label\nclassification problem.\nSubtask 1: Detection of Stereotypes\nParticipants that tackled this problem had\nto determine whether the sentences in a com-\nment contain at least one stereotype (positive\nexample) or none (negative example) consid-\nering the full distribution of labels provided\nby the annotators. The gold standard of this\nsubtask is left as a proxy to determine the\nsubset of sentences that will be evaluated in\nthe posterior subtask. For this subtask, we\nalso invited participants to consider a learn-\ning with disagreements approach, proposed\nin SemEval 2021 Task 12 (Uma et al., 2021),\nin which the authors state that there does\nnot necessarily exist a single gold standard\nfor every sample in the dataset.\nSubtask 2: Classification of stereotypes\nThis subtask consists of determining whether\na sentence contains at least one stereotype\nor none and assigning those sentences previ-\nously marked as positive (with stereotypes)\nto at least one of the ten categories that\npresent immigrants as: 1) \u2018victims of xeno-\nphobia\u2019, 2) \u2018suffering victims\u2019, 3) \u2018economic\nresources\u2019, 4) a problem of \u2018migration con-\ntrol\u2019, 5) people with \u2018cultural and religious\ndifferences\u2019, 6) people that take advantage\nof welfare \u2018benefits\u2019, 7) a problem for \u2018pub-\nlic health\u2019, 8) a threat to \u2018security\u2019, 9) \u2018dehu-\nmanization\u2019 and 10) \u2018other\u2019 types of stereo-\ntypes. Since a sentence can contain multiple\nstereotypes belonging to different categories,\nthis subtask is presented as a multi-label hi-\nerarchical classification problem.\nTeams were allowed (and encouraged) to\nsubmit multiple runs (max. 5). Subtask 2\nwas optional.4 Dataset\nThe DETESTS dataset consists of 5,629 sen-\ntences, with an average of 24% of them con-\ntaining stereotypes. It is made up of two\nparts -one from the NewsCom-TOX corpus\n(Taul\u00b4 e et al., 2021) (3,306 sentences) and\nthe other from the StereoCom corpus (2,323\nsentences), which was created especially for\nthis task. Both corpora consist of comments\npublished in response to different articles\nextracted from Spanish online newspapers\n(ABC, elDiario.es, El Mundo, NIUS, etc.)\nand discussion forums (such as Men\u00b4 eame1).\nIn the case of NewsCom-TOX, the dates of\nthe articles range from August 2017 to Au-\ngust 2020, while in StereoCom they range\nfrom June 2020 to November 2021.\nTo collect the NewsCom-TOX corpus, a\nkeyword-based approach was used to search\nfor articles related mainly to racism and\nxenophobia. Then, the articles were man-\nually selected based on their controversial\nsubject matter, potential toxicity and the\nnumber of published comments (minimum 50\ncomments per article). Since the NewsCom-\nTOX corpus was designed primarily to study\ntoxicity and not stereotypes, we used only\nthe part of the corpus with the highest per-\ncentage of stereotypes, which had been anno-\ntated previously. In order to obtain a suffi-\ncient and balanced data volume in terms of\nthe presence or absence of stereotypes, the\nsame content was also collected for the Stere-\noCom corpus, i.e., comments in response to\nimmigration-related news items in Spanish\ndigital media, selected by subject matter on\nthe basis of a keyword search.\nThe comments were presented in the same\norder in which they appeared in the tempo-\nral web thread, along with the conversational\nthread. Each comment was segmented into\nsentences, and the comment to which every\nsentence belongs and its position within the\ncomment are indicated.\nThe default dataset includes the gold stan-\ndard annotation. If the participants wish\nto apply methods of learning with disagree-\nments, we will provide, upon request, the pre-\naggregated annotation, that is, the annota-\ntion of each annotator.\n1https://www.meneame.net\nOverview of DETESTS at IberLEF 2022: DETEction and classification of racial STereotypes in Spanish\n2194.1 Annotation Scheme\nTo accomplish the classification tasks, we an-\nnotated the dataset with the main labels to\nindicate the presence or absence of stereo-\ntypes and the category/ies of the stereotype\nto which they belong. Moreover, we an-\nnotated extra features that could help the\nparticipants to train their systems. Since\nmore than one stereotype corresponding to\ndifferent categories can appear in one sen-\ntence, this is a multi-label task. We based\nour stereotype categories on the work pro-\nposed by S\u00b4 anchez-Junquera et al. (2021). All\nthe labels are annotated with binary values\n(0=absence of the feature and 1=presence of\nthe feature).\n4.1.1 Main labels\nFor each sentence, annotators had to decide\nwhether there was at least one stereotype re-\nlated to a target group.\nStereotype: There is a process of homoge-\nnization of one characteristic of an individual\nor part of a group that is applied to the entire\ngroup based on their place of origin, ethnic-\nity or religion. Stereotypes can be expressed\nexplicitly or implicitly.\nAll sentences annotated with stereotypes\nare also annotated with at least one of the\ncategories listed below (see examples on the\ntask\u2019s website2):\nXenophobia Victims: The members of\nthe target group are perceived as victims of\nxenophobia and discrimination.\nSuffering Victims: The members of the\ntarget group are portrayed as victims of\npoverty and violence in their places of ori-\ngin, and as having to face difficult situations\nin their host countries.\nEconomic Resource: The members of the\ntarget group are seen as an economic re-\nsource. They do the jobs that locals do not\nwant to do, pay taxes, and solve the problems\narising from low population growth.\nMigration Control: Immigration\npresents a threat due to massive influxes and\na lack of control at the borders. Immigrants\nare illegal and they should be expelled. It is\nseen as an invasion.\nCultural and Religious Differences:\nThe major threat consists of the loss of the\n2https://detestsiberlef.wixsite.com/detests/tasksingroup\u2019s values and traditions, and the re-\nplacement of the target group\u2019s customs and\nreligions. Immigrants are also seen as unedu-\ncated and should adapt to their host country.\nBenefits: The target group competes with\nthe ingroup for resources such as public sub-\nsidies, school places, jobs, health care and\npensions. There is a perception of the tar-\nget group being priveliged over the ingroup.\nPublic Health: Immigrants are thought\nto be carriers of infections and diseases such\nas COVID-19, Ebola and HIV.\nSecurity: Immigration brings security is-\nsues. Due to immigration, there is an in-\ncrease in crime, domestic violence, robbery,\ndrug use, sexual assault, murder, terrorist at-\ntacks and public disorders.\nDehumanization: The members of the\ntarget group are seen as inferior beings and\nare compared with animals, parasites or\nscum. Their lives have less value than those\nof the ingroup.\nOthers: Any other racial stereotype that is\nnot covered in the previous categories.\n4.1.2 Additional labels\nThe DETESTS dataset has also been anno-\ntated with three other labels that may pro-\nvide extra features at the disposal of the par-\nticipants to use optionally to train their sys-\ntems. These additional labels are:\nRacial target: The target group is defined\nby place of origin, ethnicity or religion.\nOther target: The target group corre-\nsponds to other minorities or oppressed\ngroups based on gender, sexual orientation,\nphysical or mental health conditions or age,\namong others.\nImplicitness: This category refers to\nwhether the stereotype in the sentence is\nexpressed implicitly or explicitly.\n4.2 Annotation Process\nOnce we had defined what we understand by\nstereotypes, which categories we can observe\nin our data, and in which ways they can be\nmanifested in texts, we drew up annotation\nguidelines for the annotators.\nThe annotation process consisted of two\nstages. In the first stage, the annotation\nof the categories \u2018stereotype\u2019, \u2018racial target\u2019,\n\u2018other target\u2019 and \u2018implicitness\u2019 was carried\nout. The second stage consisted of the\nAlejandro Ariza-Casabona, Wolfgang S. Schmeisser-Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique Amig\u00f3, Berta Chulvi, Paolo Rosso\n220annotation of the categories of the stereo-\ntypes. Then, disagreements were discussed\nby the annotators and a senior annotator un-\ntil agreement was reached. The team of an-\nnotators involved in the task consisted of two\nexpert linguists and two trained annotators\nwho are students of linguistics.\nEach sentence was annotated in parallel\nby three annotators and an inter-annotator\nagreement (IAA) test was performed once all\nthe sentences had been annotated. As shown\nin Table 1, overall, the IAA test gave high\nresults, excluding the feature \u2018other target\u2019,\nwhich had a Fleiss\u2019 Kappa of 0.139. This may\nbe due to the scarcity of data correspond-\ning to that feature, since the average pair-\nwise % agreement is still one of the highest.\nA similar case, although with higher results,\ncan be observed for the category \u2018others\u2019. It\nis worth noticing as well that the categories\nof stereotypes with less IAA correlate with\nthe categories with the highest distribution\namong the sentences (see Table 2). These\ncategories are \u2018migration control\u2019, \u2018security\u2019,\n\u2018benefits\u2019 and \u2018culture\u2019. Moreover, these cat-\negories also co-occur together with a higher\nfrequency than other categories (see Figure 5\nin Appendix A).\nAv. pairwise Fleiss\u2019\nLabel % Agreement. Kappa\nstereotype 84.36% 0.573\nxenophobia 97.65% 0.348\nsuffering 94.55% 0.523\neconomic 96.86% 0.593\nmigration control 83.86% 0.669\nculture 90.68% 0.65\nbenefits 91.61% 0.764\nhealth 98.92% 0.744\nsecurity 89.85% 0.735\ndehumanization 93.43% 0.488\nothers 92.74% 0.372\nracial target 84.05% 0.619\nother target 98.61% 0.139\nimplicitness 81.66% 0.412\nTable 1: Inter-annotator agreement test.\n4.3 Training and Test Datasets\nParticipants were provided with 70% of the\ncorpus to train and validate their models on\n(3,817 comments) and the remaining 30%\nof the corpus (1,812 comments) was used\nas a test set to evaluate their performanceagainst unseen sentences3. In order to avoid\ndata leakage from the NewsCom-TOX cor-\npus released in the DETOXIS shared task, all\ntest sentences were extracted from the newly\nadded StereoCom corpus in a stratified man-\nner to keep a similar label distribution to the\none found in the training set. Note that, de-\nspite the fact that the training dataset con-\ntains all gold standard categories (see Sec-\ntion 3) together with three additional fea-\ntures \u2013 \u2018racial target\u2019, \u2018other target\u2019, \u2018implic-\nitness\u2019 \u2013 none of this information is provided\nin the test set, which merely includes com-\nment and sentence identifiers of each instance\n\u2013 the identifier of the sentence it replies to (if\nany), and the sentence text.\nCategory Comments Percentage\nxenophobia 21 1.55%\nsuffering 113 8.31%\neconomic 62 4.56%\nmigration 553 40.69%\nculture 265 19.50%\nbenefits 315 23.18%\nhealth 37 2.72%\nsecurity 376 27.67%\ndehumanization 100 7.36%\nothers 90 6.62%\nTable 2: Category distribution of sentences\nthat contain at least one type of stereotype.\nTable 2 shows the category distribution\nfor the subset of examples that are anno-\ntated as containing at least one stereotype.\nThis subset contains 1,359 sentences (out of\n5,629), that is, 24.14% of the whole corpus.\nIt is important to mention that, given the\nmulti-label nature of the task, some sentences\nmay contain stereotypes belonging to multi-\nple categories and the amount of overlapping\namong categories can be noticed in the his-\ntogram provided in Figure 1.\n5 Systems and Results\nThis section contains a brief description of\nthe proposed baselines, as well as an overview\nof the systems submitted by the participants,\na brief comparison of such models regarding\nthe selected evaluation metrics for each sub-\n3To avoid any conflict with the sources of the\ncomments regarding their intellectual property rights\n(IPR), a password to access the data was sent pri-\nvately to each participant who was interested in the\ntask after filling in a registration form. This dataset\nwill only be made available for research purposes.\nOverview of DETESTS at IberLEF 2022: DETEction and classification of racial STereotypes in Spanish\n221Figure 1: Multi-label distribution.\ntask, and a short analysis of their multi-label\ncapabilities. A Github repository is publicly\navailable with the implementation of the offi-\ncial metrics, the baselines, the systems eval-\nuation, and an overview analysis4.\n5.1 Baselines\nIn order to analyze certain performance\nboundaries in both subtasks, five different\nbaselines have been considered as reference\nmodels to be compared with the participant\u2019s\nsystems: AllOnes, AllZeros, RandomClassi-\nfier, TFIDF+SVC and FastText+SVC. Due\nto the fact that the second subtask consists of\na hierarchical multi-label classification task,\nwe have extended these baselines in a hier-\narchical fashion by first determining whether\nthe sentence contains at least one stereotype\nand a set of new baseline classifiers is trained\nupon those positive cases to predict each of\nthe stereotype categories (to tackle the multi-\nlabel classification problem).\nEach baseline is briefly introduced below:\nAllOnes: This baseline maps all instances\nto the positive class it is trying to classify.\nAllZeros: Analogously to AllOnes, this\nbaseline maps all instances to the negative\nclass. Therefore, this baseline is only consid-\nered in subtask 2 in which the negative class\nis actually accounted for by the evaluation\nmetrics.\nRandomClassifier: A weighted random\nclassifier picks a random class with proba-\nbilities based on the label distribution learnt\nfrom the training set.\nTFIDF+SVC: A TF-IDF vectorizer is\nused to extract sentence-level features based\n4https://github.com/alarca94/detestson the learnt 10,000, unicode, lowercased vo-\ncabulary of n-grams with sizes 1 to 3. The\nclassifier selected to classify instances based\non the extracted features is a Support Vector\nClassifier (SVC) with a linear kernel.\nFastText+SVC: This baseline replaces\nthe classical TF-IDF vectorizer with a word\nvector extractor based on the FastText al-\ngorithm followed by a mean pooling opera-\ntion for sentence-level representation. A SVC\nclassifier with a linear kernel is also used as\na component of this baseline.\nAll baselines have been implemented us-\ning Python language, together with the fol-\nlowing libraries: Numpy5, Pandas6, Scikit-\nlearn7, and SpaCy8.\n5.2 Systems Overview\nThe DETESTS shared task received submis-\nsions from 39 teams for subtask 1, although\nonly five of these teams decided to tackle sub-\ntask 2 as well. Participants were allowed to\nprovide up to five submissions per subtask.\nAmong the top-performing systems, we ob-\nserve an extended use of pre-trained language\nmodels for the Spanish language including\nboth BERT and RoBERTa. The main dif-\nferences that lead to the leaderboard rank-\ning presented in Tables 3 and 4 depended\non how they approached problems such as\ndata unbalance, the multi-label problem or\ncontextual information (for the ranking in-\ncluding the total of participants, visit task\u2019s\nwebsite9). Despite their lower performance,\nmore classical machine learning and NLP\ntechniques were considered either as base-\nlines or submission systems by multiple par-\nticipants. These participants provided en-\nsemble architectures and bagging strategies\nwith Bag-of-Words representations and mod-\nels such as SVC, Random Forest Classifier\nand/or Logistic Regression. It is worth not-\ning that both DETESTS subtasks are really\nchallenging, especially for those classical ma-\nchine learning models whose representational\ncapabilities depend mainly on the quality of\nthe input features. Another main problem\nthat participants had to face in this compe-\ntition was the fact that the variety of pre-\n5https://numpy.org/doc/stable/index.html\n6https://pandas.pydata.org/\n7https://scikit-learn.org/stable/\n8https://spacy.io/\n9https://detestsiberlef.wixsite.com/detests/\nevaluation-results\nAlejandro Ariza-Casabona, Wolfgang S. Schmeisser-Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique Amig\u00f3, Berta Chulvi, Paolo Rosso\n222trained and fine-tuned language models for\nSpanish, although continuously increasing, is\nstill very limited. The most interesting ap-\nproaches in the competition are summed up\nbelow.\nFirst, the top scoring team I2CIII\n(V\u00b4 azquez et al., 2022) opted for two merg-\ning multiple strategies that tackled the prob-\nlems of unbalanced data and semantic tex-\ntual representation. On the one hand, they\ntried to balance the dataset with both under-\nsampling and Bagging of the majority class,\nand oversampling of the minority class with\na double translation from Spanish to English\nand back. Moreover, I2C III implemented\nan ensemble architecture combining not only\nbalancing techniques but different pretrained\nlanguage models to increase the semantic rep-\nresentation capabilities of the system.\nSecond, UMUTeam (Garc\u00b4 \u0131a-D\u00b4 \u0131az,\nJim\u00b4 enez-Zafra, and Valencia-Garc\u00b4 \u0131a, 2022)\nmade use of their own UMUTextStats tool\nto extract a set of 389 linguistic feature\nsets that were combined together with some\nnegation features, non-contextual word\nvector representations (FastText) and con-\ntextual pre-trained language modelling using\nboth BETO and RoBERTa. In the end,\ntheir model combined these representations\nvia either knowledge integration or ensemble\nlearning, thereby proving the importance\nof good feature selection. It is important\nto note that negation features only boosted\ntheir model for subtask 2, which may indi-\ncate a bigger impact on the discriminative\npower of the models for stereotype category\nclassification, as opposed to their influence\non simpler stereotype binary detection.\nAn important point regarding the submit-\nted models is that none of them tries to en-\nrich the contextual information by extract-\ning representations from other sentences in\nthe same comment. However, the LakNLP\nteam (Laknani and Garc\u00b4 \u0131a-Martinez, 2022)\nbenefits from the additional features (\u2018im-\nplicitness\u2019 and \u2018racial target\u2019) included in the\ntraining set that participants were provided\nwith. Given the fact that these features were\nnot part of the test dataset, Lak NLP develop\na meta-classifier to learn this additional fea-\nture distribution and included its prediction\nas auxiliary input to the pre-trained BETO\nmodel leading to an overall good performance\nin both subtasks.\nFurthermore, the DaMinCi team(Cabestany, Adsuar, and L\u00b4 opez, 2022) tried\nto distinguish itself from the rest of the\nparticipants by incorporating Adapters to\nthe fine-tuning strategy of the pre-trained\nlanguage models. This adapter-based model\nconsists of incorporating bottleneck layers\nbetween the existing hidden layers of the\nselected model (RoBERTa in their case)\nand freezing pre-existing model weights\nduring fine-tuning. According to their own\nvalidation and their final score on subtask 1,\nthis approach outperforms other interesting\nalternatives such as a fine-tuned RoBERTa\nmodel on auxiliary tasks that leverage\nknowledge learnt from related domains.\nLast but not least, the MALNIS team\n(Ramirez-Orta1 et al., 2022) approached\nthe DETESTS shared task as a Multi-Task\nLearning problem in which a final classifica-\ntion head per stereotype category is stacked\non top of a pre-trained RoBERTa model and\nfine-tuned using a point-wise Cross-Entropy\nloss function. Their system showed the im-\nportance of jointly modelling the distribu-\ntion of all stereotype categories in the over-\nall model performance for both subtasks by\nranking first in subtask 2. Although not all\nparticipants mentioned their preprocessing\nstrategies in their respective working notes,\npre-processing may play an important role\nin the behavior of the models, especially if\nwe are considering classical machine learning\nmodels built from scratch. Some of the steps\nthat have been implemented by several par-\nticipants range from common tokenization,\nstopwords removal, lowercasing, numbers re-\nmoval, URL and user tags masking, as well\nas spell correction.\n5.3 Metrics\nSubtasks 1 and 2 have been evaluated with\ndifferent metrics. Subtask 1 is a binary classi-\nfication problem and the F-measure combin-\ning Precision and Recall on the positive class\n(stereotype) was applied. In addition, sub-\ntask 2 was interpreted as a two-level multi-\nclass hierarchical classification problem. The\nfirst level corresponds to the binary classi-\nfication of the previous task (stereotype or\nnon-stereotype). On a second level, the posi-\ntive class is decomposed into the ten subcat-\negories described in Section 4.1. The multi-\nclass classification metrics can be label or\ninstance-based. Label-based metrics evalu-\nate systems independently for each class. We\nOverview of DETESTS at IberLEF 2022: DETEction and classification of racial STereotypes in Spanish\n223have discarded this type of metrics as they\ndo not consider the specificity and relative\nweight of the classes. In contrast, instance-\nbased metrics evaluate label sets item by\nitem. Within this family we have consid-\nered the following three metrics. The first\nis label propensity applied over precision and\nrecall for single items. Each accurate class\nin the intersection is weighted according to\nthe class propensity pc(Jain, Prabhu, and\nVarma, 2016). In particular, we have con-\nsidered the variant proposed by Amig\u00b4 o and\nDelgado (2022), with s(i) and g(i) being the\nset of classes assigned to item iin the system\noutput and gold standard respectively.\nPropP(i) =P\nc\u2208s\u2032(i)\u2229g\u2032(i)1\npcP\nc\u2208s\u2032(i)1\npc\nPropR(i) =P\nc\u2208s\u2032(i)\u2229g\u2032(i)1\npcP\nc\u2208g\u2032(i)1\npc\nwhere s\u2032(i) =s(i)\u222a {c\u2205}andg\u2032(i) =g(i)\u222a\n{c\u2205}. The reason for adding the empty class\nc\u2205is to capture the specificity of classes\nin mono-label items. The propensity fac-\ntorpcfor each class is computed as: pc=\n1\n1+Ce\u2212Alog2(Nc+B)where Ncis the number of\ndata points annotated with label cin the ob-\nserved ground truth data set of size Nand\nA,Bare application specific parameters and\nC= (logN \u22121)(B + 1)A. In this evaluation\ncampaign, we set the recommended param-\neter values A= 0.55 and B= 1.5. Propen-\nsity F-measure (PROP-F) is computed as the\nharmonic mean of these values.\nThe previous metric captures the speci-\nficity of classes appropriate in unbalanced\ndata sets. However, it does not capture hier-\narchical relationships. For this, we also ap-\nplied hierarchical-based metrics that consider\nthe ancestor overlap (Kiritchenko, Matwin,\nand Famili, 2004; Costa et al., 2007). More\nconcretely, hierarchical precision and recall\nare computed as the intersection of ances-\ntor divided by the amount of ancestors of the\nsystem output category and of the gold stan-\ndard respectively. In our evaluation, when\ncomputing the ancestor overlap we consider\nthe common empty label (root class) in order\nto avoid undefined situations. Their combi-\nnation is the Hierarchical F-measure (HF).\nSince these metrics are based on category\nset overlap, they can be applied as example\nbased multi-label classification by joining an-cestors and computing the F measure. Their\ndrawback is that the specificity of categories\nis not strictly captured since they assume a\ncorrespondence between specificity and hier-\narchical deepness. However, this correspon-\ndence is not necessarily true. Categories in\nfirst levels can be infrequent whereas leaf cat-\negories can be very common in the data set.\nIn order to capture both aspects simulta-\nneously, the official metric in this campaign\nis the Information Contrast Model (ICM)\n(Amig\u00b4 o and Delgado, 2022), which is a sim-\nilarity measure that unifies measures based\non both object feature sets and Information\nTheory (Amig\u00b4 o et al., 2020). Given two class\nsetss(i) and g(i), ICM is computed as:\nICM(A, B ) =\u03b11I(s(i))+\u03b1 2I(g(i))\u2212\u03b2I (s(i)\u222ag(i))\nwhere I(X) represents the information con-\ntent (\u2212log (P(X)) of the class set X. The\nintuition is that the more unlikely the cate-\ngory sets are to occur simultaneously (large\nI(s(i)\u222ag(i))), the less they are similar. Given\na fixed joint IC, the more the category sets\nare specific ( I(s(i)) and I(g(i))), the more\nthey are similar. ICM is grounded on simi-\nlarity axioms supported by the literature in\nboth information access and cognitive sci-\nences (Amig\u00b4 o et al., 2020). According to\nAmig\u00b4 o and Delgado (2022), the information\ncontent of a class set can be computed as:\nI({c1, c2, .., c n}) =\nI(c1) +I [\ni=2..n{ci}!\n\u2212I [\ni=2..n{lso(c 1, ci)}!\nwhere lso(c i, cj) represents the common an-\ncestor of the classes ciandcj.\n5.4 Subtask 1\nTable 3 shows the ranking of participating\nsystems for subtask 1 according to the F-\nmeasure on the positive class. The table in-\ncludes the best run per team that sent work-\ning notes. All the systems show better results\nthan the baselines. The random classifier is\nthe worst baseline and labeling all items as\npositive achieves an F-score of 0.42.\nFigure 2 plots the precision and recall\nscores for every run. As the figure shows,\nsome systems manage to distinguish them-\nselves from the rest in both precision and\nrecall by following the diagonal in the di-\nAlejandro Ariza-Casabona, Wolfgang S. Schmeisser-Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique Amig\u00f3, Berta Chulvi, Paolo Rosso\n224Figure 2: Precision vs. Recall in subtask 1.\nRanking Team Name F-Score\nGold Standard 1.0000\n1 I2C III 0.7042\n3 UMUTeam 0.6990\n5 Lak NLP 0.6627\n6 DaMinCi 0.6596\n9 MALNIS 0.6382\nFastText+SVC 0.4861\nTFIDF+SVC 0.4706\nAllOnes 0.4243\nRandomClassifier 0.2295\nTable 3: Evaluation results in subtask 1.\nrection of the (1,1) point of the gold stan-\ndard. This distribution indicates that the\nstandard F-measure weighting (precision and\nrecall equally weighted) is appropriate for es-\ntablishing the official ranking.\n5.5 Subtask 2\nTable 4 shows the ranking of systems ac-\ncording to the metrics ICM, hierarchical F-\nmeasure (HF) and Propensity F (Prop-F).\nAgain, the baseline systems (AllZeros, Ran-\ndomClassifier and AllOnes) obtain lower re-\nsults than those obtained by the participating\nsystems. In particular, assigning all possible\nlabels to all items (AllOnes) is penalized by\nall metrics and especially by ICM since the\nsystem introduces a lot of missing informa-\ntion in relation to very specific classes. All\nthree metrics agree that not assigning any\nclass (AllZeros) is a better option than any\nother arbitrary baseline.\nFigure 3 shows the relationship between\nHF and Prop-F scores. As the figure shows,\nthese metrics correlate in this benchmark.\nFigure 3: Hierarchical F-measure vs.\nPropensity F-measure in subtask 2.\nFigure 4: ICM vs. Propensity F-measure in\nsubtask 2.\nThis suggests that both the hierarchical dis-\ntance captured by HF and the class speci-\nficity captured by Prop-F are not determin-\nistic aspects in this task. This is because the\nhierarchical structure is quite simple and the\nclasses are relatively balanced in the data set.\nHowever, as Figure 4 shows, there is a\nslight mismatch between ICM and the other\ntwo metrics. This is because both HF and\nPROP-F compare, for each item, the set of\nlabels assigned by the system and the set of\nclasses to which it belongs through the F-\nmeasure on Precision and Recall. Note that\nPrecision and Coverage are ratio-based simi-\nlarity criteria between intersection and one\nof the sets (system output in the case of\nPrecision and gold standard in the case of\nOverview of DETESTS at IberLEF 2022: DETEction and classification of racial STereotypes in Spanish\n225Ranking Team Name ICM HF Prop-F\nGold Standard 1.6676 1.0000 1.0000\n1 MALNIS -0.2380 0.8813 0.8717\n2 UMUTeam -0.3298 0.8818 0.8718\n4 Lak NLP -0.4242 0.8606 0.8470\nTFIDF+SVC -0.6954 0.8552 0.8442\nAllZeros -1.1280 0.8317 0.8215\nFastText+SVC -1.1348 0.8314 0.8154\nRandomClassifier -2.0403 0.7493 0.7308\nAllOnes -36.3162 0.2224 0.1354\nTable 4: Evaluation results in subtask 2.\nRecall). In contrast, the similarity scheme\nused in ICM considers the individual sets and\ntheir union. In other words, for evaluation\npurposes, our results suggest that the multi-\nlabeling and the way in which the label sets\nare compared has more effect than the hier-\narchical structure or the class balance.\n6 Conclusions and Future Work\nThis paper has described the DETESTS chal-\nlenge at IberLEF 2022 and summarized the\nparticipation of several teams in both sub-\ntasks, emphasizing the relevant differences\nthat led to the final ranking. It is clear\nhow important pre-trained language models\nare for complex natural language tasks such\nas stereotype classification and the fact that\nnew model checkpoints for the Spanish lan-\nguage are increasingly being shared, allow-\ning participants to achieve better results and\ncome up with innovative solutions that cou-\nple well with state-of-the-art systems. Re-\ngarding the actual task, it has been designed\nas a hierarchical task that aims for stereo-\ntype detection and classification in Spanish\nsentences. Each sentence can contain up to\nten different stereotype categories and three\nadditional features are included to aid in the\npattern representation of the models. Also,\nour dataset (by explicit request) also incor-\nporates the labels of all annotators prior to\ntheir aggregation in case participants want\nto apply methods of learning with disagree-\nments.\nThe winners of both subtasks tackled the\nmajor problems directly. On the one hand,\nfor this first subtask, I2C III noticed the neg-\native effect of the unbalanced data and in-\ncorporated UnderBagging and Oversampling\nstrategies to overcome it while employing\npowerful language models in an ensemble ar-\nchitecture. On the other hand, for the sec-ond subtask, MALNIS modeled the joint cat-\negory distribution with a Multi-Task Learn-\ning strategy giving their system an important\nboost in terms of ICM, HF and Prop-F.\nUnfortunately, the effect of data balanc-\ning was not explored for subtask 2 and,\nthus, remains open for future work. Other\nfuture research directions worth following\nthat did not appear in any participant\u2019s\nmodel includes methods of learning with\ndisagreements, adding more contextual in-\nformation to the current sentences such as\ncomment-level representation or topic mod-\nelling, among others. Finally, despite the\nfact that the DaMinCi team tried to use fine-\ntuned models on related tasks, it would be\ninteresting to verify domain commonalities\nand try to transfer complementary informa-\ntion to these pre-trained architectures more\nefficiently.\nAcknowledgements\nThis work is supported by the following\nprojects: \u2018STERHEOTYPES: STudying\nEuropean Racial Hoaxes and sterEO-\nTYPES\u2019 funded by Fondazione Com-\npagnia di San Paolo and grant \u2018XAI-\nDisInfodemics: eXplainable AI for disin-\nformation and conspiracy detection during\ninfodemics\u2019 (PLEC2021-007681) funded by\nMCIN/AEI/10.13039/501100011033 and,\nas appropriate, by the \u201cEuropean Union\nNextGenerationEU/PRTR\u201d. The work of\nPaolo Rosso was carried out within the\nframework of the research project PROM-\nETEO/2019/121 (DeepPattern) by the\nGeneralitat Valenciana.\nReferences\nAllport, G. W., K. Clark, and T. Pettigrew.\n1954. The nature of prejudice. Addison-\nwesley Reading, MA.\nAlejandro Ariza-Casabona, Wolfgang S. Schmeisser-Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique Amig\u00f3, Berta Chulvi, Paolo Rosso\n226Amig\u00b4 o, E. and A. D. Delgado. 2022.\nEvaluating extreme hierarchical multi-\nlabel classification. In Proceedings of the\n60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022 , pages 5809\u20135819.\nAmig\u00b4 o, E., F. Giner, J. Gonzalo, and\nF. Verdejo. 2020. On the foundations of\nsimilarity in information access. Inf. Retr.\nJ., 23(3):216\u2013254.\nBasile, V., M. Fell, T. Fornaciari, D. Hovy,\nS. Paun, B. Plank, M. Poesio, and\nA. Uma. 2021. We need to consider dis-\nagreement in evaluation. In Proceedings\nof the 1st Workshop on Benchmarking:\nPast, Present and Future , pages 15\u201321,\nOnline, August. Association for Compu-\ntational Linguistics.\nCabestany, D., C. Adsuar, and M. L\u00b4 opez.\n2022. DaMinCi at IberLEF-2022 DE-\nTESTS task: Detection and Classification\nof Racial Stereotypes in Spanish. In Pro-\nceedings of the Iberian Languages Evalua-\ntion Forum (IberLEF 2022). CEUR Work-\nshop Proceedings, CEUR-WS.org.\nChiril, P., F. Benamara, and V. Moriceau.\n2021. \u201cBe Nice to your wife! The Restau-\nrants are Closed\u201d: Can Gender Stereotype\nDetection Improve Sexism Classification?\nInFindings of the Association for Compu-\ntational Linguistics: EMNLP 2021, pages\n2833\u20132844, Punta Cana, Dominican Re-\npublic, November. Association for Com-\nputational Linguistics.\nCosta, E. P., A. C. Lorena, A. C. Carvalho,\nand A. A. Freitas. 2007. A review of per-\nformance evaluation measures for hierar-\nchical classifiers. AAAI Workshop - Tech-\nnical Report , 01.\nCryan, J., S. Tang, X. Zhang, M. Metzger,\nH. Zheng, and B. Y. Zhao, 2020. Detecting\nGender Stereotypes: Lexicon vs. Super-\nvised Learning Methods, page 1\u201311. As-\nsociation for Computing Machinery, New\nYork, NY, USA.\nFersini, E., P. Rosso, and M. E. Anzovino.\n2018. Overview of the task on automatic\nmisogyny identification at ibereval 2018.\nInIberEval@SEPLN .\nFokkens, A., N. Ruigrok, C. Beukeboom,\nS. Gagestein, and W. Van Atteveldt.2019. Studying muslim stereotyping\nthrough microportrait extraction. In\nH. Isahara, B. Maegaard, S. Piperidis,\nC. Cieri, T. Declerck, K. Hasida, H. Mazo,\nK. Choukri, S. Goggi, J. Mariani,\nA. Moreno, N. Calzolari, J. Odijk, and\nT. Tokunaga, editors, Proceedings of the\nLREC 2018, Eleventh International Con-\nference on Language Resources and Eval-\nuation, pages 3734\u20133741. European Lan-\nguage Resources Association (ELRA).\nConference date: 07-05-2018 Through 12-\n05-2018.\nGarc\u00b4 \u0131a-D\u00b4 \u0131az, J. A., S. M. Jim\u00b4 enez-Zafra, and\nR. Valencia-Garc\u00b4 \u0131a. 2022. UMUTeam\nat IberLEF-2022 DETESTS task: Fea-\nture Engineering for the Identification and\nCategorization of Racial Stereotypes in\nSpanish. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2022). CEUR Workshop Proceedings,\nCEUR-WS.org.\nJain, H., Y. Prabhu, and M. Varma. 2016.\nExtreme multi-label loss functions for rec-\nommendation, tagging, ranking & other\nmissing label applications. In Proceed-\nings of the 22nd ACM SIGKDD Inter-\nnational Conference on Knowledge Dis-\ncovery and Data Mining, KDD \u201916, page\n935\u2013944, New York, NY, USA. Associa-\ntion for Computing Machinery.\nKiritchenko, S., S. Matwin, and F. Famili.\n2004. Hierarchical text categorization as\na tool of associating genes with gene on-\ntology codes. Proceedings of the 2nd Euro-\npean Workshop on Data Mining and Text\nMining in Bioinformatics, 01.\nLaknani, F. and M. Garc\u00b4 \u0131a-Martinez. 2022.\nLakNLP at IberLEF-2022 DETESTS\ntask: Automatic Classification of Stereo-\ntypes in Text. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022). CEUR Workshop Pro-\nceedings, CEUR-WS.org.\nRamirez-Orta1, J., M. V. Sabando,\nM. Maisonnave1, and E. Milios. 2022.\nMALNIS at IberLEF-2022 DETESTS\nTask: A Multi-Task Learning Approach\nfor Low-Resource Detection of Racial\nStereotypes in Spanish. In Proceedings\nof the Iberian Languages Evaluation\nForum (IberLEF 2022). CEUR Workshop\nProceedings, CEUR-WS.org.\nOverview of DETESTS at IberLEF 2022: DETEction and classification of racial STereotypes in Spanish\n227Rodr\u00b4 \u0131guez-S\u00b4 anchez, F., J. C. de Albornoz,\nL. Plaza, J. Gonzalo, P. Rosso, M. Comet,\nand T. Donoso. 2021. Overview of exist\n2021: sexism identification in social net-\nworks. Procesamiento del Lenguaje Natu-\nral, 67(0):195\u2013207.\nSanguinetti, M., G. Comandini, E. di\nNuovo, S. Frenda, M. Stranisci, C. Bosco,\nT. Caselli, V. Patti, and I. Russo. 2020.\nHaspeede 2 @ evalita2020: Overview of\nthe evalita 2020 hate speech detection\ntask. In V. Basile, D. Croce, M. Di Maro,\nand L. Passaro, editors, Proceedings of the\nSeventh Evaluation Campaign of Natu-\nral Language Processing and Speech Tools\nfor Italian. Final Workshop (EVALITA\n2020), volume 2765. CEUR Workshop\nProceedings (CEUR-WS.org). Conference\ndate: 17-12-2020.\nSap, M., S. Gabriel, L. Qin, D. Jurafsky,\nN. A. Smith, and Y. Choi. 2020. So-\ncial bias frames: Reasoning about Social\nand power implications of language. In\nProceedings of the 58th Annual Meeting\nof the Association for Computational Lin-\nguistics, pages 5477\u20135490, Online, July.\nAssociation for Computational Linguis-\ntics.\nSchmeisser-Nieto, W., M. Nofre, and\nM. Taul\u00b4 e. 2022. Criteria for the anno-\ntation of implicit stereotypes. In Proceed-\nings of the Language Resources and Eval-\nuation Conference , pages 753\u2013762, Mar-\nseille, France, June. European Language\nResources Association.\nS\u00b4 anchez-Junquera, J., B. Chulvi, P. Rosso,\nand S. P. Ponzetto. 2021. How do you\nspeak about immigrants? taxonomy and\nstereoimmigrants dataset for identifying\nstereotypes about immigrants. Applied\nSciences, 11(8).\nTajfel, H. 1984. Grupos humanos y catego-\nrias sociales. Herder.\nTajfel, H., A. A. Sheikh, and R. C. Gardner.\n1964. Content of stereotypes and the in-\nference of similarity between members of\nstereotyped groups. Acta Psychologica,,\n22(3):191\u2013201.\nTaul\u00b4 e, M., A. Ariza, M. Nofre, E. Amig\u00b4 o, and\nP. Rosso. 2021. Overview of DETOXIS at\nIberLEF 2021: DEtection of TOXicity in\ncomments In Spanish. Procesamiento del\nLenguaje Natural , 67(0):209\u2013221.Uma, A., T. Fornaciari, A. Dumitrache,\nT. Miller, J. Chamberlain, B. Plank,\nE. Simpson, and M. Poesio. 2021.\nSemEval-2021 Task 12: Learning with\nDisagreements. In Proceedings of the\n15th International Workshop on Semantic\nEvaluation (SemEval-2021), pages 338\u2013\n347, Online, August. Association for Com-\nputational Linguistics.\nV\u00b4 azquez, J. M., V. P. \u00b4Alvarez, C. T. Taybi,\nand P. P. S\u00b4 anchez. 2022. I2C at IberLEF-\n2022 DETESTS task: Detection of Racist\nStereotypes in Spanish Comments using\nUnderBagging and Transformers. In Pro-\nceedings of the Iberian Languages Evalua-\ntion Forum (IberLEF 2022). CEUR Work-\nshop Proceedings, CEUR-WS.org.\nA Appendix: Co-occurrence of\nStereotype Categories within a\nsentence\nThis appendix provides a heatmap of the co-\noccurrence of stereotype categories within a\nsentence to visually spot those categories that\nare used together more often (see Figure 5).\nFigure 5: Heatmap representation of the\nsentence-level co-ocurrence of stereotype cat-\negories with the occurrence count of each cat-\negory coloured in gray.\nAlejandro Ariza-Casabona, Wolfgang S. Schmeisser-Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique Amig\u00f3, Berta Chulvi, Paolo Rosso\n228Overview of EXIST 2022:sEXism Identification in Social neTworks\nOverview de EXIST 2022:\nIdentificaci\u00b4 on de Sexismo en Redes Sociales\nFrancisco Rodr\u00b4 \u0131guez-S\u00b4 anchez1, Jorge Carrillo-de-Albornoz1, Laura Plaza1,\nAdri\u00b4 an Mendieta-Arag\u00b4 on1, Guillermo Marco-Rem\u00b4 on1,\nMaryna Makeienko1, Mar\u00b4 \u0131a Plaza1, Julio Gonzalo1,\nDamiano Spina2, Paolo Rosso3\n1Universidad Nacional de Educaci\u00b4 on a Distancia\n2RMIT University, Australia\n3Universitat Polit` ecnica de Val` encia\nfrodriguez.sanchez@invi.uned.es,\n{jcalbornoz, lplaza, gmarco,julio }@lsi.uned.es,\n{mmakeienko,amendieta }@cee.uned, maria.plaza.morales95@gmail.com,\ndamiano.spina@rmit.edu.au, prosso@dsic.upv.es\nAbstract: The paper describes the organization, goals, and results of the sEXism\nIdentification in Social neTworks (EXIST)2022 challenge, a shared task proposed\nfor the second year at IberLEF. EXIST 2022 consists of two challenges: sexism\nidentification and sexism categorization of tweets and gabs, both in Spanish and\nEnglish. We have received a total of 45 runs for the sexism identification task\nand 29 runs for the sexism categorization task, submitted by 19 different teams.\nIn this paper, we present the dataset, the evaluation methodology, an overview of\nthe proposed systems, and the results obtained. The final dataset consists of more\nthan 12,000 annotated texts from two social networks (Twitter and Gab) labelled\nfollowing two different procedures: external contributors and trained experts.\nKeywords: Sexism Detection, Twitter, Gab, Spanish-English.\nResumen: El art\u00b4 \u0131culo describe la organizaci\u00b4 on, objetivos y resultados de EXIST\n2022 (sEXism Identification in Social neTworks), una competici\u00b4 on que se celebra\npor segundo a\u02dc no consecutivo en el foro IberLEF. EXIST 2022 consta de dos tareas:\ndetecci\u00b4 on de sexismo y categorizaci\u00b4 on de sexismo de tweets y gabs, tanto en espa\u02dc nol\ncomo en ingl\u00b4 es. Hemos recibido un total de 45 ejecuciones para la tarea de detecci\u00b4 on\nde sexismo y 29 ejecuciones para la tarea de categorizaci\u00b4 on de sexismo, enviadas por\n19 equipos diferentes. En el presente art\u00b4 \u0131culo, presentamos el conjunto de datos,\nla metodolog\u00b4 \u0131a de evaluaci\u00b4 on, una descripci\u00b4 on general de los sistemas propuestos\ny los resultados obtenidos. El conjunto final de datos consta de m\u00b4 as de 12.000\ntextos anotados de dos redes sociales (Twitter y Gab) etiquetados siguiendo dos\nprocedimientos diferentes: colaboradores externos y expertos en el dominio.\nPalabras clave: Detecci\u00b4 on de Sexismo, Twitter, Gab, Espa\u02dc nol-Ingl\u00b4 es.\n1 Introduction\nThe Oxford English Dictionary defines sex-\nism as \u201cprejudice, stereotyping or discrimi-\nnation, typically against women, on the ba-\nsis of sex\u201d. As stated in (Rodr\u00b4 \u0131guez-S\u00b4 anchez,\nCarrillo-de Albornoz, and Plaza, 2020), sex-\nism is frequently found in many forms in\nsocial networks, includes a wide range of\nbehaviours (such as stereotyping, ideologi-\ncal issues, sexual violence, etc.) (Donoso-V\u00b4 azquez and Rebollo-Catal\u00b4 an, 2018; Manne,\n2017) and may be expressed in different forms\n(direct, indirect, descriptive, reported, etc.)\n(Mills, 2008; Chiril et al., 2020). Subtle forms\nof sexism are particularly dangerous as they\ncan go unnoticed, and affect women in many\nfacets of their lives (Swim et al., 2001; Berg,\n2006).\nHowever, research on sexism in online\nplatforms has focused on detecting violent\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 229-240\nrecibido 05-07-2022 revisado 22-07-2022 aceptado 25-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-20\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalsexism and hate against women (Waseem,\n2016; Waseem and Hovy, 2016; Frenda et\nal., 2019). The previous edition of EXIST\n(Rodr\u00b4 \u0131guez-S\u00b4 anchez et al., 2021) was the first\nattempt to automatically detect and classify\nsexism in a broad sense, from explicit misog-\nyny to other subtle expressions that involve\nimplicit sexism behaviours. Therefore, the\nEXIST 2022 challenge is the second shared\ntask on sexism detection in social networks\nwhose aim is to identify and classify sexism\nin a broad sense. Like its first edition in 2021,\nEXIST 2022 has been proposed at IberLEF.\nDuring the first edition, we received a total\nof 70 runs for the sexism identification task\nand 61 for the sexism categorization chal-\nlenge, submitted by 31 different teams from\n11 countries, showing the great interest of the\ncommunity around sexism detection in social\nnetworks.\nThe EXIST 2022 shared task has been\nfocused on the same tasks as its first edi-\ntion: sexism identification and categoriza-\ntion. Furthermore, we proposed a new test\nset labelled by six experts trained to perform\nthe task. Thus, this new edition focuses on\naugmenting the quality of the labels and com-\nparing the dataset labelled by crowdsourcing\nto expert annotators. Moreover, balance be-\ntween the genders of the annotators was en-\nsured in order to avoid gender bias in the\nlabeling process. Annotators of different age\ngroups were also considered.\nIn this second edition of EXIST, we have\nreceived a total of 45 runs for the sexism iden-\ntification task and 29 runs for the sexism cat-\negorization task, submitted by 19 different\nteams. Results have improved with respect\nto the previous edition for task 1 (sexism\nidentification) and have remained similar for\ntask 2 (sexism categorization), which seems\nto indicate that classifying sexist expressions\naccording to the facet of women they under-\nmine is a difficult task that requires further\nresearch.\n2 Tasks\n2.1 Task Description\nThe EXIST 2022 shared task is defined as\na multilingual classification task. In particu-\nlar, the EXIST challenge is organized accord-\ning to two main subtasks: (i) sexism iden-\ntification (task 1), which aims to identify if\na message or post contains sexist content;\nand (ii) sexism categorization (task 2), whichaims to classify the type of sexism contained\nin a given sexist message or post. Partici-\npants were welcome to present systems that\nattempt both subtasks or one of them.\nTask 1 is defined as a binary classification\nproblem, where every system should deter-\nmine whether a text or message is sexist or\nnot. It includes any type of sexist expression\nor related phenomena, like descriptive or re-\nported assertions where the sexist message is\na report or a description of a sexist event. In\nparticular, we consider two labels:\n\u2022Sexist: the tweet or gab expresses sexist\nbehaviours or discourses.\n\u2022Non-Sexist: the tweet or gab does\nnot express any sexist behaviour or dis-\ncourse.\nOnce a message has been classified as sex-\nist, task 2 aims to categorize the message\naccording to the type of sexism it encloses.\nThe categorization has been revised by two\nexperts in gender issues, Trinidad Donoso\nand Miriam Comet from the University of\nBarcelona, and takes into account the differ-\nent aspects of women that are undermined.\nThis task is defined as a multi-class classifi-\ncation problem where each sexist tweet or gab\nmust be categorized in one of the 5 following\nclasses:\n\u2022Ideological and inequality: The text\ndiscredits the feminist movement, rejects\ninequality between men and women, or\npresents men as victims of gender-based\noppression.\n\u2022Stereotyping and dominance: The\ntext expresses false ideas about women\nthat suggest they are more suitable to\nfulfill certain roles (mother, wife, family\ncaregiver, faithful, tender, loving, sub-\nmissive, etc.), or inappropriate for cer-\ntain tasks (driving, hardwork, etc), or\nclaims that men are somehow superior\nto women.\n\u2022Objectification: The text presents\nwomen as objects apart from their dig-\nnity and personal aspects, or assumes or\ndescribes certain physical qualities that\nwomen must have in order to fulfill tra-\nditional gender roles (compliance with\nbeauty standards, hypersexualization of\nfemale attributes, women\u2019s bodies at the\ndisposal of men, etc.).\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrillo-de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta-Arag\u00f3n, Guillermo Marco-Rem\u00f3n, \nMaryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n230Text Task\n1 Task\n2\nWhere are\nall the white women at? non-sexist non-sexist\nFeminism\nis a war on men, but it\u2019s also a war on women. It\u2019s a war on female nature, a war on femininity. sexist ideological-inequalit y\nWoman\ndriving, be careful! sexist stereot yping-dominance\nNooffense\nbut I\u2019ve never seen an attractive african american hooker. Not a single one sexist obj\nectification\nIw\nanna touch your tits..you can\u2019t imagine what I can do on your body. sexist sexual-violence\nI hate\nmisogyny more than I hate women sexist misogyny-non-sexual-violence\nTable 1: Examples of messages for each task.\n\u2022Sexual violence: Sexual suggestions,\nrequests for sexual favors or harassment\nof a sexual nature (rape or sexual as-\nsault) are made.\n\u2022Misogyny and non-sexual violence:\nThe text expresses hatred and violence\ntowards women.\nExamples of each category for both tasks\nare reported in Table 1.\nA substantial difference between EXIST\n2022 and its first edition in 2021 is that, in\n2022, the test set was labelled by 6 experts\ntrained to perform the task, and therefore an-\nnotation quality is considerably higher. EX-\nIST 2021, on the other hand, presented a big-\nger test set labelled by crowdsourcing anno-\ntators using the Amazon Mechanical Turk1\n(MTurk) platform. Moreover, EXIST 2022\ntakes into account women and men may dif-\nfer in their perception of what is sexism, and\ntherefore the annotation group is composed\nof three women and three men.\n2.2 Evaluation Measures and\nBaselines\nIn order to evaluate the performance of the\ndifferent approaches proposed by the partici-\npants, we will use the Evaluation Framework\nEvALL2(Amig\u00b4 o et al., 2017; Amig\u00b4 o, Spina,\nand Carrillo-de Albornoz, 2018; Amig\u00b4 o et al.,\n2020). Within this framework, we will evalu-\nate the system outputs as classification tasks\n(binary and multi-class respectively) using\nstandard evaluation metrics, including Accu-\nracy, Precision, Recall, and macro-averaged\nF1-score.\nIn task 1, Sexism Identification, the re-\nsults of participants will be ranked using Ac-\ncuracy, as the distribution between sexist and\nnon-sexist categories is balanced. Besides,\nother measures will be computed, such as\nPrecision, Recall, and F1. All metrics will\nbe also computed by language. In particular,\nAccuracy has been computed as follows:\n1https://www.mturk.com/\n2www.evall.uned.esAccuracy =number of correctly predicted instances\nnumber of instances\nIn task 2, Sexism Categorization, we will\nuse macro-averaged F1-score to rank the sys-\ntem outputs. Similarly, we will compute\nother measures such as Precision and Recall.\nThe F1-score was computed as follows:\nF1=F1(sexism categorization)\n6\nwhere F1(sexism categorization) is calcu-\nlated as the sum of all classes (including non-\nsexist):\nF1(sexism categorization) =\nF1(non-sexist) + F1(ideological-inequality) +\nF1(misogyny-non-sexual-violence) +\nF1(objectification) + F1(sexual-violence) +\nF1(stereotyping-dominance)\nWe propose two different baselines so that\nwe can establish an expected performance of\nthe submitted runs. First, we provided a\nbenchmark (BASELINE ) based on Support\nVector Machine (linear kernel) trained on tf-\nidf features built from the texts unigrams.\nSecond, a model that labels each record based\non the majority class (Majority Class).\n3 Dataset\nThe EXIST 2022 shared task employs data\nfrom Twitter and Gab in English and Span-\nish. In particular, this edition uses the EX-\nIST 2021 dataset for training and a new test\nset labeled by experts in the task for testing.\nTherefore, Twitter data was used for both\ntraining and testing while Gab was only in-\ncluded in the EXIST 2022 training set. This\nway, participants can analyse whether includ-\ning data from a social network without \u201ccon-\ntent control\u201d in the training phase improves\nthe performance of their systems. In order to\nbuild the testing data for both tasks, we em-\nployed the same terms used in EXIST 2021.\nIn particular, the final set contains 116 seed\nterms for Spanish and 109 for English.\nTo create the new test set for this edi-\ntion, we used the Twitter API to search for\nOverview of EXIST 2022: sEXism Identification in Social neTworks\n231tweets written in English or Spanish contain-\ning some of the selected keywords. The setup\nof our crawler implies collecting 100 tweets\nfor each term daily. Crawling was performed\nduring the period from the 1st of January\n2022 until the 31st of January 2022, gather-\ning 170,210 tweets for Spanish and 206,549\nfor English. We have removed those with\nless than 60 tweets to ensure an appropri-\nate balance between seeds. The final set of\nseeds used contains 91 seeds for Spanish and\n94 seeds for English.\nRegarding the sampling process, approx-\nimately 7 tweets were randomly selected for\neach seed term within the period from 1st\nto 31st of January 2022. We randomly re-\nsampled these tweets for each language to\nbuild the final sampled set composed of 600\ntweets. The whole sampling process was de-\nfined taking into account different sources of\nbias. In particular, we considered three main\nsources: seed, temporal and user bias. We\ntried to mitigate seed bias by including a wide\nrange of terms that are used in both sexist\nand non-sexist contexts (116 terms for Span-\nish and 109 for English). Temporal bias be-\ntween training and testing data is mitigated\nsince there is a temporal gap of almost one\nyear between both sets. We also checked the\ntemporal gap between tweets for each seed\nto ensure that data is spread all over the pe-\nriod. Finally, we checked messages generated\nby users to ensure an appropriate balance.\nWe also took into account this principle to\nsplit the dataset into training and test sets\nand removed from the test set users who were\nalso present in the training set to avoid user\nbias.\nThe sampled data set was labelled through\na majority voting approach by six expert an-\nnotators trained to perform this task. Ini-\ntially, we developed an annotation guide in\nEnglish and Spanish in which we provided a\nclear explanation of each label along with a\nnumber of examples. We presented and ex-\nplained the guidelines to ensure that all ex-\nperts understood the task. Then, we did an\nannotation experiment proposing the 6 ex-\nperts to annotate the 20% of the test set ob-\ntaining a 0.387 kappa for task 1 and 0.336 for\ntask 2. These results indicated poor agree-\nment and were used to modify the annota-\ntion guide and revise all the problems with\nthe annotators. We repeated the experiment\nand obtained a 0.57 kappa for task 1 and 0.47for task 2 showing a moderate agreement that\naligns with the fact that the sexism detection\ntask from a broad perspective is not simple.\nSexism is even more subjective than misog-\nyny or hate speech to women thus the label-\ning process is harder. The final labels were\nselected according to the majority vote be-\ntween the 6 expert annotators in all cases.\nIn the case of a tie, the tweet/gap was dis-\ncarded. The final agreement for the whole\ndataset was 0.589 kappa for task 1 and 0.485\nfor task 2. Texts with disagreement for any\nof the classes were removed. The final EXIST\n2022 test set consists of 1058 tweets, where all\ntexts were randomly selected from the 1200\nsampled set.\nWe have also tried to avoid gender bias\nin the annotation process by employing three\nfemale annotators and three men annotators.\nGender bias may lead to algorithm bias.\nThe training data was provided as tab-\nseparated, according to the following fields:\n\u2022testcase: contains the string \u201cEX-\nIST2021\u201d or \u201cEXIST2022\u201d needed for\nthe evaluation tool EvALL.\n\u2022id: denotes a unique identifier of the\ntext.\n\u2022source: denotes the data source; it takes\nvalues \u201ctwitter\u201d or \u201cgab\u201d.\n\u2022language: denotes the language of the\ntext; it takes values \u201cen\u201d or \u201ces\u201d.\n\u2022text: contains the actual text.\n\u2022task1: defines whether the text is sexist\nor not; it takes values \u201csexist\u201d and \u201cnon-\nsexist\u201d.\n\u2022task2: defines the type of sexism (if ap-\nplicable); it takes values as:\n\u2013\u201cideological-inequality\u201d: denotes\nthe category \u201cIdeological and in-\nequality\u201d;\n\u2013\u201cmisogyny-non-sexual-violence\u201d:\ndenotes the category \u201cMisogyny\nand non-sexual violence\u201d;\n\u2013\u201cobjectification\u201d: denotes the cate-\ngory \u201cObjectification\u201d;\n\u2013\u201csexual-violence\u201d: denotes the cat-\negory \u201cSexual violence\u201d;\n\u2013\u201cstereotyping-dominance\u201d: denotes\nthe category \u201cStereotyping and\ndominance\u201d;\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrillo-de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta-Arag\u00f3n, Guillermo Marco-Rem\u00f3n, \nMaryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n232\u2013\u201cnon-sexist\u201d: denotes that the\ntweet or gab does not express any\nsexist behaviours or discourses.\nConcerning the test data, we removed\n\u201ctask1\u201d and \u201ctask2\u201d labels from the file that\nwas provided to the participants.\nThe entire EXIST dataset contains 12,403\nlabeled texts, 11,345 for training correspond-\ning to EXIST 2021 and a new test set con-\nsisting of 1,058 tweets. Table 2 summarizes\nthe description of the dataset, as well as the\nnumber of texts per class for both training\nand test sets, and the distribution by lan-\nguage.\n4 Overview of the Submitted\nApproaches\n60 groups from 14 countries signed up for EX-\nIST 2022, 19 of them submitted runs for task\n1, and 15 for task 2. In this challenge, each\nteam had the chance to submit a maximum\nof 6 runs, 3 runs for each task. We received\na total of 45 runs for task 1 and 29 runs for\ntask 2.\nRegarding the classification approaches,\nall of the participants submitted their results\nusing some sort of transformer-based system\nfor both tasks with the exception of one team.\nIn particular, 18 teams used some sort of\ntransformer architecture, of which 8 teams\nused BERT (Devlin et al., 2019) (or mul-\ntilingual BERT - mBERT), 5 used a Span-\nish version of BERT called BETO (Canete\net al., 2020), 4 used RoBERTa (Liu et al.,\n2019), 3 used DeBERTa v3 (He, Gao, and\nChen, 2021), 2 used a multilingual version\nof RoBERTa called XLM-R (Conneau et al.,\n2019) or other transformer versions. Tradi-\ntional machine learning methods like decision\ntrees or Logistic Regression (LR) have been\nadopted by only one team. This year, none\nof the teams experimented with other deep\nlearning methods (i.e. Long short-term mem-\nory networks - LSTM) or libraries. Following,\nwe list the participants and briefly describe\nthe approaches used by each group.\n2539404758 participated in both tasks\nand submitted one run for each task. They\nfine-tuned BERT for English texts and\nBETO for Spanish.\nAI-UPV participated in both tasks and\nsubmitted 3 runs for each task. Their sys-\ntem was based on an ensemble of transformer\nmodels in a single-language and multilin-gual configuration. In particular, they used\nBERT, RoBERTa, ELECTRA (Clark et al.,\n2020) and GPT-2 (Radford et al., 2019) as\ntransformer models.\nAIT FHSTP participated in both tasks\nand submitted 3 runs for each task. They\nexperimented with two multilingual trans-\nformer models, such as mBERT and XLM-\nR, and a monolingual (English) T5 model\n(Raffel et al., 2019). To train the models,\nthey used a two step approach. First, un-\nsupervised pre-training with additional data\nand second, supervised fine-tuning with ad-\nditional as well as augmented data. For these\nexperiments, they employed the MeTwo\ndataset (Rodr\u00b4 \u0131guez-S\u00b4 anchez, Carrillo-de Al-\nbornoz, and Plaza, 2020), HatEval 2019\ndataset (Basile et al., 2019) and other hate\nspeech related datasets.\navacaondata participated in both tasks\nand submitted 3 runs for each task. Their\nbest approach to the task is based on an en-\nsemble of different transformer models with\nBERTweet-large (Nguyen, Vu, and Nguyen,\n2020), RoBERTa and DeBERTa v3 for En-\nglish, and BETO, BERTIN (De la Rosa et al.,\n2022), MarIA-base (Guti\u00b4 errez-Fandi\u02dc no et al.,\n2021) and RoBERTuito (P\u00b4 erez et al., 2021)\nfor Spanish. Models were trained in two\nphases. First, a validation set was used for\nhyperparameter optimization, second, mod-\nels were trained using the whole training set.\nbesiguenza submitted one run for each\ntask. Their best system was based on mul-\ntilingual DeBERTa v3 and used back trans-\nlation techniques to augment the EXIST\ndataset.\nCIMATCOLMEX only participated in\ntask 1 with three different runs. Their best\napproach consisted in an ensemble of 10\nRoBERTuito and 10 BERT models each of\nthem is trained individually using different\nseeds.\nCompLingKnJ only participated in task\n1 with two different runs. Their best run\nwas based on transformers, where BETO was\nused for Spanish messages and BERT for\nEnglish. They experimented with a system\nbased on tf-idf features and traditional ma-\nchine learning techniques.\nELiRF-VRAIN participated in both tasks\nand submitted three runs for each task. Their\nsystem was based in a ensemble of 5 different\nmodels for Spanish (XLM-R, RoBERTa and\n3 BERT models) and other 5 models for En-\nOverview of EXIST 2022: sEXism Identification in Social neTworks\n233Training Testing\nTwitter Gab Twitter\nSpanish English Spanish English Spanish English Total\nSexist 2599 2494 265 300 254 215 6127\nNon-sexist 2612 2658 225 192 271 305 6263\nIdeological-inequalit y 695 619 73 100 97 64 1648\nMisogyn y-non-sexual-violence 600 436 58 63 32 25 1214\nObjectification 368 377 50 29 18 21 863\nSexual-violence 304 494 71 48 44 43 1004\nStereot yping-dominance 632 568 13 60 60 55 1388\nTable 2: Dataset distribution.\nglish (XLM-R, RoBERTa, BERT, hateBERT\n(Caselli et al., 2020) and ALBERT (Lan et\nal., 2019)). Furthermore, they translated all\nEnglish tweets to Spanish and vice versa and\nmasked randomly selected tokens to augment\nthe data available.\nI2C participated with 3 runs for task 1\nand one run for task 2. For their best sys-\ntem, they translated all Spanish tweets to\nEnglish and created an ensemble of 3 models:\nRoBERTa, BETO and SiEBERT (Hartmann\net al., 2020).\nLPtower submitted 3 runs for each task.\nFor their best run, they translated all tweets\nto 6 languages (French, Portuguese, Italian,\nGerman, Spanish and English) and created\nan ensemble of 6 models, each of them trained\nfor a different language.\nmultiaztertest submitted two runs for task\n1 and one run for task 2. In their best run,\nthey fine-tuned RoBERTa for English texts\nand BETO for Spanish.\nNIT Agartala NLP Team submitted one\nrun for each task. Their system was based on\nLogistic Regression trained on tf-idf features\nbuilt from the texts unigrams.\nshm2022 submitted two runs for task 1\nand one run for task 2. They trained the\nmultilingual model LaBSE (Feng et al., 2020)\nto classify both English and Spanish tweets.\nSINAI only participated in task 1 with\nthree different runs. The best run was a sys-\ntem based on DistilBERT (Sanh et al., 2019).\nThey experimented with other datasets for\ndata augmentation.\nSINAI-TL only participated in task 1 with\nthree different runs. They followed a multi-\ntask learning approach using different auxil-iary tasks. BETO for Spanish and BERT for\nEnglish were used as base models. Their best\nrun used the emotion detection task as the\nauxiliary one by training a shared model with\nthe Universal Joy dataset (Lamprinidis et al.,\n2021) for Spanish and, for English, they used\na BERT model without auxiliary.\nThangCIC submitted 3 runs for each task.\nTheir best system was based on an majority\nvote ensemble of 2 different models: mBERT\nand DeBERTa.\nUMUTeam submitted 3 runs for each task.\nTheir system combined linguistic features\nand state-of-the-art transformers using en-\nsemble techniques. Their best model is based\non a weighted ensemble model using trans-\nformers.\nUNED-UPM submitted two runs for each\ntask. For both tasks, they used a Multi-\nlingual Universal Sentence Encoder (Yang et\nal., 2019) as textual representation and com-\nputed the nearest neighbors to find the defini-\ntive class.\nxaiTUD only participated in task 1 with\na run. Their system was based on a com-\nbination of byte-level model ByT5 (Xue et\nal., 2022) with tabular modeling via TabNet\n(Ar\u0131k and Pfister, 2021).\n5 System Results\nTasks 1 and 2 were evaluated independently.\nIn the following subsections, we show the re-\nsults for each task and language. Teams were\nranked by accuracy for task 1 and macro-\naveraged F1-score (F1) for task 2. However,\nwe also report standard evaluation metrics\nsuch as Precision and Recall.\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrillo-de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta-Arag\u00f3n, Guillermo Marco-Rem\u00f3n, \nMaryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n234Ranking Team run Accuracy Precision Recall F1\n1 task1 avacaondata 1 0.7996 0.7982 0.7975 0.7978\n2 task1 CIMATCOLMEX 1 0.7949 0.7935 0.7952 0.794\n3 task1 I2C1 0.7883 0.7889 0.7912 0.788\n4 task1 SINAI-TL 1 0.7845 0.7846 0.7868 0.7841\n5 task1 multiaztertest 1 0.7836 0.7831 0.7853 0.783\n6 task1 ELiRF-VRAIN 2 0.7694 0.7684 0.7704 0.7686\n7 task1 UMU 1 0.7647 0.7647 0.7668 0.7642\n8 task1 2539404758 0.7637 0.7619 0.7628 0.7623\n9 task1 AI-UPV 3 0.7637 0.7652 0.7671 0.7635\n10 task1 ThangCIC 3 0.7609 0.7598 0.7616 0.76\n11 task1 LPtower 1 0.758 0.7561 0.7558 0.7559\n12 task1 shm2022 1 0.7533 0.754 0.756 0.753\n13 task1 AIT FHSTP 3 0.7505 0.7494 0.7512 0.7496\n14 task1 CompLingKnJ 1 0.7457 0.7446 0.7463 0.7448\n15 task1 SINAI 1 0.7316 0.7353 0.7362 0.7315\n16 task1 besiguenza 1 0.7306 0.729 0.726 0.7269\n17 task1 NIT Agartala NLP Team 1 0.7098 0.7075 0.7059 0.7065\n18 task1 BASELINE 0.6928 0.6919 0.685 0.6859\n19 task1 UNED-UPM 1 0.6824 0.7131 0.6968 0.6792\n20 Majority Class 0.5444 0.5444 0.5 0.3525\n21 task1 xaiTUD 1 0.4811 0.5034 0.5026 0.46\nTable 3: Results task 1 (best run).\n5.1 Task 1\n19 teams participated in task 1 for both\nEnglish and Spanish, presenting 45 runs in\ntotal. In Table 3, the best run for each\nteam is shown, as well as the two baselines:\ntask1 BASELINE and Majority Class. All\nruns ranking is available at the task website3.\nRegarding the best run ranking, 14\nteams achieved an Accuracy above the\ntask1 BASELINE, while only 5 teams were\nbelow the baseline. For the Majority Class\nbaseline, 16 teams achieved a higher Accu-\nracy, whereas only 3 teams were below. The\nbest performing team is avacondata, which\nachieved an overall F1 of 0.7996. This team\nexploited an ensemble of transformers models\nfor different hyper-parameter configurations.\nThe baseline based on majority vote was one\nof the worst performing solutions.\nAlthough the official ranking considered\nboth languages, we also presented two sepa-\nrate rankings by language (English and Span-\nish) for each task. Table 4 shows the top-10\nruns for English and Table 5 for Spanish. Re-\ngarding the English results, the winning team\navacaondata achieved the best results with\nan accuracy of 0.8422. Regarding the Spanish\nresults, CIMATCOLMEX ranked first with\n3http://nlp.uned.es/exist2022/an accuracy of 0.7801. They used an ensem-\nble of 10 RoBERTuito and 10 BERT models,\neach of them trained individually using dif-\nferent seeds. The winning team avacaondata\nranked fifth with more than 2% of difference\nin terms of accuracy.\nAs expected, transformer-based models\nperformed better than the other techniques,\nsince the top-10 teams are all based on these\ntechniques. Traditional machine learning ap-\nproaches did not perform well even using\nextra features based on external resources.\nSimilarly, the use of external datasets has\nbeen explored by some teams with rela-\ntive success. Specific-domain transformers\nhave been successfully employed by the top-\nperformed teams. This may suggest that\ntransformer-based models benefit from train-\ning with data from the same source (e.g.\nTwitter).\nIt is interesting to highlight the perfor-\nmance difference (around 6%) between En-\nglish and Spanish tasks. As we expected,\ntransformer models perform better in En-\nglish since they have been trained on corpus\nmainly composed of English texts. However,\nsince Spanish is well-represented in these\ndatasets, multilingual transformers perform\nvery well for this language.\nOverview of EXIST 2022: sEXism Identification in Social neTworks\n235Ranking Team run Accuracy Precision Recall F1\n1 task1 avacaondata 1.tsv en 0.8422 0.8388 0.8365 0.8376\n2 task1 SINAI-TL 1.tsv en 0.8194 0.8148 0.8206 0.8166\n3 task1 CIMATCOLMEX 3.tsv en 0.8137 0.8087 0.8132 0.8103\n4 task1 I2C1.tsv en 0.8137 0.8107 0.8182 0.8117\n5 task1 AI-UPV 3.tsv en 0.8118 0.807 0.8122 0.8087\n6 task1 multiaztertest 2.tsv en 0.8023 0.7981 0.794 0.7958\n7 task1 ELiRF-VRAIN 3.tsv en 0.7947 0.7893 0.7893 0.7893\n8 task1 LPtower 1.csv en 0.7852 0.7795 0.7811 0.7802\n9 task1 ThangCIC 3.tsv en 0.7852 0.7795 0.7805 0.78\n10 task1 AI-UPV 1.tsv en 0.7795 0.7789 0.7862 0.7779\n11 task1 BASELINE.tsv en 0.7167 0.7092 0.7053 0.7068\n12 Majority Class 0.5798 0.5798 0.5 0.367\nTable 4: Top-10 results task 1 English.\nRanking Team run Accuracy Precision Recall F1\n1 task1 CIMATCOLMEX 1.tsv es 0.7801 0.7808 0.7805 0.7801\n2 task1 multiaztertest 1.tsv es 0.7744 0.7753 0.7749 0.7744\n3 task1 I2C3.tsv es 0.7707 0.7706 0.7707 0.7706\n4 task1 I2C1.tsv es 0.7632 0.7652 0.7639 0.763\n5 task1 UMU 3es 0.7613 0.7614 0.7614 0.7613\n6 task1 avacaondata 1.tsv es 0.7575 0.7574 0.7574 0.7574\n7 task1 ELiRF-VRAIN 1.tsv es 0.7556 0.7608 0.7569 0.755\n8 task1 ThangCIC 1.tsv es 0.7556 0.7567 0.7549 0.755\n9 task1 UMU 1es 0.7556 0.757 0.7563 0.7556\n10 task1 2539404758.tsv es 0.7538 0.7539 0.7534 0.7535\n11 task1 BASELINE.tsv es 0.6692 0.6747 0.6673 0.6649\n12 Majority Class 0.5094 0.5094 0.5 0.3375\nTable 5: Top-10 results task 1 Spanish.\nRanking Team run Accuracy Precision Recall F1\n1 task2 avacaondata 1 0.7013 0.5907 0.5351 0.5106\n2 task2 ELiRF-VRAIN 3 0.7042 0.587 0.5057 0.4991\n3 task2 UMU 2 0.6767 0.5552 0.492 0.4741\n4 task2 multiaztertest 1 0.6786 0.5451 0.4826 0.4706\n5 task2 ThangCIC 8 0.6626 0.5414 0.5001 0.4706\n6 task2 I2C1 0.6465 0.5255 0.518 0.47\n7 task2 AIT FHSTP 3 0.6522 0.5301 0.4999 0.4675\n8 task2 LPtower 1 0.6569 0.5477 0.4748 0.4635\n9 task2 AI-UPV 3 0.6267 0.519 0.5005 0.4516\n10 task2 besiguenza 1 0.6285 0.4941 0.4231 0.4198\n11 task2 2539404758 0.6153 0.4511 0.3939 0.3809\n12 task2 UNED-UPM 1 0.5274 0.4141 0.4279 0.3708\n13 task2 BASELINE 0.5784 0.4299 0.3395 0.342\n14 task2 NIT Agartala NLP Team 1 0.6229 0.5736 0.281 0.3194\n15 Majority Class 0.5539 0.5539 0.1429 0.1018\n16 task2 shm2022 1 0.138 0.38 0.1637 0.056\nTable 6: Results task 2 (best run).\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrillo-de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta-Arag\u00f3n, Guillermo Marco-Rem\u00f3n, \nMaryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n236Ranking Team run Accuracy Precision Recall F1\n1 task2 avacaondata 1.tsv en 0.7471 0.6184 0.5532 0.5337\n2 task2 AI-UPV 3.tsv en 0.6996 0.5718 0.5631 0.5133\n3 task2 ELiRF-VRAIN 3.tsv en 0.73 0.5954 0.5218 0.5084\n4 task2 ThangCIC 8.tsv en 0.6939 0.549 0.5058 0.4792\n5 task2 UMU 2en 0.7091 0.5461 0.4899 0.4751\n6 task2 AI-UPV 1.tsv en 0.673 0.5353 0.5177 0.474\n7 task2 multiaztertest 1.tsv en 0.711 0.552 0.4789 0.4689\n8 task2 I2C1.tsv en 0.6654 0.5112 0.5193 0.4658\n9 task2 AIT FHSTP 3.tsv en 0.6635 0.5196 0.4767 0.4545\n10 task2 LPtower 1.csv en 0.6806 0.5289 0.461 0.4515\n11 task2 BASELINE.tsv en 0.5722 0.3836 0.3471 0.3276\n12 Majority Class 0.5932 0.5932 0.1429 0.1064\nTable 7: Top-10 results task 2 English.\nRanking Team run Accuracy Precision Recall F1\n1 task2 ELiRF-VRAIN 3.tsv es 0.6786 0.5891 0.4881 0.4867\n2 task2 avacaondata 1.tsv es 0.656 0.5718 0.5169 0.4864\n3 task2 UMU 1es 0.6541 0.5985 0.5026 0.4855\n4 task2 ELiRF-VRAIN 1.tsv es 0.6767 0.5818 0.4964 0.4841\n5 task2 AIT FHSTP 3.tsv es 0.641 0.5416 0.5215 0.4775\n6 task2 I2C1.tsv es 0.6278 0.5432 0.5152 0.4714\n7 task2 LPtower 1.csv es 0.6335 0.5666 0.4889 0.4709\n8 task2 ThangCIC 4.tsv es 0.6316 0.5494 0.5131 0.4699\n9 task2 multiaztertest 1.tsv es 0.6466 0.5457 0.4863 0.4679\n10 task2 AI-UPV 2.tsv es 0.5545 0.4754 0.4405 0.3974\n11 task2 BASELINE.tsv es 0.5846 0.4827 0.3317 0.3488\n12 Majority Class 0.515 0.515 0.1429 0.0971\nTable 8: Top-10 results task 2 Spanish.\n5.2 Task 2\n15 teams participated in task 2 for both En-\nglish and Spanish, for a total of 29 runs. In\nTable 6, the best run for each team is shown,\nas well as the two baselines. Among all the\nruns, 11 teams achieved an F1 above the\ntask2 BASELINE, while only 4 teams were\nbelow it. For the Majority Class baseline, 14\nteams achieved a higher F1, whereas only 1\nteam is below the baseline.\nIt is interesting to highlight the strong dif-\nference between the best and the worst sys-\ntems. The best performing team for task 2\nis again avacaondata. The worst results have\nbeen obtained by teams that employ tradi-\ntional machine learning methods. Further-\nmore, the difference between the first and\nsecond team is more significant. This could\nbe due to the fact that, unlike the first task,\nthe second team does not employ a domain-\nadapted transformer.\nTables 7 and 8 show results for the top-\n10 teams in English and Spanish, respec-tively. Again, the task winner avacaondata\nperformed better in English than in Span-\nish, they ranked first and second respectively.\nInterestingly, ELiRF-VRAIN performed well\nin Spanish by using an ensemble of 5 differ-\nent models for Spanish and other 5 models\nfor English.\nIn this task, the difference in performance\nbetween English and Spanish is very similar\nto task 1. However, it is important to notice\nthat most participants achieved relatively low\nresults, demonstrating the difficulty of this\ntask and the need for further research.\n6 Conclusion\nIn this paper, we have presented the results\nof the second shared task on sexism detec-\ntion in a broad sense, from explicit misog-\nyny to other subtle expressions that involve\nimplicit sexist behaviours. The task setup\nprovided an opportunity to test classifica-\ntion systems in multilingual scenarios (En-\nglish and Spanish) along with different social\nOverview of EXIST 2022: sEXism Identification in Social neTworks\n237networks (Twitter and Gab) and labelling\nprocedures (crowdsourcing and expert anno-\ntators). Perhaps the main contribution of\nthis new edition is its high-quality dataset,\nwhich comprises more than 1,000 tweets la-\nbelled by experts trained to perform both\ntasks in the competition. We think that this\ndataset is a useful resource for researchers in\nonline sexism detection.\nCompared to the previous EXIST edition,\nthe runs submitted show that the problem\nof sexism identification can be better ad-\ndressed by using transformer-based models\nadapted to the Twitter domain. However,\nthe sexism categorization still remains a chal-\nlenging problem. Like in the previous edi-\ntion, we found out that modern transformer-\nbased models considerably overcome tradi-\ntional machine learning approaches. Overall,\nthe results confirm that sexism detection in\nsocial networks is challenging but there is still\nroom for improvement.\nAgain, the high number of participating\nteams at EXIST 2022 confirms the growing\ninterest of the community around sexism de-\ntection in social networks.\nAcknowledgments\nThis work was partially supported by the\nSpanish Ministry of Science and Innovation\nunder the project \u201cFairTransNLP: Midiendo\ny Cuantificando el sesgo y la justicia en\nsistemas de PLN\u201d(PID2021-124361OB-C31\nand PID2021-124361OB-C32). This work\nwas also partially funded by the Spanish\nMinistry of Economy and Competitiveness,\nas part of the research cooperation project\n\u201cSpace for Observation of AI in Spanish\u201d\n(UNED and RED.ES, M.P., ref. C039/21-\nOT). The work of Paolo Rosso was in the\nframework of the research project PROME-\nTEO/2019/121 (DeepPattern) by the Gener-\nalitat Valenciana.\nReferences\nAmig\u00b4 o, E., J. Carrillo-de Albornoz,\nM. Almagro-C\u00b4 adiz, J. Gonzalo,\nJ. Rodr\u00b4 \u0131guez-Vidal, and F. Verdejo.\n2017. Evall: Open access evaluation\nfor information access systems. In\nProceedings of the 40th International\nACM SIGIR Conference on Research and\nDevelopment in Information Retrieval ,\npages 1301\u20131304.Amig\u00b4 o, E., J. Gonzalo, S. Mizzaro, and\nJ. Carrillo-de Albornoz. 2020. An effec-\ntiveness metric for ordinal classification:\nFormal properties and experimental re-\nsults. arXiv preprint arXiv:2006.01245.\nAmig\u00b4 o, E., D. Spina, and J. Carrillo-de Al-\nbornoz. 2018. An axiomatic analysis of\ndiversity evaluation metrics: Introducing\nthe rank-biased utility metric. In The 41st\nInternational ACM SIGIR Conference on\nResearch & Development in Information\nRetrieval , pages 625\u2013634.\nAr\u0131k, S. O. and T. Pfister. 2021. Tabnet: At-\ntentive interpretable tabular learning. In\nAAAI, volume 35, pages 6679\u20136687.\nBasile, V., C. Bosco, E. Fersini, N. Deb-\nora, V. Patti, F. M. R. Pardo, P. Rosso,\nM. Sanguinetti, et al. 2019. Semeval-\n2019 task 5: Multilingual detection of hate\nspeech against immigrants and women in\ntwitter. In 13th International Workshop\non Semantic Evaluation , pages 54\u201363. As-\nsociation for Computational Linguistics.\nBerg, S. H. 2006. Everyday sexism and\nposttraumatic stress disorder in women:\nA correlational study. Violence Against\nWomen, 12(10):970\u2013988.\nCanete, J., G. Chaperon, R. Fuentes, and\nJ. P\u00b4 erez. 2020. Spanish pre-trained bert\nmodel and evaluation data. PML4DC at\nICLR, 2020.\nCaselli, T., V. Basile, J. Mitrovi\u00b4 c, and\nM. Granitzer. 2020. Hatebert: Retrain-\ning bert for abusive language detection in\nenglish. arXiv preprint arXiv:2010.12472 .\nChiril, P., V. Moriceau, F. Benamara,\nA. Mari, G. Origgi, and M. Coulomb-\nGully. 2020. He said \u201cwho\u2019s gonna take\ncare of your children when you are at\nACL?\u201d: Reported sexist acts are not sex-\nist. In Proceedings of the 58th Annual\nMeeting of the Association for Computa-\ntional Linguistics, pages 4055\u20134066, On-\nline, July. Association for Computational\nLinguistics.\nClark, K., M.-T. Luong, Q. V. Le, and\nC. D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators\nrather than generators. arXiv preprint\narXiv:2003.10555.\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrillo-de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta-Arag\u00f3n, Guillermo Marco-Rem\u00f3n, \nMaryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n238Conneau, A., K. Khandelwal, N. Goyal,\nV. Chaudhary, G. Wenzek, F. Guzm\u00b4 an,\nE. Grave, M. Ott, L. Zettlemoyer, and\nV. Stoyanov. 2019. Unsupervised cross-\nlingual representation learning at scale.\narXiv preprint arXiv:1911.02116.\nDe la Rosa, J., E. G. Ponferrada, M. Romero,\nP. Villegas, P. G. de Prado Salas, and\nM. Grandury. 2022. Bertin: Efficient pre-\ntraining of a spanish language model us-\ning perplexity sampling. Procesamiento\ndel Lenguaje Natural, 68:13\u201323.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. In Proceedings of\nthe 2019 Conference of the North Amer-\nican Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers) , pages 4171\u20134186, Minneapolis,\nMinnesota, June. Association for Compu-\ntational Linguistics.\nDonoso-V\u00b4 azquez, T. and Rebollo-Catal\u00b4 an.\n2018. Violencias de g\u00b4 enero en entornos\nvirtuales. Ediciones Octaedro.\nFeng, F., Y. Yang, D. Cer, N. Arivazhagan,\nand W. Wang. 2020. Language-agnostic\nbert sentence embedding. arXiv preprint\narXiv:2007.01852.\nFrenda, S., B. Ghanem, M. Montes-y G\u00b4 omez,\nand P. Rosso. 2019. Online hate speech\nagainst women: Automatic identifica-\ntion of misogyny and sexism on twitter.\nJournal of Intelligent & Fuzzy Systems ,\n36(5):4743\u20134752.\nGuti\u00b4 errez-Fandi\u02dc no, A., J. Armengol-\nEstap\u00b4 e, M. P` amies, J. Llop-Palao,\nJ. Silveira-Ocampo, C. P. Carrino,\nA. Gonzalez-Agirre, C. Armentano-Oller,\nC. Rodriguez-Penagos, and M. Villegas.\n2021. Spanish language models. arXiv\npreprint arXiv:2107.07253.\nHartmann, J., M. Heitmann, C. Siebert, and\nC. Schamp. 2020. More than a feel-\ning: Accuracy and application of senti-\nment analysis.\nHe, P., J. Gao, and W. Chen. 2021.\nDebertav3: Improving deberta using\nelectra-style pre-training with gradient-\ndisentangled embedding sharing. arXiv\npreprint arXiv:2111.09543.Lamprinidis, S., F. Bianchi, D. Hardt, and\nD. Hovy. 2021. Universal joy: A data\nset and results for classifying emotions\nacross languages. In The 16th Conference\nof the European Chapter of the Associa-\ntion for Computational Linguistics . Asso-\nciation for Computational Linguistics.\nLan, Z., M. Chen, S. Goodman, K. Gimpel,\nP. Sharma, and R. Soricut. 2019. Albert:\nA lite bert for self-supervised learning of\nlanguage representations. arXiv preprint\narXiv:1909.11942.\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettle-\nmoyer, and V. Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nManne, K. 2017. Down girl: The logic of\nmisogyny. Oxford University Press.\nMills, S. 2008. Language and sexism . Cam-\nbridge University Press.\nNguyen, D. Q., T. Vu, and A. T. Nguyen.\n2020. Bertweet: A pre-trained language\nmodel for english tweets. arXiv preprint\narXiv:2005.10200.\nP\u00b4 erez, J. M., D. A. Furman, L. A. Ale-\nmany, and F. Luque. 2021. Robertu-\nito: a pre-trained language model for so-\ncial media text in spanish. arXiv preprint\narXiv:2111.09453.\nRadford, A., J. Wu, R. Child, D. Luan,\nD. Amodei, I. Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li,\nand P. J. Liu. 2019. Exploring the\nlimits of transfer learning with a unified\ntext-to-text transformer. arXiv preprint\narXiv:1910.10683.\nRodr\u00b4 \u0131guez-S\u00b4 anchez, F., J. Carrillo-de Al-\nbornoz, L. Plaza, J. Gonzalo, P. Rosso,\nM. Comet, and T. Donoso. 2021.\nOverview of exist 2021: sexism identifi-\ncation in social networks. Procesamiento\ndel Lenguaje Natural, 67:195\u2013207.\nRodr\u00b4 \u0131guez-S\u00b4 anchez, F., J. Carrillo-de Al-\nbornoz, and L. Plaza. 2020. Automatic\nclassification of sexism in social networks:\nAn empirical study on twitter data. IEEE\nAccess, 8:219563\u2013219576.\nOverview of EXIST 2022: sEXism Identification in Social neTworks\n239Sanh, V., L. Debut, J. Chaumond, and\nT. Wolf. 2019. Distilbert, a distilled ver-\nsion of bert: smaller, faster, cheaper and\nlighter. arXiv preprint arXiv:1910.01108.\nSwim, J., L. Hyers, L. Cohen, and M. Fer-\nguson. 2001. Everyday sexism: Evidence\nfor its incidence, nature, and psychologi-\ncal impact from three daily diary studies.\nJournal of Social Issues , 57:31 \u2013 53.\nWaseem, Z. 2016. Are you a racist or am\nI seeing things? annotator influence on\nhate speech detection on Twitter. In Pro-\nceedings of the First Workshop on NLP\nand Computational Social Science , pages\n138\u2013142, Austin, Texas, November. Asso-\nciation for Computational Linguistics.\nWaseem, Z. and D. Hovy. 2016. Hateful\nsymbols or hateful people? predictive fea-\ntures for hate speech detection on Twitter.\nInProceedings of the NAACL Student Re-\nsearch Workshop, pages 88\u201393, San Diego,\nCalifornia, June. Association for Compu-\ntational Linguistics.\nXue, L., A. Barua, N. Constant, R. Al-\nRfou, S. Narang, M. Kale, A. Roberts,\nand C. Raffel. 2022. Byt5: Towards a\ntoken-free future with pre-trained byte-\nto-byte models. Transactions of the As-\nsociation for Computational Linguistics ,\n10:291\u2013306.\nYang, Y., D. Cer, A. Ahmad, M. Guo,\nJ. Law, N. Constant, G. H. Abrego,\nS. Yuan, C. Tar, Y.-H. Sung, et al. 2019.\nMultilingual universal sentence encoder\nfor semantic retrieval. arXiv preprint\narXiv:1907.04307.\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrillo-de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta-Arag\u00f3n, Guillermo Marco-Rem\u00f3n, \nMaryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n240Mention detection, normalization & classification of\nspecies, pathogens, humans and food in clinical\ndocuments: Overview of the LivingNER shared task\nand resources\nDetecci\u00b4 on, normalizaci\u00b4 on y clasificaci\u00b4 on de especies,\npat\u00b4 ogenos, humanos y alimentos en documentos cl\u00b4 \u0131nicos:\nresumen de la tarea y los recursos LivingNER.\nAntonio Miranda-Escalada, Eul` alia Farr\u00b4 e-Maduell, Salvador Lima-L\u00b4 opez,\nDarryl Estrada, Luis Gasc\u00b4 o, Martin Krallinger\nBarcelona Supercomputing Center, Spain\nantoniomiresc@gmail.com\nAbstract: There is a pressing need to generate tools for finding mentions of species,\npathogens, or food from medical texts. To promote the development of such tools we\norganized the LivingNER task. LivingNER relied on a large Gold Standard corpus\nof 2000 carefully selected clinical cases in Spanish covering diverse specialties. It was\nmanually annotated with species mentions that were also carefully mapped to their\ncorresponding NCBI Taxonomy identifiers. Besides, we have generated Silver Stan-\ndard versions of LivingNER for 7 languages: English, Portuguese, Galician, Catalan,\nItalian, French, and Romanian. LivingNER had three subtasks: LivingNERSpecies\nNER (species mention detection sub-task), LivingNER-Species Norm (species men-\ntion detection and normalization to NCBI taxonomy Ids), and LivingNERClinical\nIMPACT (a document classification task related to the detection of pets, animals-\ncausing injuries, food, and nosocomial entities). We received and evaluated 62 sys-\ntems from 20 teams from 11 countries worldwide, obtaining highly competitive re-\nsults. Successful approaches typically modified pre-trained transformer-like language\nmodels (BERT, BETO, RoBERTa, etc.) and employed embedding distance metrics\nfor entity linking. LivingNER corpus: doi.org/10.5281/zenodo.6376662\nKeywords: named entity recognition, pathogens text mining, entity linking, NCBI\nTaxonomy.\nResumen: Existe la necesidad de generar herramientas para encontrar y normalizar\nmenciones de especies, pat\u00b4 ogenos o alimentos en textos m\u00b4 edicos. Para promover el\ndesarrollo de tales herramientas hemos organizado la tarea LivingNER. La tarea\nLivingNER se bas\u00b4 o en un corpus en espa\u02dc nol de 2000 casos cl\u00b4 \u0131nicos cuidadosamente\nseleccionados, representando una diversidad de especialidades. El corpus fue anotado\nmanualmente por expertos que tambi\u00b4 en asignaron a las menciones sus correspon-\ndientes identificadores de la NCBI Taxonomy. Adem\u00b4 as, hemos generado versiones\nde LivingNER para otros 7 idiomas: ingl\u00b4 es, portugu\u00b4 es, gallego, catal\u00b4 an, italiano,\nfranc\u00b4 es y rumano. LivingNER se estructur\u00b4 o en tres subtareas: 1) LivingNER-Species\nNER (subtarea de detecci\u00b4 on de menciones de especies), 2) LivingNER-Species Norm\n(detecci\u00b4 on de especies y normalizaci\u00b4 on a identificadores de NCBI Taxonomy) y 3)\nLivingNER-Clinical IMPACT (tarea de clasificaci\u00b4 on relacionada con la detecci\u00b4 on\nde mascotas, animales causantes de lesiones, alimentos y entidades nosocomiales).\nRecibimos y evaluamos 62 sistemas de 20 equipos de 11 pa\u00b4 \u0131ses a nivel mundial,\nobteniendo resultados altamente competitivos. Generalmente, los enfoques m\u00b4 as exi-\ntosos hicieron modificaciones a modelos de lenguaje basados en transformers (BERT,\nBETO, RoBERTa, etc.) y emplearon m\u00b4 etricas de distancia de embeddings para la\nnormalizaci\u00b4 on de entidades. Corpus LivingNER: doi.org/10.5281/zenodo.6376662\nPalabras clave: reconocimiento de entidades nombradas, miner\u00b4 \u0131a de textos de\npat\u00b4 ogenos, normalizaci\u00b4 on de entidades, NCBI Taxonomy.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 241-253\nrecibido 05-07-2022 revisado 22-07-2022 aceptado 25-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-21\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introduction\nThe semantic annotation of species or living\norganisms is critical to scientific disciplines\nlike medicine, biology, ecology/biodiversity,\nnutrition, and agriculture. For instance, de-\ntecting species in clinical records underscores\nthe burden of disease caused by pathogens\nin the case of infectious diseases; and iden-\ntifying organisms and foods can reveal the\ncause of allergy-related conditions. Despite\nthis undisputed relevance, organisms/species\nhave relatively scarcely featured in NLP stud-\nies, particularly for non-English content.\nBecause of the significance of this task,\nhierarchical taxonomic relations have been\ndeveloped over 250 years to determine\nrules and conventions to catalog species.\nAnd they have been recently transformed\ninto computer-based terminological resources\nsuch as NCBI taxonomy (Schoch et al., 2020;\nFederhen, 2012), the Thompson scientific\nname list, the Catalogue of Life, the Global\nNames Index database, and the ITIS Cat-\nalogue. However, these efforts have not\nbeen adequately aligned with the develop-\nment of automatic systems for semantic anal-\nysis of species mentions in text, especially\nwhen considering documents beyond English.\nCommon challenges encountered are name\nchanges (obsolete species names); homonymy\nwith commonly used words (e.g., \u201cspot\u201d\nrefers to the species Leiostomus xanthurus\nor \u201cpermit\u201d to Trachinotus falcatus ); ab-\nbreviations and acronyms (sometime highly\nambiguous like EC, which can be used for\nthe bacteria \u201dEscherichia coli \u201d and \u201cEnter-\nobacter cloacae,\u201d among others); misspelled\nnames (Escerichia coli forEscherichia coli );\ncoordinations and nested expressions (\u201chu-\nman immunodeficiency viruses types 1 and\n2\u201d); vernacular forms (common names); and\nrole names (e.g., athletes, responders).\nTo overcome these limitations, cor-\npora and tools are already available for\nspecies identification in the English-language\nbiomedical literature and their standardiza-\ntion to controlled vocabularies. For example,\nLINNAEUS (Gerner, Nenadic, and Bergman,\n2010) and the SPECIES tool (Pafilis et al.,\n2013) are capable of detecting species men-\ntions. Additionally, there have been shared\ntasks on information related to microorgan-\nisms/species, such as the Infectious Diseases\n(ID) task of BioNLP 2011 (Pyysalo et al.,\n2011). And the importance of detectingspecies mentions for gene mention entity link-\ning to database records has been addressed\nusing biomedical literature data in English\n(Krallinger, Leitner, and Valencia, 2010).\nHowever, adapting these resources to lan-\nguages other than English and document\ntypes different from biomedical literature is\nnot trivial. This is aggravated by the lack of\nresources, common evaluation scenarios, and\nshared tasks in other languages.\nThe LivingNER task addressed these is-\nsues through (1) a challenge on Named En-\ntity Recognition (NER) of species mentions,\nentity linking, and document classification\nand; (2) providing a manually, exhaustively\nannotated large corpus of Spanish clinical\ncases. All annotated organism/species men-\ntions were manually mapped to the NCBI\ntaxonomy and classified into four information\naxes related to relevant use cases.\nThe National Center for Biotechnology In-\nformation (NCBI) Taxonomy includes names\nof organisms classified primarily based on a\nphylogenetic hierarchy. The NCBI Taxon-\nomy is a universal database, used by the\nInternational Nucleotide Sequence Database\nCollaboration (INSDC), which includes Gen-\nBank, the European Molecular Biology Lab-\noratory (EMBL), and DNA Data Bank of\nJapan (DDBJ) as a single source of taxo-\nnomic classification to maintain consistency\nbetween databases. In NCBI, each unique\ncode identifies a specific type of organism\n(e.g., Taxonomy ID: 5476 for Candida Albi-\ncans) or groups of organisms (Taxonomy ID:\n40674 for mammals). NCBI Taxonomy was\nthe controlled vocabulary chosen in the LIN-\nNAEUS corpus to standardize citations.\nThe corpus also distinguishes between\nantibiotic-resistant pathogens and hospital-\nacquired (nosocomial) infections, an increas-\ning cause of morbidity and mortality when\nexisting drugs become ineffective in eliminat-\ning some bacteria. It also references and\nstandardizes mentions of the different floras\nof the human organism in preparation for the\nliterature and clinical cases related to the hu-\nman microbiome. For clinical and microbio-\nlogical use, all forms of parasitic cycles are\nalso noted. Some of the most relevant appli-\ncations are associated with extracting infor-\nmation about highly prevalent sexually trans-\nmitted diseases, animals causing injuries, and\nanimal-transmitted diseases (zoonoses) origi-\nnating from pets and animal husbandry. The\nAntonio Miranda-Escalada, Eul\u00e0lia Farr\u00e9-Maduell, Salvador Lima-L\u00f3pez, Darryl Estrada, Luis Gasc\u00f3, Martin Krallinger \n242correct extraction of species and infectious\ndiseases facilitates the classification of bacte-\nria in the context of antibiotic resistance and\nnosocomial pathogens and diseases. Another\npotential application relates to food (infec-\ntion, intoxication, healthy and unhealthy di-\nets, etc.), allergy triggers, and epidemiologi-\ncally relevant mentions such as close contacts,\npeople living in the same household, and rel-\natives.\nLivingNER is the first track on com-\nprehensive species mention recognition and\ngrounding of non-English content with a clear\npotential for multilingual adaptation, partic-\nularly for pathogens, to generate high-quality\nliving being mention recognition components.\nThe LivingNER annotation guidelines and\ncorpus are indispensable resources for de-\ntecting and classifying species and infectious\ndiseases in Spanish-language literature and\nmedical reports.\n2 Task Description\n2.1 Shared Task goal\nThe LivingNER shared task explores the au-\ntomatic recognition of species mentions in\nclinical documents in the Spanish language,\nthe assignment of NCBI Tax IDs, and the\nclassification of each mention into four cat-\negories. Notably, LivingNER incorporates a\nsubtask in which participants solve four real-\nworld health use cases.\n2.2 Sub-tasks\nThe LivingNER track contains three inde-\npendent subtasks that are built one on top\nof the other:\nLivingNER-Species NER track (Species\nmention entity recognition): given a plain\ntext clinical case report document collection,\nparticipants must return the exact character\noffsets of all species mentions, both human\nand non-human.\nLivingNER-Species Norm track (Species\nmention normalization): given a plain text\nclinical case report document collection, par-\nticipating systems have to return all species\nmentions, together with their corresponding\nNCBI taxonomy concept identifiers.\nLivingNER-Clinical IMPACT track given\na collection of plain text documents, systems\nmust (1) Perform a document classification\naccording to information relevant to high-\nimpact, real-world clinical use cases. The\nclassification is multi-label, meaning that a\nFigure 1: LivingNER example sentences an-\nnotated with Clinical Impact entities. (A) for\nPets and farm animals, (B) for food species.\nsingle document may belong to several cat-\negories. And (2) Retrieve the list of NCBI\nTax IDs that support the binary classifica-\ntion. Systems have to categorize the docu-\nments into the following information axes:\n\u2022Pets and farm animals in close contact\nwith the patient (important for detect-\ning animal-transmitted diseases such as\ntoxoplasmosis, salmonellosis, cat-scratch\ndisease, etc.).\n\u2022Animals causing injuries. Parasites are\nNOT included.\n\u2022Food species. It includes ingested al-\niments and any other food mentioned\nin the document. It excludes ingested\nitems that are not food.\n\u2022Nosocomial entities : mentions cor-\nresponding to nosocomial/healthcare-\nassociated infections.\n2.3 Evaluation metrics\nThe micro-average f1-score has been the\nprimary evaluation metric in the three\nsubtasks. Additionally, micro-average pre-\ncision and recall have been computed. The\nLivingNER evaluation library is available on\nGitHub (github.com/tonifuc3m/livingner-\nevaluation-library).\n2.4 Baseline\nFor the LivingNER-Species NER subtask, we\nhave employed the PathoTagIt-Base system.\nThis competitive baseline is a deep neural\nnetwork system trained with the LivingNER\ntraining dataset. The network is a customiza-\ntion of the BiLSTM-CRF architecture, and\nit employs word embeddings optimized for\nbiomedical Spanish language (Soares et al.,\n2019). For a more in-depth description of the\nMention detection, normalization & classification of species, pathogens, humans and food in clinical documents: Overview of the \nLivingNER  shared task and resources\n243system, check the PharmaCoNER tagger pa-\nper (Armengol-Estap\u00b4 e et al., 2019). The code\nis available on GitHub (github.com/TeMU-\nBSC/PharmaCoNER-Tagger). There is also\na web demo of the PathoTagIt-Base system\n(see temu.bsc.es/livingner/).\nFinally, we have followed an indirect ap-\nproach to create the document classifier of\nthe LivingNER-Clinical Impact subtask. We\nhave trained four different NER systems.\nThe first NER system recognizes pet and\nfarm animal mentions; the second mentions\nanimals causing injuries; the third, food men-\ntions; and the last, nosocomial entities. The\nfour NER systems were run on the test set\ndocuments. The document containing it is\nautomatically classified into the mention cat-\negory if a mention is detected. For instance,\nin Figure 1 B, as soon as the NER system\nof food mentions recognizes \u201dchufa\u201d, \u201dmelo-\ncot\u00b4 on\u201d, or \u201dmanzana\u201d, the document would\nbe classified as a \u201dfood document\u201d.\n3 Corpus and Resources\n3.1 LivingNER Gold Standard\nCorpus\nThe LivingNER corpus is a collection of\n2,000 clinical cases in Spanish from 20 med-\nical specialties: infectious diseases (includ-\ning Covid-19 cases), cardiology, neurology,\noncology, ENT, dentistry, pediatrics, en-\ndocrinology, primary care, allergology, ra-\ndiology, psychiatry, ophthalmology, psychi-\natry, urology, internal medicine, emergency\nand intensive care medicine, radiology, trop-\nical medicine, and dermatology annotated\nwith species [SPECIES] (including living or-\nganisms and microorganisms) and infectious\ndiseases [ENFERMEDAD] mentions. Each\nmention in the corpus has been standard-\nized to NCBI Taxonomy terminology. Fi-\nnally, the species mentions have been clas-\nsified into four classes of clinical interest to\nimprove their usability (companion animals,\nanimals causing injuries, food, and nosoco-\nmial entities).\nThe infectious diseases annotations are\nnot used in the LivingNER shared task.\nDocument selection. The objective of\ndocument selection was to obtain a sufficient\ndiversity of mentions representative of species\nin the clinical domain. We were mainly lim-\nited by the availability of relevant documents\nfor certain specialties. For instance, obtain-\ning clinical reports on tropical diseases wasmuch easier than on pediatric allergies. The\ndocuments were also selected based on the\nrichness of mentions, favoring the reports\nwith a larger variety of species. Finally, we\nrevised that certain diseases of great interest,\nnotably COVID-19, but also zoonoses and\nparasite infections, AIDS, hepatitis C and\nothers, were not excluded from our selection.\nCorpus annotation. The LivingNER\ncorpus has been annotated and standardized\nby a domain specialist with the support of a\nclinical specialist, who was also in charge of\nreviewing the mentions and their associated\ncodes to arrive at a final version. The process\nof annotation and normalization of the cor-\npus took place between 2020 and 2021, last-\ning approximately five months using the brat\ntool. Before starting the annotation, a first\ndraft of these guides was created based on our\nprevious annotation experiences MEDDO-\nCAN (Marimon et al., 2019), CANTEMIST\n(Miranda-Escalada, Farr\u00b4 e, and Krallinger,\n2020) or MEDDOPROF (Lima-L\u00b4 opez et al.,\n2021) among others), and previous related\nwork (Pafilis et al., 2013; Gerner, Nenadic,\nand Bergman, 2010). The annotation guide-\nlines were refined by several rounds of inter-\nannotator agreement (IAA) consisting of par-\nallel annotation of 5% of the corpus. After\nseveral rounds, a total IAA score of 0.942 for\nspecies and 0.885 for infectious diseases was\nreached. In addition, during the remainder\nof the LivingNER annotation, a random 10%\nof the papers were thoroughly reviewed to\nensure that quality was maintained. There\nwas also ongoing discussion about the con-\ntent of the corpus, especially about difficult\nand ambiguous cases, with the aim of achiev-\ning the highest possible quality and refining\nthese guidelines as much as possible.\nThe NCBI Taxonomy terminology was\nused to assign an identifier to each manual\nannotation, ensuring the usability of the cor-\npus citations. The final version of the Liv-\ningNER corpus includes 30886 species men-\ntions, of which 43.9% correspond to humans,\n4580 are unique, and 29411 are normalized\nto NCBI Taxonomy. In addition, it contains\n11841 infectious disease mentions, 4093 of\nwhich are unique, and 2283 are normalized\nto NCBI Taxonomy. The total is 42727 men-\ntions. Finally, all species entries have been\nclassified into four classes of clinical interest\nto improve their use (companion animals, an-\nimals causing injuries, food, nosocomial enti-\nAntonio Miranda-Escalada, Eul\u00e0lia Farr\u00e9-Maduell, Salvador Lima-L\u00f3pez, Darryl Estrada, Luis Gasc\u00f3, Martin Krallinger \n244Figure 2: Annotated clinical case visualized\nwith Brat tool and annotation tab-separated\nformat.\nties, and antibiotic-resistant bacteria).\nCorpus format . The LivingNER clin-\nical case documents are released in plain\ntext format with UTF-8 encoding. The an-\nnotations are included in a tab-separated\ndocument. In the LivingNER-SPECIES\nNER task, the annotations file has the fol-\nlowing columns: filename, mark (identifier\nmention mark), label (SPECIES or HU-\nMAN), off0 (starting position of the men-\ntion in the document), off1 (ending posi-\ntion of the mention in the document) and\nspan. The LivingNER-Species Norm file,\nin addition to these columns, includes four\nmore columns: isH (whether the span is nar-\nrower than the NCBITax assigned code), isN\n(whether the mention corresponds to a noso-\ncomial infection), iscomplex (whether the\nspan has assigned a combination of NCBITax\ncodes) and NCBITax (mention code in the\nNCBI Taxonomy). Finally, the LivingNER-\nClinical Impact annotation file has the fol-\nlowing columns: filename, isPet, PetIDs\n(NCBITaxonomy codes of pet and farm an-\nimals present in document), isAnimalInjury,\nAnimalInjuryIDs (NCBITaxonomy codes of\nanimals causing injuries present in docu-\nment), IsFood, FoodIDs, (NCBITaxonomy\ncodes of food mentions present in document),\nisNosocomial and NosocomialIDs (NCBITax-\nonomy codes of nosocomial species mentions\npresent in document) (see Figure 3).\nCorpus statistics. The LivingNER\ncorpus contains 1,985 documents, which\namounts to 65,373 sentences and 1,234,579\ntokens. The corpus was randomly split into\nthree subsets: training, validation, and test\nset. The test set is used for evaluation pur-\nposes of participating teams and consists of\n485 records (15 extra records will be re-\nFigure 3: LivingNER Clinical Impact data\nformat.\nleased shortly). Species and human men-\ntions are found in all 1,985 documents. There\nare 30,604 such mentions (17158 species and\n13446 human mentions) manually mapped to\nan NCBI Taxonomy ID. All human mentions\nhave the 9606 NCBI Taxonomy ID, and there\nare 2,672 other unique codes. See Table 1 for\nthe LivingNER corpus general statistics.\nThe 15 most common SPECIES mentions\nare shown in Figure 4B. It is noteworthy that\nseven out of the ten most common have the\nHUMAN label, despite there being fewer HU-\nMAN annotations. This is because it is a\nmore homogeneous entity type. Indeed, there\nare 707 different HUMAN mentions, while\nthere are 3818 different SPECIES mentions.\nIn Figure 4.A, the 15 most common\nSPECIES NCBI Tax IDs are displayed. The\nmain term of the code is shown instead of\nthe numeric ID for clarity. While some\nterms are general (prokaryotes, viruses, eu-\nkaryotes), others are specific (HIV, Entero-\nbius vermicularis, etc.) HIV appears very\nfrequently partially because it is commonly\nmentioned in the context of patient serology\nresults.\nTraining\nValidation Test Total\nDocumen\nts 1000 500 485 1,850\nSPECIES Annotations 9090 3817 4251 17158\nHUMAN Annotations 7007 3289 3150 13446\nTotal Annotations 16097 7106 7401 30604\nUnique codes 6738 2833 3101 12672\nSentences 34261 15107 16005 65373\nTokens 642813 296161 295605 1234579\nPets and farm animals 45 14 21 80\nAnimal causing injuries 107 12 22 141\nFood species 255 107 163 525\nNosocomial entities 67 21 10 98\nTable 1: DrugProt Gold Standard corpus\nstatistics.\n3.2 LivingNER Annotation\nGuidelines\nThe annotation guidelines posed many chal-\nlenges, since many mentions of species in\nclinical documents are not identical to what\nMention detection, normalization & classification of species, pathogens, humans and food in clinical documents: Overview of the \nLivingNER  shared task and resources\n245Figure 4: Number of appearances of (A) the\nmain terms of the 15 most common codes,\nand (B) the 15 most common entities in the\nLivingNER Gold Standard.\nwe can find in the terminologies (for in-\nstance, hepatitis C virus is usually found as\nacronym, i.e., HCV, and Staphilococcus au-\nreus as Staph A. and even its vernacular\nform, i.e., estafilococo). However, language\nwas not the only challenge. We had to deter-\nmine whether to include mentions undoubt-\nedly related to infectious diseases and thus to\npathogens, such as the term vaccine, if prac-\ntically pathognomonic laboratory tests could\nbe equated to the infection and thus the mi-\ncroorganism (i.e., VDRL to syphilis and thus\ntoTreponema pallidum ), and if we should dis-\ntinguish between humans since their preva-\nlence and responses to infection might greatly\ndiffer (for instance, neonates, children, males,\nfemales, the elderly). Naturally, these find-\nings and decisions weighed heavily on the\nnormalisation. We also wanted to include\nreferences to various parasite phases, since\nthey are very important for microscopic di-\nagnosis, and to the human microbiome, par-\nticularly the gut microbiome, since its study\nhas imploded in the last decades, and the\nuse of faecal transplant is already used to\ntreat resistant Clostridium difficile infections\nand a large number of clinical trials to treat\nother conditions with human flora are un-\nder way. The current annotation guidelines\nare well adapted to capture pathogens andspecies, and also to expand with the ad-\nvance of molecular microbiology and scien-\ntific knowledge.\n3.3 LivingNER Multilingual Silver\nStandard\nTo foster the development of multilingual\ntools and generate systems not only for Span-\nish but also for content in English and various\nRomance languages, we have developed the\nannotated (and normalized to NCBI Taxon-\nomy) LivingNER corpus in 7 languages: En-\nglish, Portuguese, Galician, Catalan, Italian,\nFrench, and Romanian. The overview statis-\ntics of the Silver Standard are shown in Ta-\nble 2. We refer to the DisTEMIST overview\npaper (Miranda-Escalada et al., 2022) for a\ncomplete description of the generation pro-\ncess since it is equivalent to that corpus. Find\nthe Multilingual Silver Standard at Zenodo\n(doi.org/10.5281/zenodo.6376662)\nDocumen\nts Annotations Unique NCBI Sentences Tokens\nTax IDs\nCatalan T\nraining 1000 14803 832 34173 642926\nValid 500 6724 533 15073 297012\nTest 485 7709 548 15979 296124\nEnglish Training 1000 13772 776 34430 624437\nValid 500 6332 493 15180 287164\nTest 485 7225 513 16075 286419\nFrench Training 1000 12419 754 34552 697869\nValid 500 5540 471 15225 322198\nTest 485 6428 498 16107 321766\nItalian Training 1000 12945 759 34373 649831\nValid 500 5846 470 15130 299907\nTest 485 6703 499 16038 299470\nPortuguese Training 1000 12420 727 34330 641095\nValid 500 5642 470 15143 295942\nTest 485 6738 490 16038 295587\nRomanian Training 1000 10522 699 34334 651595\nValid 500 4799 427 15130 300773\nTest 485 5617 478 16029 300297\nGalician Training 1000 16633 875 34188 616216\nValid 500 7319 555 15065 284023\nTest 485 7672 546 15983 283808\nTable 2: LivingNER Multilingual Silver\nStandard corpus statistics.\n3.4 LivingNER Terminology\nIt is the official NCBI Taxonomy FTP dump\n(ftp.ncbi.nlm.nih.gov/pub/taxonomy/) with\nthe terms translated to Spanish by a Neu-\nral Machine Translator fine-tuned for the\nbiomedical domain. It is a tab-separated\nfile with the following columns: taxid(the\nNCBI Taxonomy ID of node associated with\nthis name), name txt(the NCBI Taxonomy\nname), unique name (the unique variant of\nthis name if the name is not unique), name\nclass (synonym, common name, scientific\nname, ...), Spanish name (the NCBI Taxon-\nomy name in Spanish).\nBesides, we have added the following\nterms: 2560602 (Mumps orthorubulavirus),\n2560526 (Human orthorubulavirus 4),\nAntonio Miranda-Escalada, Eul\u00e0lia Farr\u00e9-Maduell, Salvador Lima-L\u00f3pez, Darryl Estrada, Luis Gasc\u00f3, Martin Krallinger \n246Figure 5: LivingNER multilingual corpus overview.\n2847144 (hepatitis C virus genotype 1a),\nNOCODE ( out of NCBI Taxonomy scope).\nThe first three were added because they\nappear in the LivingNER corpus, and are\npresent in the browser version of NCBI\nTaxonomy. The last one ( NOCODE ) is\nadded to identify terms in the LivingNER\ncorpus that are not present in the NCBI\nTaxonomy.\nThe terminology is available at Zenodo\n(doi.org/10.5281/zenodo.6390506).\n4 Results\n4.1 Participation Overview\nThe community has shown an active in-\nterest in LivingNER. There were 56 teams\nregistered in LivingNER, and 20 success-\nfully submitted their system results, total-\ning 62 submissions. 20 teams participated\nin LivingNER-SPECIES NER [41 runs], 8\nalso submitted their system predictions for\nLivingNER-SPECIES Norm [15 runs], and\n5 did it for the LivingNER Clinical Impact\ntrack [6 runs]. Besides, as Table 3 shows,\nparticipants belonged to institutions (indus-\ntry or academia) from different countries, in-\ncluding Spain, Romania, China and M\u00b4 exico.\n4.2 System Results\nTable 5 shows the best-run results by\nall teams for subtasks LivingNER-SpeciesNER and LivingNER-Species Norm. In\nLivingNER-Species NER, the Vicomtech\nNLP team obtained the highest micro-\naverage F1-score, 0.951. Team RACAI\nF1-score was almost tied with Vicomtech\n(0.9503), and it reached the highest precision\n(0.9622) and recall (0.9439) in different sub-\nmissions.\nIn LivingNER-Species Norm, the highest\nF1-score (0.9304) and recall (0.9234) were\nobtained once again by the Vicomtech NLP\nteam. The highest precision was obtained by\nthe ClaC team (0.9641).\nTable 4 contains the best-run results of the\nthird subtask, LivingNER-Clinical Impact.\nIn this case, participants had to classify the\ntest set documents into four categories and\ninclude the NCBI Taxonomy codes justify-\ning the classification. Results were computed\nfor the document classification task and the\ndocument classification + code justification.\nThe baseline system was available for the first\ntask (document classification), and none of\nthe participant teams outperformed it. We\ndiscuss this in the Discussion section. We\nmust outline that only 4 test set documents\nwere positive Nosocomial documents. There-\nfore, the results for this fourth classification\naxis are challenging to interpret.\nFinally, the complete results of all\nruns, plus the disaggregated results\nMention detection, normalization & classification of species, pathogens, humans and food in clinical documents: Overview of the \nLivingNER  shared task and resources\n247Team\nName Affiliation Country Tasks Ref. Tool URL\nVicom tec\nh NLP Vicomtech Spain NE/No/C (Zotova et al., 2022) \u2013\nracai Research Institute for Artificial Romania NE (Avram, Mitrofan, and Pais, 2022) \u2013\nIntelligence \u201dMihai Draganescu\u201d\nREAD-Biomed RMIT University Australia NE (Jimeno Yepes and Verspoor, 2022) \u2013\nSINAI Universidad de Ja\u00b4 en Spain NE/No/C (Chizhikova et al., 2022) \u2013\nplncmm CMM, University of Chile Chile NE/No/C (Rojas et al., 2022) (plncmm, 2022)\nSumam Francis KU Leuven Belgium NE (Francis and Moens, 2022) \u2013\nClac Concordia University Canada NE/No (Bagherzadeh, Verma, and Bergler, 2022) \u2013\njohn snowlabs John\nSnow Labs USA NE (Kocaman et al., 2022) \u2013\navacaondata IIC (ADIC) Spain NE/No/C (Vaca, 2022) \u2013\nPumas Universidad Nacional Aut\u00b4 onoma M\u00b4 exico NE/No/C (del Moral et al., 2022) \u2013\nIAM University of Bordeaux France NE (Cossin, Diallo, and Jouhet, 2022) (IAM, 2022)\nIGES IGES Institut GmbH Germany NE/No (Chapman, Schwarz, and H\u00a8 aussler, 2022) \u2013\nNLP-CIC-WFU Instituto Polit\u00b4 ecnico Nacional M\u00b4 exico & USA NE/No (Tamayo, Burgos, and Gelbukh, 2022) (NLP-CIC-WFU, 2022)\nWake Forest University\nVitor Universidade Federal do Rio de Janeiro Brasil NE \u2013 \u2013\nzzz Yunnan University China NE (Zhu and Wang, 2022) (zzz, 2022)\nKformer-OEG Universidad Polit\u00b4 ecnica de Madrid Spain NE \u2013 \u2013\nMark \u2013 \u2013 NE (Hanjie and Xiaobing, 2022) (Mark, 2022)\nHan Yunnan University China NE (Han and Ding, 2022) (tutorial, 2022)\nSapphire \u2013 \u2013 NE \u2013 \u2013\nboun-ner Bogazici University Turkey NE \u2013 \u2013\nTable 3: LivingNER team overview. In the Tasks column, NE stands for LivingNER-Species\nNER, No for LivingNER-Species Norma and C for LivingNER-Clinical Impact.\nPets\nand farm animals Animals causing injuries Food species Nosocomial entities\nTeam Name MiP MiR MiF MiP MiR MiF MiP MiR MiF MiP MiR MiF\nLivingNER-Clinical Impact\nwith codes\nVicom tec\nh 0 0 0 .0006 .125 .0012 .0088 .1154 .0164 0 0 0\nSINAI 0 0 0 0 0 0 0 0 0 0 0 0\nplncmm .0317 .3636 .0584 0 0 0 .02 .3846 .038 0 0 0\navacaondata 0 0 0 0 0 0 0 0 0 0 0 0\nPumas .024 .25 .0438 0 0 0 .0211 .2692 .0391 0 0 0\nLivingNER-Clinical Impact\nVicom tec\nh .0326 .25 .0577 .0058 .5 .0115 .0235 .3077 .0437 .0016 .75 .0032\nSINAI 0 0 0 0 0 0 0 0 0 0 0 0\nplncmm .0397 .4167 .0725 .0282 .5 .0533 .0479 .9231 .0911 .006 .5 .0118\navacaondata 0 0 0 .0021 .125 .0041 0 0 0 0 0 0\nPumas .024 .25 .0438 .0167 .25 .0312 .0211 .2692 .0391 0 0 0\nPathoTagIt-Base 1 .1667 .2857 .032 1 .062 .8 .9231 .8571 .0513 .5 .093\nTable 4: Results of LivingNER-Clinical Impact systems. MiP, MiR and MiF stands for micro-\naveraged precision, recall and F1-score.\nby label (HUMAN and SPECIES),\nare published on a dedicated webpage\n(temu.bsc.es/livingner/results/).\n4.3 Methodologies\nTable 5 briefly describe the methodolo-\ngies used by LivingNER participants, and\nfor an in-depth description, we refer to\ntheir scientific articles, listed in Table 3.\nWe have observed that the most success-\nful approaches to LivingNER-Species NER\nincluded non-standard fine-tuning of pre-\ntrained transformer-based language models.\nTypically, these language models are do-\nmain and language-specific, such as bsc-bio-\nes RoBERTa (Carrino et al., 2021), em-\nployed by teams READ-Biomed and SINAI,\namong others; or cross-lingual, such as XLM-\nRoBERTa (Conneau et al., 2019), chosen\nby team racai. The highest-scoring partici-\npant of LivingNER-Species NER, Vicomtech,has fine-tuned a transformer-based language\nmodel using a sliding windows technique that\navoids hard, meaningless segmentation cuts\nthat typically occur in these scenarios (Zo-\ntova et al., 2022).\nIn LivingNER-Species Norm, participants\nwith the highest scores used a robust NER\nsystem to detect the species mentioned. And\nthey were mapped to NCBI Taxonomy using\ntraditional approaches such as string match-\ning using Levenshtein distance and setting\na heuristic cutoff. Additionally, other par-\nticipants used, for instance, word embed-\ndings similarity (Pumas) or TF-IDF match-\ning (SINAI).\nFinally, in LivingNER-Clinical Impact,\nthe most successful approach has been the\nbaseline: to train a simple NER system to\nrecognize the entities of interest and label\nas positive any document with a detected\nnamed entity.\nAntonio Miranda-Escalada, Eul\u00e0lia Farr\u00e9-Maduell, Salvador Lima-L\u00f3pez, Darryl Estrada, Luis Gasc\u00f3, Martin Krallinger \n248SPECIES NER SPECIES\nNorm\nTeam Name MiP MiR MiF Description MiP MiR MiF Description\nVicom tec\nh NLP .9583 .9438 .951sophisticated fine-tune\ntransformer\nmodel0.9376 0.9234 0.9304Semantic Text Search\napproaches\nracai .9569 .9439 .9503fine-tune XLM-RoBER\nTa\nwith lateral inibitory\nlayer- - - -\nREAD-Biomed .954\n.9411 .9475Fine-tune RoBERTa\n(bsc-bio-es)- - - -\nSINAI .9571\n.9346 .9457fine-tune RoBERTa\n(roberta-base-bne,\nbsc-bio-es & roberta-\nbiomedical-clinical-es).8733 .8527 .8629character-level TF-IDF\nmatching and string\nmatching w. Levenshtein\ndistance\nplncmm .9455\n.9373 .9414Fine-tune RoBERTa\n(bsc-bio-es) w. FLERT.9139 .906 .9099string matc\nhing w.\nLevenshtein distance\nSumam F\nrancis .9443 .9307 .9375Fine-tune BERT\n(BETO) pre-trained w.\ncontrastive loss- - - -\nClac .9385\n.9256 .932 mi-RIM model .9495 .891 .9193string matc\nhing w.\nLevenshtein distance\njohn snowlabs .916\n.9327 .9243Bi-LSTM-CNN-Char &\nBertForTokenClassifica-\ntion- - -\nav\nacaondata .9228 .908 .9153Domain adaptation of\nMarIA-Large.512 .4799 .4954 -\nPumas .9284\n.8899 .9087fine-tune RoBERTa\n(bsc-bio-es).9389 .8075 .8682word embedding\nsimilarity\nIAM .9209\n.8733 .8965Complex dictionary\nlookup- - - -\nIGES .9112\n.8638 .8869 SAPBert-XLMR + CRF .8979 .8512 .874FAISS indexes containing\nencoded synonyms\nNLP-CIC-WFU .8303\n.8704 .8499fine-tune mBERT &\npost-processing rules.7768 .8143 .7951 dictionary lookup\nVitor .9492\n.5634 .7071 - - - - -\nzzz .8012\n.6138 .6951fine-tune\nBERT+BiLSTM- - - -\nKformer-OEG .7306\n.6057 .6623 - - - - -\nMark *p\nw .8214 .6145 .703BERT(BETO)+BiGRU+\nCRF + adversarial\nlearning- - - -\nHan *p\nw .5399 .1965 .2881 fine-tune BERT (BETO) - - - -\nSapphire .6875\n.0149 .0291 - - - - -\nBoun-ner 0.126\n0.078 0.0963 fine-tune BERT - - - -\nPathoT\nagIt-Base 0.9461 0.8507 0.8958 Section 2.5 - - - -\nTable 5: Results of LivingNER systems, subtasks SPECIES NER and SPECIES Norm. *pw\nmeans post-workshop submissions. MiP, MiR and MiF stands for micro-averaged precision,\nrecall and F1-score.\n4.4 LivingNER Spanish Silver\nStandard\nThe LivingNER test set was released together\nwith a background set: an additional collec-\ntion of 13,000 clinical case documents from\nvarious medical disciplines, all Spanish. The\nbackground set helps examine whether sys-\ntems could scale to more extensive data col-\nlections and avoid manual annotation correc-\ntion. Participants have generated automatic\npredictions for the test and the background\nset, although they were only evaluated on the\ntest set predictions in the three subtasks.\nTherefore, the background set predic-\ntions include automatic mention annotations\n(LivingNER-Species NER predictions), nor-\nmalized to NCBI Taxonomy (LivingNER-Species Norm predictions) and document\nclassifications with evidence (LivingNER-\nClinical Impact predictions). The back-\nground set predictions from all participants\nwill be harmonized and constitute the Liv-\ningNER Spanish Silver Standard corpus,\nsimilar to the CALBC initiative (Rebholz-\nSchuhmann et al., 2010), to the Cantemist\n(Miranda-Escalada, Farr\u00b4 e, and Krallinger,\n2020), CodiEsp (Miranda-Escalada et al.,\n2020), MESINESP2021 (Gasco et al., 2021),\nProfNER (Miranda-Escalada et al., 2021),\nand PharmaCoNER (Gonzalez-Agirre et al.,\n2019) shared tasks.\nConsidering the large precision and re-\ncall of most LivingNER systems, the Liv-\ningNER Spanish Silver Standard will be a\nMention detection, normalization & classification of species, pathogens, humans and food in clinical documents: Overview of the \nLivingNER  shared task and resources\n249high-quality collection of annotated, normal-\nized, and classified clinical documents in\nSpanish. Besides, it will serve to foster the\ndevelopment of species recognition and link-\ning resources, as well as to generate more an-\nnotated data. The LivingNER Spanish Sil-\nver Standard will be released on the Zenodo\nMedical NLP community.\n5 Discussion\nThere is a clear need to generate, extend\nand provide access to multilingual terminolo-\ngies and glossaries for the biomedical domain.\nProviding access to bilingual medical glos-\nsaries such as MeSpEN, curated for species\ninformation and other clinical entities, might\nbe helpful to foster exploitation for multilin-\ngual semantic annotation efforts (Villegas et\nal., 2018).\nIn this direction, the LivingNER initiative\npioneers to structure the species information\nin clinical documents written in languages\nother than English. To foster the develop-\nment of species NER and linking resources,\nwe have released the LivingNER corpus: the\nfirst Gold Standard corpus of Spanish clinical\ndocuments with species mentions, manually\nmapped to the NCBI Taxonomy.\nThe LivingNER corpus was created fol-\nlowing strict annotation guidelines that are\nmade public to allow the corpus extension\nand adaptation to other languages or do-\nmains. It contains HUMAN annotations (a\nbuilding block to collect relevant informa-\ntion from patient history, hereditary diseases,\netc.) and SPECIES annotations. The latter\nis essential for diverse clinical applications,\nsuch as epidemiology.\nTo enhance the interoperability between\ndifferent data sources, and taking into ac-\ncount (1) multilingual scenarios, (2) the mul-\ntilingual potential of species mentions, and\n(3) the general lack of annotated data in\nother languages, we have released the Liv-\ningNER Multilingual Corpus. It contains the\nLivingNER corpus documents, translated to\n7 languages (English, French, Italian, Por-\ntuguese, Catalan, Romanian, and Galician),\nand automatically generated species mention\nannotations mapped to NCBI Taxonomy.\nThe resources and the task have gener-\nated considerable interest in the community.\nParticipant teams have developed 62 com-\npetitive systems based on pre-trained trans-\nformer language models evaluated against\nFigure 6: Actual examples of annotated\nspecies mentions and automatically recog-\nnized profession mentions.\nthe LivingNER corpus manual annotations.\nAdditionally, they have generated automatic\npredictions for nearly 13,000 documents that\nwill be harmonized to create the LivingNER\nSpanish Silver Standard.\nThese resources can be used to obtain\nactionable information from clinical narra-\ntives. An example would be linking the\nspecies with the text\u2019s occupational infor-\nmation to fine-tune the work-related disease\nstatistics. This linking is seen in Figure\n6, in which Gold Standard SPECIES an-\nnotations are combined with an automatic\nsystem that recognizes profession mentions\n(trained with MEDDOPROF (Lima-L\u00b4 opez et\nal., 2021) corpus).\nAs future directions, we plan to generate\nmore granular annotations for the HUMAN\nmentions that are needed for real-world ap-\nplications. In addition, the third subtask on\nClinical Impact applications lacked enough\ntraining and test data, and we plan to correct\nthis issue in the future. Finally, the Multi-\nlingual Silver Standard will be manually re-\nviewed to generate manually-generated par-\nallel annotations in eight languages.\nAcknowledgements\nWe acknowledge the Encargo of Plan TL (SE-\nDIA) to BSC for funding and the scientific\ncommittee for their guidance and help. Due\nto the relevance of species for biomaterials\nand implants, this project is supported by\nthe European Union\u2019s Horizon Europe Co-\nordination & Support Action under Grant\nAgreement No 101058779. We acknowledge\nthe support from the AI4PROFHEALTH\nproject (PID2020-119266RA-I00). We thank\nthe organization of IberLEF and SEPLN, and\nBitac for collaboration during the corpus and\nguidelines construction.\nAntonio Miranda-Escalada, Eul\u00e0lia Farr\u00e9-Maduell, Salvador Lima-L\u00f3pez, Darryl Estrada, Luis Gasc\u00f3, Martin Krallinger \n250References\nArmengol-Estap\u00b4 e, J., F. Soares, M. Mari-\nmon, and M. Krallinger. 2019. Pharma-\nconer tagger: a deep learning-based tool\nfor automatically finding chemicals and\ndrugs in spanish medical texts. Genomics\n& informatics , 17(2).\nAvram, A.-M., M. Mitrofan, and V. Pais.\n2022. Species entity recognition using a\nneural inhibitory mechanism.\nBagherzadeh, P., H. Verma, and S. Bergler.\n2022. Multi-input rim for named-entity\nrecognition in spanish clinical reports.\nCarrino, C. P., J. Armengol-Estap\u00b4 e,\nA. Guti\u00b4 errez-Fandi\u02dc no, J. Llop-Palao,\nM. P` amies, A. Gonzalez-Agirre, and\nM. Villegas. 2021. Biomedical and clini-\ncal language models for spanish: On the\nbenefits of domain-specific pretraining in\na mid-resource scenario. arXiv preprint\narXiv:2109.03570.\nChapman, K., M. Schwarz, and B. H\u00a8 aussler.\n2022. Multilingual medical entity recog-\nnition and cross-lingual zero-shot linking\nwith faiss.\nChizhikova, M., J. Collado-Monta\u02dc nez,\nP. L\u00b4 opez- \u00b4Ubeda, M. C. D\u00b4 \u0131az-Galiano,\nL. A. Ure\u02dc na-L\u00b4 opez, and M. T. Mart\u00b4 \u0131n-\nValdivia. 2022. Sinai at livingner shared\ntask 2022: Species mention recognition\nand normalization using transfer learning\nand string matching techniques.\nConneau, A., K. Khandelwal, N. Goyal,\nV. Chaudhary, G. Wenzek, F. Guzm\u00b4 an,\nE. Grave, M. Ott, L. Zettlemoyer, and\nV. Stoyanov. 2019. Unsupervised cross-\nlingual representation learning at scale.\narXiv preprint arXiv:1911.02116.\nCossin, S., G. Diallo, and V. Jouhet. 2022.\nIam at iberlef 2022: Ner of species men-\ntions.\ndel Moral, R., J. Reyes-Aguill\u00b4 on, O. Ramos-\nFlores, H. G\u00b4 omez-Adorno, and G. Bel-\nEnguix. 2022. Species mention entity\nrecognition, linking and classification us-\ning roberta in combination with spanish\nmedical embeddings.\nFederhen, S. 2012. The ncbi taxon-\nomy database. Nucleic acids research,\n40(D1):D136\u2013D143.Francis, S. and M.-F. Moens. 2022. Task-\naware contrastive pre-training for spanish\nnamed entity recognition in livingner chal-\nlenge.\nGasco, L., A. Nentidis, A. Krithara,\nD. Estrada-Zavala, R. T. Murasaki,\nE. Primo-Pe\u02dc na, C. Bojo Canales,\nG. Paliouras, M. Krallinger, et al.\n2021. Overview of bioasq 2021-mesinesp\ntrack. evaluation of advance hierarchical\nclassification techniques for scientific\nliterature, patents and clinical trials.\nCEUR Workshop Proceedings.\nGerner, M., G. Nenadic, and C. M. Bergman.\n2010. Linnaeus: a species name identi-\nfication system for biomedical literature.\nBMC bioinformatics, 11(1):1\u201317.\nGonzalez-Agirre, A., M. Marimon, A. In-\ntxaurrondo, O. Rabal, M. Villegas, and\nM. Krallinger. 2019. Pharmaconer: Phar-\nmacological substances, compounds and\nproteins named entity recognition track.\nInProceedings of The 5th Workshop on\nBioNLP Open Shared Tasks, pages 1\u201310.\nHan, S. and H. Ding. 2022. Named entity\nrecognition for livingner-species based on\nbert and span detection.\nHanjie, M. and Z. Xiaobing. 2022. Clini-\ncal text entity recognition based on pre-\ntrained model and bigru-crf.\nIAM. 2022. Iamsystem.\nhttps://github.com/scossin/IAMsystem.\nJimeno Yepes, A. and K. Verspoor. 2022.\nThe read-biomed team in livingner task 1\n(2022): Adaptation of an english annota-\ntion system to spanish.\nKocaman, V., G. Pirge, B. Polat, and\nD. Talby. 2022. Biomedical named entity\nrecognition in eight languages with zero\ncode changes.\nKrallinger, M., F. Leitner, and A. Valencia.\n2010. Analysis of biological processes and\ndiseases using text mining approaches.\nBioinformatics Methods in Clinical Re-\nsearch, pages 341\u2013382.\nLima-L\u00b4 opez, S., E. Farr\u00b4 e-Maduell,\nA. Miranda-Escalada, V. Briv\u00b4 a-Iglesias,\nand M. Krallinger. 2021. Nlp applied to\noccupational health: Meddoprof shared\ntask at iberlef 2021 on automatic recog-\nnition, classification and normalization of\nMention detection, normalization & classification of species, pathogens, humans and food in clinical documents: Overview of the \nLivingNER  shared task and resources\n251professions and occupations from med-\nical texts. Procesamiento del Lenguaje\nNatural, 67:243\u2013256.\nMarimon, M., A. Gonzalez-Agirre, A. In-\ntxaurrondo, H. Rodriguez, J. L. Martin,\nM. Villegas, and M. Krallinger. 2019. Au-\ntomatic de-identification of medical texts\nin spanish: the meddocan track, corpus,\nguidelines, methods and evaluation of re-\nsults. In IberLEF@ SEPLN, pages 618\u2013\n638.\nMark. 2022. 33da.\nhttps://github.com/33Da/.\nMiranda-Escalada, A., E. Farr\u00b4 e, and\nM. Krallinger. 2020. Named entity\nrecognition, concept normalization and\nclinical coding: Overview of the cantemist\ntrack for cancer text mining in spanish,\ncorpus, guidelines, methods and results.\nIberLEF@ SEPLN , pages 303\u2013323.\nMiranda-Escalada, A., E. Farr\u00b4 e-Maduell,\nS. Lima-L\u00b4 opez, L. Gasc\u00b4 o, V. Briva-\nIglesias, M. Ag\u00a8 uero-Torales, and\nM. Krallinger. 2021. The profner\nshared task on automatic recognition\nof occupation mentions in social media:\nsystems, evaluation, guidelines, embed-\ndings and corpora. In Proceedings of the\nSixth Social Media Mining for Health (#\nSMM4H) Workshop and Shared Task,\npages 13\u201320.\nMiranda-Escalada, A., L. Gasc\u00b4 o, S. Lima-\nL\u00b4 opez, E. Farr\u00b4 e-Maduell, D. Estrada,\nA. Nentidis, A. Krithara, G. Katsimpras,\nG. Paliouras, and M. Krallinger. 2022.\nOverview of distemist at bioasq: Auto-\nmatic detection and normalization of dis-\neases from clinical texts: results, methods,\nevaluation and multilingual resources.\nMiranda-Escalada, A., A. Gonzalez-Agirre,\nJ. Armengol-Estap\u00b4 e, and M. Krallinger.\n2020. Overview of automatic clinical\ncoding: Annotations, guidelines, and so-\nlutions for non-english clinical cases at\ncodiesp track of clef ehealth 2020. In\nCLEF (Working Notes) .\nNLP-CIC-WFU. 2022. Nlp-\ncic-wfu- contribution-to-\nlivingner-shared-task-2022.\nhttps://github.com/ajtamayoh/NLP-\nCIC-WFU- Contribution-to-LivingNER-\nshared-task-2022.Pafilis, E., S. P. Frankild, L. Fanini,\nS. Faulwetter, C. Pavloudi,\nA. Vasileiadou, C. Arvanitidis, and\nL. J. Jensen. 2013. The species and\norganisms resources for fast and accurate\nidentification of taxonomic names in text.\nPloS one, 8(6):e65390.\nplncmm. 2022. Livingner.\nhttps://github.com/maranedah/LivingNER.\nPyysalo, S., T. Ohta, R. Rak, D. Sullivan,\nC. Mao, C. Wang, B. Sobral, J. Tsujii, and\nS. Ananiadou. 2011. Overview of the in-\nfectious diseases (id) task of bionlp shared\ntask 2011. In Proceedings of BioNLP\nShared Task 2011 Workshop, pages 26\u201335.\nRebholz-Schuhmann, D., A. J. J. Yepes,\nE. M. Van Mulligen, N. Kang, J. Kors,\nD. Milward, P. Corbett, E. Buyko,\nE. Beisswanger, and U. Hahn. 2010.\nCalbc silver standard corpus. Journal of\nbioinformatics and computational biology ,\n8(01):163\u2013179.\nRojas, M., J. Barros, M. Araneda, and\nJ. Dunstan. 2022. Flert-matcher: A two-\nstep approach for clinical named entity\nrecognition and normalization.\nSchoch, C. L., S. Ciufo, M. Domrachev,\nC. L. Hotton, S. Kannan, R. Khovan-\nskaya, D. Leipe, R. Mcveigh, K. O\u2019Neill,\nB. Robbertse, et al. 2020. Ncbi taxon-\nomy: a comprehensive update on cura-\ntion, resources and tools. Database, 2020.\nSoares, F., M. Villegas, A. Gonzalez-Agirre,\nM. Krallinger, and J. Armengol-Estap\u00b4 e.\n2019. Medical word embeddings for Span-\nish: Development and evaluation. In Pro-\nceedings of the 2nd Clinical Natural Lan-\nguage Processing Workshop , pages 124\u2013\n133, Minneapolis, Minnesota, USA, June.\nAssociation for Computational Linguis-\ntics.\nTamayo, A., D. A. Burgos, and A. Gel-\nbukh. 2022. Partner: Paragraph tuning\nfor named entity recognition on clinical\ncases in spanish using mbert + rules.\ntutorial. 2022. ner.\nhttps://github.com/songhan123123/ner.\nVaca, A. 2022. Named entity recognition for\nhumans and species with domain-specific\nand domain-adapted transformer models.\nAntonio Miranda-Escalada, Eul\u00e0lia Farr\u00e9-Maduell, Salvador Lima-L\u00f3pez, Darryl Estrada, Luis Gasc\u00f3, Martin Krallinger \n252Villegas, M., A. Intxaurrondo, A. Gonzalez-\nAgirre, M. Marimon, and M. Krallinger.\n2018. The mespen resource for english-\nspanish medical machine translation and\nterminologies: census of parallel corpora,\nglossaries and term translations. LREC\nMultilingualBIO: multilingual biomedical\ntext processing.\nZhu, Z. and L. Wang. 2022. Bert-bilstm\nmodel for entity recognition in clinical\ntext.\nZotova, E., A. Garc\u00b4 \u0131a-Pablos, N. Perez,\nP. Tur\u00b4 on, and M. Cuadros. 2022. Vi-\ncomtech at livingner 2022.\nzzz. 2022. 2251821381.\nhttps://github.com/2251821381.\nMention detection, normalization & classification of species, pathogens, humans and food in clinical documents: Overview of the \nLivingNER  shared task and resources\n253254Overview of PAR-MEX at Iberlef 2022: Paraphrase\nDetection in Spanish Shared Task\nResumen de PAR-MEX en IberLEF 2022: Tarea Compartida\npara la Detecci\u00b4 on de Par\u00b4 afrasis en Espa\u02dc nol\nGemma Bel-Enguix1, Gerardo Sierra1, Helena G\u00b4 omez-Adorno2,\nJuan-Manuel Torres-Moreno3,Jesus-German Ortiz-Barajas4,Juan V\u00b4 asquez4\n1Instituto de Ingenier\u00b4 \u0131a (UNAM)\n2Instituto de Investigaciones en Matem\u00b4 aticas Aplicadas y en Sistemas (UNAM)\n3Laboratoire Informatique d\u2019Avignon (Avignon Universit\u00b4 e)\n4Posgrado en Ciencia e Ingenier\u00b4 \u0131a de la Computaci\u00b4 on\n{gbele,gsierram }@iingen.unam.mx, helena.gomez@iimas.unam.mx,\njuan-manuel.torres@univ-avignon.fr, {jgermanob,juanmv}@comunidad.unam.mx\nAbstract: Paraphrase detection is an important unresolved task in natural lan-\nguage processing; especially in the Spanish language. In order to address this issue,\nand contribute to the creation of high-performance paraphrase detection automated\nsystems, we propose a shared task called PAR-MEX. For this task, we created a\ncorpus, in Spanish, with topics in the domain of Mexican gastronomy. Afterwards,\nthe participants in this task submitted their classification results on our corpus. In\nthis paper we explain the steps followed for the creation of the corpus, we summa-\nrize the results obtained by the various participants, and propose some conclusions\nregarding the paraphrase-detection task in Spanish.\nKeywords: PAR-MEX, paraphrase detection, Iberlef.\nResumen: La detecci\u00b4 on de par\u00b4 afrasis es una tarea importante no resuelta en proce-\nsamiento del lenguaje natural; especialmente en la lengua espa\u02dc nola. Para atacar\neste problema, y para contribuir a la creaci\u00b4 on de sistemas de detecci\u00b4 on autom\u00b4 atica\nque obtengan resultados competitivos, proponemos la tarea compartida llamada\nPAR-MEX. Para esto, creamos un corpus en espa\u02dc nol con temas dentro del campo\nsem\u00b4 antico de gastronom\u00b4 \u0131a mexicana. Despu\u00b4 es los participantes en esta tarea en-\nviaron los resultados de sus sistemas de clasificaci\u00b4 on sobre nuestro corpus. En este\npaper explicamos los pasos seguidos para la creaci\u00b4 on del corpus, resumimos los resul-\ntados obtenidos por los participantes, y proponemos algunas conclusiones al respecto\nde la detecci\u00b4 on de par\u00b4 afrasis en espa\u02dc nol.\nPalabras clave: PAR-MEX, detecci\u00b4 on par\u00b4 afrasis, Iberlef.\n1 Introduction\nTwo texts, or two sentences, are paraphrase\nwhen they are semantically equivalent, re-\ngardless of the cause that led to that equiv-\nalence (Das and Smith, 2009). Detecting\nparaphrased text is a task that has aroused\nthe interest of the Natural Language Process-\ning (NLP) community, due to the fact that\nit has multiple applications, such as plagia-\nrism detection, question-answering and ma-\nchine translation (Kong et al., 2020).\nParaphrase construction includes different\nmechanisms, such as lexical changes through\nsynonymy, sentence rearrangement, breakingof a sentence into several parts, and joining\nmore than one phrase into another. There-\nfore, addressing the problem of paraphrase\ndetection requires an analysis that encom-\npasses different levels, both lexical and se-\nmantic, as well as syntactic.\nTo deal with the problem of paraphrase\ndetection using supervised machine learn-\ning methods, researchers use data sets that\ntypically include pairs of sentences that are\nidentified as paraphrase or non-paraphrase.\nThere are various ways of elaborating or com-\npiling these corpora: news collections, pla-\ngiarism pairs, manual creation, relational ac-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 255-263\nrecibido 05-07-2022 revisado 18-07-2022 aceptado 21-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-22\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalTopic of the document No. of lines\nsushi 28\nmolecular cuisine 21\ntequila 25\nkebab 25\nday of the dead 25\nvegan food 25\nstreet food 25\nTable 1: Topic and number of lines in each\nof the seven original documents.\nquisition, back-translation, multiple transla-\ntions.\nThe PARMEX task has been organized\nfor the first time at IberLeF 2022 (Montes-\ny-G\u00b4 omez et al., 2002), a shared evaluation\ncampaign for NLP systems in Spanish and\nother iberian languages, which is part of the\nSEPLN congress. The task is based on the\nGastronomy Corpus, elaborated by the Lan-\nguage Engineering Group, which is divided\ninto seven sub-corpora that deal with dif-\nferent topics related to cuisine, preferably,\nbut not exclusively, Mexican. The corpus\nhas been manually compiled in Mexico and,\ntherefore, contains some terms and expres-\nsions specific to the Mexican variant of Span-\nish.\nThe rest of their paper is organised as\nfollows. Section 2 presents the evaluation\nframework used at PARMEX 2022. Section\n3 shows an overview of different approaches\ntaken to tackle the problem. Section 4 re-\nports and analyses the results obtained by\nthe teams that have participated. Finally,\nSection 5 presents our conclusions from this\nshared task.\n2 PARMEX 2022 Corpus and\nevaluation framework\nFor the PAR-MEX at Iberlef 2022 task, we\ncreated a corpus comprised of sentence pairs\nin Mexican Spanish. For the creation of the\nsentence pairs, first we produced seven orig-\ninal texts with gastronomical topics. Each\none of these seven texts had a variable num-\nber of lines. On Table 1 the exact number of\nlines and topics per document are shown.\nThe second step in the creation of the cor-\npus was the generation of the paraphrased\ndocuments. These new documents were cre-\nated by humans who were tasked with writing\none document with identical semantic con-\ntent and same number of lines as in the orig-TopicNo. of\nparaphrased\ndocuments\nsushi 7\nmolecular cuisine 31\ntequila 7\nkebab 7\nday of the dead 8\nvegan food 6\nstreet food 6P72\nTable 2: Topics of the original seven docu-\nments, and their respective number of para-\nphrased documents.\ninal document. For example, for the docu-\nment sushi.txt, an original document with\n28 lines, seven paraphrased documents were\ncreated. The 28 lines in each one of these\nseven paraphrased documents contained the\nexact same meaning as the 28 lines in the\noriginal document.\nThe process described above was repeated\nfor every one of the seven original documents.\nThen, we generated a total of 72 paraphrased\ndocuments. The exact numbers can be seen\non Table 2.\nThe next step in the elaboration of the\ntask\u2019s corpus was the creation of the sentence\npairs, and their respective labels. For this, we\npaired each line in every original document\nwith each line in every paraphrased docu-\nment. If the sentence pair was made up of\na line in an original document with an index\nofi, and one line in a paraphrased document\nwith an index i, then it would be labeled as\n\u201cparaphrase\u201d. In the opposite case, the one\nin which a sentence in the original document\nwith index iwas matched with a sentence\nfrom another document but with an index of\nj(given that i\u0338=j), then that sentence pair\nwould be labeled as \u201cnot paraphrase\u201d. It is\nimportant to mention that even if the index\nof an original document and the index of a\nparaphrased document were equal, it was also\nverified that the line from the paraphrased\ndocument belonged to the same topic as the\nline from the original document. For exam-\nple, if line ifrom document vegan food.txt\nwas paired with line ifrom a paraphrased\ndocument related to tequila.txt, this pair\nwould not be labeled as paraphrase since\ntheir semantic contents would differ due to\ntheir topics even though their indices were\nGemma Bel-Enguix, Gerardo Sierra, Helena G\u00f3mez-Adorno, Juan-Manuel Torres-Moreno, Jesus-German Ortiz-Barajas, Juan V\u00e1squez\n256TopicNo. of\nhigh-level\nsentence-pairs\nsushi 41\nmolecular cuisine 214\ntequila 84\nkebab 63\nday of the dead 75\nvegan food 42\nstreet food 51P750\nTable 3: Number of high-level paraphrase\npairs per original document.\nthe same. Therefore, in order to obtain the\nparaphrase sentence-pairs, the topic and the\nindices were compared.\nThe final step in the creation of the cor-\npus was the addition of the high-level para-\nphrase pairs. For this, we requested hu-\nmans to write several original documents\nwith high-level paraphrase. During this step,\nwe did not ask them to write paraphrased\ndocuments with the same number of lines\nas the original documents. Once created\nthese novel documents with high-level para-\nphrases, we extracted some lines and paired\nthem with the sentences in the original doc-\numents. This process generated less para-\nphrase pairs than the initial step with low-\nlevel paraphrases, and the exact number of\nhigh-level paraphrase-pairs can be observed\nin Table 3.\nAfter the pairing of the sentences, and\nthe creation of their respective labels, a to-\ntal of 10,298 sentence-pairs were obtained.\nFrom this set, 1,844 sentence-pairs were la-\nbeled as paraphrase, while the remaining\n8,454 sentence-pairs were labeled as non-\nparaphrase. This represented an approx-\nimate of 20% of sentence-pairs labeled as\nparaphrase, with the remaining 80% labeled\nas not paraphrase. From this set, we created\nthe training, validation and test partitions.\nThe distribution of these sets is shown on Ta-\nble 4.\n3 Overview of the Submitted\nApproaches\nIn this edition, six teams submitted one or\nmore solutions to the task through the co-\ndalab platform1. CodaLab Competitions is\n1https://codalab.lisn.upsaclay.fr/competitions/2345PartitionTotal\nsentence-\npairsParaphrase\nsentence-\npairs\nTraining 7,382 1,282\nValidation 97 20\nTest 2,819 542\nTotal 10,298 1,844\nTable 4: Number and distribution of\nsentence-pairs in the training, validation and\nevaluation sets.\na robust open-source framework for running\ncompetitions that involve results or code sub-\nmission. The evaluation methodology of a\ncompetition in this platform consists of re-\nceiving as input the predictive outputs of sys-\ntems. It returns a performance evaluation\nbased on the metrics defined for each task.\nThis section presents a summary of the\nsubmitted systems in terms of preprocess-\ning, feature extraction, and classification al-\ngorithms. In Table 5 we indicate the gen-\neral approach used by each team. It can be\nappreciated that participants used two gen-\neral approaches: transformers and traditional\nML. Following this, we briefly describe each\nof the participants methods.\nApproach\nNLP-CIC-TA\nGE\nT\u00a8\nu-Par\nThang CIC\nAbu\nFRSCIC\nUC3M-DEEPNLP\nTransformers X X X\nTraditional ML X XX\nTable 5: General approach of each partici-\npating team.\n\u2022Using Transformers on Noisy vs. Clean\nData for Paraphrase Identification in\nMexican Spanish (Tamayo, Burgos, and\nGelbukh, 2022)\n\u2013 Team name: NLP-CIC-TAGE\n\u2013 Summary: The participants pre-\nsented a transfer learning approach\nusing transformers to tackle para-\nphrase identification on noisy vs.\nclean data in Spanish. They used\nBERTIN, a pre-trained model on\nthe Spanish portion of a massive\nOverview of PAR-MEX at Iberlef 2022: Paraphrase Detection in Spanish Shared Task\n257multilingual web corpus. The fine-\ntuning and parameter tunning of\nBERTIN was performed on noisy\ndata and used to identify para-\nphrase on clean data.\n\u2022PAR-MEX Shared Task Submission De-\nscription: Identifying Spanish Para-\nphrases Using Pretrained Models and\nTranslations (Girrbach, 2022)\n\u2013 Team name: T\u00a8 u-Par\n\u2013 Summary: The participants pro-\nposed an approach based on a clas-\nsical machine learning pipeline con-\nsisting of feature extraction, super-\nvised learning, and evaluation. The\nfeature extraction consists in encod-\ning Spanish sentences (or their En-\nglish translations) by a pretrained\nsentence encoder, then concatenat-\ning the sentence embeddings or rep-\nresenting the sentences by a sim-\nilarity score. Different classifiers\nwere used depending on the feature\ntype\u2014a logistic regression model\nand a random forest model on the\nsimilarity features, and multi-layer\nperceptrons on the sentence embed-\ndings features.\n\u2022GAN-BERT, an Adversarial Learning\nArchitecture for Paraphrase Identifica-\ntion (Ta et al., 2022)\n\u2013 Team name: Thang CIC\n\u2013 Summary: The participants used\ntext embeddings from pre-trained\ntransformer models for training by\nGAN-BERT, adversarial learning.\nThey modified noises for the gen-\nerator, which have a random rate\nand the exact size of the hidden\nlayer of transformers. They also in-\ncluded a rule of thumb based on\nthe pair similarity to remove possi-\nble wrong sentence pairs in positive\nexamples and additional unlabelled\ndata in the same domain to improve\nthe model performance.\n\u2022Paraphrase Identification: Lightweight\neffective methods based features frompre-trained models (Rahman et al.,\n2022)\n\u2013 Team name: Abu\n\u2013 Summary: The participants intro-\nduced two lightweight methods: lin-\near regression and multilayer per-\nceptron, trained on six features:\nthe difference in sentences\u2019 length,\ncommon lemmas between 2 sen-\ntences, sentences\u2019 similarity, etc.\nAfter performing Component Anal-\nysis (PCA) to reduce the dimen-\nsion, they filter noises in the posi-\ntive examples by introducing a rule\nof thumb on the pair similarity.\n\u2022Mexican Spanish Paraphrase Identifica-\ntion using Data Augmentation (Meque\net al., 2022)\n\u2013 Team name: FRSCIC\n\u2013 Summary: The participants per-\nformed a data augmentation step\non the training set using transla-\ntion. The text vectorization process\nconsisted of sentence transformers,\nspaCy vectors, traditional word n-\ngrams, and bi-tri syntactic n-grams\nusing TF-IDF. They proposed a\nsimilarity vector using three differ-\nent similarity algorithms for the fi-\nnal representation: Jaccard, Cosine,\nand spaCy. For the classification\nstep, they used a soft-voting ensem-\nble model with three estimators.\n\u2022UC3M at PAR-MEX@IberLef 2022:\nFrom Cosine Distance to Transformer\nModels for Paraphrase Identification\nin Mexican Spanish (Brando-Le-Bihan,\nKarbushev, and Segura-Bedmar, 2022)\n\u2013 Team name: UC3M-\nDEEPNLP\n\u2013 Summary: The participants eval-\nuated a baseline method based\non the cosine similarity of two\ntext pairs representation: TF-\nIDF model on bag-of-words and\nword embedding models provided\nby spaCy. For the final submission,\nthey used the \u201cbert-base-cased-\nfinetuned-mrpc\u201d model, which is\nGemma Bel-Enguix, Gerardo Sierra, Helena G\u00f3mez-Adorno, Juan-Manuel Torres-Moreno, Jesus-German Ortiz-Barajas, Juan V\u00e1squez\n258fine-tuned for paraphrase detection\nby using the MRPC corpus. They\nalso proposed strategies such as\nclass balancing or data augmenta-\ntion to improve the generalization\ncapability. However, they did not\npresent these strategies in the final\nsubmission.\n4 Experimental Evaluation and\nAnalysis of the Results\nThis section reviews the results obtained\nby the participants of PAR-MEX at Iber-\nlef 2022: Paraphrase Detection in Spanish\nShared Task. For this purpose, we analyse\nand compare the submitted solutions\u2019 per-\nformance on the test partition. We used the\nF1-score metric on the paraphrase (P) as the\nprimary performance measure and to rank\nall the participants. We launched a Codalab\ncompetition to manage the shared task stages\nand compute the performance metric for all\nsubmissions.\nWe propose a transformers-based ap-\nproach as a baseline. It consists of the\nBidirectional Encoder Representation from\nTransformer (BERT) model (Devlin et al.,\n2019). We use the base model for our base-\nline, consisting of twelve Transformer blocks\nand the pre-trained model BETO (Ca\u02dc nete et\nal., 2020), a BERT model trained on an enor-\nmous Spanish corpus. We use four epochs\nand the Adam optimizer for the fine-tuning\nstage with a learning rate of 2e-5. We use\nthe HuggingFace implementation (Wolf et al.,\n2020) for Tensorflow (Abadi et al., 2015). In\norder to have comparable results with the\nparticipant submissions, we report the best\nresult in five runs using different random\nseeds.\nTable 6 summarises the results obtained\nby each team and our baseline in the PAR-\nMEX shared task. We report the F1 score in\nboth Paraphrase and Non-paraphrase classes,\nthe macro F1 score, and the accuracy. In\nthis edition of the PAR-MEX shared task, the\napproach submitted by the NLP-CIC-TAGE\nteam outperformed all the other approaches\nand the baseline. The NLP-CIC-TAGE team\nused an approach based on a transformer\narchitecture; they fine-tuned the RoBERTa\nmodel pre-trained in a Spanish corpus. In\ncontrast, the second-best approach proposedby the T\u00a8 u-Par team used a Random For-\nest classifier using similarity-based features.\nThese results show that classic approaches\nare still competitive for this task compared\nto deep learning.\nWe use the Maximum Possible Accu-\nracy (MPA) and Coincident Failure Diversity\n(CFD) metrics (Tang, Suganthan, and Yao,\n2006) to analyse the complementariness and\nthe diversity of the predictions of the sub-\nmitted approaches. The MPA is analogous\nto accuracy, defined as the correct classified\ninstances divided into the total number of in-\nstances. To consider an instance correctly\nclassified, at least one of the teams needs\nto assign the correct label to it. Using the\nMPA metric, we can detect the misclassified\ninstances by all teams. The CFD metric has\na minimum value of 0 when all classifiers are\nalways correct or when all classifiers are ei-\nther correct or wrong. On the other hand, it\nhas a maximum value of 1 when at most one\nclassifier will fail on any randomly chosen in-\nstance (Kuncheva and Whitaker, 2003). The\nCFD is defined in equation 1.\nCFD =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, p0= 1.0;\n1\n1\u2212p 0PL\ni=1L\u2212i\nL\u22121pip0<1\n(1)\nTable 7 shows the results of these met-\nrics by grouping the proposed approaches\nbased on their similar features. We cre-\nate four groups: all teams, all teams who\nsend their paper, Transformers-based ap-\nproaches, traditional-machine-learning-based\napproaches. All of the groups men-\ntioned above have at least two mem-\nbers. All participants sent their papers\nbut one. Transformers-based approaches in-\nclude the following teams: NLP-CIC-TAGE,\nThang CIC, and UC3M-DEEPNLP. T\u00a8 u-Par,\nFRSCIC, and ThangCIC conform traditional\nmachine learning based approaches group.\nIn terms of the general approach, tradi-\ntional machine learning performs better in\nterms of MPA than Transformers-based solu-\ntions. The above suggests that the different\nfeatures used to train these machine learn-\ning models complement each other. In the\nsame way, the combination of transformers\nand machine learning approaches obtain the\nhighest MPA performance and have an av-\nerage increment of 0.66% compared to those\nOverview of PAR-MEX at Iberlef 2022: Paraphrase Detection in Spanish Shared Task\n259Team F1-score (P) F1-score (NP) Accuracy Macro F1-score\nNLP-CIC-TAGE 0.9424 0.9869 0.9787 0.9647\nT\u00a8 u-Par 0.9373 0.9853 0.9762 0.9613\nThang CIC 0.9022 0.9775 0.9635 0.9399\nAbu 0.8867 0.9751 0.9592 0.9309\nFRSCIC 0.8754 0.9730 0.9557 0.9242\nUC3M-DEEPNLP 0.8450 0.9679 0.9468 0.9065\ntemu bsc 0.8441 0.9567 0.9322 0.9004\nbaseline 0.834936 0.953075 0.926924 0.894006\nTable 6: Result summary for the PAR-MEX shared task on the test set.\nApproachBest\naccuracyMPA CFDNumber\nof systems\nAll teams 0.9787 0.9936 0.0408 7\nall teams (with submission) 0.9787 0.9915 0.0341 6\nTransformers 0.9787 0.9847 0.0331 3\nTraditional ML 0.9762 0.9883 0.0373 3\nTable 7: MPA and CFD comparison results among the different proposed approaches.\nindividual approaches. Finally, the values for\nthe CFD score are comparable among all ap-\nproaches, which means that their predictions\nare complementary to an extent; this leads us\nto conclude that traditional and transformer-\nbased approaches learn different information\nfrom text pairs.\nTable 8 shows the results of the F1 score\nfor the paraphrase class divided by topic in\nthe test set. The kebab category achieved the\nhighest performance with an average F1 score\nof 0.9483; on the other hand, the sushi topic\nhad the worst performance with an average\nF1 score of 0.7659. The NLP-CIC-TAGE\nteam obtained the best performance in two\nof the seven topics. In contrast, the T\u00a8 u-Par\nteam obtained the best performance in four\ntopics, including the sushi, which is the hard-\nest. Nevertheless, the difference was in the\nfood truck topic. The NLP-CIC-TAGE ob-\ntained a 0.9153 F1 score, while the T\u00a8 u-Par\nteam obtained 0.8673. For this result, the\nNLP-CIC-TAGE achieved first place in the\nPAR-MEX shared task.\nTables 9 and 10 show the performance of\neach team by topic and low-level paraphrase\nand high-level paraphrase, respectively. In\norder to compute these metrics, we filtered\nthe paraphrase examples and kept the non-\nparaphrase examples unchanged. Only day of\nthe dead, vegan food, and food truck topics\nhave examples of high-level paraphrase. Re-\ngarding high-level paraphrase, the food truck\ntopic obtains the highest performance whilethe sushi topic obtains the lowest; however,\nthe sushi topic only has one example of this\ntype of paraphrase. When comparing high-\nlevel and low-level paraphrase performance,\nonly the food truck topic performs better on\nhigh-level paraphrase than on low-level para-\nphrase. These results suggest that, in gen-\neral, detecting high-level paraphrase exam-\nples is more challenging for the proposed ap-\nproaches. The most substantial difference is\nin the vegan food topic; the average result in\nhigh-level paraphrases is 0.6509, while in low-\nlevel paraphrases is 0.9095, which means a\n0.2586 between both levels. This topic has 41\nhigh-level paraphrase examples and 36 low-\nlevel paraphrase examples; because the ex-\namples of this topic are nearly balanced, we\ncan conclude that the performance difference\nis due to the difficulty of identifying high-\nlevel paraphrase features.\nIn terms of proposed approaches,\nTransformers-based models outperform all\nteams in two of the three topics with high-\nlevel paraphrase examples; in the remaining\ntopic, Transformers-based and traditional\nmachine learning approaches have the same\nperformance. Therefore, we can conclude\nthat Transformers can learn better features\nto identify high-level paraphrases. On the\nother hand, when dealing with low-level\nparaphrases, a traditional machine learning\napproach outperform all teams in 4 of 7\ntopics. A Transformers-based approach has\nthe highest performance in the remaining\nGemma Bel-Enguix, Gerardo Sierra, Helena G\u00f3mez-Adorno, Juan-Manuel Torres-Moreno, Jesus-German Ortiz-Barajas, Juan V\u00e1squez\n260TeamMolecular\ncusineDay of\nthe deadKebab TequilaVegan\nfoodSushiFood\ntruck\nNLP-CIC-TAGE 0.9878 0.9714 0.9792 0.9231 0.8261 0.8333 0.9153\nT\u00a8 u-Par 0.9762 0.9859 0.98 0.9362 0.8252 0.8772 0.8673\nThang CIC 0.9687 0.8400 0.9216 0.9091 0.8444 0.7692 0.8468\nAbu 0.9495 0.9489 0.9574 0.8864 0.8000 0.6567 0.7573\nFRSCIC 0.9254 0.8806 0.9293 0.8764 0.7852 0.8077 0.7810\nUC3M-DEEPNLP 0.9010 0.8000 0.9462 0.8989 0.7576 0.6818 0.7358\ntemu bsc 0.8460 0.8675 0.9245 0.7132 0.8591 0.7353 0.9167\nBaseline 0.7871 0.9863 0.8596 0.7833 0.8387 0.7692 0.918\nAverage 0.9177 0.9096 0.9372 0.8658 0.8181 0.7654 0.8412\nTable 8: Results for the PAR-MEX shared task on the test set by topic.\nTeamMolecular\ncusineDay of\nthe deadKebab TequilaVegan\nfoodSushiFood\ntruck\nNLP-CIC-TAGE 0.9878 0.9636 0.9792 0.9231 0.9333 0.8511 0.9189\nT\u00a8 u-Par 0.9762 1 0.98 0.9362 0.9114 0.8727 0.8406\nThang CIC 0.9687 0.8099 0.9216 0.9091 0.9577 0.7843 0.806\nAbu 0.9495 0.9541 0.9574 0.8864 0.9429 0.6667 0.6885\nFRSCIC 0.9254 0.8571 0.9293 0.8764 0.8919 0.8 0.7302\nUC3M-DEEPNLP 0.901 0.7473 0.9462 0.8989 0.8919 0.6977 0.6769\ntemu bsc 0.846 0.8382 0.9245 0.7132 0.9 0.7273 0.9067\nBaseline 0.7871 0.9828 0.8596 0.7833 0.8471 0.7619 0.9091\nAverage 0.9177 0.8941 0.9372 0.8658 0.9095 0.7702 0.8096\nTable 9: Results for the PAR-MEX shared task on the test set by topic and low-level paraphrases.\nthree topics. With these results, we can\nconclude that machine learning models can\nhandle low-level paraphrasing better than\ncomplex models like transformers when using\nsimilarity-based features as the primary type\nof characteristics.\nFinally, Table 11 shows each team\u2019s\nperformance only on the paraphrase type.\nAgain, the results are consistent with what\nwe show in tables 7 and 8. Although the\nNLP-CIC-TAGE team does not obtain the\nbest result in every topic in the test set, their\noverall performance is the best on both levels\nof paraphrasing.\n5 Conclusions\nThis paper described the design and re-\nsults of the PAR-MEX shared task collocated\nwith IberLef 2022. PAR-Mex is focused in\nparaphrase identification in Mexican Spanish\ntexts. This has been the first edition of the\ntask.\nThe data set of PAR-MEX included both,\nlow-level and high-level pairs of paraphrases,\nalthough they were not distinguished for the\nparticipants. The analysis of the results\nshows that, whereas low-level paraphrase iscurrently an easy task for natural language\nprocessing (0.90 of average), high-level para-\nphrase is a problem that has not been conve-\nniently approached yet.\nThe best results in this shared task were\nobtained by a team that proposed to ap-\nproach the problem with a method based on\ntransformers. However, traditional machine\nlearning strategies obtained very similar re-\nsults. Indeed, while deep learning techniques\nhave the best scores in the sub-corpora of\nmolecular cuisine, vegan food sushi and food\ntruck, traditional methods lead in day of the\ndead, kebab and tequila. The only topic\nin which transformers reach a clearly bet-\nter score is food truck. This shows this is a\ncomplex task and that collaboration between\nmodels and the use of multiple variables can\nimprove the final outcome of the research.\nAcknowledgments\nWe acknowledge the support of the projects\nCONACyT CB A1-S-27780, and DGAPA-\nUNAM PAPIIT references TA400121 and\nTA101722. The authors thank CONA-\nCYT for the computing resources provided\nthrough the Deep Learning Platform for Lan-\nOverview of PAR-MEX at Iberlef 2022: Paraphrase Detection in Spanish Shared Task\n261TeamDay of\nthe deadVegan\nfoodSushi Food truck\nNLP-CIC-TAGE 1 0.6567 0 0.9091\nT\u00a8 u-Par 0.9286 0.6479 0.2222 0.9091\nThang CIC 0.6364 0.7077 0 0.9091\nAbu 0.9286 0.623 0 0.8571\nFRSCIC 0.875 0.6061 0.25 0.8571\nUC3M-DEEPNLP 0.9655 0.5397 0 0.7727\ntemu bsc 0.5769 0.7273 0.1 0.913\nBaseline 0.9375 0.6988 0.1176 0.8936\nAverage 0.8561 0.6509 0.0862 0.8776\nTable 10: Results for the PAR-MEX shared task on the test set by topic and high-level para-\nphrases. Molecular cuisine, kebab and tequila do not have high-level paraphrase examples.\nTeamF1-score\nhigh-level paraphraseF1-score\nlow-level paraphrase\nNLP-CIC-TAGE 0.7755 0.9602\nT\u00a8 u-Par 0.6951 0.9538\nThang CIC 0.6552 0.9137\nAbu 0.6494 0.905\nFRSCIC 0.6795 0.8884\nUC3M-DEEPNLP 0.6575 0.8605\ntemu bsc 0.4167 0.8378\nBaseline 0.3976 0.8265\nAverage 0.6158 0.8927\nTable 11: Results for the PAR-MEX shared task on the test set by paraphrase type.\nguage Technologies of the INAOE Supercom-\nputing Laboratory.\nReferences\nAbadi, M., A. Agarwal, P. Barham,\nE. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, I. Goodfellow, A. Harp,\nG. Irving, M. Isard, Y. Jia, R. Jozefow-\nicz, L. Kaiser, M. Kudlur, J. Levenberg,\nD. Man\u00b4 e, R. Monga, S. Moore, D. Mur-\nray, C. Olah, M. Schuster, J. Shlens,\nB. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan,\nF. Vi\u00b4 egas, O. Vinyals, P. Warden, M. Wat-\ntenberg, M. Wicke, Y. Yu, and X. Zheng.\n2015. TensorFlow: Large-scale machine\nlearning on heterogeneous systems. Soft-\nware available from tensorflow.org.\nBrando-Le-Bihan, A., R. Karbushev, and\nI. Segura-Bedmar. 2022. UC3M at\nPAR-MEX@IberLef 2022: from cosine dis-\ntance to transformer models for para-\nphrase identification in mexican spanish.\nInProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2022).Ca\u02dc nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u00b4 erez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. In PML4DC at ICLR 2020 .\nDas, D. and N. A. Smith. 2009.\nParaphrase identification as probabilistic\nquasi-synchronous recognition. In Pro-\nceedings of the Joint Conference of the\n47th Annual Meeting of the ACL and\nthe 4th International Joint Conference\non Natural Language Processing of the\nAFNLP, pages 468\u2013476, Suntec, Singa-\npore, August. Association for Computa-\ntional Linguistics.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. In Proceedings of\nthe 2019 Conference of the North Amer-\nican Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers) , pages 4171\u20134186, Minneapolis,\nMinnesota, June. Association for Compu-\ntational Linguistics.\nGemma Bel-Enguix, Gerardo Sierra, Helena G\u00f3mez-Adorno, Juan-Manuel Torres-Moreno, Jesus-German Ortiz-Barajas, Juan V\u00e1squez\n262Girrbach, L. 2022. PAR-MEX shared task\nsubmission description: Identifying span-\nish paraphrases using pretrained mod-\nels and translations. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022) .\nKong, L., Z. Han, Y. Han, and H. Qi. 2020.\nA deep paraphrase identification model in-\nteracting semantics with syntax. Com-\nplexity, 2020:14 pages.\nKuncheva, L. and C. Whitaker. 2003. Mea-\nsures of diversity in classifier ensembles\nand their relationship with the ensemble\naccuracy. Machine Learning, 51:181\u2013207,\n05.\nMeque, A., F. Balouchzahi, G. Sidorov, and\nA. Gelbukh. 2022. Mexican spanish para-\nphrase identification using data augmen-\ntation. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2022) .\nMontes-y-G\u00b4 omez, M., J. Gonzalo, F. Rangel,\nM. Casavantes, M. \u00b4Alvarez-Carmona,\nG. Bel-Enguix, H. Escalante, L. Freitas,\nA. Miranda-Escalada, F. Rodr\u00b4 \u0131guez-\nS\u00b4 anchez, A. Ros\u00b4 a, M. Sobrevilla-\nCabezudo, M. Taul\u00b4 e, and R. Valencia-\nGarc\u00b4 \u0131a. 2002. Proceedings of IberLeF\n2002.\nRahman, A., H. Ta, L. Najjar, and A. Gel-\nbukh. 2022. Paraphrase identification:\nLightweight effective methods based fea-\ntures from pre-trained models. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2022).\nTa, H., A. Rahman, L. Najjar, and A. Gel-\nbukh. 2022. GAN-BERT, an adversarial\nlearning architecture for paraphrase iden-\ntification. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2022).\nTamayo, A., D. A. Burgos, and A. Gelbukh.\n2022. Using transformers on noisy vs.\nclean data for paraphrase identification\nin mexican spanish. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022) .\nTang, E. K., P. N. Suganthan, and X. Yao.\n2006. An analysis of diversity measures.\nMachine learning , 65(1):247\u2013271.\nWolf, T., L. Debut, V. Sanh, J. Chau-\nmond, C. Delangue, A. Moi, P. Cis-\ntac, T. Rault, R. Louf, M. Funtowicz,J. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y. Jernite, J. Plu, C. Xu, T. L.\nScao, S. Gugger, M. Drame, Q. Lhoest,\nand A. M. Rush. 2020. Transformers:\nState-of-the-art natural language process-\ning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural\nLanguage Processing: System Demonstra-\ntions, pages 38\u201345, Online, October. As-\nsociation for Computational Linguistics.\nOverview of PAR-MEX at Iberlef 2022: Paraphrase Detection in Spanish Shared Task\n263264Overview of PoliticEs 2022: Spanish Author Profiling\nfor Political Ideology\nResumen de la tarea PoliticEs 2022: Perfilado del Autor\nEspa\u02dc nol por su Ideolog\u00b4 \u0131a Pol\u00b4 \u0131tica\nJos\u00b4 e Antonio Garc\u00b4 \u0131a-D\u00b4 \u0131az1, Salud Mar\u00b4 \u0131a Jim\u00b4 enez-Zafra2,\nMar\u00b4 \u0131a-Teresa Mart\u00b4 \u0131n Valdivia2,Francisco Garc\u00b4 \u0131a-S\u00b4 anchez1,\nL. Alfonso Ure\u02dc na-L\u00b4 opez2,Rafael Valencia-Garc\u00b4 \u0131a1\n1Facultad de Inform\u00b4 atica, Universidad de Murcia, Campus de Espinardo, 30100, Spain\n2Computer Science Department, SINAI, CEATIC, Universidad de Ja\u00b4 en, 23071, Spain\n{joseantonio.garcia8,frgarcia,valencia }@um.es\n{sjzafra,maite,laurena }@ujaen.es\nAbstract: This paper presents the PoliticEs 2022 shared task, organized at Iber-\nLEF 2022 workshop, within the framework of the 38th International Conference of\nthe Spanish Society for Natural Language Processing. This task aims to extract the\npolitical ideology from a given user\u2019s set of tweets. Specifically, it focused on the\nidentification of the gender and the profession, as demographic traits, and the po-\nlitical ideology from a binary and multi-class perspective, as a psychographic trait.\nThe PoliticEs task attracted 63 teams that registered through CodaLab. Finally, 20\nsubmitted results and 14 presented working notes describing their systems. Most of\nthe teams proposed transformer-based approaches, although some of them also used\ntraditional machine learning algorithms or even a combination of both approaches.\nKeywords: Author profiling, political ideology, author analysis, demographic and\npsychographic traits.\nResumen: Este art\u00b4 \u0131culo presenta la tarea PoliticEs 2022, organizada en el taller\nIberLEF 2022, en el marco de la 38 edici\u00b4 on del Congreso Internacional de la Sociedad\nEspa\u02dc nola para el Procesamiento del Lenguaje Natural. Esta tarea tiene como obje-\ntivo extraer la ideolog\u00b4 \u0131a pol\u00b4 \u0131tica de un usuario a partir de un conjunto de tuits pub-\nlicados por \u00b4 el. En concreto, se centr\u00b4 o en la identificaci\u00b4 on del g\u00b4 enero y la profesi\u00b4 on,\ncomo rasgos demogr\u00b4 aficos, y la ideolog\u00b4 \u0131a pol\u00b4 \u0131tica desde una perspectiva binaria y\nmulticlase, como rasgo psicogr\u00b4 afico. La tarea PoliticEs atrajo a 63 equipos que se\ninscribieron a trav\u00b4 es de CodaLab. Finalmente, 20 enviaron resultados y 14 presen-\ntaron art\u00b4 \u0131culos describiendo sus sistemas. La mayor\u00b4 \u0131a de los equipos propusieron\nenfoques basados en transformers, aunque algunos de ellos tambi\u00b4 en utilizaron algo-\nritmos tradicionales de aprendizaje autom\u00b4 atico o incluso una combinaci\u00b4 on de ambos\nenfoques.\nPalabras clave: Perfilado de usuarios, ideolog\u00b4 \u0131a pol\u00b4 \u0131tica, an\u00b4 alisis de autores, rasgos\ndemogr\u00b4 aficos y psicogr\u00b4 aficos.\n1 Introduction\nPolitical ideology is a psychographic trait\nthat can be used to understand individual\nand social behaviour, including moral and\nethical values as well as inherent attitudes,\nappraisals, biases, and prejudices (Verhulst,\nEaves, and Hatemi, 2012). The relation-\nship between personality traits and political\nideology was demonstrated in Fatke (2017).\nThe author gathered data from 21 countries\nand found a correlation between political\nideology and the big five personality traits.For instance, he found that conscientiousness\nis strongly correlated with the right wing,\nwhereas openness to experience and agree-\nability were notably more correlated to the\nleft wing. Moreover, our political ideology\nhas a great influence in our daily lives. For\nexample, Baumgaertner, Carlisle, and Just-\nwan (2018) found a correlation between po-\nlitical ideology and the attitude of citizens to\nvaccination campaigns of infectious diseases.\nThe PoliticEs shared task organized at\nIberlEF 2022 (Montes-y G\u00b4 omez et al., 2022)\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 265-272\nrecibido 05-07-2022 revisado 18-07-2022 aceptado 21-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-23\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalaims to extract political ideology information\nfrom texts. For this, an author profiling task\nis proposed. It is focused on the identification\nof the gender, the profession, and the polit-\nical spectrum from a binary and multi-class\nperspective.\nIn recent years, several shared tasks have\nbeen organized on author analysis under\nthe PAN workshop series (Bevendorff et al.,\n2021). The novelty of the PoliticEs task is\nthat, to the best of our knowledge, none of\nthese previous tasks have focused on politi-\ncal ideology.\nThe rest of the paper is organized as fol-\nlows. Section 2 describes the PoliticEs shared\ntask. Section 3 presents the dataset provided\nin the competition. Section 4 summarized\nthe participant approaches. Section 5 shows\nthe results and a discussion thereof. Finally,\nSection 6 concludes the paper with some in-\nsights and future works.\n2 Task description\nThe PoliticEs shared task consists of extract-\ning the gender and the profession as demo-\ngraphic traits, and the political ideology as\na psychographic trait from a given user\u2019s set\nof tweets. Political ideology is considered as\na binary (pib) and as a multiclass problem\n(pim). The possible categories of each trait\nare as follows:\n\u2022gender: male, female.\n\u2022profession: political, journalist.\n\u2022pib: left, right.\n\u2022pim: left, moderate left, right, moderate\nright.\nThe challenges involved in this shared task\nare:\n1. Extracting political ideology from a text\ncollection. To the best of our knowledge,\nthis is the first Spanish shared task fo-\ncused on this.\n2. Multi-class classification. The author\nprofiling task should be addressed from a\nbinary and multi-class perspective with\nfour different classes.\nThe competition was organized through\nCodaLab and is accessible at the following\nlink: https://codalab.lisn.upsaclay.\nfr/competitions/1948. It was divided into3 phases: Practice, Evaluation and Post-\nevaluation. In the Practice phase, the par-\nticipants were provided with a subset of the\ntraining data to familiarize with the train-\ning data format, and with a notebook with\na baseline based on Bag of Words (BoW) to\nhave a starting point for system development.\nLater, they were provided with the full train-\ning set to develop their approaches. For this,\nthey were allowed to make a maximum of 100\nsubmissions in CodaLab. It should be men-\ntion that in the Evaluation phase, the test\npartition was provided for the participants\nto label it using the developed systems. This\npartition was used to evaluate the teams.\nThey were allowed to make a maximum of 10\nsubmissions through CodaLab, from which\neach team had to select the best one for rank-\ning. The ranking was determined using the\narithmetic mean of the macro f1-score of the\ngender, profession, binary political ideology,\nand multi-class political ideology.\n3 Dataset\nThe dataset for this shared task is an exten-\nsion of the Spanish PoliCorpus 2022 (Garc\u00b4 \u0131a-\nD\u00b4 \u0131az, Colomo-Palacios, and Valencia-Garc\u00b4 \u0131a,\n2022), which consists of a set of tweets\nfrom the timelines of the Twitter accounts\nof politicians and journalists in Spain. The\npoliticians are members of the government,\ncongress and senate of Spain along with may-\nors, presidents of the autonomous commu-\nnities, former politicians, and collaborators\nwhereas the journalist accounts belong to\njournalists associated to political press, from\nSpanish newspapers such as ABC, El Pa\u00b4 \u0131s,\nElDiario, El Mundo or La Raz\u00b4 on among oth-\ners. The dataset was compiled using the\nUMUCorpusClassifier tool (Garc\u00b4 \u0131a-D\u00b4 \u0131az et\nal., 2020).\nThe users of the dataset are labelled with\ntheir gender (male, female), profession (polit-\nical, journalist), and their political spectrum\non a binary axis (left, right) and a multi-\nclass axis (left, moderate left, moderate right,\nright).\nRegarding the tweets collected from each\nuser, we discarded retweets and tweets that\ncontains headlines from news sites. We also\nremoved tweets written in languages other\nthan Spanish. Moreover, we anonymised\nthem by replacing all mentions with the token\n@user, except for the real users, that were en-\ncoded with the token @user and a correlative\nJos\u00e9 Antonio Garc\u00eda-D\u00edaz, Salud Mar\u00eda Jim\u00e9nez-Zafra, Mar\u00eda-Teresa Mart\u00edn Valdivia, Francisco Garc\u00eda-S\u00e1nchez, L. Alfonso Ure\u00f1a-L\u00f3pez, \nRafael Valencia-Garc\u00eda\n266number. We did this to hinder the author\u2019s\ntraits identification.\nThe final dataset is composed of around\n400 different users with at least 120 tweets.\nFor the shared task, training and test sets\nwere released (80%-20%). We released the\ndataset in two splits: training and testing.\nHowever, in the first stages of the competi-\ntion, we released an early birds dataset com-\nposed by a subset of 5,000 tweets from the\ntraining dataset. It is worth noting that the\naccounts from training and testing are com-\npletely independent in order to prevent auto-\nmatic classifiers learn to identify the authors\nrather than the traits. The number of users\nper set and trait are shown in Table 1.\n4 Participant approaches\nThe PoliticEs shared task attracted 63 teams\nthat registered in CodaLab, of which 20\nsubmitted results and 14 presented working\nnotes describing their systems. The follow-\ning is a brief summary of the participants\u2019\nproposals:\n\u2022(1st) LosCalis (Carrasco and\nRosillo, 2022). This system is based\non transformers (Vaswani et al., 2017).\nIt combines BETO (Ca\u02dc nete et al., 2020)\nand MarIA (Guti\u00b4 errez Fandi\u02dc no et al.,\n2022), and employs both architectures\nfor document level characteristics ex-\ntraction together with a Multi-Layer\nPerceptron for labels decoding.\n\u2022(2nd) NLP-CIMAT (Villa-Cueva\net al., 2022). The authors propose\nPolitiBETO, a pretrained BETO model\n(Ca\u02dc nete et al., 2020) in the political\ndomain, based on the use of domain\nadaptation and ensemble learning. They\ncompose an ensemble using several in-\nstances of pretrained adapted BETO\nmodels, which predicts the test data at\na tweet level. These predictions are then\nmerged through a majority vote to deter-\nmine the labels of a given author based\non their tweets.\n\u2022(3rd) Alejando Mosquera (Mos-\nquera, 2022). He explores the use of\nL2-regularized logistic regression model\nbased on word and character n-grams\nfeatures along with readability features.\nThis work is notable for the analysis of\nadversarial attacks on the author profil-\ning challenge.\u2022(4th) CIMAT 2021 (Santib\u00b4 a\u02dc nez-\nCort\u00b4 es et al., 2022). This team de-\nfines different classification models per\neach trait. Specifically, they use fine-\ntuned BERT (Kenton and Toutanova,\n2019) models for the gender and profes-\nsion, XGBoost for binary ideology, and\nLogistic Regression for multiclass ideol-\nogy.\n\u2022(5th) HalBERT (Holgado and\nSinha, 2022). The authors evaluate\nmultiple feature sets, and deep learning\nand machine learning models. They also\nexplore data augmentation and ensem-\nble learning techniques. They find that\nGloVE embedding features and term-\nfrequency based features, like TF-IDF,\ncan be very helpful and can provide\ncomparative results to deep learning ap-\nproaches.\n\u2022(7th) I2C (Ramos et al., 2022).\nTheir proposal is based on the used of\ntransformers (Vaswani et al., 2017). For\ngender extraction, they build an ensem-\nble as a set of pre-trained transform-\ners models (RoBERTa (Liu et al., 2019),\nALBERTI1and BERTIN (De la Rosa et\nal., 2022)). For the identification of the\nprofession, the tweets of each user are\nmerged to optimize the models. Finally,\nfor the binary and multi-class classifica-\ntion of political ideology, the ROBERTA\nmodel was fine-tuned.\n\u2022(8th) TeamMX (Ochoa-Hern\u00b4 andez\nand Alem\u00b4 an, 2022). The authors an-\nalyze several methods for feature selec-\ntion and machine learning (Random For-\nest and SVM) and deep learning (Multi-\nLayer Perceptron) classifiers. Finally,\nfor determining the gender, they select\nthe best 200 Pearson\u2019s correlation words,\nusing TF-IDF and SVM classifier. For\nthe identification of the profession, they\nuse transition point analysis with lem-\nmas using TF-IDF and Random Forest\nclassifier. For the binary classification of\nthe ideology, they select the set based\nstudy using TF-IDF and SVM classi-\nfier. For the multi-class classification of\nthe ideology they use an average analysis\nwith lemmas using frequency and Multi-\nLayer Perceptron classifier.\n1https://huggingface.co/flax-community/\nalberti-bert-base-multilingual-cased\nOverview of PoliticEs 2022: Spanish Author Profiling for Political Ideology\n267Trait Training Test Total\nGenderMale 177 69 246\nFemale 136 36 172\nProfessionPolitician 251 80 331\nJournalist 61 26 87\nBinary ideologyLeft 178 57 235\nRight 135 48 183\nMulticlass ideologyModerate left 102 36 138\nLeft 76 21 97\nModerate right 94 31 125\nRight 41 17 58\nTable 1: Corpus statistics per trait.\n\u2022(9th) UniRetro (Manea and Dinu,\n2022). The authors propose two ap-\nproaches: the first one based on us-\ning TF-IDF on SentencePiece pretrained\nand custom tokens obtained by Named\nEntity Encapsulation, and the second\none consisting of fine-tuning BETO\n(Ca\u02dc nete et al., 2020) and DistilBETO\n(Ca\u02dc nete et al., 2022).\n\u2022(13th) UNED (Rodrigo, Fabregat,\nand Centeno, 2022). This team ex-\nplores two approaches. The first is\nbased on approximate nearest neigh-\nbours, which obtains low scores for in-\ndividual results but a great score when\ncombining several outputs. The second\nuses some fine-tuned BERT systems, ob-\ntaining the best results.\n\u2022(14th) THANGCIC (Ta et al.,\n2022). They present a system based\non multilingual BERT (Kenton and\nToutanova, 2019), fine-tuned for senti-\nment analysis, which has been trained\nwith product reviews written in differ-\nent languages.\n\u2022(15th) URJC-Team (Rodr\u00b4 \u0131guez-\nGarc\u00b4 \u0131a, Montalvo Herranz, and\nMart\u00b4 \u0131nez Unanue, 2022). This\nteam explores two machine learning al-\ngorithms (Logistic Regression and SVM)\nusing a pre-processing module that\ncleans the tweets and a feature extrac-\ntor module that combines character and\nword features with two different settings,\nwith and without stopwords. Finally,\nthey select SVM classifier with stop-\nwords, which provides the best results.\n\u2022(16th) SINAI (Espin-Riofrio,\nOrtiz-Zambrano, and Montejo-R\u00b4 aez, 2022). The authors propose a\nvoting classifier model that leverages\nthe use of several classical classifiers\n(Logistic Regression, Random Forest,\nDecision Trees, Multi-Layer Perceptron,\nand Gradient Tree Boosting) using as\nfeatures the combination of stylometry\nmeasures with embeddings obtained\nfrom MarIA (Guti\u00b4 errez Fandi\u02dc no et al.,\n2022), a Spanish RoBERTa model for\ntext representation.\n\u2022(19th) UC3MDeep (Garc\u00b4 \u0131a-Ochoa\nMart\u00b4 \u0131n-Forero, Massotti L\u00b4 opez,\nand Segura-Bedmar, 2022). The\nauthors explore several machine learn-\ning approaches (K-Nearest Neighbours,\nRandom Forest and Logistic Regression)\nwith different configurations. They ob-\ntain the best scores using Logistic Re-\ngression without penalty and with a saga\nsolver.\n\u2022INFOTEC-LaBD (Cabrera, Tellez,\nand Miranda, 2022). The proposal\nof these authors is based on a low-\ndimensional stacking model approach,\nwhich was designed to create both trans-\nparent and competitive user profiling\nmodels. The results of this team were\nlate in the challenge, due to a confusion\nwith the deadline.\n5 Results and discussion\nThe official leaderboard of the PoliticEs\nshared task is shown in Table 2. It can be\nseen the results of the 19 participants that\nsubmitted results in time, plus the results of\nthe baseline provided as a notebook, plus the\nresults of the INFOTEC-LaBD team, which\nsubmitted a few hours late due to a mistake.\nJos\u00e9 Antonio Garc\u00eda-D\u00edaz, Salud Mar\u00eda Jim\u00e9nez-Zafra, Mar\u00eda-Teresa Mart\u00edn Valdivia, Francisco Garc\u00eda-S\u00e1nchez, L. Alfonso Ure\u00f1a-L\u00f3pez, \nRafael Valencia-Garc\u00eda\n268Team Average F1-gender F1-profession F1-ideology F1-ideology\nMacro-F1 (binary) (m-class)\nLosCalis 0.90226 (01) 0.90287 (01) 0.94433 (01) 0.96162 (01) 0.80023 (04)\nNLP-CIMAT 0.89096 (02) 0.78484 (06) 0.92125 (03) 0.96148 (02) 0.89628 (01)\nAlejandro Mosquera 0.88918 (03) 0.82671 (03) 0.93345 (02) 0.95152 (03) 0.84504 (03)\nCIMAT 2021 0.87976 (04) 0.83683 (02) 0.89500 (05) 0.94167 (04) 0.84553 (02)\nHalBERT 0.82532 (05) 0.72602 (13) 0.89776 (04) 0.92176 (05) 0.75574 (06)\nBernardo 0.81996 (06) 0.79178 (04) 0.84982 (08) 0.91315 (06) 0.72511 (08)\nI2C 0.79998 (07) 0.74377 (11) 0.86756 (07) 0.86215 (09) 0.72646 (07)\nTeamMX 0.79849 (08) 0.78222 (07) 0.82681 (11) 0.82143 (11) 0.76349 (05)\nUniRetro 0.78694 (09) 0.73798 (12) 0.88346 (06) 0.90220 (07) 0.62412 (12)\njoseluisUS 0.78164 (10) 0.79178 (04) 0.79532 (13) 0.91315 (06) 0.62631 (11)\nErHulio 0.77040 (11) 0.75278 (09) 0.71208 (16) 0.89207 (08) 0.72466 (09)\nAzaelCC 0.75180 (12) 0.78050 (08) 0.89500 (05) 0.79935 (14) 0.53234 (18)\nUNED 0.74089 (13) 0.74716 (10) 0.83331 (09) 0.81827 (12) 0.56482 (15)\nTHANGCIC 0.72724 (14) 0.69146 (15) 0.81471 (12) 0.75769 (16) 0.64511 (10)\nURJC-Team 0.72192 (15) 0.65987 (16) 0.83298 (10) 0.80811 (13) 0.58672 (13)\nSINAI 0.72147 (16) 0.78571 (05) 0.75395 (15) 0.78469 (15) 0.56154 (16)\nUC3M-DEEPLNLP-2 0.64315 (17) 0.69388 (14) 0.47324 (17) 0.82917 (10) 0.57629 (14)\nprobatzen 0.61084 (18) 0.59167 (18) 0.77987 (14) 0.67453 (18) 0.39729 (20)\nUC3MDeep 0.58644 (19) 0.64892 (17) 0.40341 (19) 0.74638 (17) 0.54704 (17)\nBASELINE 0.51123 (20) 0.57621 (19) 0.43243 (18) 0.59567 (19) 0.44060 (19)\nINFOTEC-LaBD 0.72426 (15) 0.71275 (14) 0.61111 (17) 0.95152 (03) 0.72426 (10)\nTable 2: PoliticEs official leaderboard (ranking per metric is shown between parenthesis).\nThe system that obtained the overall high-\nest performance was LosCalis, with an aver-\nage macro-f1 of 0.90226, combining BETO\nand MarIA for document level characteris-\ntics extraction together with a Multi-Layer\nPerceptron classifier for labels decoding. It\nwas followed by NLP-CIMAT and Alejan-\ndro Mosquera with an average macro-f1 of\n0.89096 and 0.88918, respectively. The NLP-\nCIMAT team proposed PolitiBETO, based\non domain adaptation and ensemble learning.\nAlejandro Mosquera used word and character\nn-grams features along with readability fea-\ntures with a L2-regularized logistic regression\nclassifier.\nRegarding the results per trait, on the one\nhand, in relation to the demographic traits,\ngender has been the most difficult for the par-\nticipants to classify and, on the other hand,\nwith respect to the psychographic trait, po-\nlitical ideology, the multi-class classification\nhas been the most complex.\nConcerning the approaches used, most\nof the teams propose approaches based\non transformers (BETO, MarIA, RoBERTa,\nALBERTI, BERTIN, DistilBERT, and mul-\ntilingual BERT), mainly fine-tuning the pre-\ntrained models. Some of them also use tra-\nditional machine learning algorithms, being\nSVM and Logistic Regression the most fre-\nquent. There are teams that define differ-ent models for the identification of each trait,\nalthough most use a single model for all of\nthem. Some of them also combine different\napproaches through ensemble learning and\nonly one team explores data augmentation\ntechniques.\n6 Conclusions\nThis paper presents the first edition of the\nPoliticEs task at IberLEF 2022. It is an au-\nthor profiling task for political ideology in\nSpanish. So far, several tasks on author-\nship analysis have been organized in the PAN\nworkshop series (Bevendorff et al., 2021),\nbut none of them focuses on political ide-\nology. Political ideology is a psychographic\ntrait that can be used to understand individ-\nual and social behavior. Because of its rel-\nevance, we intend to promote author profil-\ning research for political ideology in Spanish\nthrough the organization of this shared task.\nWe are very pleased with the impact\nof the PoliticEs task, as 63 teams regis-\ntered for it through CodaLab, the plat-\nform on which the competition was orga-\nnized, which is accessible at the following\nlink: https://codalab.lisn.upsaclay.\nfr/competitions/1948. Finally, of all the\nregistered teams, 20 submitted results and\n14 presented working notes to describe their\nsystems, which are summarized in this paper.\nOverview of PoliticEs 2022: Spanish Author Profiling for Political Ideology\n269As expected, approaches based on transform-\ners are the trend solutions presented by par-\nticipating teams, but some of them also used\ntraditional machine learning systems or even\na combination of them. Finally, it should be\nmentioned that gender and political ideology\nmulti-class have been the traits most difficult\nto classify for the participants.\nAs future work, we plan to extend the\ndataset by including more users who are nei-\nther politicians nor journalists. For this, we\nask users to voluntarily sent their tweets at\nthe same time they define their political spec-\ntrum. Another idea is to include more sub-\ntasks concerning author analysis. For exam-\nple, we are planing to add a subtask related\nto stance detection, in order to determine\nwhich authors are in favor of certain topics\nand which users are against. We can use this\ninformation to define clusters of users and to\nobserve whether there is a relationship be-\ntween the topics and the political ideology.\nAcknolwedgements\nThis work was supported by Project\nLaTe4PSP (PID2019-107652RB-I00) funded\nby MCIN/AEI/10.13039/501100011033,\nProject AlInFunds (PDC2021-\n121112-I00) funded by\nMCIN/AEI/10.13039/501100011033 and\nby the European Union NextGenera-\ntionEU/PRTR, Project LIVING-LANG\n(RTI2018-094653-B-C21) funded by\nMCIN/AEI/10.13039/501100011033 and\nby ERDF A way of making Europe, and\nBig Hug project (P20 00956, PAIDI 2020)\nand WeLee project (1380939, FEDER An-\ndaluc\u00b4 \u0131a 2014-2020) funded by the Andalusian\nRegional Government. In addition, Jos\u00b4 e\nAntonio Garc\u00b4 \u0131a-D\u00b4 \u0131az has been supported by\nBanco Santander and University of Murcia\nthrough the industrial doctorate programme,\nand Salud Mar\u00b4 \u0131a Jim\u00b4 enez-Zafra has been\npartially supported by a grant from Fondo\nSocial Europeo and Administraci\u00b4 on de la\nJunta de Andaluc\u00b4 \u0131a (DOC 01073).\nReferences\nBaumgaertner, B., J. E. Carlisle, and F. Just-\nwan. 2018. The influence of political ide-\nology and trust on willingness to vacci-\nnate. PloS one, 13(1):e0191728.\nBevendorff, J., B. Chulvi, G. L. D. L.\nPe\u02dc na Sarrac\u00b4 en, M. Kestemont, E. Man-javacas, I. Markov, M. Mayerl, M. Pot-\nthast, F. Rangel, P. Rosso, et al. 2021.\nOverview of PAN 2021: authorship ver-\nification, profiling hate speech spreaders\non twitter, and style change detection.\nInInternational Conference of the Cross-\nLanguage Evaluation Forum for European\nLanguages, pages 419\u2013431. Springer.\nCabrera, H., E. S. Tellez, and S. Miranda.\n2022. INFOTEC-LaBD at PoliticES 2022:\nLow-dimensional Stacking Model for Po-\nlitical Ideology Profiling. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022). CEUR Workshop Pro-\nceedings, CEUR-WS, A Coru\u02dc na, Spain .\nCa\u02dc nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u00b4 erez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. Pml4dc at iclr , 2020:1\u201310.\nCa\u02dc nete, J., S. Donoso, F. Bravo-Marquez,\nA. Carvallo, and V. Araujo. 2022. Al-\nbeto and distilbeto: Lightweight span-\nish language models. arXiv preprint\narXiv:2204.09145.\nCarrasco, S. S. and R. C. Rosillo. 2022.\nLosCalis at PoliticEs 2022: Political Au-\nthor Profiling using BETO and MarIA. In\nProceedings of the Iberian Languages Eval-\nuation Forum (IberLEF 2022). CEUR\nWorkshop Proceedings, CEUR-WS, A\nCoru\u02dc na, Spain .\nDe la Rosa, J., E. G. Ponferrada, M. Romero,\nP. Villegas, P. G. de Prado Salas, and\nM. Grandury. 2022. Bertin: Efficient pre-\ntraining of a spanish language model us-\ning perplexity sampling. Procesamiento\ndel Lenguaje Natural, 68:13\u201323.\nEspin-Riofrio, C., J. Ortiz-Zambrano, and\nA. Montejo-R\u00b4 aez. 2022. SINAI at\nPoliticEs 2022: Exploring Relative Fre-\nquency of Words in Stylometrics for\nProfile Discovery. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022). CEUR Workshop Pro-\nceedings, CEUR-WS, A Coru\u02dc na, Spain .\nFatke, M. 2017. Personality traits and po-\nlitical ideology: A first global assessment.\nPolitical Psychology , 38(5):881\u2013899.\nGarc\u00b4 \u0131a-D\u00b4 \u0131az, J. A., \u00b4A. Almela, G. Alcaraz-\nM\u00b4 armol, and R. Valencia-Garc\u00b4 \u0131a. 2020.\nUMUCorpusClassifier: Compilation and\nJos\u00e9 Antonio Garc\u00eda-D\u00edaz, Salud Mar\u00eda Jim\u00e9nez-Zafra, Mar\u00eda-Teresa Mart\u00edn Valdivia, Francisco Garc\u00eda-S\u00e1nchez, L. Alfonso Ure\u00f1a-L\u00f3pez, \nRafael Valencia-Garc\u00eda\n270evaluation of linguistic corpus for Nat-\nural Language Processing tasks. Proce-\nsamiento del Lenguaje Natural, 65(0):139\u2013\n142.\nGarc\u00b4 \u0131a-D\u00b4 \u0131az, J. A., R. Colomo-Palacios, and\nR. Valencia-Garc\u00b4 \u0131a. 2022. Psycho-\ngraphic traits identification based on po-\nlitical ideology: An author analysis study\non spanish politicians\u2019 tweets posted in\n2020. Future Generation Computer Sys-\ntems, 130:59\u201374.\nGarc\u00b4 \u0131a-Ochoa Mart\u00b4 \u0131n-Forero, \u00b4A., A. Mas-\nsotti L\u00b4 opez, and I. Segura-Bedmar. 2022.\nUC3MDeep at PoliticEs 2022: Exploring\nTraditional Machine Learning Algorithms\nfor Political Ideology Detection. In Pro-\nceedings of the Iberian Languages Eval-\nuation Forum (IberLEF 2022). CEUR\nWorkshop Proceedings, CEUR-WS, A\nCoru\u02dc na, Spain .\nGuti\u00b4 errez Fandi\u02dc no, A., J. Armengol Estap\u00b4 e,\nM. P` amies, J. Llop Palao, J. Sil-\nveira Ocampo, C. Pio Carrino, C. Ar-\nmentano Oller, C. Rodriguez Penagos,\nA. Gonzalez Agirre, and M. Villegas.\n2022. MarIA: Spanish language models.\nProcesamiento del Lenguaje Natural , 68.\nHolgado, C. G. and A. Sinha. 2022.\nHalBERT at PoliticEs 2022: Are Ma-\nchine Learning Algorithms better for Au-\nthor Profiling? In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022). CEUR Workshop Pro-\nceedings, CEUR-WS, A Coru\u02dc na, Spain .\nKenton, J. D. M.-W. C. and L. K. Toutanova.\n2019. BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL-\nHLT, pages 4171\u20134186.\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettle-\nmoyer, and V. Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nManea, A.-A. and L. P. Dinu. 2022.\nUniRetro at PoliticEs@IberLef 2022: Po-\nlitical Ideology Profiling using Language\nModels. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2022). CEUR Workshop Proceedings,\nCEUR-WS, A Coru\u02dc na, Spain .Montes-y G\u00b4 omez, M., J. Gonzalo, F. Rangel,\nM. Casavantes, M. \u00b4A.\u00b4Alvarez-Carmona,\nG. Bel-Enguix, H. Jair Escalante, L. Fre-\nitas, A. Miranda-Escalada, F. Rodr\u00b4 \u0131guez-\nS\u00b4 anchez, A. Ros\u00b4 a, M. A. Sobrevilla-\nCabezudo, M. Taul\u00b4 e, and R. Valencia-\nGarc\u00b4 \u0131a, editors. 2022. Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022) .\nMosquera, A. 2022. Alejandro Mosquera\nat PoliticEs 2022: Towards Robust Span-\nish Author Profiling and Lessons Learned\nfrom Adversarial Attacks. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2022). CEUR Work-\nshop Proceedings, CEUR-WS, A Coru\u02dc na,\nSpain.\nOchoa-Hern\u00b4 andez, J. L. and Y. Alem\u00b4 an.\n2022. TeamMX at PoliticEs 2022: Analy-\nsis of Feature Sets in Spanish Author Pro-\nfiling for Political Ideology. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2022). CEUR Work-\nshop Proceedings, CEUR-WS, A Coru\u02dc na,\nSpain.\nRamos, P. C., J. M. V\u00b4 azquez, V. P.\n\u00b4Alvarez, and J. L. D. Olmedo. 2022.\nI2C at PoliticEs 2022: Using Trans-\nformers to Identify Political Ideology\nin Spanish Tweets. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022). CEUR Workshop Pro-\nceedings, CEUR-WS, A Coru\u02dc na, Spain .\nRodrigo, \u00b4A., H. Fabregat, and R. Centeno.\n2022. UNED at PoliticEs 2022: Test-\ning Approximate Nearest Neighbors and\nSpanish Language Models for Author Pro-\nfiling in Political Ideology. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2022). CEUR Work-\nshop Proceedings, CEUR-WS, A Coru\u02dc na,\nSpain.\nRodr\u00b4 \u0131guez-Garc\u00b4 \u0131a, M. \u00b4A., S. Montalvo Her-\nranz, and R. Mart\u00b4 \u0131nez Unanue. 2022.\nURJC-Team at PoliticEs 2022: Politi-\ncal Ideology Prediction using Linear Clas-\nsifiers. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2022). CEUR Workshop Proceedings,\nCEUR-WS, A Coru\u02dc na, Spain .\nSantib\u00b4 a\u02dc nez-Cort\u00b4 es, E., A. Carrillo-\nCabrera, Y. A. Castillo-Castillo,\nOverview of PoliticEs 2022: Spanish Author Profiling for Political Ideology\n271D. Moctezuma, and V. Mu\u02dc niz-S\u00b4 anchez.\n2022. CIMAT 2021 at PoliticEs 2022:\nEnsemble Based Classification Algo-\nrithms for Author Profiling in Spanish\nLanguage. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2022). CEUR Workshop Proceedings,\nCEUR-WS, A Coru\u02dc na, Spain .\nTa, H. T., A. B. S. Rahman, L. Najjar,\nand A. Gelbukh. 2022. THANGCIC\nat PoliticEs 2022: Term-based BERT for\nExtracting Political Ideology from Span-\nish Author Profiling. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2022). CEUR Workshop Pro-\nceedings, CEUR-WS, A Coru\u02dc na, Spain .\nVaswani, A., N. Shazeer, N. Parmar,\nJ. Uszkoreit, L. Jones, A. N. Gomez,\n L. Kaiser, and I. Polosukhin. 2017. At-\ntention is all you need. Advances in neural\ninformation processing systems, 30.\nVerhulst, B., L. J. Eaves, and P. K. Hatemi.\n2012. Correlation not causation: The re-\nlationship between personality traits and\npolitical ideologies. American journal of\npolitical science, 56(1):34\u201351.\nVilla-Cueva, E., I. Gonz\u00b4 alez-Franco,\nF. Sanchez-Vega, and A. P. L\u00b4 opez-\nMonroy. 2022. NLP-CIMAT at PoliticEs\n2022: PolitiBETO, a Domain-Adapted\nTransformer for Multi-class Political\nAuthor Profiling. In Proceedings of\nthe Iberian Languages Evaluation Fo-\nrum (IberLEF 2022). CEUR Workshop\nProceedings, CEUR-WS, A Coru\u02dc na,\nSpain.\nJos\u00e9 Antonio Garc\u00eda-D\u00edaz, Salud Mar\u00eda Jim\u00e9nez-Zafra, Mar\u00eda-Teresa Mart\u00edn Valdivia, Francisco Garc\u00eda-S\u00e1nchez, L. Alfonso Ure\u00f1a-L\u00f3pez, \nRafael Valencia-Garc\u00eda\n272Overview of QuALES at IberLEF 2022:\nQuestion Answering Learning from Examples in\nSpanish\nOverview de QuALES en IberLEF 2022:\nPreguntas y Respuestas Autom\u00b4 aticas sobre Ejemplos en\nEspa\u02dc nol\nAiala Ros\u00b4 a1, Luis Chiruzzo1, Luc\u00b4 \u0131a Bouza1, Alina Dragonetti1,\nSantiago Castro2, Mathias Etcheverry1, Santiago G\u00b4 ongora1, Santiago Goycoechea1,\nJuan Machado1, Guillermo Moncecchi1, Juan Jos\u00b4 e Prada1, Dina Wonsever1\n1Universidad de la Rep\u00b4 ublica, Montevideo, Uruguay\n{aialar, luischir, lucia.bouza, alina.dragonetti, mathiase, sgongora,\nsgoycoechea, juan.machado, gmonce, prada, wonsever }@fing.edu.uy\n2University of Michigan, Anne Arbor, USA\nsacastro@umich.edu\nAbstract: We present the results of the QuALES task, which addresses the problem\nof Extractive Question Answering from texts. For both training and evaluation we\nuse the QuALES corpus, a corpus of Uruguayan media news about the Covid-19\npandemic and related topics. We describe the systems developed by seven partici-\npants, all of them based on different BERT-like language models. The best results\nwere obtained using the multilingual RoBERTa model pre-trained with SQUAD-Es-\nV2, with a fine tuning on the QuALES corpus.\nKeywords: Question Answering for Spanish, Language Models, Datasets for Ques-\ntion Answering.\nResumen: Presentamos los resultados de la tarea QuALES, que aborda el problema\nde B\u00b4 usqueda de Respuestas extractiva a partir de textos. Tanto para entrenamiento\ncomo para evaluaci\u00b4 on utilizamos el corpus QuALES, un corpus de noticias de medios\nuruguayos sobre la pandemia por Covid-19 y temas relacionados. Describimos los\nsistemas desarrollados por siete participantes, todos ellos basados en diferentes mo-\ndelos de lenguaje tipo BERT. Los mejores resultados se obtuvieron usando el modelo\nRoBERTa multiling\u00a8 ue preentrenado con SQUAD-Es-V2, con una fine tuning sobre\nel corpus QuALES.\nPalabras clave: B\u00b4 usqueda de Respuestas en Espa\u02dc nol, Modelos de Lenguaje, Cor-\npus para B\u00b4 usqueda de Respuestas.\n1 Introduction\nQuestion Answering (QA) is a classical Na-\ntural Language Processing task that is cu-\nrrently gaining great relevance. QA can be\nroughly divided into two main categories (Ju-\nrafsky and Martin, 2021): semantic analysis,\nwhere the question is transformed to a query\nto a knowledge database; and open domain\nquestion answering, where, starting from a\nquestion written in natural language and a\nset of documents, the answer to the question\nis obtained using information retrieval and\ninformation extraction techniques.\nOpen domain question answering involves\ntwo main stages: a) getting the relevant do-cuments, generally using methods from the\nInformation Retrieval field (IR) (Manning,\nRaghavan, and Sch\u00a8 utze, 2010), possibly one\nof the most widely studied topics in NLP,\nwith web search engines as their most noti-\nceable product, b) extracting the answer from\nthose documents. Each of these stages has its\nown challenges, and the whole task requires\na successful outcome for each of them and for\ntheir integration.\nIn this task we address the problem of ex-\ntractive QA in Spanish, based on a corpus\nof a specific domain: press news about the\nCovid-19 pandemic. We focus on the second\nstage of the task: given a text, extracting the\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 273-280\nrecibido 05-07-2022 revisado 21-07-2022 aceptado 24-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-24\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalanswer to a question, if there is one.\nThe rest of the paper is structured as fo-\nllows: section 2 describes the background of\nQA, focusing on QA for Spanish; section 3\ndescribes the corpus created for this task and\nsome other resources; section 4 describes the\nQuALES task; section 5 presents the partici-\npants systems and analyzes the results; and,\nfinally, section 6 shows some conclusions.\n2 Background\nStarting last decade, along with the popula-\nrization of distributional semantic methods\nbased on neural networks (Le and Mikolov,\n2014; LeCun, Bengio, and Hinton, 2015), this\nkinds of methods started to be applied to the\nQA task, achieving significant results impro-\nvement (Yu et al., 2014; Min et al., 2018;\nXiong, Zhong, and Socher, 2017; Seo et al.,\n2016).\nAll these supervised learning approaches\nwere possible due to the existence of research\noriented publicly available datasets. These\ndatasets have enabled not only model trai-\nning, but also constant monitoring of this\narea\u2019s state of the art. Probably the most\npopular is SQuAD (Rajpurkar et al., 2016).\nTo build this dataset, annotators were pre-\nsented with a Wikipedia paragraph and as-\nked to write questions that could be ans-\nwered from the given text. Natural Ques-\ntions (Kwiatkowski et al., 2019b) was crea-\nted from actual Google Search queries, where\nannotators marked the answer into Wikipe-\ndia article snippets. TriviaQA (Joshi et al.,\n2017) contains a set of Trivia questions and\nanswers. CuratedTREC (Baudi\u02c7 s and \u02c7Sediv\u00b4 y,\n2015) dataset generated by the QA track of\nthe NIST TREC conferences contains ques-\ntions and answers. NewsQA (Trischler et al.,\n2016) is a machine comprehension dataset\nof over 100,000 human-generated question-\nanswer pairs, based on set of over 10,000 news\narticles from CNN.\nIn the last few years, after the publica-\ntion of models based on the Transformers\narchitecture (Vaswani et al., 2017) for sol-\nving sequence to sequence transformation\nproblems, and particularly language models\nsuch as BERT (Devlin et al., 2018) and AL-\nBERT (Lan et al., 2019), there has been a\nnew push in system performance, particu-\nlarly for the English language. These kinds of\nmodels are trained in an self-supervised way,\nusing large volumes of data and computingpower. After that stage (called pretraining),\nthey can be easily fine-tuned to apply them\nto different tasks. Regarding this shared task,\nwe are particularly interested in fine-tuning\nthem to the open domain question answering\ntask.\nThe study of the QA area is currently very\nactive, as evidenced by the inclusion of a tu-\ntorial1on this topic in ACL 2020, the main\nNLP event worldwide.\nThroughout the last few years, several QA\nrelated tasks have been proposed. Since 2015,\none of the tasks of each SemEval annual in-\nternational workshop on Semantic Evalua-\ntion has been related to some form of the\nQA Task. For example, SemEval-2015 Task\n3: \u201cAnswer Selection in Community Ques-\ntion Answering\u201d (Nakov et al., 2019b) pro-\nposed, given a question, to classify a cer-\ntain answer as good, bad, or potential, and\nanswer yes/no questions. The challenge was\nproposed for Arabic and English. SemEval-\n2017 Task 3: \u201cCommunity Question Ans-\nwering\u201d (Nakov et al., 2019a) proposed th-\nree different subtasks: Question-Comment Si-\nmilarity, Question-Question Similarity, and\nQuestion-External Comment Similarity. Ad-\nditionally, for the Arabic language, another\ntask was added: reranking correct answers for\na new question.\nSemEval-2022 includes the task\n\u201cCompetence-based Multimodal Ques-\ntion Answering\u201d (Task 09)2, designed to\nquery how well a system understands the\nsemantics of recipes derived from the R2VQ\ndataset, a multimodal dataset of cooking\nrecipes and videos.\nIn (Reddy, Chen, and Manning, 2019) the\nCoQA (Conversational Question Answering)\ndataset is presented as a challenge. The da-\ntaset includes 127k questions with answers,\nobtained from 8k conversations about text\npassages. The goal of the CoQA challenge\nis to measure the ability of machines to un-\nderstand a text passage and answer a series\nof interconnected questions that appear in a\nconversation.\nThe Stanford Question Answering Data-\nset (SQuAD) (Rajpurkar et al., 2016) web-\nsite includes the highest performing systems\non the dataset, measuring Exact Match and\n1https://github.com/danqi/acl2020-openqa-\ntutorial\n2https://competitions.codalab.org/competitions\n/34056\nAiala Ros\u00e1, Luis Chiruzzo, Luc\u00eda Bouza, Alina Dragonetti, Santiago Castro, Mathias Etcheverry, Santiago G\u00f3ngora, Santiago Goycoechea, \nJuan Machado, Guillermo Moncecchi, Juan Jos\u00e9 Prada, Dina Wonsever\n274F1 values (see the next section for a descrip-\ntion of these metrics). These systems should\nanswer reading comprehension questions, in-\ncluding questions that do not have an answer\non the dataset.\nBased on Google\u2019s Natural Questions Da-\ntaset (Kwiatkowski et al., 2019a), the Tensor-\nflow 2.0 machine learning platform includes a\nQuestion Answering competition, where the\ngoal is to predict short and long answer res-\nponses to real questions about Wikipedia ar-\nticles. Using the same dataset, the 2020 Neu-\nrIPS Conference an open domain question\nanswering challenge (Min et al., 2021) was\nalso proposed, including three tracks where\nthe objective is to build self-contained ques-\ntion answering systems.\nFor English, the BioASQ challenge for\n2022 proposes a task relative to the Covid-19\ndomain, using a dataset composed by biome-\ndical articles.\nQA research for Spanish has evolved much\nmore slowly. However, similar language re-\nsources have been created for this language,\nwhich makes us think it is possible to study\nand fine-tune current architectures to obtain\ncompetitive results. In particular, there is a\nrecently developed version of BERT for Spa-\nnish, dubbed BETO (Ca\u02dc nete et al., 2020),\nand a version of SQuAD (the main dataset\nfor training and evaluating open domain QA\nsystems) translated to Spanish (Rajpurkar et\nal., 2016; Carrino, Costa-juss` a, and Fonollo-\nsa, 2019). The Spanish Question Answering\nCorpus (SQAC) is an extractive QA data-\nset created from texts extracted from a mix\nfrom different news-wire and literature sour-\nces, and it includes 18,817 questions with the\nannotation of their answer spans from 6,247\ntextual contexts (Guti\u00b4 errez Fandi\u02dc no et al.,\n2022).\nFrom 2003 to 2014, the CLEF Ques-\ntion Answering Track has proposed diffe-\nrent campaigns related to question answe-\nring, some of which included Spanish data-\nsets. For example, together with the CLEF\n2009 forum, ResPubliQA, a Question Ans-\nwering Task over European legislation was\nproposed (Pe\u02dc nas et al., 2009). The task con-\nsisted of extracting a relevant paragraph of\ntext that included the answer to a natural\nlanguage question. During CLEF 2010, the\ntask was expanded (Pe\u02dc nas et al., 2010) to in-\nclude an answer selection task (i.e. besides re-\ntrieving the relevant paragraph, systems we-re required to identify the exact answer to\nthe question). It also proposed several cross-\nlingual tasks, working on two multilingual pa-\nrallel corpus: the JRC-ACQUIS Multilingual\nParallel Corpus (10,700 parallel and aligned\ndocuments), and the Europarl collection (150\nparallel and aligned document per language),\nwith 200 question-answer pairs provided for\nevaluation.\nUnlike the task we present here, the CLEF\ntasks have addressed domain-general ques-\ntions, or questions for some specific domains,\nbut different from the one selected for QuA-\nLES. In addition, they have worked with sma-\nller amounts of training and testing data. So-\nme of these CLEF tasks have some characte-\nristics that differ from our proposal, such as\ndatasets oriented to answer multiple choice\nquestions, or natural language questions to\nbe answered from DBPedia structured data\n(instead of plain text), among other.\n3 Corpus\nWe provided a corpus of around 2,600\nquestion-answer pairs (the QuALES corpus).\nThe training set contains 1,000 of these pairs,\nwhile the dev and test sets have around\n800 pairs each. Participants could use any\nother data for training as well, in particu-\nlar SQuAD (Rajpurkar et al., 2016) or News-\nQA (Trischler et al., 2016). The data is avai-\nlable at the Codalab competition site3.\nThe QuALES corpus is original and it was\ncreated manually by the members of the team\nand students. It is a Question-Answering cor-\npus in Spanish obtained from a set of Covid-\n19 related news published in two important\nnews media from Uruguay (La Diaria4and\nMontevideo Portal5). It consists of a set of\nfactoid questions mostly about Covid-19 and\nits repercussions in Uruguay and the world.\nTable 1 shows the statistics of the dataset.\nThe corpus annotation was made in two\nstages: first, we annotated questions by\nreading only the title and first sentence of\nthe article; then, we thought of questions de-\nrived from the reading of the whole article.\nFor each question, we annotated the answer\nfound (if there was any) and the whole sen-\ntence context which included it. For the an-\nnotation of the answer, we selected the shor-\n3https://codalab.lisn.upsaclay.fr/competitions\n/2619\n4https://ladiaria.com.uy/\n5https://www.montevideo.com.uy/\nOverview of QuALES at IberLEF 2022: Question Answering Learning from Examples in Spanish\n275Split Train Dev Test\nArticles 176 146 143\nQuestions 948 773 759\nAnswers 1000 800 821\nEmpty answers 165 132 103\nTable 1: Statistics of the QuALES corpus\nshowing number of articles, number of ques-\ntions, total number of answers and total num-\nber of questions without answers (empty ans-\nwers) by split.\ntest span of text contained in the sentence\nthat consisted in a complete answer for the\nquestion. All the answers were directly ex-\ntracted from the text. Some questions may\nhave more than one answer in a given text,\nin such cases, a set of answers is generated\nfor this question.\nWe measured inter-annotator agreement\nbetween six pairs of annotators. Each pair\nanswered a set of 25 questions, generating\na total of 150 questions with two different\nannotator answers. We obtained an avera-\nge Exact Match of 0.61 and an average F1\nof 0.76. These results are quite low, which\nshows the complexity of the task, even for\nhumans. The difference between Exact Match\nand F1 shows the difficulty in defining the li-\nmits of the answer, in general the differences\nare due to the inclusion or not of elements\nsuch as prepositions or determiners. The low\nF1 shows that selecting the fragment that\ncontains the answer, or deciding that a cer-\ntain fragment has no answer in the text, is\nalso a highly complex task.\nWe also published some resources to au-\ntomatically generate a Spanish version of\nthe NewsQA (Trischler et al., 2016) corpus.\nThe complete NewsQA corpus was translated\nusing a machine translation model and after\nthat we aligned the answers. This alignment\nstage is necessary because, when translating\neach fragment with its associated question\nand answer, the substring corresponding to\nthe answer within the fragment, can be trans-\nlated differently from the associated answer,\nwhich is translated decontextualized. In our\ntranslation of the corpus, this alignment pro-\nblem was detected in 49 % of the cases. To\nsolve this problem we worked on two approa-\nches: on the one hand we trained a neural\nmodel from pairs of aligned texts, and, on\nthe other hand, we tested some heuristics de-\nfined from the analysis of different examples.In order to evaluate the two approaches, we\nperformed a manual evaluation of a subset\nof 2,000 question-answer pairs. A portion of\nthis curated corpus was used for parameter\ntuning of the neural model. The neural model\nfor alignment achieved better results than the\nheuristics approach. Due to licensing issues,\nit is not possible to provide a link to this data-\nset, but the resources to recreate this process\nare available at our github repository6.\n4 Task\nThe aim of the QuALES task is to develop\nquestion answering systems that can answer\nquestions based on news articles written in\nSpanish. The systems get a full news arti-\ncle and a question, and must find the shor-\ntest span of text in the article (if it exists)\nthat answers the question. It should be no-\nted that for some questions there may not be\nan answer in the given text. est\u00b4 a hablando.\nThe training, development and test datasets\nwere generated from the QuALES corpus, as\nmentioned above. Originally, we planned to\nhave two separate corpora for evaluation, but\nseeing that the texts often contain Covid-19\nrelated news mixed with other topics, we de-\ncided to annotate only one set. Most of the\nquestions in the dataset are about Covid-19\nmatters, but some of them are also about\nother topics.\nTable 2 shows a sample text with two\nquestions. The answer to one of the questions\ncan be found in the text, while the other is\nnot present.\nAs one of our evaluation metrics, we mea-\nsure average Exact Match for all the da-\ntaset instances, following the approach of\nSQuAD (Rajpurkar et al., 2016). We also\nreport, following (Reddy, Chen, and Man-\nning, 2019), the macro-average F1 score of\nword overlap: we compare each individual\nprediction against the different human gold\nstandard answers and select the maximum\nvalue as system F1 score for that instan-\nce; the system performance is the macro-\naverage of all those F1 scores. Some determi-\nners, specifically, definite articles, and pun-\nctuation marks were ignored when calcula-\nting this evaluation metric.\nSome of the questions in the dataset have\nmore than one possible answer, but the sys-\ntems are expected to generate at most only\n6https://github.com/pln-fing-udelar/newsqa-es\nAiala Ros\u00e1, Luis Chiruzzo, Luc\u00eda Bouza, Alina Dragonetti, Santiago Castro, Mathias Etcheverry, Santiago G\u00f3ngora, Santiago Goycoechea, \nJuan Machado, Guillermo Moncecchi, Juan Jos\u00e9 Prada, Dina Wonsever\n276Comenzaron las clases presenciales en 344 escuelas rurales, con baja asistencia. A\nlas 8.45 dos perros paseaban por el patio de la escuela rural 27 de La Macana, en\nFlorida. Dos maestras con t\u00b4 unicas blancas y tapabocas esperaban a los alumnos que\nreanudar\u00b4 \u0131an las clases presenciales luego de cinco semanas de conexi\u00b4 on virtual. Ya\nestaba instalado el micr\u00b4 ofono y el parlante en el patio, hab\u00b4 \u0131an llegado los inspectores\nregionales junto con la directora general del Consejo de Educaci\u00b4 on Inicial y Primaria\n(CEIP), Irup\u00b4 e Buzzetti, que junto a la prensa local esperaban a los ni\u02dc nos. De los 28\nalumnos que asisten regularmente, 14 hab\u00b4 \u0131an dicho que no iban a ir y los otros no\nhab\u00b4 \u0131an confirmado. A las 9.00, cuando deb\u00b4 \u0131an comenzar las clases en la escuela de La\nMacana, no hab\u00b4 \u0131a ning\u00b4 un ni\u02dc no. (...) La situaci\u00b4 on de La Macana se repiti\u00b4 o en varias\nde las escuelas que abrieron este mi\u00b4 ercoles. De las 547 escuelas habilitadas abrieron\n344, confirm\u00b4 o a la diaria Limber Santos, director del departamento de Educaci\u00b4 on\nRural del CEIP. De esas escuelas, cerca de 90 no recibieron alumnos; Santos estim\u00b4 o\nque en la ma\u02dc nana del mi\u00b4 ercoles 1.030 ni\u02dc nos concurrieron a las escuelas, de un\ntotal de 3.900 que concurren a las 547 habilitadas y de 2.838 alumnos que tienen\nmatriculadas las 344 escuelas que abrieron. La asistencia, por tanto, lleg\u00b4 o a 36 % en\nel primer d\u00b4 \u0131a.\nQ1:\u00bfCu\u00b4 antas escuelas rurales hay en Uruguay?\nA1:De las [547] escuelas habilitadas abrieron 344, confirm\u00b4 o a la diaria Limber\nSantos, director del departamento de Educaci\u00b4 on Rural del CEIP.\nQ2:\u00bfCu\u00b4 ando vuelven las clases presenciales a todas las escuelas?\nA2: \u2013not found in the text\u2013\nTable 2: Example of a short text that could be found in the corpus, and two possible questions\nfor the text. Q1 has the answer 547, found in the text, but Q2 does not have an answer in the\ntext.\none answer. Because of this, when there are\nmultiple answers for a question, the metrics\nevaluate the answer candidate provided by\nthe system against all the possible answers,\nand get the maximum value.\n5 Competition\nThe competition was run in two phases: a de-\nvelopment phase, for which we released the\ntraining dataset with annotations and deve-\nlopment dataset without annotations; and an\nevaluation phase, for which we released the\nannotations of the development dataset and\na test dataset without annotations. Partici-\npants could train their models using other\navailable corpora, such as the Spanish ver-\nsion of SQuAD or NewsQA.\n5.1 Description of the systems\nEighteen participants registered for the com-\npetition in our Codalab site, eight of them\nsubmitted results for the development phase\n(73 submissions in total), and seven of them\nsubmitted results for the evaluation phase (46\nsubmissions in total). All of the participants\nthat sent results in the evaluation phase used\nBERT-like models, analyzing if fine-tuning\nthem with proper data improved their per-formance.\nThe language model most commonly used\nby the participants was RoBERTa for Spa-\nnish, trained with the corpus from the Biblio-\nteca Nacional de Espa\u02dc na7. BETO8, multilin-\ngual RoBERTa9, multilingual BERT10, and\ndistill BERT for Spanish11were also used.\nThe corpora used, in addition to the\nQuALES corpus, were SQuAD 2.0, NewsQA\nand SQAC (Spanish versions).\nThe participant smaximo (M\u00b4 aximo, 2022)\nfollowed a curriculum learning strategy con-\nsisting of fine-tuning BETO and RoBERTa\nfor Spanish on a series of QA datasets. The\nauthor found out that the top performance\nwas achieved using RoBERTa first trained\non SQAC, then on the Spanish version of\nSQuAD (SQuAD-ES-v2) and finally on the\nQuALES corpus.\n7https://huggingface.co/PlanTL-GOB-\nES/roberta-large-bne\n8https://github.com/dccuchile/beto\n9https://huggingface.co/docs/transformers/main/\nen/model doc/roberta\n10https://github.com/google-\nresearch/bert/blob/master/multilingual.md\n11https://huggingface.co/mrm8488/distill-bert-\nbase-spanish-wwm-cased-finetuned-spa-squad2-es\nOverview of QuALES at IberLEF 2022: Question Answering Learning from Examples in Spanish\n277The participant alvarory (Rodrigo and\nPe\u02dc nas, 2022) tried three main approaches. In\nthe first one they fine-tuned RoBERTa for\nSpanish (base and large versions) and BETO\nfor 10 epochs on the QuALES training set\nwith datasets containing both training and\ndevelopment splits of the task, for a total of\n1,800 question-answer pairs. For the second\napproach they used even more data than the\navailable in QuALES, in order to study the\ntransferability among different datasets when\nusing two pretrained models: RoBERTa and\nmultilingual BERT. The third approach was\nbased on combining different models for re-\nturning a single output using two voting sche-\nmes.\nThe participant avacaondata (Vaca-\nSerrano, 2022) addresses extractive QA th-\nrough an ensemble system composed of three\nlarge pre-trained language models in Spanish:\nMarIA-base, MarIA-large and RigoBERTa.\nThese models were fine-tuned on data from\nthe Spanish version of SQuAD (SQUAD-ES-\nv2), a Spanish version of NewsQA, generated\nby the author, and QuALES. The best mo-\ndel is an ensemble that gives scores to each\nanswer based on multiple criteria such as the\nnumber of models that predict it and the mo-\ndels\u2019 scores. The final predictions were per-\nformed by aggregating the output of the re-\nsulting models, referred as a meta-ensemble.\nA number of ensemble strategies were tried,\nwhere finally Grouped Score Aggregation per-\nform best. This strategy consists on selecting\nthe answer by the count of each answer mul-\ntiplied by a scaling factor based on the vali-\ndation scores of the models.\nThe participant Bernardo fine-tuned Ro-\nBERTa for Spanish for 3 epochs using th est\u00b4 a\nhablando.e train subsets of SQAC, SQUAD-\nES-v2 and QuALES. ichramm performed ex-\nperiments using RoBERTa for Spanish pre-\ntrained with the SQAC corpus, and distill-\nBERT pretrained with SQUAD-ES-v2. His\nsubmitted outputs were calculated using the\nRoBERTa model, fine-tuned on QuALES.\nHe also experimented including the NewsQA\nversion for Spanish, obtaining lower results\n(one point less in each metric). The partici-\npant gberger also used the distill-BERT mo-\ndel.\nsebastianvolti ranked first in both me-\ntrics of the competition. He reached his top\nperformance using XML-RoBERTa, a multi-\nlingual model, pretrainend with SQUAD-ES-v2 and fine tuned using the QuALES corpus.\nHe also tested a model that included a fine\ntuning stage with 2,000 examples from the\nSpanish translation of the NewsQA corpus,\nprior to fine tuning with the QuALES cor-\npus, which yielded slightly lower results.\n5.2 Results\nWe show the best result for each user for\neach metric. Please notice that the best exact\nmatch and F1 scores might have been obtai-\nned in different submissions by the same user.\nTable 3 shows the best exact match scores\nfor each user:\nUser EM\nsebastianvolti 0.5349\nichramm 0.4677\nsmaximo 0.4598\nBernardo 0.4427\navacaondata 0.3992\ngberger 0.3715\nalvarory 0.3175\nTable 3: Results for the exact match metric.\nTable 4 shows the best F1 overlap scores\nfor each user.\nUser F1\nsebastianvolti 0.7282\nBernardo 0.6159\nsmaximo 0.6142\navacaondata 0.5877\nichramm 0.5581\ngberger 0.4500\nalvarory 0.4293\nTable 4: Results for the overlap F1 metric.\nAs can be seen in the tables, the best re-\nsults achieved (F1: 0.73 and EM: 0.53) are far\nfrom those reported on the SQuAD corpus for\nEnglish on the official SQuAD site (F1: 0.93\nand EM: 0.91). Our task differs from what\nis reported there in that the evaluation texts\nbelong to a specific domain (news about the\nCovid-19 pandemic), and also in the size of\nthe context provided to search for the ans-\nwers. In our case, the context is a complete\nnews article, which are longer than the con-\ntexts included in the SQuAD dataset.\nThe best results were obtained by\nsebastianvolti, whose best model is based\non RoBERTa pretrained on SQuAD 2.0, fine\ntuned on the QuALES corpus, and was the\nonly participant who used the multilingual\nAiala Ros\u00e1, Luis Chiruzzo, Luc\u00eda Bouza, Alina Dragonetti, Santiago Castro, Mathias Etcheverry, Santiago G\u00f3ngora, Santiago Goycoechea, \nJuan Machado, Guillermo Moncecchi, Juan Jos\u00e9 Prada, Dina Wonsever\n278version of RoBERTa, four other participants\nused RoBERTa for Spanish (trained on the\nBNE corpus).\nAlso note that none of the systems have\nreached the inter-annotator agreement levels,\nboth for EM and F1, although for F1 the best\nsubmission by sebastianvolti is the closest\nby around 5 %.\n6 Conclusions\nWe presented the results of the QuALES\ncompetition on Question Answering Learning\nfrom Examples in Spanish. Seven partici-\npants submitted systems to the competition,\nand the best systems achieved 0.53 in exact\nmatch and 0.73 in average F1 overlap.\nThe extractive Q&A task, although sho-\nwing very good results on the main available\nbenchmark, the SQuAD corpus, still presents\ngreat challenges when working with different\ndata and searching for answers in larger con-\ntexts.\nThe QuALES corpus, despite its rather\nsmall size, provided significant improvements\nin training, complementing other larger cor-\npora taken as a base, mainly SQuAD (Spa-\nnish version) and SQAC.\nReferences\nBaudi\u02c7 s, P. and J. \u02c7Sediv\u00b4 y. 2015. Mode-\nling of the question answering task in the\nyodaqa system. In J. Mothe, J. Savoy,\nJ. Kamps, K. Pinel-Sauvagnat, G. Jones,\nE. San Juan, L. Capellato, and N. Fe-\nrro, editors, Experimental IR Meets Multi-\nlinguality, Multimodality, and Interaction,\npages 222\u2013228, Cham. Springer Interna-\ntional Publishing.\nCarrino, C. P., M. R. Costa-juss` a, and J. A.\nFonollosa. 2019. Automatic spanish\ntranslation of the squad dataset for mul-\ntilingual question answering. arXiv pre-\nprint arXiv:1912.05200.\nCa\u02dc nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u00b4 erez. 2020. Spanish\npre-trained bert model and evaluation da-\nta.Pml4dc at iclr , 2020:1\u201310.\nDevlin, J., M.-W. Chang, K. Lee, and K. Tou-\ntanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for lan-\nguage understanding. arXiv preprint ar-\nXiv:1810.04805.Guti\u00b4 errez Fandi\u02dc no, A., J. Armengol Es-\ntap\u00b4 e, M. P` amies, J. Llop Palao, J. Silvei-\nra Ocampo, C. Pio Carrino, C. Armen-\ntano Oller, C. Rodriguez Penagos, A. Gon-\nzalez Agirre, and M. Villegas. 2022. Ma-\nria: Spanish language models. Procesa-\nmiento del Lenguaje Natural , 68.\nJoshi, M., E. Choi, D. S. Weld, and L. Zettle-\nmoyer. 2017. Triviaqa: A large scale\ndistantly supervised challenge dataset for\nreading comprehension.\nJurafsky, D. and J. H. Martin. 2021. Speech\nand language processing. 3rd edition draft.\nUS: Prentice Hall.\nKwiatkowski, T., J. Palomaki, O. Redfield,\nM. Collins, A. Parikh, C. Alberti, D. Eps-\ntein, I. Polosukhin, J. Devlin, K. Lee, et al.\n2019a. Natural questions: a benchmark\nfor question answering research. Transac-\ntions of the Association for Computational\nLinguistics, 7:453\u2013466.\nKwiatkowski, T., J. Palomaki, O. Redfield,\nM. Collins, A. Parikh, C. Alberti, D. Eps-\ntein, I. Polosukhin, M. Kelcey, J. Devlin,\nK. Lee, K. N. Toutanova, L. Jones, M.-\nW. Chang, A. Dai, J. Uszkoreit, Q. Le,\nand S. Petrov. 2019b. Natural questions:\na benchmark for question answering re-\nsearch. Transactions of the Association of\nComputational Linguistics .\nLan, Z., M. Chen, S. Goodman, K. Gimpel,\nP. Sharma, and R. Soricut. 2019. Albert:\nA lite bert for self-supervised learning of\nlanguage representations. arXiv preprint\narXiv:1909.11942.\nLe, Q. and T. Mikolov. 2014. Distribu-\nted representations of sentences and docu-\nments. In International conference on ma-\nchine learning , pages 1188\u20131196. PMLR.\nLeCun, Y., Y. Bengio, and G. Hinton. 2015.\nDeep learning. nature, 521(7553):436\u2013\n444.\nManning, C., P. Raghavan, and H. Sch\u00a8 utze.\n2010. Introduction to information re-\ntrieval. Natural Language Engineering,\n16(1):100\u2013103.\nMin, S., J. Boyd-Graber, C. Alberti, D. Chen,\nE. Choi, M. Collins, K. Guu, H. Hajishir-\nzi, K. Lee, J. Palomaki, et al. 2021. Neu-\nrips 2020 efficientqa competition: Systems,\nOverview of QuALES at IberLEF 2022: Question Answering Learning from Examples in Spanish\n279analyses and lessons learned. In Neu-\nrIPS 2020 Competition and Demonstra-\ntion Track, pages 86\u2013111. PMLR.\nMin, S., V. Zhong, R. Socher, and C. Xiong.\n2018. Efficient and robust question ans-\nwering from minimal context over docu-\nments. arXiv preprint arXiv:1805.08092.\nM\u00b4 aximo, S. 2022. Supervised domain adap-\ntation for extractive question answering in\nspanish.\nNakov, P., D. Hoogeveen, L. M` arquez,\nA. Moschitti, H. Mubarak, T. Baldwin,\nand K. Verspoor. 2019a. Semeval-2017\ntask 3: Community question answering.\narXiv preprint arXiv:1912.00730.\nNakov, P., L. M` arquez, W. Magdy, A. Mos-\nchitti, J. Glass, and B. Randeree. 2019b.\nSemeval-2015 task 3: Answer selection in\ncommunity question answering. arXiv\npreprint arXiv:1911.11403.\nPe\u02dc nas, A., P. Forner, \u00b4A. Rodrigo, R. Sutclif-\nfe, C. For\u02d8 ascu, and C. Mota. 2010. Over-\nview of respubliqa 2010: Question answe-\nring evaluation over european legislation.\nInCLEF.\nPe\u02dc nas, A., P. Forner, R. Sutcliffe, \u00b4A. Rodri-\ngo, C. For\u02d8 ascu, I. Alegria, D. Giampiccolo,\nN. Moreau, and P. Osenova. 2009. Over-\nview of respubliqa 2009: Question answe-\nring evaluation over european legislation.\nInWorkshop of the Cross-Language Eva-\nluation Forum for European Languages ,\npages 174\u2013196. Springer.\nRajpurkar, P., J. Zhang, K. Lopyrev, and\nP. Liang. 2016. Squad: 100,000+ ques-\ntions for machine comprehension of text.\narXiv preprint arXiv:1606.05250.\nReddy, S., D. Chen, and C. D. Manning.\n2019. Coqa: A conversational question\nanswering challenge. Transactions of the\nAssociation for Computational Linguis-\ntics, 7:249\u2013266.\nRodrigo, A. and A. Pe\u02dc nas. 2022.\nUned@quales 2022: Testing the per-\nformance of transformer-based language\nmodels for spanish question-answering.\nSeo, M., A. Kembhavi, A. Farhadi, and\nH. Hajishirzi. 2016. Bidirectional atten-\ntion flow for machine comprehension. ar-\nXiv preprint arXiv:1611.01603.Trischler, A., T. Wang, X. Yuan, J. Harris,\nA. Sordoni, P. Bachman, and K. Sule-\nman. 2016. Newsqa: A machine com-\nprehension dataset. arXiv preprint ar-\nXiv:1611.09830.\nVaca-Serrano, A. 2022. Adversarial ques-\ntion answering in spanish with transfor-\nmer models.\nVaswani, A., N. Shazeer, N. Parmar, J. Usz-\nkoreit, L. Jones, A. N. Gomez,  L. Kaiser,\nand I. Polosukhin. 2017. Attention is all\nyou need. Advances in neural information\nprocessing systems, 30.\nXiong, C., V. Zhong, and R. Socher. 2017.\nDcn+: Mixed objective and deep residual\ncoattention for question answering. arXiv\npreprint arXiv:1711.00106.\nYu, L., K. M. Hermann, P. Blunsom, and\nS. Pulman. 2014. Deep learning for ans-\nwer sentence selection. arXiv preprint ar-\nXiv:1412.1632.\nAiala Ros\u00e1, Luis Chiruzzo, Luc\u00eda Bouza, Alina Dragonetti, Santiago Castro, Mathias Etcheverry, Santiago G\u00f3ngora, Santiago Goycoechea, \nJuan Machado, Guillermo Moncecchi, Juan Jos\u00e9 Prada, Dina Wonsever\n280Overview of ReCoRES at IberLEF 2022: Reading\nComprehension and Reasoning Explanation for\nSpanish\nOverview de ReCoRES en IberLEF 2022: Comprensi\u00b4 on de\nLectura y Explicaci\u00b4 on de Razonamiento en Espa\u02dc nol\nMarco Antonio Sobrevilla Cabezudo1, Diego Diestra2, Rodrigo L\u00b4 opez2,\nErasmo G\u00b4 omez2, Arturo Oncevay3, Fernando Alva-Manchego4\n1University of S\u02dc ao Paulo\n2Department of Engineering, Pontificia Universidad Catolica del Per\u00b4 u\n3School of Informatics, University of Edinburgh\n4Cardiff University\nmsobrevillac@usp.br, {ddiestra, a20112387, hector.gomez}@pucp.pe,\na.oncevay@ed.ac.uk, alvamanchegof@cardiff.ac.uk\nAbstract: This paper presents the ReCoRES task, organized at IberLEF 2022,\nwithin the framework of the 38th edition of the International Conference of the\nSpanish Society for Natural Language Processing. The main goal of this shared-task\nis to promote the task of Reading Comprehension and Verbal Reasoning. This task\nis divided into two sub-tasks: (1) identifying the correct alternative in reading com-\nprehension questions and (2) generating the reasoning used to select an alternative.\nIn general, 3 teams participated in this event, mainly proposing transformer-based\nneural models in conjunction with additional strategies. The results of this event,\ninsights and some challenges are presented, opening a range of possibilities for future\nwork.\nKeywords: Reading Comprehension, Reasoning Explanation, Spanish.\nResumen: Este art\u00b4 \u0131culo presenta la tarea ReCoRES, organizada en IberLEF 2022,\nen el marco de la 38 edici\u00b4 on de la Conferencia Internacional de la Sociedad Espa\u02dc nola\npara el Procesamiento del Lenguaje Natural. El objetivo de esta tarea es promover\nla tarea de Comprensi\u00b4 on de Lectura y Razonamiento Verbal. Esta tarea es dividida\nen dos sub-tareas: (1) la identificaci\u00b4 on de la alternativa correcta en preguntas de\ncomprensi\u00b4 on de lectura y (2) la generaci\u00b4 on del razonamiento usado para seleccionar\nuna alternativa. En general, 3 equipos participaron de este evento proponiendo\nmayormente modelos neuronales basados en transformers con algunas estrategias\nadicionales. Los resultados de este evento as\u00b4 \u0131 como aprendizajes y algunos desaf\u00b4 \u0131os\nson presentados, abriendo un abanico de posibilidades como trabajos futuros.\nPalabras clave: Comprensi\u00b4 on de Lectura, Explicaci\u00b4 on del Razonamiento, Espa\u02dc nol.\n1 Introduction\nQuestion Answering (QA) consists of return-\ning an accurate and short answer given a Nat-\nural Language question. According to Rogers\net al. (2020), QA can be approached from\ntwo main perspectives: Open QA, in which\nresponses are recovered from several sources\nsuch as Web pages and knowledge bases, and\nReading Comprehension (RC), where the an-\nswer is recovered from a single document.\nRC datasets are classified into three cat-\negories according to their answer type: (1)span-selection datasets, where the text ex-\nplicitly includes the answer, (2) multiple-\nchoice datasets, where systems have to se-\nlect an answer from a list of candidates; and\n(3) freeform answers dataset, where answers\nare written in freeform. Most RC datasets\nare in the first category, with the most pop-\nular being SQuAD (Rajpurkar et al., 2016).\nAn explicit limitation of these span-selection\ndatasets is that they can only target infor-\nmation explicitly mentioned in the text and\noften get solved with shallow lexical match-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 281-287\nrecibido 05-07-2022 revisado 18-07-2022 aceptado 21-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-25\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturaling (Rogers et al., 2020).\nUsing a multiple-choice dataset is a com-\nmon and realistic way to measure read-\ning comprehension in humans (Echegoyen,\n\u00b4Alvaro Rodrigo, and Pe\u02dc nas, 2020). In ad-\ndition, Rogers et al. (2020) point out that\nmultiple-choice is a better format to assess\nlanguage understanding of automatic sys-\ntems. It is because it requires a high degree\nof textual inference and the development of\nstrategies for selecting the correct answer.\nFor English, there are diverse Multiple-\nChoice QA datasets, such as RACE (Lai\net al., 2017), Entrance Exams (Pe\u02dc nas et\nal., 2011) and QuAIL (Rogers et al., 2020).\nHowever, that is not the case for most lan-\nguages. For Spanish, in particular, there are\ntwo QA datasets available: SQuAD-es (Car-\nrino, Costa-juss` a, and Fonollosa, 2020) and\nEntrance Exams (EE) (Pe\u02dc nas et al., 2011).\nHowever, these datasets present some limita-\ntions originated by the nature of the dataset\nor some aspects like the size. For example,\nSQuAD-es is a span-based QA dataset, i.e.,\nthe answers are included in the text explic-\nitly. In the case of EE, it is a multiple-\nchoice QA dataset in which, even though\nquestions demand a certain level of reasoning,\nthe dataset size is quite small (43 texts and\n191 questions), constraining the exploration\nof current State-of-the-Art approaches.\nIn order to contribute to the development\nof research in Question-Answering/Reading\nComprehension for Spanish, this shared-task\naims to:\n\u2022Introduce a new and more extensive\nmultiple-choice QA dataset for Spanish\nbased on university entrance examina-\ntions, where questions aim to evaluate\nhumans instead of computers and in-\nclude extra information about the rea-\nsoning used to choose an alternative.\n\u2022Evaluate multiple-choice question an-\nswering, and reasoning generation ap-\nproaches on this dataset.\n2 Task Description\nThis shared-task consists of two sub-tasks:\n\u2022Sub-task 1 - Machine Reading Compre-\nhension: given a text, a question, and a\nset of candidate answers, a system must\nselect the correct answer.\u2022Sub-task 2 - Reasoning Explanation:\ngiven a text and a question, a system\nmust generate an explanation for its an-\nswer selection\n3 Dataset\nThe dataset used in this shared-task was ex-\ntracted from actual university entrance ex-\naminations provided by Peruvian institutions\nthat train students for entrance examina-\ntions and includes diverse topics and ques-\ntion types that require a certain level of rea-\nsoning. Source documents that compose the\ndataset were initially available in PDF for-\nmat. This way, we built the dataset by ap-\nplying two strategies: (1) using an OCR to\nconvert the PDF documents to TXT format\nand then manually correcting them to fix pos-\nsible OCR problems, and (2) transcripting\nthe PDF files. Eight collaborators and two\norganization committee members performed\nmanual revision and transcription.\nThe whole dataset comprises 439 texts,\nand 1,822 questions with 2-7 candidate an-\nswers each, divided into training, develop-\nment, and test sets with 257 texts (1,073\nquestions), 91 texts (363 questions), and 91\ntexts (386 questions), respectively. Addition-\nally, each question-answer pair instance in-\ncludes a short explanation as reasoning sup-\nport for choosing a candidate answer1.\nFigure 1 shows an example of a long text,\na question with five alternatives, and the\ncorresponding reasoning. It is worth noting\nthat texts are long, and questions are not de-\nscribed most typically -using question mark-\ners and wh-questions; instead, these are de-\nscribed as a sentence that needs to be com-\npleted.\n4 Experimental Setup\n4.1 Baseline\nWe use two baselines for sub-task 1. The\nfirst consists of randomly choosing an answer\namong the alternatives for each question, and\nthe second is a BERT-based baseline2, simi-\nlar to the one used by Rogers et al. (2020). It\nworks this way: for each answer option, the\ncontext, question, and choice are joined and\n1The dataset is available at\nhttps://github.com/ddiestra/mrc-dataset.\n2We use the BERT model available at\nhttps://huggingface.co/dccuchile/bert-base-spanish-\nwwm-cased.\nMarco Antonio Sobrevilla Cabezudo, Diego Diestra, Rodrigo L\u00f3pez, Erasmo G\u00f3mez, Arturo Oncevay, Fernando Alva-Manchego\n282T ext: \u201cEl trabajo es en primer t\u00e9rmino un proceso entre la naturaleza y el hombre, proceso en que este realiza, regula y \ncontrola mediante su propia acci\u00f3n su intercambio de materias con la naturaleza. En este proceso, el hombre se enfrenta \ncomo un poder natural con la materia de la naturaleza. Pone en acci\u00f3n las fuerzas naturales que forman su corporeidad, los \nbrazos y las piernas, la cabeza y la mano, para de ese modo asimilarse, bajo una forma \u00fatil para su propia vida, las materias \nque la naturaleza le brinda. Y a la par que de ese modo act\u00faa sobre la naturaleza exterior a \u00e9l y la transforma, transforma su \npropia naturaleza, desarrollando las potencias que dormitan en \u00e9l y sometiendo el juego de sus fuerzas a su propia \ndisciplina. Aqu\u00ed no vamos a ocuparnos de las primeras formas de trabajo, formas instintivas y de tipo animal. Aqu\u00ed, partimos \ndel supuesto del trabajo plasmado ya bajo una forma en la que pertenece exclusivamente al hombre. Una ara\u00f1a ejecuta \noperaciones que semejan a las manipulaciones del tejedor, y la construcci\u00f3n de los panales de las abejas podr\u00eda avergonzar, \npor su perfecci\u00f3n, a m\u00e1s de un maestro de obras. Pero, hay algo en que el peor maestro de obras aventaja, desde luego, a \nla mejor abeja, y es el hecho de que, antes de ejecutar la construcci\u00f3n, la proyecta en su cerebro. Al final del proceso de \ntrabajo, brota un resultado que antes de comenzar el proceso exist\u00eda ya en la mente del obrero; es decir, un resultado que \nten\u00eda ya existencia ideal. El obrero no se limita a hacer cambiar de forma la materia que le brinda la naturaleza, sino que, al \nmismo tiempo, realiza en ella su fin, fin que \u00e9l sabe que rige como una ley las modalidades de su actuaci\u00f3n y al que tiene \nnecesariamente que supeditar su voluntad. Y esta supeditaci\u00f3n no constituye un acto aislado. Mientras permanezca \ntrabajando, adem\u00e1s de esforzar los \u00f3rganos que trabajan, el obrero ha de aportar esa voluntad consciente del fin a que \nllamamos atenci\u00f3n, atenci\u00f3n que deber\u00e1 ser tanto m\u00e1s reconcentrada cuanto menos atractivo sea el trabajo, por su car\u00e1cter \no por su ejecuci\u00f3n, para quien lo realiza, es decir, cuanto menos disfrute de \u00e9l el obrero como de un juego de sus fuerzas \nf\u00edsicas y espirituales.\u201d\nQuestion:  Medularmente, el autor intenta dilucidar:\nAlternatives:\nA. las diferencias entre lo instintivo y lo planificado.\nB. la naturaleza del trabajo exclusivamente humano.\nC. el car\u00e1cter pernicioso del trabajo en la actualidad.\nD. la supremac\u00eda de la naturaleza frente a la humanidad.\nE. las etapas que componen el proceso productivo.\nAnswer:  B\nReason:  El autor busca caracterizar el trabajo humano frente a lo instintivo, se\u00f1ala as\u00ed que el trabajo humano est\u00e1 \nsupeditado a un fin.Figure 1: ReCoRES\u2019s Example.\nused as input, and the output is its probabil-\nity, and the most likely option is selected as\nthe answer. Among the settings, we train the\nmodel for 1 epoch and use a learning rate of\n3e-5 with Adam optimizer.\nThe baseline for sub-task 2 is a T5-based\none that receives the text and the question as\ninputs and returns the reason3. In addition,\nwe evaluate a two-stage approach. Firstly, we\nselect the two most important sentences4for\na specific question according to cosine simi-\nlarity.5. Then we train a T5-based model,\nsimilar to the first baseline. The parameters\n3We use the T5 model available at\nhttps://huggingface.co/flax-community/spanish-\nt5-small.\n4We used the two most important sentences be-\ncause it produced the best results in the development\nset.\n5This strategy is inspired by query-based auto-\nmatic summarization (Hovy, 2005).used were: input length and output length o\n512 and 100 tokens, respectively, a learning\nrate of 0.003 with Adafactor optimizer, and\na batch size of 8 with gradient accumulation\nof 4 steps. Besides, we freeze the embedding\nlayer. Finally, we select the model with the\nbest perplexity in the development set after\n7 epochs. During prediction, we use a beam\nsize of 5.\n4.2 Evaluation\nSub-task 1 is evaluated in two ways. Firstly,\nwe will evaluate the accuracy, i.e., the num-\nber of correct answers in relation to the total\nnumber of questions. The second measure\nis c@1 (Pe\u02dc nas and Rodrigo, 2011), used at\nCLEF (Rodrigo et al., 2015). c@1 is a con-\nservative metric that penalizes incorrect an-\nswers, encouraging systems to not choose an\nanswer unless they are certain.\nSub-task 2 is evaluated in two ways as\nOverview of ReCoRES at IberLEF 2022: Reading Comprehension and Reasoning Explanation for Spanish\n283well. The first one will consist of running au-\ntomatic semantic metrics BERTScore (Zhang\net al., 2020) to measure the similarity be-\ntween the generated explanation and its man-\nual reference. We will use this metric instead\nof classical BLEU (Papineni et al., 2002), or\nMETEOR (Banerjee and Lavie, 2005) be-\ncause \u201creasons\u201d can be open and diverse.\nThe second one is a manual evaluation of\nthree quality criteria:\n\u2022Accuracy, to measure how accurate is\nthe output system in relation to the orig-\ninal output;\n\u2022Fluency (Howcroft et al., 2020), that\nmeasures the degree to which a text\n\u201cflows well\u00a8 and is not e.g. a sequence of\nunconnected parts.\n\u2022Readability (Howcroft et al., 2020), that\nmeasures if the output system is under-\nstandable or easy to read.\nTo perform the manual evaluation, we re-\ncruit some crowdworkers. In particular, these\nwere undergraduate students who had expe-\nrience in this task (Reading Comprehension).\nThe crowdworkers were guided to rate each\ncriteria using an interval of 1-5, being 1 the\nworst and 5 the best.\n5 Participants\nIn this edition, 3 teams registered on the task\nand submitted results. However, two of them\npresented working notes describing their sys-\ntems. The following is a brief summary of\nthe final proposals submitted:\n5.1 MRCPUCP\nThis team only participated in the sub-task\n16. They proposed a BERT-based approach\nin which all text, alternatives, and reason-\ning are concatenated and used as input, and\nthe output is one of the alternatives. They\nused BETO as BERT-based model for Span-\nish (similar to the baseline) and finetune the\nmodel on the dataset they built.\n5.2 SADA (Baggetto et al., 2022)\nThis team only participated in the sub-task\n1. The authors explore using encoder models,\ngenerative models, clue generation systems,\nand dataset expansion. In experiments, the\n6This team did not present working notes for the\npresent shared-task.Sub-task 1\nTeam Accuracy\nc@1\nBaseline (Random) 0.2514\n0.2514\nBaseline (BERT) 0.1917 0.1917\nBaseline (BERT) + Threshold 0.0492 0.0896\nVersae\n& Nandezgarcia 0.4067 0.4067\nSADA 0.7254 0.7254\nMRCPUCP 0.7591 0.7591\nSub-task 2\nTeam BER\nTScore\nBaseline T5 0.6579\nT5\n- 2 sentences 0.6652\nVersae\n& Nandezgarcia 0.6867\nTable 1: Results of Automatic Evaluation.\nbest model was a pre-trained multilingual T5\nmodel finetuned on an expanded multilingual\ndataset.\n5.3 Versae & Nandezgarcia (De la\nRosa and Fern\u00b4 andez, 2022)\nThis team participated in both sub-tasks.\nThe authors tested several methods for clas-\nsic fine-tuning of encoder-only language mod-\nels for the task of reading comprehension and\na zero-shot approach for reasoning explana-\ntion using a decoder-only model.\n6 Results and Discussion\nTable 1 presents the results for the sub-task\n1 and sub-task 2. For sub-task 1 (Reading\nComprehension), the best performance was\nobtained by the MRCPUCP team, and the\nSADA team obtained the second-best one,\nonly 3 points lower than the first one.\nIt is worth noting that all teams used pre-\ntrained models such as BERT (Devlin et al.,\n2019) or T5 (Raffel et al., 2020) in conjunc-\ntion with some additional strategies. Among\nthe strategies, we can highlight the use of\nreasoning as part of the input and its help-\nfulness in getting the correct answers in the\nreading comprehension task. On the other\nhand, multilingual information has proven to\nbe helpful, even when the domains are differ-\nent.\nConcerning sub-task 2, the best perfor-\nmance in the automatic evaluation was ob-\ntained by the work of Versae & Nandezgarcia,\nbeing almost 2 points higher than the strong\nT5-based baseline. It is worth noting that the\nwinning proposal used a zero-shot approach,\ni.e., no training data of this task was used for\nlearning to generate the reasoning.\nDue to input texts in our dataset being\nlong, we wonder how much do text length\ninfluence the performance? To verify it, we\nMarco Antonio Sobrevilla Cabezudo, Diego Diestra, Rodrigo L\u00f3pez, Erasmo G\u00f3mez, Arturo Oncevay, Fernando Alva-Manchego\n284divide the test set in text subsets according to\nits length, as shown in the X-axis in Figures 2\nand 3.\nFigure 2 shows how the accuracy changes\naccording to the text length for all proposals\n(baseline is the BERT-based one). We can\nnote that, as was expected, the performance\ndecreases when the texts are longer, except\nfor the cases where the length is higher than\n500 tokens. This result is suspicious as most\nproposals were BERT-based models. Thus,\nthe maximum length was defined as 512 to-\nkens. However, the proposal of SADA uses\na T5-based model that can deal with these\nlengths. In the case of the longest texts (be-\ntween 850 and 900 tokens), we must note\nthat the performance was almost 0.25 be-\ncause the models usually chose an alternative\nby chance, and it was correct for all questions\nthat had the same alternative as correct.\nLength0,00000,25000,50000,75001,0000\n0-50\n50-100100-150 150-200 200-250 250-300 300-350 350-400 400-450 450-500 500-550 550-600 600-650 650-700 700-750 750-800 800-850 850-900baseline versae_fernandez sada mrcpucp\nFigure 2: Analysis of the performance on the\nMachine Comprehension task according dif-\nferent text lengths.\nFigure 3 shows how BERTScore changes\naccording to the text length for all proposals.\nWe note that even when the values obtained\nby Versae & Nandezgarcia are a bit higher\nin all subsets, these are almost the same (a\nbit higher than 0.60). These results can sug-\ngest that models can deal with different text\nlengths in the same way or that metric is not\ngood enough to determine what is the best\nproposal. However, a deeper study is neces-\nsary to determine the actual reason for get-\nting these results.\nFinally, Table 2 presents the human evalu-\nation results. It shows that fluency and read-\nability achieve similar scores (almost 4) for all\nproposals, being a bit better for the proposal\nof Versae & Nandezgarcia. This is expected\nas all models are based on big language mod-\nLength0,00000,20000,40000,60000,8000\n0-50\n50-100100-150 150-200 200-250 250-300 300-350 350-400 400-450 450-500 500-550 550-600baseline baseline + sum versae_fernandezFigure 3: Analysis of the performance on the\nReasoning Explanation task according differ-\nent text lengths.\nAccuracy Fluency\nReadability\nBaseline T5 1.08\u00b1\n0.40 4.04 \u00b11.16 3.95 \u00b11.26\nT5 - 2 sentences 1.20 \u00b10.51 4.08 \u00b11.07 3.93 \u00b11.22\nVersae\n& Nandezgarcia 2.35 \u00b11.39 4.33 \u00b10.90 4.33 \u00b10.92\nTable 2: Human Evaluation. Accuracy, Flu-\nency and Readability were rated in an inter-\nval of 1-5. Results are shown in terms of\nmean\u00b1standard deviation.\nels that can usually generate fluent and read-\nable texts. In the case of accuracy, we can see\nthat the proposal of Versae & Nandezgarcia\nobtained the best results. However, results\nare still lower than 3, proving that this task\nis harder and the automatic evaluation met-\nric could not be suitable.\n7 Conclusion\nWe presented the first edition of the\nReCoRES task at IberLEF, including two\nsub-tasks: reading comprehension and rea-\nsoning explanation.\nIn general, three teams participated in this\nshared-task: three for sub-task 1 and one for\nsub-task 2. However, only two teams sent\ntheir working notes. All proposals were based\non pre-trained language models with some\nadditional strategies.\nOverall, the winner of sub-task 1 was the\nMRCPUCP team, and the winner of sub-task\n2 was the Versae-Nandezgarcia team. About\nthe results, some interesting findings about\nthe helpfulness of incorporating reasoning in-\nformation and multilingual datasets in the\nreading comprehension task and the need to\nuse more suitable metrics and other strategies\nto deal with the reasoning explanation task\nas this one has proven to be complicated.\nAs future work, we plan to extend the cur-\nOverview of ReCoRES at IberLEF 2022: Reading Comprehension and Reasoning Explanation for Spanish\n285rent corpus for both sub-tasks and annotate\ndifferent question types according to the tax-\nonomy proposed by Rogers et al. (2020) to\nverify what are the actual abilities of pre-\ntrained language models. Besides, we plan to\nannotate text segments that explain the rea-\nsoning to build an extractive reasoning expla-\nnation dataset instead of an abstractive one\nlike the one used in this shared-task.\nAcknowledgements\nThe authors acknowledge the support of the\nundergraduate students at the department of\nSoftware Engineering at the Universidad Na-\ncional Mayor de San Marcos in the manual\nevaluation.\nReferences\nBaggetto, P., S. Ramos, J. Garc\u00b4 \u0131a, and J. R.\nNavarro. 2022. Study on text comprehen-\nsion and MCQA in spanish. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2022), A Coru\u02dc na, Spain.\nCEUR Workshop Proceedings.\nBanerjee, S. and A. Lavie. 2005. ME-\nTEOR: An automatic metric for MT eval-\nuation with improved correlation with hu-\nman judgments. In Proceedings of the\nACL Workshop on Intrinsic and Extrinsic\nEvaluation Measures for Machine Trans-\nlation and/or Summarization, pages 65\u2013\n72, Ann Arbor, Michigan, June. Associa-\ntion for Computational Linguistics.\nCarrino, C. P., M. R. Costa-juss` a, and\nJ. A. R. Fonollosa. 2020. Automatic\nSpanish translation of SQuAD dataset for\nmulti-lingual question answering. In Pro-\nceedings of the 12th Language Resources\nand Evaluation Conference , pages 5515\u2013\n5523, Marseille, France, May. European\nLanguage Resources Association.\nDe la Rosa, J. and A. Fern\u00b4 andez. 2022. Zero-\nshot Reading Comprehension and Rea-\nsoning for Spanish with BERTIN GPT-\nJ-6B. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2022) ,\nA Coru\u02dc na, Spain. CEUR Workshop Pro-\nceedings.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. In Proceedings ofthe 2019 Conference of the North Amer-\nican Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers) , pages 4171\u20134186, Minneapolis,\nMinnesota, June. Association for Compu-\ntational Linguistics.\nEchegoyen, G., \u00b4Alvaro Rodrigo, and\nA. Pe\u02dc nas. 2020. Cross-lingual training\nfor multiple-choice question answering.\nProcesamiento del Lenguaje Natural ,\n65(0):37\u201344.\nHovy, E. 2005. Text summarisation. The\nOxford Handbook of computational lin-\nguistics, pages 583\u2013598.\nHowcroft, D. M., A. Belz, M.-A. Clinciu,\nD. Gkatzia, S. A. Hasan, S. Mahamood,\nS. Mille, E. van Miltenburg, S. San-\nthanam, and V. Rieser. 2020. Twenty\nyears of confusion in human evaluation:\nNLG needs evaluation sheets and stan-\ndardised definitions. In Proceedings of the\n13th International Conference on Natu-\nral Language Generation , pages 169\u2013182,\nDublin, Ireland, December. Association\nfor Computational Linguistics.\nLai, G., Q. Xie, H. Liu, Y. Yang, and\nE. Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examina-\ntions. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natu-\nral Language Processing , pages 785\u2013794,\nCopenhagen, Denmark, September. Asso-\nciation for Computational Linguistics.\nPapineni, K., S. Roukos, T. Ward, and W.-\nJ. Zhu. 2002. Bleu: a method for auto-\nmatic evaluation of machine translation.\nInProceedings of the 40th Annual Meet-\ning of the Association for Computational\nLinguistics, pages 311\u2013318, Philadelphia,\nPennsylvania, USA, July. Association for\nComputational Linguistics.\nPe\u02dc nas, A., E. H. Hovy, P. Forner, \u00b4A. Ro-\ndrigo, R. F. E. Sutcliffe, C. Forascu,\nand C. Sporleder. 2011. Overview of\nQA4MRE at CLEF 2011: Question an-\nswering for machine reading evaluation.\nIn V. Petras, P. Forner, and P. D. Clough,\neditors, CLEF 2011 Labs and Work-\nshop, Notebook Papers, 19-22 September\n2011, Amsterdam, The Netherlands, vol-\nume 1177 of CEUR Workshop Proceed-\nings. CEUR-WS.org.\nMarco Antonio Sobrevilla Cabezudo, Diego Diestra, Rodrigo L\u00f3pez, Erasmo G\u00f3mez, Arturo Oncevay, Fernando Alva-Manchego\n286Pe\u02dc nas, A. and A. Rodrigo. 2011. A sim-\nple measure to assess non-response. In\nProceedings of the 49th Annual Meeting\nof the Association for Computational Lin-\nguistics: Human Language Technologies ,\npages 1415\u20131424, Portland, Oregon, USA,\nJune. Association for Computational Lin-\nguistics.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li,\nand P. J. Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-\nto-text transformer. Journal of Machine\nLearning Research , 21(140):1\u201367.\nRajpurkar, P., J. Zhang, K. Lopyrev, and\nP. Liang. 2016. SQuAD: 100,000+\nquestions for machine comprehension of\ntext. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natu-\nral Language Processing , pages 2383\u20132392,\nAustin, Texas, November. Association for\nComputational Linguistics.\nRodrigo, \u00b4A., A. Pe\u02dc nas, Y. Miyao, E. H.\nHovy, and N. Kando. 2015. Overview\nof CLEF QA entrance exams task 2015.\nIn L. Cappellato, N. Ferro, G. J. F.\nJones, and E. SanJuan, editors, Work-\ning Notes of CLEF 2015 - Conference and\nLabs of the Evaluation forum, Toulouse,\nFrance, September 8-11, 2015, volume\n1391 of CEUR Workshop Proceedings.\nCEUR-WS.org.\nRogers, A., O. Kovaleva, M. Downey, and\nA. Rumshisky. 2020. Getting closer to\nai complete question answering: A set\nof prerequisite real tasks. Proceedings of\nthe AAAI Conference on Artificial Intel-\nligence , 34(05):8722\u20138731, Apr.\nZhang, T., V. Kishore, F. Wu, K. Q. Wein-\nberger, and Y. Artzi. 2020. Bertscore:\nEvaluating text generation with bert.\nInInternational Conference on Learning\nRepresentations.\nOverview of ReCoRES at IberLEF 2022: Reading Comprehension and Reasoning Explanation for Spanish\n287288Overview of Rest-Mex at IberLEF 2022:Recommendation System, Sentiment Analysis and\nCovid Semaphore Prediction for Mexican Tourist\nTexts\nResumen de la tarea Rest-Mex en IberLEF 2022: Sistema de\nRecomendaci\u00b4 on, An\u00b4 alisis de Sentimiento y Predicci\u00b4 on de\nSem\u00b4 aforo Covid para Textos Tur\u00b4 \u0131sticos Mexicanos\nMiguel \u00b4A.\u00b4Alvarez-Carmona1,2,\u00b4Angel D\u00b4 \u0131az-Pacheco1, Ram\u00b4 on Aranda1,2,\nAnsel Y. Rodr\u00b4 \u0131guez-Gonz\u00b4 alez1,2,Daniel Fajardo-Delgado3,\nRafael Guerrero-Rodr\u00b4 \u0131guez4,L\u00b4 azaro Bustio-Mart\u00b4 \u0131nez5\n1Centro de Investigaci\u00b4 on Cient\u00b4 \u0131fica y de Educaci\u00b4 on Superior de Ensenada\n2Consejo Nacional de Ciencia y Tecnolog\u00b4 \u0131a\n3Tecnol\u00b4 ogico Nacional de M\u00b4 exico Campus Ciudad Guzm\u00b4 an\n4Universidad de Guanajuato\n5Universidad Iberoamericana, Ciudad de M\u00b4 exico\n{malvarez, diazpacheco, aranda, ansel }@cicese.edu.mx\ndaniel.fd@cdguzman.tecnm.mx, r.guerrero-rodriguez@ugto.mx,\nlazaro.bustio@ibero.mx\nAbstract: This paper presents the framework and results from the Rest-Mex task\nat IberLEF 2022. This task considered three tracks: Recommendation System,\nSentiment Analysis and Covid Semaphore Prediction, using texts from Mexican\ntouristic places. The Recommendation System task consists in predicting the degree\nof satisfaction that a tourist may have when recommending a destination of Nayarit,\nMexico, based on places visited by the tourists and their opinions. On the other\nhand, the Sentiment Analysis task predicts the polarity of an opinion issued and the\nattraction by a tourist who traveled to the most representative places in Mexico. We\nhave built corpora for both tasks considering Spanish opinions from the TripAdvisor\nwebsite. As a novelty, the Covid Semaphore Prediction task aims to predict the color\nof the Mexican Semaphore for each state, according to the Covid news in the state,\nusing data from the Mexican Ministry of Health. This paper compares and discusses\nthe participants\u2019 results for all three tacks.\nKeywords: Rest-Mex 2022, Sentiment Analysis, Covid Prediction, Mexican Tourist\nText.\nResumen: Este art\u00b4 \u0131culo presenta el marco y los resultados de la tarea Rest-Mex\nen IberLEF 2022. Esta tarea consider\u00b4 o tres sub tareas: Sistema de recomendaci\u00b4 on,\nAn\u00b4 alisis de sentimiento y Predicci\u00b4 on de sem\u00b4 aforo Covid, utilizando textos de lugares\ntur\u00b4 \u0131sticos mexicanos. La tarea del Sistema de Recomendaci\u00b4 on consiste en predecir el\ngrado de satisfacci\u00b4 on que puede tener un turista al recomendar un destino de Nayarit,\nM\u00b4 exico, con base en los lugares visitados por los turistas y sus opiniones. Por otro\nlado, la tarea de An\u00b4 alisis de Sentimiento predice la polaridad de una opini\u00b4 on emitida\ny la atracci\u00b4 on por parte de un turista que viaj\u00b4 o a los lugares m\u00b4 as representativos\nde M\u00b4 exico. Hemos construido corpus para ambas tareas teniendo en cuenta las\nopiniones en espa\u02dc nol de TripAdvisor. Como novedad, la tarea de Predicci\u00b4 on de\nSem\u00b4 aforo Covid tiene como objetivo predecir el color del Sem\u00b4 aforo Mexicano para\ncada estado, de acuerdo a las noticias Covid en el estado, utilizando datos de la\nSecretar\u00b4 \u0131a de Salud de M\u00b4 exico. Este documento compara y discute los resultados de\nlos participantes para las tres sub tareas.\nPalabras clave: Rest-Mex 2022, An\u00b4 alisis de sentimientos, Predicci\u00b4 on de covid,\nTextos Tur\u00b4 \u0131sticos Mexicanos.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 69, septiembre de 2022, pp. 289-299\nrecibido 05-07-2022 revisado 18-07-2022 aceptado 21-07-2022\nISSN 1135-5948. DOI 10.26342/2022-69-26\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introduction\nTourism is a social, cultural, and economic\nphenomenon related to people\u2019s movement to\nplaces outside their usual place of residence\nfor personal or business/professional reasons\n(Guerrero-Rodriguez et al., 2021). This ac-\ntivity is vital in various countries, includ-\ning Mexico ( \u00b4Alvarez-Carmona et al., 2022)1,\nwhere tourism represents 8.7% of the national\nGDP, generating around 4.5 million direct\njobs (Arce-Cardenas et al., 2021).\nIn 2021, Rest-Mex emerged, which is an\nevaluation forum ( \u00b4Alvarez-Carmona et al.,\n2021). This forum is the first that seeks to\nspecialize in text analysis from tourism to\nprovide solutions to different tasks for Mexi-\ncan Spanish. In its 2021 edition, the Rest-\nMex proposed two different tasks. Analy-\nsis of recommendation systems and sentiment\nanalysis. For both tasks, data was collected\nfrom the TripAdvisor site.\nFor this Rest-Mex edition, we proposed\nthree sub-tasks: Recommendation System,\nSentiment Analysis on Mexican tourist texts,\nand as a novelty, the task of Determining the\ncolor of the Mexican Covid-19 epidemiologi-\ncal semaphore is added.\nFor this purpose, 3 data sets have been\nbuilt. We collected 2,263 instances from\n2,011 users who visited 18 touristic places\nin Nayarit, Mexico, for the recommendation\nsystem task. For the sentiment analysis task,\nthe data is labeled to determine the polarity\nand origin of each opinion. For this, 43,150\nopinions were collected from various tourist\nspots in Mexico. Finally, for determining\nthe epidemiologic semaphore, 131,471 news\nitems referring to covid were collected for all\nthe states of the Mexican Republic, grouped\ninto2,656 weeks.\nThe remainder of this paper is organized\nas follows: Section 2 describes this forum\u2019s\ncollection-building process and the evalua-\ntion metrics. Section 3 summarizes the solu-\ntions submitted for the tasks and shows the\nresults obtained by the participants\u2019 systems\nand the analysis. Finally, Section 4 presents\nthe conclusions obtained by this evaluation\nforum.\n1Mexico is in the world\u2019s top ten and the second\nIberoamerican country related to the arrival of inter-\nnational tourists.2 Evaluation framework\nThis section outlines the construction of the\nthree used corpora, highlighting particular\nproperties, challenges, and novelties. It also\npresents the evaluation measures used for the\ntasks.\n2.1 Recommendation System\ncorpus\nThe first subtask consists of a classification\ntask where the participating system can pre-\ndict the degree of satisfaction a tourist may\nhave when recommending a destination.\nThe collection consists of 2,263 in-\nstances with 2,011 tourists and 18 touris-\ntic places from Nayarit, Mexico. This col-\nlection was obtained from the tourists who\nshared their satisfaction on TripAdvisor be-\ntween 2010 and 2020. Each class of satisfac-\ntion is an integer between [1, 5], where {1:\nVery bad, 2: Bad, 3: Neutral, 4: Good,5:\nVery good }. Each instance consists of two\nparts:\n1.User information :\n\u2022Gender: The tourist\u2019s gender.\n\u2022Place: The tourist place that the\ntourist recommends a visit.\n\u2022Location: The place of origin of\nthe tourist (the central, northeast,\nnorthwest, west, and southeast re-\ngions refer to the regions of Mexico).\n\u2022Date: Date when the recommenda-\ntion was issued.\n\u2022Type: Type of trip that the tourist\nwould do. The type would be\nin [Family, Friends, Alone, Couple,\nBusiness]\n\u2022History: The history of the places\nthe tourist has visited and his/her\nopinions on each of these places.\n2.Place information: A brief text de-\nscription of the place and a series of rep-\nresentative characteristics of the place as\na type of tourism that can be done there\n(adventure, beach, relaxation, among\nothers.), If it is a family atmosphere, pri-\nvate or public, it is free or paid, among\nothers.\nWe use a 70/30 partition to divide into\ntrain and test. This means that we used\nMiguel \u00c1. \u00c1lvarez-Carmona, \u00c1ngel D\u00edaz-Pacheco, Ram\u00f3n Aranda, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez, Daniel Fajardo-Delgado, \nRafael Guerrero-Rodr\u00edguez, L\u00e1zaro Bustio-Mart\u00ednez\n290Class Train instances Test instances\n1 45 20\n2 53 24\n3 167 72\n4 457 196\n5 860 369\n\u03a3 1582 681\nTable 1: Instances distribution for the rec-\nommendation system task.\n1,582 labeled instances for the training par-\ntition while we used 681 unlabeled instances\nfor the test partition.\nTable 1 shows the distribution of the in-\nstances for the recommendation system task\nfor the train and test partitions.\nThe class imbalance is clear since class 5\nrepresents around 50 % of the total instances,\nmaking this task very difficult.\nFormally the problem of this task is de-\nfined as:\n\u201cGiven a TripAdvisor tourist and a Mexi-\ncan tourist place, the goal is to automatically\nobtain the degree of satisfaction (between 1\nand 5) the tourist may have when visiting\nthat place.\u201d\n2.2 Sentiment Analysis corpus\nThe second subtask is a classification task\nwhere the participating system can predict\nthe polarity and the tourist attraction of an\nopinion issued by a tourist who traveled to\nthe representative Mexican places. This col-\nlection was obtained from the tourists who\nshared their opinion on TripAdvisor between\n2002 and 2021. Each opinion\u2019s polarity is an\ninteger between [1, 5], where {1: Very bad,\n2: Bad, 3: Neutral, 4: Good,5: Very good }.\nAlso, the participants must determine the\nattractiveness of the opinion being issued.\nThe possible classes are Attractive, Hotel and\nRestaurant.\nThe corpus consists of 43,150 opinions\nshared by tourists. Like the recommenda-\ntion task, we use a 70/30 partition to di-\nvide into train and test. This means that\nwe used 30,212 labeled instances for the train\npartition, while we used 12,938 unlabeled in-\nstances for the test partition.\nTable 2 shows the distribution of the in-\nstances for the sentiment analysis task for the\ntrain and test partitions for polarity and at-\ntraction.\nAs with the recommendation system sub-\ntask, the class imbalance is clear since class 5Pol Attr\nClass Train Test Class Train Test\n1 547 256 Attractive 5197 2216\n2 730 315 Hotel 16565 7100\n3 2121 884 Restaurant 8450 3622\n4 5878 2423 -\n5 20936 9060 -\n\u03a3 30212 12938 - 30212 12938\nTable 2: Instances distribution for polarity\nand attraction traits on sentiment analysis\ntask.\nand the class Hotel represents around 50 % of\nthe total instances, making this a task with\na significant degree of difficulty too.\nFormally the problem of this task is de-\nfined as:\n\u201cGiven an opinion about a Mexican\ntourist place, the goal is to determine the po-\nlarity, between 1 and 5, of the text and the\nvisited attraction, which could be an attrac-\ntion, a hotel, or a restaurant.\u201d\n2.3 Covid Semaphore Prediction\nThe last subtask is a classification task where\nthe participating system can predict the fu-\nture of the covid semaphore through the\nnews. This collection was obtained from\nnews websites that published reports regard-\ning covid from June 2020 to December 2021.\nFor this task, 131,471 news items referring\nto covid were collected for all the states of\nthe Mexican Republic, grouped into 2,656\nweeks. Like the previous tasks, a 70/30\npartition was made for training and testing.\nTherefore, 94,540 news items distributed in\n1912 weeks were selected for the training cor-\npus. The test corpus consists of 36,931 news\nitems distributed over 744 weeks.\nEach week or instance consists of 4 labels.\nThese labels correspond to the semaphore\ncolor of the instance after fweeks in the\nfuture. The possible colors to detect are:\nred, orange, yellow, and green, where red\nis the color that places the most restric-\ntions on public activities and green is the\ncolor that corresponds to the best possi-\nble situation. The participants must pre-\ndict the color of the semaphore for the weeks\nf={0,2,4,8}. For more information re-\ngarding the covid semaphore, you can con-\nsult (Alvarez-Carmona and Aranda, 2022),\n(\u00b4Alvarez-Carmona et al., 2022b).\nTable 3 shows the distribution of the in-\nstances for eaxh fvalue.\nOverview of Rest-Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid Semaphore Prediction for Mexican \nTourist Texts\n291f= 0 f= 2 f= 4 f= 8\nClass Train Test Train Test Train Test Train Test\nRed 248 87 201 71 179 63 139 42\nOrange 680 273 680 275 673 261 655 252\nYellow 545 216 554 221 568 227 615 232\nGreen 439 168 477 177 492 193 503 218\n\u03a3 1912 744 1912 744 1912 744 1912 744\nTable 3: Instances distribution for semaphore prediction.\nLike the other tasks, for this corpus, it\ncan be seen that the red class is the minority,\nwhich could be the most crucial class to pre-\ndict, so this task has considerable complexity\nto solve.\nFormally the problem of this task is de-\nfined as:\n\u201cGiven the news set for a week fin a state\nof the Mexican Republic x, each system must\nreturn the color of the covid epidemiological\nsemaphore for weeks f,f+2,f+4, and f+8\nfor the xstate.\u201d\n2.4 Performance measures\nSystems are evaluated using standard eval-\nuation metrics, including accuracy and F-\nmeasure, but MAE (mean absolute error) will\nrank the submissions for the recommendation\nsystem task. MAE are defined as equation 1.\nMAE Sx=1\nnnX\ni=1|T(i)\u2212Sx(i)| (1)\nWhere Sxis a participating system x,T(i)\nis the result of the instance iaccording to the\nGround Truth, and Sx(i) is the output of the\nparticipant system x, for instance, i. Finally,\nnis the number of instances in the collection.\nWe proposed a measure to evaluate the\nsentiment analysis task for this edition. This\nmeasure is defined as shown in the equation\n2.\nmeasure S=1\n1+MAE p+FA\n2(2)\nWhere FAis the average among the micro\nF-measure for each class (hotel, restaurant,\nand attractive), and MAE evaluates the po-\nlarity.\nFinally, for evaluating the semaphore task,\nwe proposed a measure that gives more\nweight to well-ranked coming weeks to ob-\ntain a final result. This measure is defined in\nthe equation 3.measure C=Fw0+ 2\u2217Fw2+ 4\u2217Fw4+ 8\u2217Fw8\n15\n(3)\nFinally, it is essential to mention that the\nchosen baseline is the majority class for the\nthree tasks.\n3 Overview of the Submitted\nApproaches\nThis section presents the results obtained by\nthe participants for the different tasks.\n3.1 Recommendation system\noverview\nFor this study, three teams have submitted\ntheir solutions for the recommendation sys-\ntem task.\nThe authors of (Callejas-Hern\u00b4 andez et\nal., 2022) noted that using simpler repre-\nsentations (BoW) independent of the lan-\nguage is well suited for the recommen-\ndation task. A similar simple approach\nis also applied in (Morales-Murillo, Pinto-\nAvenda\u02dc no, and Rojas L\u00b4 opez, 2022). Fi-\nnally, In (Veigas-Ram\u00b4 \u0131rez, Mart\u00b4 \u0131nez-Davies,\nand Segura-Bedmar, 2022), a Bert represen-\ntation is proposed.\nTable 4 shows a summary of the re-\nsults obtained by each team for the rec-\nommendation system task. The MAE was\nused to rank participants. The approach of\nthe GPI-CIMAT team (Callejas-Hern\u00b4 andez\net al., 2022) obtained the best performance.\nSurprisingly, the simple approach overcomes\nthe Bert-based approach. This would be the\nresult of the small relativity database. Fi-\nnally, it was expected that the F-measure of\nthe baseline would not have good results; this\nis evident since all the experiments surpassed\nthe baseline in this metric, although again,\nthe result exceeded the baseline by 0.05.\nAlso, Table 4 shows the result of the team\nthat obtained the best result in last year\u2019s\nedition. Since this is the only task of this\nMiguel \u00c1. \u00c1lvarez-Carmona, \u00c1ngel D\u00edaz-Pacheco, Ram\u00f3n Aranda, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez, Daniel Fajardo-Delgado, \nRafael Guerrero-Rodr\u00edguez, L\u00e1zaro Bustio-Mart\u00ednez\n292edition that is identical to that of the pre-\nvious year, it is possible to make this com-\nparison. It is possible to see that no team\ncould beat the Alumni-MCE 2GEN team of\nthe 2021 edition (Arreola et al., 2021).\nTable 5 shows the best F-measure results\nby class in the recommendation task. These\nresults show that the minority classes (1 and\n2) were not well represented, which is why the\nbest result of the 2021 edition is shown. From\nclass 3, it is possible to see that a different\nteam obtains a good result. It is possible to\nobserve that the GPI CIMAT team obtains\nthe best result for class 5, which explains its\nbetter MAE result.\nSomething remarkable is that all the sys-\ntems exceeded the baseline (BL).\n3.1.1 Perfect assemble for the\nrecommendation system task\nTo analyze the complementarity of the pre-\ndictions by the participants\u2019 systems, we\nbuilt a theoretically perfect ensemble (PA)\nfrom their runs, as calculated in (Arag\u00b4 on et\nal., 2019). We considered that a test in-\nstance was correctly classified if at least one\nof the participating teams classified it cor-\nrectly. Also, it is proposed to combine the\nparticipating systems to create a representa-\ntion based on the outputs of each system.\nFor this, they implemented a deep learning\n(DL) architecture like the one proposed in\n(\u00b4Alvarez-Carmona et al., 2022a).\nFrom these results, it is possible to ob-\nserve that the perfect ensemble performance\nis considerably better than the participants\u2019\napproaches, suggesting that the participants\u2019\nsystems complement each other. This phe-\nnomenon has already appeared in this type\nof task and is known as The Phenomenon of\nCompleteness over Mexican Text Classifica-\ntion ( \u00b4Alvarez-Carmona et al., 2022a).\nFigure 1 shows the number of instances\ncorrectly classified by ssystems. That is,\nwhen s= 0, all the instances that were not\nclassified well by any system are shown, while\nwhen s= 9, they are the instances that were\nclassified well by all systems. The base 2\nlogarithm was applied to the number of in-\nstances to observe the graph better. The sys-\ntems of this and the previous edition were\ntaken for this exercise.\n3.2 Sentiment analysis overview\nFor this study, 13 teams have submitted their\nsolutions and descriptions for the sentiment\nFigure 1: Instances that were correctly classi-\nfied by the different numbers of possible sys-\ntems for recommendation.\nanalysis task.\nFor this edition, the transformers-based\nrepresentation completely dominate the first\nplaces for the sentiment analysis task. The\nUMU team (Garc\u00b4 \u0131a-D\u00b4 \u0131az et al., 2022),\nUC3M (P\u00b4 erez Enr\u00b4 \u0131quez, Alonso-Menc\u00b4 \u0131a,\nand Segura-Bedmar, 2022), CIMAT MTY\nGTO (G\u00b4 omez-Espinosa, Mu\u02dc niz Sanchez,\nand L\u00b4 opez-Monroy, 2022), MCE (Mendoza,\nRamos-Zavaleta, and Rodr\u00b4 \u0131guez, 2022), GPI\nCIMAT (Callejas-Hern\u00b4 andez et al., 2022),\nCIMAT 2020 (Santib\u00b4 a\u02dc nez Cort\u00b4 es et al.,\n2022), DCI UG (Barco, Rodr\u00b4 \u0131guez Rivera,\nand Hern\u00b4 andez-Far\u00b4 \u0131as, 2022) and UCI-UC-\nCUJAE (Toledano-L\u00b4 opez et al., 2022) im-\nplemented solutions, mainly based on Bert.\nIt is possible to observe that these types of\nmethodologies are the ones that obtain the\nbest results since the lowest result, of what\ntransformers applied, is 0.84 when the best\nresult is 0.89, that is, the results are very\nclose to each other.\nOn the other hand, the rest of the\nworks proposed more straightforward meth-\nods. ESCOM-IPN-LCD Team (Alcibar-\nZubillaga et al., 2022) proposes a Logistic\nRegression classifier to train two models, one\nfor polarity prediction and the other for the\nattraction type prediction. UPTC-UDLAP\nTeam (Rico-Sulayes and Monsalve-Pulido,\n2022) applies the Naive Bayes Multinomial\nalgorithm to represent a supervised classifi-\ncation approach. The team uses unigrams,\nbigrams, and trigrams as features. The un-\nsupervised classification was carried out by\ncomputing the total polarity of the opinions\nin an intensity spectrum according to the\nscale of the data using context embeddings.\nThe SENA Team (Jurado-Buch, Bustio-\nMart\u00b4 \u0131nez, and \u00b4Alvarez-Carmona, 2022) uses\nOverview of Rest-Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid Semaphore Prediction for Mexican \nTourist Texts\n293Rank Country Institute Team MAE Accuracy F-measure\nPA - - - 0.28 86.58 0.66\nDL - - - 0.31 76.45 0.52\n2021 Mex CIMAT Alumni-MCE 2GEN Run 10.31 77.28 0.50\n1st Mex CIMAT GPI-CIMAT 0.69 52.12 0.19\n2nd Mex BUAP LKEBUAP Run k1\u221213\u2212k 2\u221218 0.70 46.64 0.22\n- Mex BUAP LKEBUAP Run k1\u221214\u2212k 2\u221218 0.70 45.88 0.21\n- Mex/Che CIMAT GPI-CIMAT Run 1 0.72 53.66 0.17\n3rd Esp UC3M UC3M-DEEPNLP Run 1 0.72 48.89 0.22\nBL Majority Class 0.74 53.30 0.13\n- Esp UC3M UC3M-DEEPNLP Run 2 0.75 52.71 0.13\nTable 4: Performance for the Recommendation System task.\nF-measure class Best result Team\n1 0.32 Alumni-MCE 2GEN Run 1(2021)\n2 0.24 Alumni-MCE 2GEN Run 1(2021)\n3 0.14 UC3M-DEEPNLP Run 1\n4 0.37 LKEBUAP Run k1\u221213\u2212k 2\u221218\n5 0.69 GPI-CIMAT\nTable 5: Performance for the Recommendation System task per class.\na representation based on Topics extracted\nby LDA and classified with simple Deep\nLearning architecture. Finally, DevsEx-\nMachina (Rivas- \u00b4Alvarez et al., 2022) pro-\nposes to extract all the terms in each class\nfrom one to four words (1...4-grams) as po-\nlarity characteristics. Also, they perform a\nchain of translations of the opinions, from\nSpanish to other languages and back to Span-\nish, to obtain meanings and synonymous\nterms.\nIt is interesting that despite being more\nstraightforward, some of the results of the\nproposals that are not based on Transform-\ners obtain close values. This seems ideal for\nenvironments with limited memory, time, or\ndata.\nTable 6 shows a summary of the results\nobtained by each team for the sentiment anal-\nysis task2. The UMU team obtained the best\nresult, although the difference with UC3M is\n0.002. Due to the closeness of the results,\nit is possible that there is no statistical sig-\nnificance between all the methods based on\ntransformers.\nTable 7 shows the best F-measure results\nby class in the sentiment analysis task. Inter-\nestingly, the UMU team does not get the best\nresults for any polarity class. However, it is\nthe best team for all three attraction classes.\nThe DCI UG team obtains the best results\nfor classes 1 and 2, which are the most diffi-\n2For systems with *, the authors did not send the\nsystem\u2019s description.cult to classify due to their clear imbalance.\nUC3M obtains the best result for class 3, and\nfinally, MCE, in its two attempts, obtains the\nbest results for the majority of classes.\n3.2.1 Perfect assemble for the\nsentiment analysis task\nAs in the section 3.1.1, the complementarity\nof the systems was analyzed for the sentiment\nanalysis task. We calculated the perfect as-\nsemble.\nTable 6 also shows the perfect assemble\n(PA) result and the Deep Learning combina-\ntion systems (DL).\nAs in the recommendation task, it is pos-\nsible to observe that the perfect ensemble\nperformance is considerably better than the\nUMU approach, suggesting that the partic-\nipants\u2019 systems are complementary to each\nother again, with an error result very close\nto zero. The DL approach improves the best\nresult obtained by the UMU team.\nFigure 2 shows the number of instances\ncorrectly classified by ssystems similar to the\nFigure 1. The color green is the polarity in-\nstances, whereas the color yellow represents\nthe attraction instances.\n3.2.2 Interesting opinions\nPA approach got only six incorrect instances\nfor the attractiveness detection task. The\npattern of these instances is tourists talking\nabout a hotel restaurant or vice versa, which\nconfuses all systems. For example:\nEste lugar era estupendo. Un mont\u00b4 on de\nMiguel \u00c1. \u00c1lvarez-Carmona, \u00c1ngel D\u00edaz-Pacheco, Ram\u00f3n Aranda, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez, Daniel Fajardo-Delgado, \nRafael Guerrero-Rodr\u00edguez, L\u00e1zaro Bustio-Mart\u00ednez\n294Rank Country Institute Team Measure SMAE P FA\nPA - - - 0.98 0.03 0.99\nDL - - - 0.91 0.19 0.99\n1st Esp UMU UMU-Team Run 1 0.89 0.25 0.99\n2nd Esp UC3M UC3M Run 1 0.89 0.26 0.98\n3rd Mex CIMAT CIMAT MTY-GTO Run 1 0.88 0.26 0.98\nHM Mex ITESM MCE-Team Run 2 0.88 0.26 0.98\n- Mex ITESM MCE-Team Run 1 0.88 0.26 0.98\n- Esp UMU UMU-Team Run 2 0.88 0.27 0.98\nHM Mex/Che CIMAT GPI-CIMAT Run 1 0.88 0.26 0.98\nHM Mex CIMAT CIMAT2020 Beto Run10.88 0.27 0.97\nHM Mex INAOE DCI-UG Run 1 0.87 0.26 0.96\nHM Cub/Bel UCI UCI-UC-CUJAE Run 2 0.87 0.30 0.97\n- Cub/Bel UCI UCI-UC-CUJAE Run 1 0.86 0.30 0.97\n- Mex CIMAT CIMAT2020 Run 2 0.86 0.31 0.97\n- Mex INAOE DCI-UG Run 2 0.86 0.30 0.96\nHM Mex IPN ESCOM-IPN-IIA* Run 2 0.85 0.32 0.96\n- Mex/Che CIMAT GPI-CIMAT Run 1 0.84 0.28 0.91\nHM Mex IPN ESCOM-IPN-LCD Run 2 0.84 0.35 0.94\n- Mex IPN ESCOM-IPN-IIA* Run 1 0.83 0.34 0.92\nHM Mex/Col UDLAP UPTC-UDLAP Run 1 0.82 0.44 0.96\nHM Col/Mex SENA SENA Team 0.80 0.47 0.92\nHM Mex UAEM DevsExMachina Run 2 0.70 0.63 0.79\n- Mex UAEM DevsExMachina Run 1 0.66 0.97 0.82\n- Mex IPN ESCOM-IPN-LCD Run 1 0.59 0.85 0.65\n- Mex/Col UDLAP UPTC-UDLAP Run 2 0.54 0.54 0.43\nBL - - Majority class 0.45 0.47 0.23\nTable 6: Performance for the Sentiment Analysis task.\nF-measure class Best result Team\n1 0.61 DCI-UG Run 1\n2 0.37 DCI-UG Run 1\n3 0.50 UC3M]\n4 0.48 MCE-Team Run 1\n5 0.88 MCE-Team Run 2\nAttractive 0.99 UMU-Team Run 1\nHotel 0.99 UMU-Team Run 1\nRestaurant 0.98 UMU-Team Run 1\nTable 7: Performance for the Sentiment Analysis task per class.\nopciones y gran comida fresca. El desayuno\nbuffet era grandes mucha fruta fresca.\nThis instance is a hotel; however, the opin-\nion refers to the food.\nAnother example is:\nEl hotel est\u00b4 a incre\u00b4 \u0131ble, pero resalt\u00b4 o el ex-\ncelente servicio en insu sky bar, muchas gra-\ncias al capit\u00b4 an Iv\u00b4 an, y a su staff Heriberto,\nGabriel, Luis, Isidoro y Hugo, las bebidas\nde Gerardo y Alexis incre\u00b4 \u0131bles y la cocina\nun placer ! En el \u00b4 area de alberca al se\u02dc nor\nWenceslao! Muchas gracias por todo !\nWhich, although it is inside a hotel, is a\nrestaurant.\nNone of these instances have the attractive\nlabel.3.3 Semaphore covid prediction\nresults\nFor this last task, six systems were received\nfrom 4 teams.\nMCE team(Ramos-Zavaleta and\nRodr\u00b4 \u0131guez, 2022) presents an approach\nbased on features extracted directly from\nthe news and the other applying transfer\nlearning. First, they propose a system based\non CorEx topics, and as a second attempt,\nthey propose a system based on Bert.\nArandanito team (Carmona-S\u00b4 anchez,\nCarmona, and \u00b4Alvarez-Carmona, 2022) pro-\nposes a method based on topic extraction.\nThis topic-based representation is applied to\na series of linear regressions, which serve as\ninput for simple deep learning architecture.\nOverview of Rest-Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid Semaphore Prediction for Mexican \nTourist Texts\n295Figure 2: Instances that were correctly classi-\nfied by the different numbers of possible sys-\ntems for sentiment analysis.\nThe last Team (Romero-Cant\u00b4 on et al.,\n2022) proposes an approach based on weigh-\ning representative words as features extracted\ndirectly from the text news. Those words\nwere weighed by using the Mutual informa-\ntion (MI) measure.\nTable 8 shows the results of the partici-\npating teams. It can be seen that both MCE\nand Arandanito have a very close results to\neach other. Curiously, both approaches are\ntopic-based.\nIt can be seen that the best results are ob-\ntained for week 2 in the future. That is, two\nweeks after the news was published. How-\never, the results of weeks 4 and 8, considering\nthe imbalance and that there are four classes,\nare competitive.\nLike the other tasks, all the participants\nmanaged to pass the Baseline (BL).\nTable 9 shows the best results for each\nclass for each evaluated week.\nFor week 0, it is possible to see that MCE\nobtains all the best results. However, from\nweek 2, it can be seen that Arandanito ob-\ntains the best result for the Red class. This\nis the most challenging class because it is the\nminority class. For all other classes, MCE\ngets the best result.\n3.3.1 Perfect assemble for the\nsemaphore prediction task\nTable 8 also shows the perfect assembly and\nthe combination of the systems, like the other\ntwo tasks.\nThe perfect ensemble is much higher than\nthe best of the individual results, which in-\ndicates that these systems also complement\neach other.\nOn the other hand, the combination of the\nFigure 3: Instances that were correctly classi-\nfied by the different numbers of possible sys-\ntems for semaphore prediction.\ntwo systems once again enhances the individ-\nual best value.\nFigure 3 shows the number of instances\ncorrectly classified by ssystems similar to the\nFigure 1. The color green is Week 0, yellow\nfor 2, blue for 4, and red for 8.\nFor more details of the results of both\ntasks, it is possible to go to the following web\npage: https://sites.google.com/cicese.\nedu.mx/rest-mex-2022/results.\n4 Conclusions\nThis paper described the design and re-\nsults of the Rest-Mex shared task collocated\nwith IberLef 2022. Rest-Mex stands for\nRecommendation system, Sentiment analy-\nsis and covid semaphore prediction in Span-\nish tourists text for Mexican places. For the\nthree tasks, 18 teams participated. Mainly,\nthe members of these teams come from in-\nstitutes in countries such as Mexico, Spain,\nCuba, Colombia, Belgium, and Switzerland.\nThirty-five different systems were received to\nbe evaluated to solve each of the three tasks\nproposed in the Rest-Mex 2021.\nFor the recommendation task, a tourist\u2019s\nsatisfaction with a recommendation of a\ntourist place for the state of Nayarit in Mex-\nico was evaluated. The best MAE result\nobtained was that of (Callejas-Hern\u00b4 andez et\nal., 2022), which belongs to the CIMAT of\nMexico. This team proposed a simple sys-\ntem based on BoW. Although all the partic-\nipating systems outperformed the Baseline,\nno system was able to obtain better results\nthan the 2021 edition. This result indicates\nthat this task still has many challenges to be\nsolved.\nMiguel \u00c1. \u00c1lvarez-Carmona, \u00c1ngel D\u00edaz-Pacheco, Ram\u00f3n Aranda, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez, Daniel Fajardo-Delgado, \nRafael Guerrero-Rodr\u00edguez, L\u00e1zaro Bustio-Mart\u00ednez\n296Rank Country Institute Team Measure CFw0Fw2Fw4Fw8\nPA - - - 0.84 0.92 0.88 0.87 0.81\nDL - - - 0.67 0.74 0.76 0.71 0.63\n1st Mex ITESM MCE-Team Run 2 0.49 0.56 0.52 0.46 0.48\n2nd Mex BUAP Arandanito 0.48 0.33 0.56 0.51 0.46\n- Mex ITESM MCE-Team Run 1 0.32 0.33 0.34 0.32 0.32\n- Mex UNAM *ML-Team Run 2 0.24 0.25 0.27 0.23 0.24\nMex UNAM *ML-Team Run 1 0.22 0.20 0.22 0.22 0.23\nHM Mex UAN The Last 0.17 0.18 0.18 0.18 0.16\nBL Majority Class 0.12 0.13 0.13 0.12 0.12\nTable 8: Performance for the semaphore prediction task.\nF-measure class Best result Team F-measure class Best result Team\nRedw0 0.38 MCE-Team Run 2 Redw2 0.37 Arandanito\nOrange w0 0.66 MCE-Team Run 2 Orange w2 0.68 MCE-Team Run 2\nYellow w0 0.45 MCE-Team Run 2 Yellow w2 0.52 MCE-Team Run 2\nGreen w0 0.73 MCE-Team Run 2 Green w2 0.74 MCE-Team Run 2\nRedw4 0.39 Arandanito Redw8 0.2 Arandanito\nOrange w4 0.66 MCE-Team Run 2 Orange w8 0.65 MCE-Team Run 2\nYellow w4 0.47 MCE-Team Run 2 Yellow w8 0.55 MCE-Team Run 2\nGreen w4 0.71 MCE-Team Run 2 Green w8 0.73 MCE-Team Run 2\nTable 9: Performance for the Semaphore Prediction task per class.\nThe sentiment analysis task aimed to iden-\ntify the polarity and precedence of an opinion\nmade about a Mexican tourist destination.\nThe polarity was evaluated with MAE while\nthe origin with F-Measure. The team that\ngot the best performance was (Garc\u00b4 \u0131a-D\u00b4 \u0131az\net al., 2022). This team represents the Uni-\nversity of Murcia in Spain. They proposed\na method based on Bert. Other teams that\nalso implemented Bert obtained results very\nclose to first place. This is further evidence\nof the importance of transformers in textual\nclassification tasks. Also, the results indicate\nthat distinguishing between opinions of ho-\ntels, restaurants, and attractions is a task\nthat can have very high results, close to 100\n%.\nThe task of determining the semaphore\ncovid was a novelty introduced for this year\u2019s\nedition. Based on the news regarding covid,\nthis task consists of determining the color\nof the epidemiological semaphore for weeks\n0, 2, 4, and 8 in the future based on the\nnews publications. The best result obtained\nfor this task can be seen in (Ramos-Zavaleta\nand Rodr\u00b4 \u0131guez, 2022). This team comes from\nITESM of Mexico. Their solution is based\nmainly on extracting topics, although they\nobtains his best result with Bert. Best re-\nsults are achieved when ranked 2 weeks into\nthe future; however, results for 4 weeks alsoseem competitive. The results at 8 weeks suf-\nfer a drop in the classification.\nFinally, it is shown that there is signif-\nicant complementarity between the partici-\npating systems. In other evaluation forums,\nattempts have been made to mix the par-\nticipating systems to obtain better results\n(\u00b4Alvarez-Carmona et al., 2018), taking the\nproposal to use a simple deep learning archi-\ntecture ( \u00b4Alvarez-Carmona et al., 2022a), it\nwas possible to improve the best results of the\nthree tasks. However, the perfect theoretical\nensemble is still above the results obtained.\nAcknowledgements\nOur special thanks go to all of Rest-Mex\u2019s\nparticipants, the organizers, and their insti-\ntutions.\nReferences\nAlcibar-Zubillaga, J., Y. De-Luna Ocampo,\nI. Pacheco-Castillo, K. Ramirez-\nMendez, J. P. M. Sainz-Takata, and\nO. Ju\u00b4 arez Gambino. 2022. Participa-\ntion of escom\u2019s data science group at\nrest-mex 2022: Sentiment analysis task.\nInProceedings of the Third Workshop\nfor Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR WS Proceedings.\n\u00b4Alvarez-Carmona, M. \u00b4A., R. Aranda,\nS. Arce-Cardenas, D. Fajardo-Delgado,\nOverview of Rest-Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid Semaphore Prediction for Mexican \nTourist Texts\n297R. Guerrero-Rodr\u00b4 \u0131guez, A. P. L\u00b4 opez-\nMonroy, J. Mart\u00b4 \u0131nez-Miranda, H. P\u00b4 erez-\nEspinosa, and A. Y. Rodr\u00b4 \u0131guez-Gonz\u00b4 alez.\n2021. Overview of rest-mex at iber-\nlef 2021: recommendation system for\ntext mexican tourism. Procesamiento del\nLenguaje Natural .\n\u00b4Alvarez-Carmona, M. \u00b4A., R. Aranda,\nR. Guerrero-Rodr\u00b4 \u0131guez, A. Y. Rodr\u00b4 \u0131guez-\nGonz\u00b4 alez, and A. P. L\u00b4 opez-Monroy.\n2022a. A combination of sentiment anal-\nysis systems for the study of online travel\nreviews: Many heads are better than one.\nComputaci\u00b4 on y Sistemas, 26(2).\n\u00b4Alvarez-Carmona, M. A., R. Aranda, A. Y.\nRodr\u00b4 \u0131guez-Gonz\u00b4 alez, L. Pellegrin, and\nH. Carlos. 2022b. Classifying the mexican\nepidemiological semaphore colour from\nthe covid-19 text spanish news. Journal\nof Information Science .\n\u00b4Alvarez-Carmona, M. \u00b4A., E. Guzm\u00b4 an-Falc\u00b4 on,\nM. Montes-y G\u00b4 omez, H. J. Escalante,\nL. Villasenor-Pineda, V. Reyes-Meza, and\nA. Rico-Sulayes. 2018. Overview of mex-\na3t at ibereval 2018: Authorship and ag-\ngressiveness analysis in mexican spanish\ntweets. In Notebook papers of 3rd se-\npln workshop on evaluation of human lan-\nguage technologies for iberian languages\n(ibereval), seville, spain, volume 6.\n\u00b4Alvarez-Carmona, M. \u00b4A., E. Villatoro-Tello,\nL. Villase\u02dc nor-Pineda, and M. Montes-y\nG\u00b4 omez. 2022. Classifying the social me-\ndia author profile through a multimodal\nrepresentation. In Intelligent Technolo-\ngies: Concepts, Applications, and Future\nDirections. Springer, pages 57\u201381.\nAlvarez-Carmona, M. \u00b4A. and R. Aranda.\n2022. Determinaci\u00b4 on autom\u00b4 atica del color\ndel sem\u00b4 aforo mexicano del covid-19 a par-\ntir de las noticias.\nArag\u00b4 on, M. E., M. A. \u00b4Alvarez-Carmona,\nM. Montes-y G\u00b4 omez, H. J. Escalante,\nL. V. Pineda, and D. Moctezuma. 2019.\nOverview of mex-a3t at iberlef 2019: Au-\nthorship and aggressiveness analysis in\nmexican spanish tweets. In IberLEF@ SE-\nPLN, pages 478\u2013494.\nArce-Cardenas, S., D. Fajardo-Delgado,\nM. \u00b4A.\u00b4Alvarez-Carmona, and J. P.\nRam\u00b4 \u0131rez-Silva. 2021. A tourist recom-\nmendation system: a study case in mex-ico. In Mexican International Conference\non Artificial Intelligence, pages 184\u2013195.\nSpringer.\nArreola, J., L. Garcia, J. Ramos-Zavaleta,\nand A. Rodr\u00b4 \u0131guez. 2021. An embeddings\nbased recommendation system for mexi-\ncan tourism. submission to the rest-mex\nshared task at iberlef 2021. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR WS Proceedings.\nBarco, G. M., G. E. Rodr\u00b4 \u0131guez Rivera, and\nD.-I. Hern\u00b4 andez-Far\u00b4 \u0131as. 2022. Sentiment\nanalysis in spanish reviews: Dataket sub-\nmission on rest-mex 2022. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2022), CEUR WS Proceedings.\nCallejas-Hern\u00b4 andez, C., E. Rivadeneira-\nP\u00b4 erez, F. S\u00b4 anchez-Vega, A. P. L\u00b4 opez-\nMonroy, and E. Villatoro-Tello. 2022.\nThe winning approach for the recom-\nmendation systems shared task @rest mex\n2022. In Proceedings of the Third Work-\nshop for Iberian Languages Evaluation Fo-\nrum (IberLEF 2022), CEUR WS Proceed-\nings.\nCarmona-S\u00b4 anchez, G., A. Carmona, and\nM. A. \u00b4Alvarez-Carmona. 2022. Com-\nbining linear regressions to determine the\nfuture of the covid in mexico from the\nnews. In Proceedings of the Third Work-\nshop for Iberian Languages Evaluation Fo-\nrum (IberLEF 2022), CEUR WS Proceed-\nings.\nGarc\u00b4 \u0131a-D\u00b4 \u0131az, J. A., M. A. Rodr\u00b4 \u0131guez-Garc\u00b4 \u0131a,\nF. Garc\u00b4 \u0131a-S\u00b4 anchez, and R. Valencia-\nGarc\u00b4 \u0131a. 2022. Umuteam at rest-mex\n2022: Polarity prediction using knowl-\nedge integration of linguistic features and\nsentence embeddings based on transform-\ners. In Proceedings of the Third Workshop\nfor Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR WS Proceedings.\nG\u00b4 omez-Espinosa, V., V. Mu\u02dc niz Sanchez, and\nA. P. L\u00b4 opez-Monroy. 2022. Automl and\nensemble transformers for sentiment anal-\nysis in mexican tourism texts. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2022), CEUR WS Proceedings.\nMiguel \u00c1. \u00c1lvarez-Carmona, \u00c1ngel D\u00edaz-Pacheco, Ram\u00f3n Aranda, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez, Daniel Fajardo-Delgado, \nRafael Guerrero-Rodr\u00edguez, L\u00e1zaro Bustio-Mart\u00ednez\n298Guerrero-Rodriguez, R., M. \u00b4A.\u00b4Alvarez-\nCarmona, R. Aranda, and A. P. L\u00b4 opez-\nMonroy. 2021. Studying online travel\nreviews related to tourist attractions us-\ning nlp methods: the case of guanajuato,\nmexico. Current issues in tourism, pages\n1\u201316.\nJurado-Buch, J. D., L. Bustio-Mart\u00b4 \u0131nez, and\nM. A. \u00b4Alvarez-Carmona. 2022. The\nrole of the topics for the sentiment anal-\nysis task on a mexican tourist collec-\ntion. In Proceedings of the Third Work-\nshop for Iberian Languages Evaluation Fo-\nrum (IberLEF 2022), CEUR WS Proceed-\nings.\nMendoza, D., J. Ramos-Zavaleta, and\nA. Rodr\u00b4 \u0131guez. 2022. A transfer learn-\ning model for polarity in touristic reviews\nin spanish from tripadvisor. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2022), CEUR WS Proceedings.\nMorales-Murillo, V. G., D. Pinto-Avenda\u02dc no,\nand F. Rojas L\u00b4 opez. 2022. A hybrid rec-\nommender model based on information re-\ntrieval for mexican tourism text in rest-\nmex 2022. In Proceedings of the Third\nWorkshop for Iberian Languages Evalua-\ntion Forum (IberLEF 2022), CEUR WS\nProceedings.\nP\u00b4 erez Enr\u00b4 \u0131quez, M., J. Alonso-Menc\u00b4 \u0131a, and\nI. Segura-Bedmar. 2022. Transformers\napproach for sentiment analysis: Classi-\nfication of mexican tourists reviews from\ntripadvisor. In Proceedings of the Third\nWorkshop for Iberian Languages Evalua-\ntion Forum (IberLEF 2022), CEUR WS\nProceedings.\nRamos-Zavaleta, J. and A. Rodr\u00b4 \u0131guez. 2022.\nA mexico\u2019s covid traffic light color pre-\ndiction system based on mexican news.\nInProceedings of the Third Workshop\nfor Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR WS Proceedings.\nRico-Sulayes, A. and J. Monsalve-Pulido.\n2022. A proposal and comparison of\nsupervised and unsupervised classifica-\ntion techniques for sentiment analysis in\ntourism data. In Proceedings of the Third\nWorkshop for Iberian Languages Evalua-\ntion Forum (IberLEF 2022), CEUR WS\nProceedings.Rivas- \u00b4Alvarez, J. C., R. A. Garc\u00b4 \u0131a-\nHern\u00b4 andez, S. I. Medina-Mart\u00b4 \u0131nez, A. M.\nMart\u00b4 \u0131nez-Ortiz, N. Hern\u00b4 andez-Casta\u02dc neda,\nJ. E. Ruiz-Melo, A. Hern\u00b4 andez-\nCasta\u02dc neda, and Y. Nikolaevna-Ledeneva.\n2022. Devs-ex-machina at rest-mex 2022\nopinion mining of the mexican tourism\nsector through sets of normalized n-grams.\nInProceedings of the Third Workshop\nfor Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR WS Proceedings.\nRomero-Cant\u00b4 on, A., A. Diaz-Pacheco,\nR. Aranda, and P. Ram\u00b4 \u0131rez-Silva. 2022.\nMexican epidemiological semaphore color\nprediction by means of mutual informa-\ntion features. In Proceedings of the Third\nWorkshop for Iberian Languages Evalua-\ntion Forum (IberLEF 2022), CEUR WS\nProceedings.\nSantib\u00b4 a\u02dc nez Cort\u00b4 es, E., A. Carrillo-Cabrera,\nY. A. Castillo-Castillo, D. A. Moctezuma-\nOchoa, and V. H. Mu\u02dc n\u00b4 \u0131z S\u00b4 anchez. 2022.\nBert model and data augmentation for\nsentiment analysis in tourism reviews for\nmexican spanish language. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2022), CEUR WS Proceedings.\nToledano-L\u00b4 opez, O. G., J. Madera,\nH. Gonz\u00b4 alez, A. Sim\u00b4 on-Cuevas, T. De-\nmeester, and E. Mannens. 2022. Fine-\ntuning mt5-based transformer via cma-es\nfor sentiment analysis. In Proceedings\nof the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2022), CEUR WS Proceedings.\nVeigas-Ram\u00b4 \u0131rez, S., D. Mart\u00b4 \u0131nez-Davies, and\nI. Segura-Bedmar. 2022. Recommenda-\ntion system rest-mex 2022 for mexican\ntourism using natural language process-\ning. In Proceedings of the Third Workshop\nfor Iberian Languages Evaluation Forum\n(IberLEF 2022), CEUR WS Proceedings.\nOverview of Rest-Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid Semaphore Prediction for Mexican \nTourist Texts\n299300  \n \n \n \n     \nInformaci\u00f3n General\n  \n Informaci\u00f3n para los Autores  \nFormato de los Trabajos \n\u2022L\na longitud m\u00e1xima admitida para las contribuciones ser\u00e1 de 10 p\u00e1ginas DIN A4 (210 x 29 7\nm\nm.), incluidas referencias y figuras.\n\u2022L\nos art\u00edculos pueden estar escritos en ingl\u00e9s o espa\u00f1ol. El t\u00edtulo, resumen y palabras clav e\nd\neben escribirse en ambas lenguas.\n\u2022E\nl formato ser\u00e1 en Word \u00f3 LaT eX\nE\nnv\u00edo  de los Trabajos \n\u2022E\nl env\u00edo de los trabajos se realizar\u00e1 electr\u00f3nicamente a trav\u00e9s de la p\u00e1gina web de la Sociedad\nEspa\u00f1ola para el Procesamiento del Lenguaje Natural ( http://www.sepln.org)\n\u2022Para los trabajos con formato LaTeX se mandar\u00e1 el archivo PDF junto a todos los fuentes\nnecesarios para compilaci\u00f3n LaTex\n\u2022P\nara los trabajos con formato Word se mandar\u00e1 el archivo PDF junto al DOC o RTF\n\u2022P\nara m\u00e1s informaci\u00f3n http://www.sepln.org/la -revista/informacion -para-autores\n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalInformaci\u00f3n Adicional  \nFunciones del Consejo de Redacci\u00f3n\nL\nas funciones del Consejo de Redacci\u00f3n o Editorial de la revista SEPLN son las siguientes: \n\u2022Controlar la selecci\u00f3n y tomar las decisiones en la publicaci\u00f3n de los contenidos que han d e\nco\nnformar cada n\u00famero de la revista\n\u2022Po\nl\u00edtica editorial\n\u2022P\nreparaci\u00f3n de cada n\u00famero\n\u2022R\nelaci\u00f3n con los evaluadores y autores\n\u2022R\nelaci\u00f3n con el comit\u00e9 cient\u00edfico\nE\nl consejo de redacci\u00f3n est\u00e1 formado por los siguientes miembros  \nL.Al\nfonso Ure\u00f1a L\u00f3pez (Director)\nUn\niversidad de Ja \u00e9n \nlaurena@ujaen.es \nPatricio Mart\u00ednez Barco (Secretario)  \nUniversidad de Alicante  \npatricio@dlsi.ua.es  \nManuel Palomar Sanz  \nUniversidad de Alicante  \nmpalomar@dlsi.ua.es  \nFelisa Verdejo Ma\u00edllo  \nUNED  \nfelisa@lsi.uned.es  \nFunciones del Consejo Asesor\nL\nas funciones d el Consejo Asesor o Cient\u00edfico de la revista SEPLN son las siguientes:  \n\u2022Marcar, orientar y redireccionar la pol\u00edtica cient\u00edfica de la revista y las l\u00edneas de investigaci\u00f3n\na potenciar\n\u2022R\nepresentaci\u00f3n\n\u2022I\nmpulso a la difusi\u00f3n internacional\n\u2022C\napacidad de atracci\u00f3 n de autores\n\u2022E\nvaluaci\u00f3n\n\u2022C\nomposici\u00f3n\n\u2022Pr\nestigio\n\u2022Al\nta especializaci\u00f3n\n\u2022I\nnternacionalidad\nE\nl Consejo Asesor est\u00e1 formado por los siguientes miembros:  \nXabier Arregi  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nManuel de Buenaga  Universidad de Alcal\u00e1 (Espa\u00f1a)  \nJos\u00e9 Camacho  Collados  Cardiff University  (Reino Unido) \nSylviane Cardey -Greenfield  Centre de recherche en linguistique et traitement automatique des \nlangues (Francia)  \nIrene Castell\u00f3n  Universidad de Barcelona (Espa\u00f1a)  \nArantza D\u00edaz de Ilarraza  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nAntonio Ferr\u00e1ndez  Universidad de Alicant e (Espa\u00f1a)  \nKoldo Gojenola  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nXavier G\u00f3mez Guinovart  Universidad de Vigo (Espa\u00f1a)  \nJos\u00e9 Miguel Go\u00f1i  Universidad Polit\u00e9cnica de Madrid (Espa\u00f1a)  \nInma Hernaez Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalElena Lloret Universidad de Alicante ( Espa\u00f1a)  \nRam\u00f3n L\u00f3pez -C\u00f3zar Delgado  Universidad de Granada (Espa\u00f1a) \nBernardo Magnini  Fondazione Bruno Kessler (Italia)  \nNuno J. Mamede Instituto de Engenharia de Sistemas e Computadores (Portugal)  \nM. A\nntonia Mart\u00ed Anton\u00edn Universidad de Barcelona (Espa\u00f1a)  \nM.T\neresa Mart\u00edn Valdivia Unive rsidad de Ja\u00e9n (Espa\u00f1a)  \nPatricio Mart\u00ednez -Barco Universidad  de Alicante (Espa\u00f1a) \nEugenio Mart\u00ednez C\u00e1mara Universidad de Granada (Espa\u00f1a) \nPaloma Mart\u00ednez Fern\u00e1ndez Universidad  Carlos III (Espa\u00f1a) \nRaquel Mart\u00ednez Unanue Universidad Nacional de Educaci\u00f3n a Di stancia (Espa\u00f1a) \nRuslan Mitkov University of Wolverhampton (Reino Unido)  \nManuel Montes y G\u00f3mez Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico) \nMariana Lara Neves German Federal Institute for Risk Assessment  (Alemania)  \nLlu\u00eds Padr\u00f3 Univer sidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a)  \nManuel Palomar Unive rsidad de Alicante (Espa\u00f1a) \nFerr\u00e1n Pla Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nGerman Rigau Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nHoracio Rodr\u00edguez Universidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a)  \nPaolo Rosso Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nLeonel Ruiz Miyares Centro de Ling\u00fc\u00edstica Aplicada de Santiago de Cuba (Cuba)  \nHoracio Saggion Universidad Pompeu Fabra  (Espa\u00f1a)  \nEmilio Sanch\u00eds Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nEncarna Segarra Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nThamar Solorio University of Houston (Esta dos Unidos de Am\u00e9rica)  \nMaite Taboada Simon Fra ser University (Canad\u00e1)  \nMariona Taul\u00e9 Universidad de Barcelona  \nJuan-Manuel Torres-Moreno Laboratoire Informatique d\u2019Avignon / Universit\u00e9 d\u2019Avignon \n(Francia)  \nJos\u00e9 Antonio Troyano Jim\u00e9nez  Universidad de Sevilla (E spa\u00f1a)  \nL.Al\nfonso Ure\u00f1a L\u00f3pez Universidad de Ja\u00e9n (Espa\u00f1a) \nRafael Valencia Garc\u00eda Universidad de Murcia (Espa\u00f1a) \nRen\u00e9 Venegas Vel\u00e1sques Pontificia Universidad Cat\u00f3lica de Valpara\u00edso (Chile)  \nFelisa Verdejo Ma\u00edllo Universidad Nacional de Educaci\u00f3n a Distanci a (Espa\u00f1a) \nManuel Vilares Universidad de Vigo  (Espa\u00f1a)  \nLuis Villase\u00f1or -Pineda Institu to Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico) \nCartas al director\nS\nociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural  \nDepartamento de Inform\u00e1tica. Universidad de Ja\u00e9n  \nCampus Las Lagunillas, Edificio A3. Despacho 127. 23071 Ja\u00e9n \nsecretaria.sepln@ujaen.es \nM\u00e1s informaci\u00f3n\nP\nara m\u00e1s informaci\u00f3n sobre la Sociedad Espa\u00f1ola del Procesamiento del Lenguaje Natural puede \nconsultar la p\u00e1gina web http://www.sepln.org.  \nSi desea inscribirse como socio de la Socie dad Espa\u00f1ola del Procesamiento del Lenguaje Natural \npuede realizarlo a trav\u00e9s del formulario web que se encuentra en esta direcci\u00f3n \nhttp://www.sepln.org/sepln/la -sociedad  \nLos n\u00fameros anteriores de la revista se encuentran disponibles en la revista electr\u00f3nica: \nhttp://journal.sepln.org/sepln/ojs/ojs/index.php/pln/issue/archive  \nLas funciones del Consejo de Redacci\u00f3n est\u00e1n disponibles en Internet a trav\u00e9s de \nhttp://www.sepln.org/la -revista/consejo -de-redaccion  \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalLas funciones del Consejo Asesor est\u00e1n disponibles Internet a trav\u00e9s de la p\u00e1gina \nhttp://www.se pln.org/la -revista/consejo -asesor  \nLa inscripci\u00f3n como nuevo socio de la SEPLN se puede realizar a trav\u00e9s de la p\u00e1gina \nhttp://www.sepln.org/sepln/inscripcion -para-nuevos -socios  \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalOverview of DETESTS at IberLEF 2022: DETEction and classification of racial STere otypes in Spanish  \nAlejandro Ariza -Casabona, Wolfgang S. Schmeisser -Nieto, Montserrat Nofre, Mariona Taul\u00e9, Enrique \nAmig\u00f3, Berta Chulvi, Paolo Rosso  ...........................................................................................................  217 \nOverview of EXIST 2022: sEXism Identification in Social neTworks  \nFrancisco Rodr\u00edguez -S\u00e1nchez, Jorge Carrillo- de-Albornoz, Laura Plaza, Adri\u00e1n Mendieta- Arag\u00f3n, \nGuillermo Marco -Rem\u00f3n, Maryna Makeienko, Mar\u00eda Plaza, Julio Gonzalo, Damiano Spina, Paolo Rosso\n ..................................................................................................................................................................  229 \nMention detection, normalization & classification of species, pathogens, humans and food in clinical \ndocuments: Overview of the LivingNER shared task and resources  \nAntonio Miranda- Escalada, Eul\u00e0lia Farr\u00e9- Maduell, Salvador Lima -L\u00f3pez, Darryl Estrada, Luis Gasc\u00f3, \nMartin Krallinger  .....................................................................................................................................  241 \nOverview of PAR -MEX at Iberlef 202 2: Paraphrase Detection in Spanish Shared Task  \nGemma Bel -Enguix, Gerardo Sierra, Helena G\u00f3mez -Adorno, Juan -Manuel Torres -Moreno, Jesus -\nGerman Ortiz -Barajas, Juan V\u00e1squez  ......................................................................................................  255 \nOverview of PoliticEs  2022: Spanish Author Profiling for Political Ideology  \nJos\u00e9 Antonio Garc\u00eda- D\u00edaz, Salud Mar\u00eda Jim\u00e9nez -Zafra, Mar\u00eda -Teresa Mart\u00edn Valdivia, Francisco \nGarc\u00eda -S\u00e1nchez, L. Alfonso Ure\u00f1a- L\u00f3pez, Rafael Valencia- Garc\u00eda  ........................................................ 265 \nOverview of QuALES at IberLEF 2022: Quest ion Answering Learning from Examples in Spanish  \nAiala Ros\u00e1, Luis Chiruzzo, Luc\u00eda Bouza, Alina Dragonetti, Santiago Castro, Mathias Etcheverry, Santiago G\u00f3ngora, Santiago Goycoechea, Juan Machado, Guillermo Moncecchi, Juan Jos\u00e9 Prada, Dina \nWonsever  .................................................................................................................................................. 273 \nOver view of ReCoRES at IberLEF 2022: Reading Comprehension and Reasoning Explanation for \nSpanish  \nMarco Antonio Sobrevilla Cabezudo, Diego Diestra, Rodrigo L\u00f3pez, Erasmo G\u00f3mez, Arturo Oncevay, Fernando Alva- Manchego ........................................................................................................................  281 \nOverview of Rest -Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid \nSemaphore Prediction for Mexican Tourist Texts  \nMiguel \u00c1. \u00c1lvarez -Carmona, \u00c1ngel D\u00edaz -Pacheco, Ram\u00f3n Aranda, Ansel Y. Rodr\u00edguez -Gonz\u00e1lez, Daniel \nFajardo- Delgado, Rafael Guerrero -Rodr\u00edguez, L\u00e1zaro Bustio- Mart\u00ednez  ................................................  289 \nI\nnformaci\u00f3n General  \nInformaci\u00f3n para los autores  ....................................................................................................................  303 \nInformaci\u00f3n adicional  ...............................................................................................................................  304 \n\u00a9 2022 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural", "language": "PDF", "image": "PDF", "pagetype": "PDF", "links": "PDF"}