{"title": "PDF", "author": "PDF", "url": "https://rua.ua.es/dspace/bitstream/10045/117498/1/PLN_67.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "id": "PDF", "license": "PDF", "body": "PDF", "comments": "PDF", "commentsbody": "PDF", "raw_text": "PDF", "text": "ISSN: 1135 -5948         \nArt\u00edculos  \nSarcasm Detection with BERT  \nElsa Scola, Isabel Segura -Bedmar  ................................ ................................ ................................ ..............  13 \nImpact of Text Length for Information Retrieval Tasks based on Probabilistic Topics  \nCarlos Badenes -Olmedo, Borja Lozano -\u00c1lvarez, Oscar Corcho  ................................ ................................  27 \nConstructing Corpus and Word Embedding for Spanish Covid -19 Data  \nKyungjin Hwang  ................................ ................................ ................................ ................................ ..........  37 \nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo  \nVictor Darriba, Yerai Doval, Elmurod Kuriyozov ................................ ................................ .......................  45 \nAutoPunct : A BERT -based Automatic Punctuation and Capitalisation System for Spanish and Basque  \nAnder Gonz\u00e1lez -Docasal, Aitor Garc\u00eda -Pablos, Haritz Arzelus, Aitor \u00c1lvarez ................................ ..........  59 \nUnimodal Feature -level improvement on Multimodal CMU -MOSEI Dataset: Uncorrelated and \nConvolv ed Feature Sets  \nDaniel Mora Melanchthon  ................................ ................................ ................................ ...........................  69 \nMasking and BERT -based Models for Stereotype Identication  \nJavier S\u00e1nchez -Junquera, Paolo Rosso, Manuel Montes -y-G\u00f3mez, Berta Chulvi  ................................ ...... 83 \nEl sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00eds ticas musicales  \nMarco Palomeque, Juan de Lucio  ................................ ................................ ................................ ...............  95 \nReconocimiento  y clasificaci\u00f3n de entidades nombradas en textos legales  en espa\u00f1ol  \nDoaa Samy  ................................ ................................ ................................ ................................ .................  103 \nUn enfoque sem\u00e1ntico en la selecci\u00f3n de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones  \nHaro ld Gonz\u00e1lez -Guerra, Alfredo Sim\u00f3n -Cuevas, Jos\u00e9 M. Perea -Ortega, Jos\u00e9 A. Olivas  ......................  115 \nInducci\u00f3n autom\u00e1tica de una taxonom \u00eda multiling\u00fce de marcadores discursivos: primeros resultados en \ncastellano, ingl \u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n  \nRogelio Nazar  ................................ ................................ ................................ ................................ ............  127 \nExtraction of Terms Semantically Related to Colponyms: Evaluation  in a Small Specialized Corpus  \nJuan Rojas -Garcia  ................................ ................................ ................................ ................................ ..... 139 \nIberLEF 2021: Res\u00famenes de las tareas de evaluaci\u00f3n  \nOverview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021  \nFlor Miriam Plaza -del-Arco, Salud Mar\u00eda Jim\u00e9nez -Zafra, Arturo Montejo -R\u00e1ez, M. Dolores Molina -\nGonz\u00e1lez, L. Alfonso Ure\u00f1a -L\u00f3pez, M. Teresa Mart\u00edn -Valdivia  ................................ ...............................  155 \nOverview of Rest -Mex at IberLEF 2021: Recommendation System for Text Mexican Tourism  \nMiguel \u00c1. \u00c1lvarez -Carmona, Ram\u00f3n Aranda, Samuel Arce -Cardenas, Daniel Fajardo -Delgado, Rafael \nGuerrero -Rodr\u00edguez, A. Pastor L\u00f3pez -Monroy, Juan Mart\u00ednez -Miranda, Humberto P\u00e9rez -Espinosa, \nAnsel Y. Rodr\u00edguez -Gonz\u00e1lez  ................................ ................................ ................................ .....................  163 \nVaxxStance@IberLEF 2021: Overview of the Task on Going Beyond Text in Cross -Lingual Stance \nDetection  \nRodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo  ........  173 \nOverview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants  \nFlor Miriam Plaza -del-Arco , Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn -Valdivia, \nArturo Montejo -R\u00e1ez, Manuel Montes -y-G\u00f3mez, Horacio Jarqu\u00edn -V\u00e1squez, Luis Villase\u00f1or -Pineda  .... 183 \nOverview of EXIST 2021: sEXism Identification in Social neTworks  \nFrancisco Rodr\u00edguez -S\u00e1nchez, Jorge Carrilo -de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, \nMiriam Comet, Trinidad Donoso  ................................ ................................ ................................ ...............  195 \nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish  \nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique Amig\u00f3, Paolo Rosso  ................................ . 209 \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalISSN: 1135 -5948         \nComit\u00e9 Editorial  \nConsejo de redacci\u00f3n  \nL. Alfonso Ure\u00f1a L\u00f3pez  Universidad de Ja\u00e9n laurena@ujaen.es  (Director)  \nPatricio Mart\u00ednez Barco  Universidad de Alicante  patricio@dlsi.ua.es  (Secretario)  \nManuel Palomar Sanz  Universidad de Alicante mpalomar@dlsi.ua.es  \nFelisa Verdejo Ma\u00edllo   UNED  felisa@lsi.uned.es  \nISSN : 1135 -5948 \nISSN electr\u00f3nico : 1989 -7553 \nDep\u00f3sito Legal : B:3941 -91 \nEditado en:  Universidad de Ja\u00e9n  \nA\u00f1o de edici\u00f3n:  2021 \nEditores:  Eugenio Mart\u00ednez C\u00e1mara  Universidad de Granada  emcamara@decsai.ugr.es  \n\u00c1lvaro Rodrigo Yuste  UNED  alvarory@lsi.uned.es  \nPaloma Mart\u00ednez Fern\u00e1ndez  Universidad Carlos III  pmf@inf.uc3m.es  \nPublicado por:  Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural  \nDepartamento de Inform\u00e1tica. Universidad de Ja\u00e9n  \nCampus Las Lagunillas, EdificioA3. Despacho 127. 23071 Ja\u00e9n  \nsecretaria.sepln@ujaen.es  \nConsejo asesor  \nManuel de Buenaga  Universidad de Alcal\u00e1  (Espa\u00f1a)  \nSylviane Cardey -Greenfield  Centre de recherche en lingu istique et traitement \nautomatique des langues (Francia)  \nJos\u00e9 Camacho Collados  Cardiff University (Reino Unido)  \nIrene Castell\u00f3n Masalles  Universidad de Barcelona (Espa\u00f1a)  \nArantza D\u00edaz de Ilarraza  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nAntonio Ferr\u00e1ndez  Universidad de Alicante (Espa\u00f1a)  \nKoldo Gojenola  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nXavier G\u00f3mez Guinovart  Universidad de Vigo (Espa\u00f1a)  \nJos\u00e9 Miguel Go\u00f1i  Universidad Polit\u00e9cnica de Madrid (Espa\u00f1a)  \nRam\u00f3n L\u00f3pez -C\u00f3zar Delgado  Universidad de Granada ( Espa\u00f1a)  \nMariana Lara Neves  German Federal Institute for Risk Assessment \n(Alemania)  \nElena Lloret  Universidad de Alicante (Espa\u00f1a)  \nBernardo Magnini  Fondazione Bruno Kessler (Italia)  \nNuno J. Mamede  Instituto de Engenharia de Sistemas e Computadores \n(Portugal)  \nM. Teresa Mart\u00edn Valdivia Universidad de Ja\u00e9n (Espa\u00f1a)  \nPatricio Mart\u00ednez -Barco Universidad de Alicante (Espa\u00f1a)  \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalEugenio Mart\u00ednez C\u00e1mara  Universidad de Granada (Espa\u00f1a)  \nPaloma Mart\u00ednez Fern\u00e1ndez  Universidad Carlos III (Espa\u00f1a)  \nRaquel  Mart\u00ednez Unanue  Universidad Nacional de Educaci\u00f3n a Distancia \n(Espa\u00f1a)  \nLeonel Ruiz Miyares  Centro de Ling\u00fc\u00edstica Aplicada de Santiago de Cuba \n(Cuba)  \nRuslan Mitkov  University of Wolverhampton (Reino Unido)  \nManuel Montes y G\u00f3mez  Instituto Nacional de Astrof \u00edsica, \u00d3ptica y Electr\u00f3nica \n(M\u00e9xico)  \nLlu\u00eds Padr\u00f3  Universidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a)  \nManuel Palomar  Universidad de Alicante (Espa\u00f1a)  \nFerr\u00e1n Pla  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nGerman Rigau  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nHoracio S aggion  Universidad Pompeu Fabra (Espa\u00f1a)  \nPaolo Rosso  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nEmilio Sanch\u00eds  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nKepa Sarasola  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nEncarna Segarra  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nThamar Solorio  University of Houston (Estados Unidos de Am\u00e9rica)  \nMaite Taboada  Simon Fraser University (Canad\u00e1)  \nMariona Taul\u00e9  Universidad de Barcelona  \nJuan-Manuel Torres -Moreno  Laboratoire Informatique d\u2019Avignon / Universit \u00e9 \nd\u2019Avignon (Francia)  \nJos\u00e9 Antonio Troyano Jim\u00e9nez  Universidad de Sevilla (Espa\u00f1a)  \nL. Alfonso Ure\u00f1a L\u00f3pez  Universidad de Ja\u00e9n (Espa\u00f1a)  \nRafael Valencia Garc\u00eda  Universidad de Murcia (Espa\u00f1a)  \nRen\u00e9 Venegas Vel\u00e1sques  Pontificia Universidad Cat\u00f3lica de Valpara\u00eds o (Chile)  \nFelisa Verdejo Ma\u00edllo  Universidad Nacional de Educaci\u00f3n a Distancia \n(Espa\u00f1a)  \nManuel Vilares  Universidad de la Coru\u00f1a (Espa\u00f1a)  \nLuis Villase\u00f1or -Pineda  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica \n(M\u00e9xico)  \n \n \nRevisores adicionales  \n \nMiguel \u00c1ngel \u00c1lvarez Carmona  Consejo Nacional de Ciencia y Tecnolog\u00eda  (M\u00e9xico) \nMario Ezra Arag\u00f3n  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica \n(M\u00e9xico)  \nV\u00edctor Manuel Darriba Bilbao  Universidad de Vigo (Espa\u00f1a)  \nAgust\u00edn Daniel Delgado Mu\u00f1oz  Universidad Nacional de Educaci\u00f3n a Distancia \n(Espa\u00f1a)  \nAna Garc\u00eda Serrano  Universidad Nacional de Educaci\u00f3n a Distancia \n(Espa\u00f1a)  \nSalud Mar\u00eda Jim\u00e9nez Zafra  Universidad de Ja\u00e9n (Espa\u00f1a)  \nFernando Mart\u00ednez Santiago  Universidad de Ja\u00e9n (Espa\u00f1a)  \nMar\u00eda Dolores Molina Gonz\u00e1lez  Universidad de Ja\u00e9n (Espa\u00f1a)  \nFlor Miriam Plaza del Arco  Universidad de Ja\u00e9n (Espa\u00f1a)  \nAnselmo Pe\u00f1as  Universidad Nacional de Educaci\u00f3n a Distancia \n(Espa\u00f1a)  \nFrancisco Manuel Rangel Pardo  Symanto (Espa\u00f1a)  \nFrancisco J. Ribadas -Pena  Universidad de Vigo (Espa\u00f1a)  \nMar\u00eda Auxiliadora Rodr\u00edguez Barrios  Universidad Complutense de Madrid (Espa\u00f1a)  \n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalISSN: 1135 -5948         \nPre\u00e1mbulo  \nLa revista Procesamiento del Lenguaje Natural  pretende ser un foro de publicaci\u00f3n de art\u00edculos \ncient\u00edfico -t\u00e9cnicos in\u00e9ditos de calidad relevante en el \u00e1mbito del Procesamiento de Lenguaje \nNatural (PLN) tanto para la comunidad cient\u00edfica nacional e internacional, como para las \nempresas del sector. Ade m\u00e1s, se quiere potenciar el desarrollo de las diferentes \u00e1reas relacionadas \ncon el PLN, mejorar la divulgaci\u00f3n de las investigaciones que se llevan a cabo, identificar las \nfuturas directrices de la investigaci\u00f3n b\u00e1sica y mostrar las posibilidades reales de  aplicaci\u00f3n en \neste campo. Anualmente la SEPLN (Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje \nNatural) publica dos n\u00fameros de la revista, que incluyen art\u00edculos originales, presentaciones de \nproyectos en marcha, rese\u00f1as bibliogr\u00e1ficas y res\u00famenes de  tesis doctorales. Esta revista se \ndistribuye gratuitamente a todos los socios, y con el fin de conseguir una mayor expansi\u00f3n y \nfacilitar el acceso a la publicaci\u00f3n, su contenido es libremente accesible por Internet.  \nLas \u00e1reas tem\u00e1ticas tratadas son las s iguientes:  \n\u2022Modelos ling\u00fc\u00edsticos, matem\u00e1ticos y psicoling\u00fc\u00edsticos del lenguaje\n\u2022Ling\u00fc\u00edstica de corpus\n\u2022Desarrollo de recursos y herramientas ling\u00fc\u00edsticas\n\u2022Gram\u00e1ticas y formalismos para el an\u00e1lisis morfol\u00f3gico y sint\u00e1ctico\n\u2022Sem\u00e1ntica, pragm\u00e1tica y discurso\n\u2022Lexicograf\u00eda y terminolog\u00eda computacional\n\u2022Resoluci\u00f3n de la ambig\u00fcedad l\u00e9xica\n\u2022Aprendizaje autom\u00e1tico en PLN\n\u2022Generaci\u00f3n textual monoling\u00fce y multiling\u00fce\n\u2022Traducci\u00f3n autom\u00e1tica\n\u2022Reconocimiento y s\u00edntesis del habla\n\u2022Extracci\u00f3n y recuperaci\u00f3n de informaci\u00f3n monoli ng\u00fce, multiling\u00fce y multimodal\n\u2022Sistemas de b\u00fasqueda de respuestas\n\u2022An\u00e1lisis autom\u00e1tico del contenido textual\n\u2022Resumen autom\u00e1tico\n\u2022PLN para la generaci\u00f3n de recursos educativos\n\u2022PLN para lenguas con recursos limitados\n\u2022Aplicaciones industriales del PLN\n\u2022Sistemas de di\u00e1logo\n\u2022An\u00e1lisis de sentimientos y opiniones\n\u2022Miner\u00eda de texto\n\u2022Evaluaci\u00f3n de sistemas de PLN\n\u2022Implicaci\u00f3n textual y par\u00e1frasis\nEl ejemplar n\u00famero 67 de la revista Procesamiento del Lenguaje Natural  contiene trabajos \ncorrespondientes a dos apartados diferentes: comunicaciones cient\u00edficas y res\u00famenes de las tareas \nde evaluaci\u00f3n competitiva de la edici\u00f3n del a\u00f1o 2021 del foro de evaluaci\u00f3n Iberian Language \nEvaluation Forum  (IberLEF). Todos ellos han si do aceptados mediante el proceso de revisi\u00f3n \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturaltradicional en la revista. Queremos agradecer a los miembros del Comit\u00e9 Asesor y a los revisores \nadicionales la labor que han realizado.  \n \nSe recibieron 37 trabajos para este n\u00famero, de los cuales 25 eran art\u00edcul os cient\u00edficos y 12 res\u00fames \nde las tareas de evaluaci\u00f3n competitiva del foro de evaluaci\u00f3n IberLEF 2021. De entre los 25 \nart\u00edculos recibidos, 12 han sido finalmente seleccionados para su publicaci\u00f3n, lo cual fija una tasa \nde aceptaci\u00f3n del 48%.  \n \nEl Comit\u00e9 Asesor de la revista se ha hecho cargo de la revisi\u00f3n de los trabajos. Este proceso de \nrevisi\u00f3n es de doble anonimato: se  mantiene oculta la identidad de los autores que son evaluados \ny de los revisores que realizan las evaluaciones.  En un primer paso, cad a art\u00edculo ha sido \nexaminado de manera ciega o an\u00f3nima por tres revisores. En un segundo paso, para aquellos \nart\u00edculos que ten\u00edan una divergencia m\u00ednima de tres puntos (sobre siete) en sus puntuaciones, sus \ntres revisores han reconsiderado su evaluaci\u00f3n en  conjunto. Finalmente, la evaluaci\u00f3n de aquellos \nart\u00edculos que estaban en posici\u00f3n muy cercana a la frontera de aceptaci\u00f3n ha sido supervisada por \nm\u00e1s miembros del comit\u00e9 editorial. El criterio de corte adoptado ha sido la media de las tres \ncalificaciones,  siempre y cuando hayan sido iguales o superiores a 5 sobre 7.  \n \n \n \n \n \nSeptiembre de 2021  \nLos editores.  \n \n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalISSN: 1135 -5948         \nPreamble  \nThe Natural Language Processing  journal aims to be a forum for the publication of high -quality \nunpublished scientific and technical papers on Natural Language Processing (NLP) for both the \nnational and international scientific community and companies. Furthermore, we want to \nstrengthen the development of different areas related to NLP, widening the dissemination of \nresearch carried out, identifying the future directions of basic research and demonstrating the \npossibilities of its application in this field. Every year, the Spanish Society  for Natural Language \nProcessing (SEPLN) publishes two issues of the journal that include original articles, ongoing \nprojects, book reviews and summaries of doctoral theses. All issues published are freely \ndistributed to all members, and contents are freel y available online.  \nThe subject areas addressed are the following:  \n\u2022Linguistic, Mathematical and Psychological models to language\n\u2022Grammars and Formalisms for Morphological and Syntactic Analysis\n\u2022Semantics, Pragmatics and Discourse\n\u2022Computational Lexicography and Terminology\n\u2022Linguistic resources and tools\n\u2022Corpus Linguistics\n\u2022Speech Recognition and Synthesis\n\u2022Dialogue Systems\n\u2022Machine Translation\n\u2022Word Sense Disambiguation\n\u2022Machine Learning in NLP\n\u2022Monolingual and multilingual Text Generati on\n\u2022Information Extraction and Information Retrieval\n\u2022Question Answering\n\u2022Automatic Text Analysis\n\u2022Automatic Summarization\n\u2022NLP Resources for Learning\n\u2022NLP for languages with limited resources\n\u2022Business Applications of NLP\n\u2022Sentiment Analysis\n\u2022Opinion Mining\n\u2022Text Mining\n\u2022Evaluation of NLP systems\n\u2022Textual Entailment and Paraphrases\nThe 65 th issue of the Procesamiento del Lenguaje Natural  journal contains scientific papers  and \nsummarie of the shared -tasks of the edition of 2021 of the evaluation forum Iberian Languages \nEvaluation Forum (IberLEF) . All of these were accepted by a peer review process. We would \nlike to thank the Advisory Committee members and additional reviewers for their work.  \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalThirty -seven  papers were submitted for this issue, from which tw enty-five were scientific papers \nand twelve  were summaries of the evaluation tasks of the evaluation forum IberLEF 2021 . From \nthese twenty -five papers, we selected twelve  (48%) for publication.  \n \nThe Advisory Committee of the journal has reviewed the papers  in a double -blind process. Under \ndouble -blind review the identity of the reviewers and the authors are hidden from each other. In \nthe first step, each paper was reviewed blindly by three reviewers. In the second step, the three \nreviewers have given a seco nd overall evaluation of those papers with a difference of three or \nmore points out of seven in their individual reviewer scores. Finally, the evaluation of those papers \nthat were in a position very close to the acceptance limit were supervised by the edit orial board. \nThe cut -off criterion adopted was the mean of the three scores given , as long as it is equal or \ngreater than 5 out of 7 . \n \n \n \nSeptember  2021 \nEditorial board.  \n \n \n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural ISSN: 1135 -5948          \n \n \n \nArt\u00edculos  \n \nSarcasm Detection with BERT  \nElsa Scola, Isabel Segura -Bedmar  ................................ ................................ ................................ ..............  13 \nImpact of Text Length for Information Retrieval Tasks based on Probabilistic Topics  \nCarlos Badenes -Olmedo, Borja Lozano -\u00c1lvarez, Oscar Corcho  ................................ ................................  27 \nConstructing Corpus and Word Embedding for Spanish Covid -19 Data  \nKyungjin Hwang  ................................ ................................ ................................ ................................ ..........  37 \nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo  \nVictor Darriba, Yerai Doval, Elmurod Kuriyozov ................................ ................................ .......................  45 \nAutoPunct: A BERT -based Automatic Punctuation and Capitalisation System for Spanish and Basque  \nAnder Gonz\u00e1lez -Docasal, Aitor Garc\u00eda -Pablos, Haritz Arzelus, Aitor \u00c1lvarez ................................ ..........  59 \nUnimodal Feature -level improvement on Multimodal CMU -MOSEI Dataset: Uncorrelated and \nConvolved Feature Sets  \nDaniel Mora Melanchthon  ................................ ................................ ................................ ...........................  69 \nMasking and BERT -based Models for Stereotype Identication  \nJavier S\u00e1nchez -Junquera, Paolo Rosso, Manuel Montes -y-G\u00f3mez, Berta Chulvi  ................................ ...... 83 \nEl sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales  \nMarco Palomeque, Juan de Lucio  ................................ ................................ ................................ ...............  95 \nReconocimiento  y clasificaci\u00f3n de entidades nombradas en textos legales  en espa\u00f1ol  \nDoaa Samy  ................................ ................................ ................................ ................................ .................  103 \nUn enfoque sem\u00e1ntico en la selecci\u00f3n de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones  \nHarold Gonz\u00e1lez -Guerra, Alfredo Sim\u00f3n -Cuevas, Jos\u00e9 M. Perea -Ortega, Jos\u00e9 A. Olivas  ......................  115 \nInducci\u00f3n autom\u00e1tica de una taxonom \u00eda multiling\u00fce de marcadores discursivos: primeros resultados en \ncastellano, ingl \u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n  \nRogelio Nazar  ................................ ................................ ................................ ................................ ............  127 \nExtraction o f Terms Semantically Related to Colponyms: Evaluation  in a Small Specialized Corpus  \nJuan Rojas -Garcia  ................................ ................................ ................................ ................................ ..... 139 \n \nIberLEF 2021: Res\u00famenes de las tareas de evaluaci\u00f3n  \nOverview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021  \nFlor Miriam Plaza -del-Arco, Salud Mar\u00eda Jim\u00e9nez -Zafra, Arturo Montejo -R\u00e1ez, M. Dolores Molina -\nGonz\u00e1lez, L. Alfonso Ure\u00f1a -L\u00f3pez, M. Teresa Mart\u00edn -Valdivia  ................................ ...............................  155 \nOverview of Rest -Mex at IberLEF 2021: Recommendation System for Text Mexican Tourism  \nMiguel \u00c1. \u00c1lvarez -Carmona , Ram\u00f3n Aranda, Samuel Arce -Cardenas, Daniel Fajardo -Delgado, Rafael \nGuerrero -Rodr\u00edguez, A. Pastor L\u00f3pez -Monroy, Juan Mart\u00ednez -Miranda, Humberto P\u00e9rez -Espinosa, \nAnsel Y. Rodr\u00edguez -Gonz\u00e1lez  ................................ ................................ ................................ .....................  163 \nVaxxStance@IberLEF 2021: Overview of the Task on Going Beyond T ext in Cross -Lingual Stance \nDetection  \nRodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo  ........  173 \nOverview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants  \nFlor Miriam Plaza -del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn -Valdivia, \nArturo Montejo -R\u00e1ez, Manuel Montes -y-G\u00f3mez, Horacio Jarqu\u00edn -V\u00e1squez, Luis Villase\u00f1or -Pineda  .... 183 \nOverview of EXIST 2021: sEXism Identification in Social neTworks  \nFrancisco Rodr\u00edguez -S\u00e1nchez, Jorge Carrilo -de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, \nMiriam Comet, Trinidad Donoso  ................................ ................................ ................................ ...............  195 \nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish  \nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enr ique Amig\u00f3, Paolo Rosso  ................................ . 209 \n \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalOverview of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task  \nHelena G\u00f3mez -Adorno, Juan Pablo Posadas -Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo  ...... 223 \nOverview of the eHealth Knowledge Discovery Challenge at IberLEF 2021  \nAlejandro Piad -Morffis, Suilan Estevez -Velarde, Yoan Gutierrez, Yudivian Almeida -Cruz, Andr\u00e9s \nMontoyo, Rafael Mu\u00f1oz  ................................ ................................ ................................ .............................  233 \nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic \nrecognition, classifica tion and normalization of professions and occupations from medical texts  \nSalvador Lima -L\u00f3pez, Eul\u00e0lia Farr\u00e9 -Maduell, Antonio Miranda -Escalada, Vicent Briv\u00e1 -Iglesias, Martin \nKrallinger  ................................ ................................ ................................ ................................ ...................  243 \nOverview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Hu mor in Spanish  \nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea  ............  257 \nOverview of the IDPT Task on Irony Detection in Portuguese at IberLEF 2021  \nUlisses B. Corr\u00eaa, Leonardo Coelho, Leonardo Santos, Larissa A. de Freit as ................................ ........  269 \nOverview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press  \nElena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta \nZamorano  ................................ ................................ ................................ ................................ ...................  277 \n \nInformaci\u00f3n General  \nInformaci\u00f3n para los autores  ................................ ................................ ................................ ......................  289 \nInformaci\u00f3n adicional  ................................ ................................ ................................ ................................  291 \n  \n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural  \n \n \n \n \n \n \n \n \nArt\u00edculos   \n Sarcasm Detection with BERT\nDetecci\u0013 on de Sarcasmo con BERT\nElsa Scola, Isabel Segura-Bedmar\nUniversidad Carlos III de Madrid, Legan\u0013 es, Spain\nscolaelsa@gmail.com, isegura@inf.uc3m.es\nAbstract: Sarcasm is often used to humorously criticize something or hurt someo-\nne's feelings. Humans often have di\u000eculty in recognizing sarcastic comments since\nwe say the opposite of what we really mean. Thus, automatic sarcasm detection in\ntextual data is one of the most challenging tasks in Natural Language Processing\n(NLP). It has also become a relevant research area due to its importance in the\nimprovement of sentiment analysis. In this work, we explore several deep learning\nmodels such as Bidirectional Long Short-Term Memory (BiLSTM) and Bidirectional\nEncoder Representations from Transformers (BERT) to address the task of sarcasm\ndetection. While most research has been conducted using social media data, we eva-\nluate our models using a news headlines dataset. To the best of our knowledge, this\nis the \frst study that applies BERT to detect sarcasm in texts that do not come\nfrom social media. Experiment results show that the BERT-based approach overco-\nmes the state-of-the-art on this type of dataset.\nKeywords: Sarcasm Detection, Deep Learning, BiLSTM, BERT.\nResumen: El sarcasmo se usa con frecuencia para realizar cr\u0013 \u0010tica o burla indirec-\nta, a veces hiriendo los sentimientos de alguien. Algunas veces, las personas tienen\ndi\fcultades para reconocer los comentarios sarc\u0013 asticos, ya que decimos lo contra-\nrio de lo que realmente queremos decir. Por lo tanto, la detecci\u0013 on autom\u0013 atica de\nsarcasmo en textos es una de las tareas m\u0013 as complicadas en el Procesamiento del\nLenguaje Natural (PLN). Adem\u0013 as, se ha convertido en un \u0013 area de investigaci\u0013 on\nrelevante debido a su importancia para mejorar el an\u0013 alisis de sentimientos. En es-\nte trabajo, exploramos varios modelos de aprendizaje profundo, como Bidirectional\nLong Short-Term Memory (BiLSTM) y Bidirectional Encoder Representations from\nTransformers (BERT) para abordar la tarea de detecci\u0013 on de sarcasmo. Si bien la\nmayor\u0013 \u0010a de los trabajos anteriores se han centrado en datasets construidos con textos\nde redes sociales, en este art\u0013 \u0010culo, evaluamos nuestros modelos utilizando un dataset\nformado por titulares de noticias. Por tanto, este es el primer estudio que aplica\nBERT para detectar el sarcasmo en textos que no provienen de las redes sociales.\nLos resultados de los experimentos muestran que el enfoque basado en BERT supera\nel estado del arte en este tipo de conjunto de datos.\nPalabras clave: Sarcasm Detection, Deep Learning, BiLSTM, BERT.\n1 Introduction\nThe Cambridge Dictionary de\fnes sarcasm\nas\\the use of remarks that mean the opposi-\nte of what they say, made to hurt someone's\nfeelings or to criticize something in a humo-\nrous way\" . However, understanding sarcasm\nis a task that is often hard for humans, as\nit is highly dependent on the context and\nsense of humor of each person(Capelli, Na-\nkagawa, and Madden, 1990). The perceptionof sarcasm can vary by multiple factors, li-\nke culture, gender or personality (Rockwell\nand Theriot, 2001). For example, Indians and\nAmericans perceive sarcasm in di\u000berent ways\n(Joshi et al., 2016). In the following senten-\nce taken from the study presented by Joshi et\nal. (2016): \\Love going to work and being sent\nhome after two hours\" , Indian annotators do\nnot agree with Americans. Indian annotators\nlabeled the instance as non-sarcastic as they\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 13-25\nrecibido 07-04-2021 revisado 04-06-2021 aceptado 07-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-1\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturaldid not have any context about long commu-\nting to work and that `being sent home' could\nmean being \fred from a job.\nMoreover, when sarcasm happens ver-\nbally, aspects like volume, voice tonality and\nspeed, contribute to express it. Often sarcasm\nis also accompanied by various gestures, like\neye and hand movement. In contrast, written\nsarcasm, which occurs in di\u000berent environ-\nments (such as emails, social media, or pro-\nduct reviews) completely lacks the aforemen-\ntioned features that contribute to the identi-\n\fcation of sarcasm, and therefore, making it\nmore di\u000ecult to detect it. This suggests that\ndetecting sarcasm is a very challenging task\nfor humans, and it is even harder for algo-\nrithms.\nAutomatic sarcasm detection is one of the\nmost challenging tasks in Natural Language\nProcessing (NLP) (Eke et al., 2020) and can\nbe used in a variety of applications, ranging\nfrom knowing the customers opinions about\nproducts or services o\u000bered by a company\nor even identifying inappropriate or harming\ncomments in social media to protect users.\nTo date, most attempts at sarcasm de-\ntection have used Twitter datasets to train\nand evaluate their models (Pt\u0013 a\u0014 cek, Haber-\nnal, and Hong, 2014). However, these data-\nsets are noisy and add di\u000eculty to the task\nbecause tweets are short texts (280 charac-\nters). They also contain very informal langua-\nge, grammatical and spelling mistakes, slang\nterms, abbreviations and non-standard lan-\nguage features such as hashtags, emoticons,\nhyperlinks and other ones, which do not oc-\ncur in standard texts. Moreover, the lack of\ncontext could also be a problem as many\ntweets are replies to previous tweets (Hernan-\ndez Farias, Patti, and Rosso, 2016). However,\nsigni\fcantly less e\u000bort has been put into ex-\nploiting other types of texts to train and eva-\nluate models for sarcasm detection.\nThis study aims to explore di\u000berent deep\nlearning techniques such as Bidirectional\nLong Short Term Memory (BiLSTM) (Ho-\nchreiter and Schmidhuber, 1997) and Bidirec-\ntional Encoder Representations from Trans-\nformers (BERT) (Devlin et al., 2019) to ad-\ndress the task of sarcasm detection from\ntexts. BERT is a model that is gaining in-\ncreasing popularity due its outstanding per-\nformance for multiple NLP tasks (Lee et al.,\n2020; Zheng and Yang, 2019; Hakala and\nPyysalo, 2019). To the best of our knowled-ge, this is the \frst study that applies BERT\nfor sarcasm detection in texts that are not\nextracted from social media.\nThis paper is organised as follows: Section\n2 discusses the main datasets used for sar-\ncasm detection. It also presents state-of-the-\nart methods for this task. Section 3 describes\nthe dataset and methods used in this work.\nIn Section 4, we show the evaluation of the\nproposed methods and discuss their results.\nFinally, Section 5 describes conclusions and\nfuture work.\n2 Related work\nSarcasm is a form of expression in which peo-\nple convey the opposite of what they say to\nhurt someone emotionally or humorously cri-\nticize something. This implicit subjectivity to\nthe problem makes it even harder for ma-\nchines to detect. Therefore, this is one of\nthe most challenging tasks in NLP nowadays.\nThe task of automatic sarcasm detection has\nbeen most commonly de\fned, in past work,\nas a classi\fcation task. That is, given a pie-\nce of text, the goal is to predict whether it\nis sarcastic or not. In this section, we review\nthe main datasets as well as the most recent\napproaches to address this task.\n2.1 Datasets for sarcasm detection\nMany datasets for sarcasm detection are\ncreated by using hashtag based supervision.\nHashtag based supervision consists in sear-\nching tweets containing hashtags like #sar-\ncasm, and assuming they were correctly \\an-\nnotated\"by their authors. The main advan-\ntage of this technique is that it allows collec-\nting large datasets, which are automatically\nlabeled with no manual annotation. However,\nthe produced labels can be highly noisy. For\ninstance, it considers all tweets without pre-\nde\fned tags as non-sarcastic, however, some\nof them could express sarcasm.\nOne of the earliest Twitter dataset for sar-\ncasm detection was proposed by Rilo\u000b et al.\n(2013). This dataset contains a total of 3,000\ntweets, of which 2,307 are non-sarcastic and\n693 sarcastic. Due to Twitter's data sharing\npolicy, only the tweet ids are permitted to\nshare, so their tweets can be directly down-\nloaded from Twitter by using those ids. Ho-\nwever, many of the original tweets have been\nremoved since 2013, and therefore, the data-\nset is a bit outdated.\nAnother Twitter dataset collected by\nElsa Scola, Isabel Segura-Bedmar\n14using hashtag based supervision is the Irony\ndetection in English tweets dataset, which\nwas part of the SemEval-2018 competition\n(Apidianaki et al., 2018). Although it is orien-\nted to the task of irony detection, it can be\nused for sarcasm detection as well, as sar-\ncasm often contains irony in it. The data-\nset contains 2,396 positive instances (sarcas-\ntic tweets) and 604 negative instances (non-\nsarcastic tweets). Despite the tweets being\ncollected making use of hashtags, all the\ntweets were manually labeled in order to\navoid noisy data. Furthermore, the corpus\nwas cleaned by removing retweets, duplica-\nted tweets, and non-English tweets.\nGhosh and Veale (2016) created one of the\nbiggest datasets for sarcasm detection. The\ntraining dataset contains 39,000 tweets, of\nwhich 18,000 are sarcastic and 21,000 non-\nsarcastic, making this dataset evenly balan-\nced. The test dataset contains 2,000 tweets\nannotated by an internal team of researchers,\nwhich is also balanced.\nAn innovative contribution was made by\nOprea and Magdy (2020). This proposal\nshows an original way of collecting sarcas-\ntic tweets. They have designed an online sur-\nvey where they ask Twitter users to provide\nlinks to one sarcastic and three non-sarcastic\ntweets. This results in the iSarcasm dataset,\nwhich contains 4,484 tweets, out of which 777\nwere labeled as sarcastic and 3,707 as non-\nsarcastic.\nApart from Twitter data, there are also\nsome available datasets from Reddit, a dis-\ncussion website. One of them is the dataset\ncollected by Khodak, Saunshi, and Vodrahalli\n(2018), which contains 1.3 million comments\nfrom Reddit. It was generated by scraping\ncomments that contained the\ns tag. This tag is often used by Redditors to\nindicate that their comment is sarcastic and\nshould not be taken seriously. Therefore, it\nmay produce noise, as happen with hashtag\nbased supervision. This dataset provides ba-\nlanced and imbalanced versions.\nContributions in a dialogue context for\nsarcasm detection have also been made.\nThe Discussion Forum dataset (Ghosh, Ri-\nchard Fabbri, and Muresan, 2017) is a co-\nllection of posts from forums. For each post,\nits replies are also included. These posts\nwere manually annotated in three catego-\nries of sarcasm: general sarcasm, hyperbole,\nand rhetorical questions. For the general sar-casm category, there are 3,260 posts per class\n(sarcastic and not-sarcastic), that is, a total\nof 6,520 posts. The hyperbole contains 582\nposts per class and rhetorical questions 851\nposts per class.\nSeveral options can be found in the mul-\ntilingual panorama for sarcasm detection.\nPt\u0013 a\u0014 cek, Habernal, and Hong (2014) created\ntwo datasets for sarcasm detection on Twit-\nter in English and Czech. While the English\ndataset was obtained by hashtag based su-\npervision (using the hashtag #sarcasm as\nan indicator of sarcastic tweets), the Czech\ndataset was manually annotated. The En-\nglish dataset is provided in two options: ba-\nlanced corpus (50,000 sarcastic and 50,000\nnon-sarcastic tweets), and imbalanced corpus\n(25,000 sarcastic and 75,000 non-sarcastic\ntweets). The Czech dataset has 325 sarcas-\ntic tweets and 6,675 non-sarcastic ones.\nRecently, the IroSvA (Irony Detection in\nSpanish Variants) shared task (Ortega-Bueno\net al., 2019) provided a dataset for irony de-\ntection in short messages (tweets and news\ncomments) written in Spanish. The corpus\nconsists of 9,000 short messages about di\u000be-\nrent topics written in Spanish {3,000 from\nCuba, 3,000 from Mexico and 3,000 from\nSpain- and annotated with irony. Approxi-\nmately, 80 % of the corpus corresponds to the\ntraining dataset, whereas the remaining 20 %\ncorresponds to the test set.\nAs can be seen from the above, most data-\nsets for sarcams detection are collected from\nsocial media. An alternative dataset was pre-\nsented by Misra and Arora (2019). They pro-\nposed a novel dataset based on new headlines\nto overcome the limitations of Twitter and\nother social media datasets. Sarcastic headli-\nnes were collected from TheOnion,1which is\na news website whose sole purpose is to pro-\nduce sarcastic content. Non-sarcastic headli-\nnes were extracted from the Hu\u000bPost,2which\nis a real news website. This dataset is descri-\nbed in detail in Subsection 3.1.\n2.2 Approaches for sarcasm\ndetection\nWe now review the main approaches that ha-\nve addressed this task.\n1https://www.theonion.com\n2https://www.hu\u000bpost.com\nSarcasm Detection with BERT\n152.2.1 Traditional Machine Learning\nEarly approaches for sarcasm detection used\ntraditional machine learning algorithms. Lie-\nbrecht, Kunneman, and van den Bosch (2013)\nproposed a model for text classi\fcation,\nwhich is based on Balanced Winnow (Littles-\ntone, 1988). This machine learning techni-\nque produces interpretable per-class weight.\nThese weights can later be used to discover\nthe highest-ranking features for one class. To\ntrain the classi\fer, the authors used a co-\nllection of tweets collected from a database\nprovided by the Netherlands eScience Centre.\nTo create the dataset, the authors collected\na sample of tweets tagged with the hashtag\n#sarcasme (the Dutch word for sarcasm) and\na random sample of tweets without it. They\nexplored the e\u000bect of balanced (50-50) and\nimbalanced (25 % sarcastic and 75 % non-\nsarcastic) data. The classi\fer provided 75 %\nTPR (True Positive Rate or recall) and 16 %\nFPR (False Positive Rate) using the balan-\nced dataset. However, although the use of an\nimbalanced dataset had a positive e\u000bect on\nFPR (5 %), TPR dropped markedly to 56 %.\nError analysis showed that sarcasm is often\nindicated by the usage of intensi\fers and ex-\nclamations. When these are not present in the\ntweet, there is often a hashtag indicating sar-\ncasm. The authors hypothesized that explicit\nmarkers, like hashtags, are the digital equiva-\nlent of nonverbal expressions that people use\nin real life to express sarcasm. One of the li-\nmitations of this study is that the tweets were\nautomatically annotated without further ma-\nnual review.\nThe same year, Rilo\u000b et al. (2013) pro-\nposed an alternative approach to de\fne and\nidentify sarcasm in text. They state that sar-\ncasm is often de\fned in terms of contrast or\n\\saying the opposite of what you mean\". As\nthey stated it in their study, \\[It] is common\non Twitter: the expression of positive sen-\ntiment (e.g., \\love\" or \\enjoy\") in referen-\nce to a negative activity or state (e.g., \\ta-\nking an exam\" or \\being ignored\". Their ap-\nproach focused on trying to identify this ty-\npe of contrast in the text by recognizing po-\nsitive sentiments with negative situations in\nsentences. To achieve this, they created a no-\nvel bootstrapping algorithm that automati-\ncally learns lists of positive sentiment phrases\nand negative situation phrases from sarcastic\ntweets. They used the bootstrapped lexicons\nto recognize sarcasm by looking for phrasesin their tweets. This system achieved an F1\nof 22 %. Additionally, they tested a Support\nVector Machine (SVM) (Cortes and Vapnik,\n1995) classi\fer providing an F1 of 48 %. Fi-\nnally, they combined both approaches in an\nattempt to improve the results. The hybrid\napproach obtained an improvement in recall\nwith a slight drop in precision, which resulted\nin an F1 of 51 %.\nIn 2014, Pt\u0013 a\u0014 cek, Habernal, and Hong\n(2014) proposed two classi\fers: Maximum\nEntropy (MaxEnt) (Nigam, 1999) and SVM\nto address this task. This research gave mo-\nre importance to feature engineering rather\nthan to the classi\fers. A Bag-of-Words ap-\nproach was applied to represent the texts.\nA number of experiments were performed by\ncombining previously selected n-grams and a\nset of language-independent features, inclu-\nding punctuation marks, emoticons, quotes,\ncapitalized words, character n-grams, and\nskip-grams as baselines. Moreover, they did\na multilingual study by using an English da-\ntaset, as well as a Czech dataset. These da-\ntasets were described above. They evaluated\nbalanced and imbalanced datasets scenarios.\nFor the English dataset, The MaxEnt clas-\nsi\fer achieved an F1 of 94.7 % and 92.4 %\non the balanced and imbalanced datasets res-\npectively. The SVM classi\fer yielded an F1\nof 91.4 % on the balanced data and 88.6 % on\nthe imbalanced data. Experiments showed lo-\nwer results for the Czech dataset. This may\nbe due to the Czech dataset being much sma-\nller than the English dataset, as well as to the\ninner grammatical complexity of the Czech\nlanguage. MaxEnt obtained an F1 of 57 %,\nwhile SVM gave the best F1 (58.2 %) on the\nCzech dataset.\nBamman and Smith (2015) approached\nthe problem from an original perspective by\nattempting to introduce one of the most rele-\nvant components to sarcasm understanding:\ncontext. To achieve this, they used Twit-\nter data combined with extra-linguistic in-\nformation from the context of the tweet.\nThis information includes properties of the\nauthor (such as author historical salient\nterms, author historical topics, author his-\ntorical sentiment, pro\fle information, pro\fle\nunigrams), the audience (author features of\nthe users involved in the Twitter conversa-\ntion), and the immediate communicative en-\nvironment (such as unigrams and bigrams in\nboth original and response tweets). As a clas-\nElsa Scola, Isabel Segura-Bedmar\n16si\fer, a binary Logistic Regression with l2re-\ngularization and tenfold cross-validation was\nused. They evaluated di\u000berent combinations\nof feature sets (about the tweet, author and\nenvironment), showing improvements in com-\nparison to only using term frequencies to re-\npresent tweets. Including all the features to-\ngether yielded the highest accuracy at 85.1 %.\nHowever, the most signi\fcant improvement\nin accuracy came from the inclusion of author\nfeatures.\n2.2.2 Deep Learning Approaches\nWe now present some of the latest work on\nsarcasm detection research, in which mostly\ndeep learning techniques are used.\nAmir et al. (2016) proposed to automati-\ncally learn and exploit user embeddings com-\nbined with lexical features to detect sarcasm.\nUser embeddings are vector representations\nthat \\encode latent aspects of users and cap-\nture homophily, by projecting similar users\ninto nearby regions of the embedding space.\"\n(Amir et al., 2016). More concretely, their ap-\nproach captures relations between users and\ntheir content. One of the main advantages of\nthis work is that it avoids the laborious fea-\nture engineering process. The authors only\nused the text of previous posts of the users to\ncreate the user embeddings. The authors used\na Convolutional Neural Network (CNN), ob-\ntaining an accuracy of 87 %. Thus, user em-\nbeddings can capture relevant user attributes\nwithout the need for elaborated feature engi-\nneering.\nGhosh, Richard Fabbri, and Muresan\n(2017) exploited the conversation context in\nsarcasm detection. The authors used the Dis-\ncussion Forum data (Oraby et al., 2016b),\nwhich contains sarcastic responses from a fo-\nrum and their corresponding context (the ori-\nginal post and its replies). They also collected\nsarcastic and non-sarcastic tweets to create\na dataset for their experiments. Several ty-\npes of LSTM networks were investigated, sho-\nwing an F1 of 70.56 % for the Discussion Fo-\nrum data and an F1 of 73.45 % on the Twitter\ndataset.\nThe most recent studies for sarcasm de-\ntection have exploited BERT, a novel lan-\nguage model based on transformers to provi-\nde deep contextual representations for words.\nThis model is gaining increasing popularity\ndue its outstanding performance for multiple\nNLP tasks.\nXu and Xu (2019) presented an investi-gation of di\u000berent models to explore the ef-\nfect of contextual information on sarcasm\ndetection. Concretely, various LSTM mo-\ndels and BERT were used. The implemented\nLSTM models were all of them unidirectional\n(from left to right) and used pre-trained Glo-\nVe(Pennington, Socher, and Manning, 2014)\nas a word embedding model. Two datasets\nare used for this study, Discussion Forum da-\nta (Oraby et al., 2016a)), and Reddit Sar-\ncasm data (Khodak, Saunshi, and Vodrahalli,\n2018)), which were described above. The re-\nsults of this study show that BERT achieved\nbetter results than all the LSTM models for\nboth datasets. In the case of the LSTM mo-\ndel, the performance varies depending on the\ndataset. For example, it obtains an accuracy\nof 73.23 % for the Discussion Forum dataset,\nbut 67.32 % for Reddit data. This is probably\ndue to the lack of \\quality.of the Reddit data-\nset as it relies on self-annotated labels, which\noften add noise. Besides, the fact that the\nDiscussion Forum dataset contains generally\nproperly written English contributes to hel-\nping the model perform better. Whereas Red-\ndit is full of typos and slang terms, which\nmakes it hard for word embeddings to un-\nderstand.\nKhatri and P (2020) proposed using ma-\nchine learning classi\fers in combination with\nBERT and GloVe embeddings in order to de-\ntect sarcasm in tweets. The authors experi-\nmented with di\u000berent classi\fers: SVM, Lo-\ngistic Regression, Gaussian Naive Bayes, and\nRandom Forest. They used a balanced da-\ntaset containing 5,000 tweets. Their experi-\nments showed that word embeddings are use-\nful for sarcasm detection as they capture the\nmeaning of the words as vector representa-\ntions. The best results were obtained with Lo-\ngistic Regression for both embeddings, achie-\nving an F-score of 63 % when BERT embed-\ndings are used, and an F1 of 69 % with Glove\nembeddings.\nMisra and Arora (2019) proposed hybrid\nneural network architecture for sarcasm de-\ntection from texts, as well as a dataset of\nnews headlines, described above, which chan-\nges the tendency of the almost exclusive usa-\nge of Twitter datasets for the task of sar-\ncasm detection. This system combined pre-\ntrained user embeddings and a BiLSTM mo-\ndule, which used an attention module to up-\ndate the weights of the encoded context for\neach epoch. As the headlines are written by\nSarcasm Detection with BERT\n17professionals in a formal tone, there are no\nspelling mistakes or slang terms. Moreover,\nthe labels of the sarcastic instances are high\nquality (TheOnion only publishes sarcastic\nnews). Furthermore, as news headlines are\nself-contained (there are not reply posts), it\nis easier to spot the sarcastic elements of the\nsentence. This hybrid architecture obtained\nan accuracy of 89.7 %.\n3 Approaches\nThis section describes the dataset used in this\nwork and the two deep learning approaches\nproposed to deal with the task of sarcasm\ndetection.\n3.1 Dataset\nSarcasm detection studies often make use of\nTwitter datasets (Cai, Cai, and Wan, 2019),\ncollected using keyword hashtags (like #sar-\ncasm). However, these datasets can turn out\nto be noisy due to the informal use of lan-\nguage in social media. Social media texts\ncontain very informal language, grammatical\nand spelling mistakes, slang terms, abbrevia-\ntions (for example, 'TBH' refers to 'To Be\nHonest') and non-standard language featu-\nres such as hashtags, emoticons, hyperlinks\nand other ones that do not occur in standard\ntexts. Furthermore, tweets are often replies\nto previous tweets, which would imply a lack\nof contextual information.\nIn order to avoid these drawbacks of\ntweets, in this study, the News Headlines Da-\ntaset For Sarcasm Detection (Misra and Aro-\nra, 2019) is used. This dataset contains news\nheadlines collected from two journal websites:\nThe Onion andHu\u000bPost. The former produ-\nces sarcastic versions of current events, whe-\nreas the latter is a well known trustworthy\nnewspaper. Some of the advantages of using\nthis dataset:\n\u2022News headlines are written without any\nspelling mistakes and informal usage of\nthe vocabulary.\n\u2022Pre-trained word embedding models\n(Mikolov et al., 2013; Pennington, So-\ncher, and Manning, 2014; Joulin et\nal., 2017) trained using formal texts\nusually provide higher coverage of voca-\nbulary than pre-trained models trained\non tweets.\n\u2022The headlines are self-contained, whe-\nreas, tweets, on the contrary, could beresponses to previous tweets, or part of\nthreads, which would translate in a lack\nof context.\n\u2022Since the sole purpose of The Onion is\nto publish sarcastic news, it could pro-\nvide a higher guarantee of the correct-\nness of the data in comparison to some\nTwitter datasets based in keyword hash-\ntags. In studies like (Rilo\u000b et al., 2013),\nit is assumed that human labeling from\nTwitter users is correct, however, the an-\nnotation of a sample showed that only\n85 % of those tweets were indeed sarcas-\ntic. Therefore, the Twitter datasets can\nbe noisy. Thus, headlines from newspa-\npers like The Onion can be an alternati-\nve that contributes to more accurately\nannotated datasets for sarcasm detec-\ntion. However, the implicit subjectivity\nin humor related tasks certainly hinders\nthe achievement of the \\perfect\"dataset.\nThe original dataset is provided in JSON\nformat. Each headline is represented by its\ntext, the link to the original news article,\nand an value of 0 (if it is a non-sacarstic\nheadline) or 1 (if it is a sarcastic headli-\nne). After removing duplicate headlines, the\ndataset is composed of 28,503 headlines, of\nwhich 14,951 are non-sarcastic and 13,552 are\nsarcastic. Finally, the dataset was split into\n70 % for training (with 9,498 sarcastic head-\nlines and 10,454 non-sarcastic ones), 10 %\nfor validation (with 1,342 sarcastic headlines\nand 1,508 non-sarcastic ones) and 20 % for\ntest (with 2,712 sarcastic headlines and 2,989\nnon-sarcastic ones). As can be seen, the three\ndatasets are balanced, that is, the amount of\npositive instances (sarcastic texts) and nega-\ntive ones (non-sarcastic texts) is roughly the\nsame. These datasets were used to train the\nmodels and evaluate their performance.\n3.2 Methods\n3.2.1 Long Short-Term Memory\n(LSTM)\nAs a baseline, we propose the Long Short-\nTerm Memory (LSTM) (Hochreiter and Sch-\nmidhuber, 1997) architecture that recently\nhas been successfully used for text classi\f-\ncation (Zhou et al., 2016; Wang et al., 2018).\nLSTM is a unidirectional model that proces-\nses the inputs from left to right, but not from\nright to left. Hence, during the training, it can\nonly preserve relevant information from the\nElsa Scola, Isabel Segura-Bedmar\n18left part of the input, but it does not know\nabout what is the information on the right\npart. However, sometimes, to correctly un-\nderstand a text, we need to take into account\nnot just the previous words, but also the co-\nming words. For example, \\Sometimes I need\nwhat only you can provide: your absence.\",\nand \\Sometimes I need what only you can\nprovide: your love.\" , are sentences that share\nthe same beginning, but having completely\ndi\u000berent meanings.\nTo overcome this drawback, a Bidirea-\ntional LSTM (BiLSTM) network is propo-\nsed. This model connects two hidden layers\nof opposite directions to the same output.\nIn this way, the output layer can get infor-\nmation from the past (forward) and future\n(backward) states simultaneously. Therefore,\nBiLSTM can capture past (left) and future\n(right) contexts information. In our experi-\nmentation, we apply a BiLSTM layer with\n128 units in each direction. The number of\nunits was set according to previous literatu-\nre work, such as (Garain, 2019; Garain and\nMahata, 2019).\nThe network is initialized with word em-\nbeddings. To do this, the headlines are toke-\nnized and each token is represented as a vec-\ntor by using a pre-trained word embedding\nmodel such as Glove (Pennington, Socher,\nand Manning, 2014), developed by Google\n. In particular, we use glove.6B.200d, which\nwas trained with the Wikipedia 2014 + Giga-\nword 5 corpora and contains 6B tokens. The\ndimension of word vectors is 200.\nIn deep learning models, it is important\nnot to take the last result of each cell, but\nrather the best result of it. For this reason,\nafter the BiLSTM layer, a global maxpooling\nlayer downsamples the entire feature map to\na single value. This is done by checking each\nsequence of results provided by each LSTM\ncell and retaining only the maximum result.\nThis allows us to identify the strongest trait\nof a headline and highlight the tokens with\nthe most relevant information. For example,\nit could identify a word that is particularly\nfunny in the headline, which would be helpful\nfor sarcasm detection.\nAfter the global maxpooling layer, we add\ntwo fully connected layers, the \frst one with\n40 units and a dropout probability of 0.5,\nand the second one with 20 units and a dro-\npout probability of 0.5. The addition of fully\nconnected layers in deep learning models hasshown to improve the performance of the text\nclassi\fcation task (Kim, 2014). ReLU, a non-\nlinear activation function capable to capture\ncomplex relationships, is used as the activa-\ntion function. As it is sparsely activated (it\nprovides zero for all negative inputs, and the-\nreby, units often do not activate at all), it's\nmore likely that neurons are actually proces-\nsing meaningful aspects of the problem.\nFor the output layer, one single unit with a\nsigmoid function has been used that allows us\nto obtain the probability of an instance (text)\nbeing sarcasm. Therefore, one single probabi-\nlity is returned. For p >0;5, it would be con-\nsidered that the instance (text) belongs to the\npositive class (sarcastic), whereas if p <0;5\nthen it would be considered as the negative\nclass (non-sarcastic).\n3.2.2 Bidirectional Encoder\nRepresentations from\nTransformers (BERT)\nBERT has been repeatedly showing state-of-\nthe-art results in a wide range of tasks (Lee\net al., 2020; Zheng and Yang, 2019; Hakala\nand Pyysalo, 2019), however, it has hardly\nbeen used for sarcasm detection (Khatri and\nP, 2020). Thus, one of the main contributions\nof our study is the use of BERT (Devlin et\nal., 2019) to address the task of sarcasm de-\ntection from news headlines.\nBERT relies on a transformer to learn the\ncontextual relationships between the words in\na text. The purpose of BERT is to generate\na language representation model. Therefore,\nan encoder is needed in which an input to-\nkenizer is used. For the implementation, the\no\u000ecial tokenization script provided by BERT\nwas used, which is progressively being upda-\nted with the latest improvements.\nAfter the encoding process, the tokens,\nas well as the masks and segments are ob-\ntained. Each of these will correspond to an\ninput layer of the network. There are di\u000be-\nrent versions of the BERT model (Devlin et\nal., 2019): BERT-Base and BERT-Large. The\nlast one is an improved and computationally\nmore intensive version of the \frst model. This\nmodel has the following parameteres: L=24,\nH=1024, A=16, where L is the number of\nstacked encoders, H is the hidden size and A\nis the number of heads in the MultiHead At-\ntention layers. Therefore, we use the BERT-\nLarge model (bert enuncased L24 H1024 A-\n16), which was pre-trained for English on Wi-\nkipedia and Books Corpus. Inputs are \\un-\nSarcasm Detection with BERT\n19cased\", which means the text is converted to\nlower-case before the WordPiece tokenization\n(e.g., `John Doe' becomes `john doe'). Addi-\ntionally, all the accent markers are stripped.\nFor the training process, random input mas-\nking is applied independently to word pieces,\nas described in (Devlin et al., 2019). The to-\nkens, as well as the masks and segments ob-\ntained after the encoding process, correspond\nto the inputs of the BERT layer.\nThe output of the BERT layer is then pro-\ncessed by the tfoplayer strided slice layer,\nwhich performs the extraction of a straded\nslice of a tensor. Then, a Sigmoid layer with\none single unit receives the output of this la-\nyer and obtains a probability of an instance\nbeing sarcasm.\nIt often occurs in the \feld of machine lear-\nning, that an algorithm performs incredibly\nwell on the training dataset, but poorly on\nthe test set. This common phenomenon is ca-\nlled over\ftting. That is, the model has a high\nvariance, which makes it di\u000ecult to genera-\nlize well on new data. To avoid over\ftting,\nwe apply di\u000berent strategies such as dropout\nand early stopping. Dropout (Srivastava et\nal., 2014) is the standard regularizer for deep\nneural networks in NLP. This regularization\ntechnique involves setting a probability of\nkeeping certain nodes or not. A value between\n0 and 1 is speci\fed, which is the fraction of\nthe input units to drop. It has been shown,\nthat a dropout rate of 0.5 is e\u000bective in most\nscenarios (Kim, 2014). Therefore, there is a\nprobability of 50 % that a node will be remo-\nved from the network. This, ultimately, re-\nsults in a much simpler network that helps\nto prevent over\ftting. Early stopping was al-\nso used to prevent the model from over\ftting.\nEarly stopping is a method in which an ar-\nbitrarily large number of training epochs are\nspeci\fed, and the training process is stopped\nonce the model performance stops improving\non the validation dataset. Therefore, loss in\nthe validation dataset was monitored. A pa-\ntience of 3 was used, which means that the\nnetwork is allowed to continue training for\nup to an additional 3 epochs, after the point\nthat validation loss stopped improving. This\nallows us to get across \rat spots or \fnd some\nadditional improvement during the training\nprocess. Then, the last best model is the one\nthat is stored for posterior predictions.\nWe used the well-known API written in\nPython, Keras (2.3.1), for building and trai-ning deep learning models. Keras runs on top\nof the machine learning platform TensorFlow.\nWe also use a TensorFlow 2 (TF2) SavedMo-\ndel, which is the recommended way to share\npre-trained models and model pieces on Ten-\nsorFlow Hub. These models can be integrated\nwith Keras by making use of TensorFlow's\nhigh-level API.\nThe chosen optimizer is Adam, which is\nan adaptive learning rate optimizer introdu-\nced by Kingma and Ba (2015). The authors\nproposed the optimizer as \\a method for e\u000e-\ncient stochastic optimization that only requi-\nres \frst-order gradients with little memory\nrequirement. The method computes indivi-\ndual adaptive learning rates for di\u000berent pa-\nrameters from estimates of \frst and second\nmoments of the gradients\". For training the\nBi-LSTM model, we used the default para-\nmeters in Keras for Adam.However, for our\nBERT model, we use the default parameters,\nexcept for the learning rate, whose value was\nmodi\fed to 2e-6. The selected loss for both\nmodels is binary cross entropy, which is the\nstandard cross-entropy loss for binary classi-\n\fcation tasks.\nFor the LSTM model, the number of\nepochs is 25 (early stopping at the 5th) and\nthe batch size is 100. For the BERT model,\nThe number of epochs is 10 (early stopping\nat the 5th) and the batch size is 20.\nAs the environment to train and test the\nmodels, Google Colab was used with GPU\nactivated. Google Colab is a Google Research\nproduct that enables running Python code\non the browser for free with computational\nresources, such as GPU. The dataset as well\nas the code to replicate the experiments can\nbe found in the GitHub repository.3\n4 Results and Discussion\nTable 1 shows the results obtained with the\nBiLSTM and BERT models for sarcasm de-\ntection, displaying di\u000berent metrics. This al-\nso includes results of an SVM classi\fer and\na CNN model(Kim, 2014). They have been\nconsidered as the baseline systems, as they\nhave been widely and successfully used in sar-\ncasm detection (Khatri and P, 2020; Pt\u0013 a\u0014 cek,\nHabernal, and Hong, 2014; Amir et al., 2016).\nAs in the BiLSTM model, the CNN mo-\ndel was also initialized with pre-trained word\n3https://github.com/ElsaScola/Sarcasm-\nDetection-with-Natural-Language-Processing-and-\nDeep-Learning\nElsa Scola, Isabel Segura-Bedmar\n20embeddings from Glove(Pennington, Socher,\nand Manning, 2014) and used the adam opti-\nmizer for the training. The convolutional la-\nyer has 128 \flters of size 5. After this layer, a\nmaxpooling layer is added to select the most\nimportant features.\nAll the deep learning models provide bet-\nter results than the baseline system based on\nSVM. CNN and BiLSTM resulted in very si-\nmilar results, being BiLSTM slightly better.\nIt can be seen that BERT performed in gene-\nral better than the BiLSTM model. In par-\nticular, BERT provides an improvement of\n4.65 % in F1 score over LSTM. More speci\f-\ncally, BERT has surpassed the LSTM model\nin both precision (6.83 % improvement) and\nrecall (2.51 % improvement), which indicates\nthe e\u000bectiveness of this approach for sarcasm\ndetection. Despite using BiLSTM to capture\nbetter the context of the sentence, the atten-\ntion mechanism of BERT surpasses it in this\ntask. To the best of our knowledge, this is\nthe \frst study that applies BERT to detect\nsarcasm in texts that are not social media\nmessages.\nMoreover, we compare our results to tho-\nse presented in (Misra and Arora, 2019) sin-\nce both studies use the same dataset. Our\nBERT model shows an improvement of \u0018\n1;7 % in the results, compared to the hybrid\nnetwork architecture proposed by (Misra and\nArora, 2019). It also outperforms the systems\ndescribed in Section 2, although our results\nare not comparable to what have been repor-\nted in systems which focused on social me-\ndia texts. BERT was also used in (Xu and\nXu, 2019; Khatri and P, 2020), but their re-\nsults were much worse than those obtained by\nour BERT model. Like BERT, our CNN and\nBiLSTM models also have signi\fcant better\nperformance than those previous deep lear-\nning systems trained and tested on social\nmedia texts (Amir et al., 2016; Ghosh, Ri-\nchard Fabbri, and Muresan, 2017; Xu and\nXu, 2019; Khatri and P, 2020). This agrees\nwith the fact of social media texts are cha-\nracterized by a lack of context, which leads\nto high ambiguity and makes the task of de-\ntecting sarcasm even more di\u000ecult.\nWe have studied a small sample of false\npositives and negatives produced by the mo-\ndels to identify their major weak points in\nwhich these models fail. Table 2 shows so-\nme headlines that were wrongly classi\fed as\nsarcastic by both models. Having a look in-to these false positives, it can be hypothesi-\nzed that both models seem to consider tho-\nse instances containing humorous or very su-\nrrealistic sentences as sarcasm. For example,\nboth models agree that the sentence \\Man\napparently opens beer with butt, inspires bar-\ntenders everywhere\" is sarcastic. This could\nbe caused by the absurdity of the sentence,\nwhich might result in the models conside-\nring it as a joke. The same situation happens\nfor sentences like \\Farting teen sparks \fght.\",\nwhich can be interpreted as jokes. This is due\nto the lack of knowledge of the model on the\ncontext. Paying attention to another senten-\nce, which both models considered sarcastic:\n\\Passport robot tells man of Asian descent\nhis eyes are too closed.\", it can be seen that\nthe models might learn to see the humor in si-\ntuations that are not necessarily humorous or\nthat might oppress certain collectives, as the\nAsian community in this case. This is an issue\nthat goes beyond the scope of this study, ho-\nwever, it is interesting to observe and analyze\nthis kind of phenomenon. The data that is gi-\nven to a model to learn humor can eventually\nlead the model to reproduce the same racist\nstereotypes that we see in society. Thus, spe-\ncial care should be put in curating the trai-\nning data and reach a consensus of what type\nof humor is funny and which is harmful or of-\nfensive.\nIn the current study, BERT was able\nto classify correctly various instances that\nBiLSTM could not. While BiLSTM failed\nto classify the headline \\Obama is like that\nreally great neighbour who's moving out.\" as\na sarcastic one, BERT was able to recog-\nnise that there was humor in the sentence\nbut was not meant to be sarcastic. That dif-\nferentiation is key in order to obtain more\naccurate results in the task. On the other\nhand, the headline \\Jailed for being too poor\"\nwas correctly classi\fed as non-sarcastic by\nBiLSTM, but wrongly as sarcastic by BERT.\nWe now review some of the False Nega-\ntives. For example, for the headline \\Angeli-\nna Jolie coming for your baby\" , both models\nagree that this is not sarcastic, even if it is\nindeed sarcastic. This is probably the lack of\ncontext handling in the two models on who\nAngelina Jolie is and what is known for. The\nsame situation happens with the sarcastic\nheadline \\Police repeatedly shoot Tim Cook\nafter mistaking Iphone for gun\" , which was\nwrongly classi\fed as non-sarcastic by both\nSarcasm Detection with BERT\n21Model Loss Acc. P R F1\nSVM 0.79 0.81 0.75 0.78\nCNN 0.4434 0.8654 0.8547 0.8639 0.8593\nLSTM 0.4391 0.8680 0.8561 0.8687 0.8623\nBERT 2.9443 0.9147 0.9244 0.8938 0.9088\nTabla 1: Results over the test dataset.\nTrump suggests Iran brought deadly terrorist attacks upon itself.\nFarting teen sparks \fght.\nMan apparently opens beer with butt, inspires bartenders everywhere.\nPassport robot tells man of Asian descent his eyes are too closed.\nDog gives priceless reaction when owner pretends to faint.\nTabla 2: Examples of false positives for BiLSTM and BERT models.\nmodels. It is needed to know that Tim Co-\nok is the CEO of Apple to understand the\nsentence. It could be helpful in identifying\nimportant subjects in sentences and getting\nsome context from them, in order to help\nthe models make more accurate predictions.\nWhile BiLSTM was able to detect sarcasm in\nthe headline \\Je\u000b Bezos named Amazon em-\nployee of the month\" , BERT classi\fed it as\nnon-sarcastic. Again, this may be due to the\nlack of context in the model.\nThere are some headlines particularly\nhard to identify as they could be real facts,\nindependently of the context the model is gi-\nven. For example, the headline \\Visit to Goo-\ngle Earth reveals house is on \fre\" , even if the\nheadline was given to a human that is aware\nof what Google Earth is and had the context\nto understand the headline, the person could\nthink is a real sentence as it is a possibility,\nand therefore, the context by itself, would not\nplay a big role for this type of sentences.\nIf the models were able to know the \\ab-\nsolute truth\" around the proposed head-\nlines, they could easily classify them as\nsarcastic/non-sarcastic. However, providing\ncontext to the model is a challenging task for\nwhich some approaches were proposed in so-\ncial network data (see Section 2). Neverthe-\nless, this is a challenging task, which we plan\nto address in future work.\n5 Conclusions\nTo the best of our knowledge, this is the \frst\nstudy that applies BERT to detect sarcasm\nin texts that are not social media texts. Our\nexperiments show that BERT achieves bet-ter performance than BiLSTM. Our BERT-\nbased approach also overcomes the hybrid\nneural architecture (based on Bi-LSTM) des-\ncribed in (Misra and Arora, 2019), which was\nalso evaluated using the news headlines da-\ntaset.\nAs future work, we plan to extend the eva-\nluation with other sarcasm datasets to mea-\nsure the results of our models on di\u000berent ty-\npes of texts. We also plan to study how to\nencode knowledge of the world in our deep\nlearning models, which will help us to obtain\na correct interpretation of any text.\nFurthermore, we will explore the results\nof multimodal sarcasm detection, by accom-\npanying texts with audio to also contribute\nto sarcasm recognition in the evolving \feld of\nvirtual assistants. Intonation in the speech of\nthe user could be indicative of sarcasm. This\ntype of research is still in the early stages,\nhowever, a few datasets have been presented\nin this direction (Castro et al., 2019).\nAcknowledgments\nThis work has been supported by the Madrid\nGovernment (Comunidad de Madrid) un-\nder the Multiannual Agreement with UC3M\nin the line of \\Fostering Young Doctors\nResearch\"(NLP4RARE-CM-UC3M), as well\nas in the line of \\Excellence of University\nProfessors\"(EPUC3M17), and in the context\nof the V PRICIT (Regional Programme of\nResearch and Technological Innovation).\nBibliograf\u0013 \u0010a\nAmir, S., B. C. Wallace, H. Lyu, P. Carvalho,\nand M. J. Silva. 2016. Modelling con-\nElsa Scola, Isabel Segura-Bedmar\n22text with user embeddings for sarcasm de-\ntection in social media. In Proceedings of\nThe 20th SIGNLL Conference on Compu-\ntational Natural Language Learning, pages\n167{177, Berlin, Germany.\nApidianaki, M., S. M. Mohammad, J. May,\nE. Shutova, S. Bethard, and M. Carpuat,\neditors. 2018. Proceedings of The 12th\nInternational Workshop on Semantic Eva-\nluation, New Orleans, Louisiana.\nBamman, D. and N. A. Smith. 2015. Con-\ntextualized sarcasm detection on twitter.\nInProceedins of the 9TH International\nAAAI Conference On Web And Social\nMedia , Oxford, UK.\nCai, Y., H. Cai, and X. Wan. 2019. Multi-\nmodal sarcasm detection in twitter with\nhierarchical fusion model. In Proceedings\nof the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages\n2506{2515, Florence, Italy.\nCapelli, C. A., N. Nakagawa, and C. M. Mad-\nden. 1990. How children understand sar-\ncasm: The role of context and intonation.\nChild Development, 61(6):1824{1841.\nCastro, S., D. Hazarika, V. P\u0013 erez-Rosas,\nR. Zimmermann, R. Mihalcea, and S. Po-\nria. 2019. Towards multimodal sarcasm\ndetection (an Obviously perfect paper).\nInProceedings of the 57th Annual Meeting\nof the Association for Computational Lin-\nguistics, pages 4619{4629, Florence, Italy.\nCortes, C. and V. Vapnik. 1995. Support-\nvector networks. Machine learning ,\n20(3):273{297.\nDevlin, J., M.-W. Chang, K. Lee, and K. Tou-\ntanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language\nunderstanding. In Proceedings of the 2019\nConference of the North American Chap-\nter of the Association for Computatio-\nnal Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, pages 4171{\n4186, Minneapolis, USA.\nEke, C. I., A. A. Norman, L. Shuib, and H. F.\nNweke. 2020. Sarcasm identi\fcation in\ntextual data: systematic review, research\nchallenges and open directions. Arti\fcial\nIntelligence Review , 53(6):4215{4258.\nGarain, A. 2019. Humor analysis based\non human annotation(haha)-2019: Humoranalysis at tweet level using deep lear-\nning. In Proceedings of the Iberian Lan-\nguages Evaluation Forum co-located with\n35th Conference of the Spanish Society for\nNatural Language Processing, IberLEF-\nSEPLN 2019 , volume 2421, pages 191{\n196, Bilbao, Spain.\nGarain, A. and S. K. Mahata. 2019. Sen-\ntiment analysis at SEPLN (TASS)-2019:\nsentiment analysis at tweet level using\ndeep learning. In Proceedings of the\nIberian Languages Evaluation Forum co-\nlocated with 35th Conference of the Spa-\nnish Society for Natural Language Pro-\ncessing, IberLEF@SEPLN 2019, volume\n2421, pages 611{617, Bilbao, Spain.\nGhosh, A. and T. Veale. 2016. Fracking\nsarcasm using neural network. In Procee-\ndings of the 7th Workshop on Computatio-\nnal Approaches to Subjectivity, Sentiment\nand Social Media Analysis , pages 161{169,\nSan Diego, California.\nGhosh, D., A. Richard Fabbri, and S. Mure-\nsan. 2017. The role of conversation con-\ntext for sarcasm detection in online inter-\nactions. In Proceedings of the 18th Annual\nSIGdial Meeting on Discourse and Dia-\nlogue , pages 186{196, Saarbr\u007f ucken, Ger-\nmany.\nHakala, K. and S. Pyysalo. 2019. Biomedi-\ncal named entity recognition with multi-\nlingual BERT. In Proceedings of The 5th\nWorkshop on BioNLP Open Shared Tasks ,\npages 56{61, Hong Kong, China.\nHernandez Farias, D., V. Patti, and P. Rosso.\n2016. Irony detection in twitter: The role\nof a\u000bective content. ACM Transactions\non Internet Technology , 16:1{24.\nHochreiter, S. and J. Schmidhuber. 1997.\nLong short-term memory. Neural compu-\ntation, 9:1735{80, 12.\nJoshi, A., P. Bhattacharyya, M. Carman,\nJ. Saraswati, and R. Shukla. 2016.\nHow do cultural di\u000berences impact the\nquality of sarcasm annotation?: A case\nstudy of Indian annotators and Ameri-\ncan text. In Proceedings of the 10th\nSIGHUM Workshop on Language Techno-\nlogy for Cultural Heritage, Social Sciences,\nand Humanities, pages 95{99, Berlin, Ger-\nmany.\nSarcasm Detection with BERT\n23Joulin, A., E. Grave, P. Bojanowski, M. Dou-\nze, H. J\u0013 egou, and T. Mikolov. 2017. Fast-\ntext. zip: Compressing text classi\fcation\nmodels. In 5th International Conference\non Learning Representations (ICLR).\nKhatri, A. and P. P. 2020. Sarcasm detection\nin tweets with bert and glove embeddings.\nInProceedings of the Second Workshop on\nFigurative Language Processing, pages 56{\n60.\nKhodak, M., N. Saunshi, and K. Vodrahalli.\n2018. A large self-annotated corpus for\nsarcasm. In Proceedings of the 11th Inter-\nnational Conference on Language Resour-\nces and Evaluation (LREC 2018) , Miya-\nzaki, Japan.\nKim, Y. 2014. Convolutional neural net-\nworks for sentence classi\fcation. In\nProceedings of the 2014 Conference on\nEmpirical Methods in Natural Language\nProcessing (EMNLP), pages 1746{1751,\nDoha, Qatar. Association for Computatio-\nnal Linguistics.\nKingma, D. P. and J. Ba. 2015. Adam:\nA method for stochastic optimization. In\n3rd International Conference on Learning\nRepresentations, ICLR 2015, Conference\nTrack Proceedings, San Diego, CA, USA.\nLee, J., W. Yoon, S. Kim, D. Kim, S. Kim,\nC. H. So, and J. Kang. 2020. Biobert: a\npre-trained biomedical language represen-\ntation model for biomedical text mining.\nBioinformatics, 36(4):1234{1240.\nLiebrecht, C., F. Kunneman, and A. van den\nBosch. 2013. The perfect solution for de-\ntecting sarcasm in tweets #not. In Procee-\ndings of the 4th Workshop on Computatio-\nnal Approaches to Subjectivity, Sentiment\nand Social Media Analysis, pages 29{37,\nAtlanta, Georgia.\nLittlestone, N. 1988. Learning quickly\nwhen irrelevant attributes abound: A\nnew linear-threshold algorithm. Machine\nLearning, 2(4):285{318.\nMikolov, T., I. Sutskever, K. Chen, G. S.\nCorrado, and J. Dean. 2013. Distribu-\nted representations of words and phrases\nand their compositionality. In Advances\nin neural information processing systems ,\npages 3111{3119.Misra, R. and P. Arora. 2019. Sarcasm de-\ntection using hybrid neural network. ar-\nXiv preprint arXiv:1908.07414.\nNigam, K. 1999. Using maximum entropy\nfor text classi\fcation. In Proceedings of\nIJCAI-99 Workshop on Machine Learning\nfor Information Filtering, pages 61{67.\nOprea, S. and W. Magdy. 2020. iSarcasm:\nA dataset of intended sarcasm. In Pro-\nceedings of the 58th Annual Meeting of\nthe Association for Computational Lin-\nguistics, pages 1279{1289, Online.\nOraby, S., V. Harrison, L. Reed, E. Hernan-\ndez, E. Rilo\u000b, and M. Walker. 2016a.\nCreating and characterizing a diverse cor-\npus of sarcasm in dialogue. In Proceedings\nof the 17th Annual Meeting of the Special\nInterest Group on Discourse and Dialo-\ngue, pages 31{41, Los Angeles, USA.\nOraby, S., V. Harrison, L. Reed, E. Hernan-\ndez, E. Rilo\u000b, and M. A. Walker. 2016b.\nCreating and characterizing a diverse cor-\npus of sarcasm in dialogue. In Procee-\ndings of the SIGDIAL 2016 Conference,\nThe 17th Annual Meeting of the Special\nInterest Group on Discourse and Dialo-\ngue, 13-15 September 2016, Los Angeles,\nCA, USA, pages 31{41. The Association\nfor Computer Linguistics.\nOrtega-Bueno, R., F. Rangel,\nD. Hern\u0013 andez Far\u0010as, P. Rosso,\nM. Montes-y G\u0013 omez, and J. E. Me-\ndina Pagola. 2019. Overview of the task\non irony detection in spanish variants. In\nProceedings of the Iberian Languages Eva-\nluation Forum (IberLEF 2019) , Bilbao,\nSpain.\nPennington, J., R. Socher, and C. D. Man-\nning. 2014. Glove: Global vectors for\nword representation. In Proceedings of the\n2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) ,\npages 1532{1543, Doha, Qatar.\nPt\u0013 a\u0014 cek, T., I. Habernal, and J. Hong. 2014.\nSarcasm detection on Czech and English\nTwitter. In Proceedings of COLING\n2014, the 25th International Conference\non Computational Linguistics: Technical\nPapers , pages 213{223, Dublin, Ireland.\nRilo\u000b, E., A. Qadir, P. Surve, L. De Silva,\nN. Gilbert, and R. Huang. 2013. Sar-\ncasm as contrast between a positive sen-\nElsa Scola, Isabel Segura-Bedmar\n24timent and negative situation. In Pro-\nceedings of the 2013 Conference on Em-\npirical Methods in Natural Language Pro-\ncessing (EMNLP) , pages 704{714, Seattle,\nWashington.\nRockwell, P. and E. M. Theriot. 2001. Cultu-\nre, gender, and gender mix in encoders of\nsarcasm: A self-assessment analysis. Com-\nmunication Research Reports , 18(1):44{\n52.\nSrivastava, N., G. Hinton, A. Krizhevsky,\nI. Sutskever, and R. Salakhutdinov. 2014.\nDropout: A simple way to prevent neu-\nral networks from over\ftting. Journal\nof Machine Learning Research, 15:1929{\n1958, 06.\nWang, J.-H., T.-W. Liu, X. Luo, and\nL. Wang. 2018. An lstm approach to short\ntext sentiment classi\fcation with word\nembeddings. In Proceedings of the 30th\nConference on Computational Linguistics\nand Speech Processing (ROCLING 2018) ,\npages 214{223, Hsinchu, Taiwan.\nXu, L. and V. Xu. 2019. Pro-\nject report: Sarcasm detection.\nhttps://web.stanford.edu/class/archive/\ncs/cs224n/cs224n.1194/project.html.\nOnline; accessed 8 July 2021.\nZheng, S. and M. Yang. 2019. A new method\nof improving bert for text classi\fcation.\nInProceedings of International Conferen-\nce on Intelligent Science and Big Data En-\ngineering , pages 442{452, Nanjing, China.\nZhou, P., Z. Qi, S. Zheng, J. Xu, H. Bao,\nand B. Xu. 2016. Text classi\fcation im-\nproved by integrating bidirectional LSTM\nwith two-dimensional max pooling. In\nProceedings of COLING 2016, the 26th In-\nternational Conference on Computational\nLinguistics: Technical Papers , pages 3485{\n3495, Osaka, Japan.\nSarcasm Detection with BERT\n25Impact of Text Length for Information Retrieval\nTasks based on Probabilistic Topics\nIn\ruencia de la Longitud del Texto en Tareas de Recuperaci\u0013 on\nde Informaci\u0013 on mediante T\u0013 opicos Probabil\u0013 \u0010sticos\nCarlos Badenes-Olmedo, Borja Lozano- \u0013Alvarez, Oscar Corcho\nOntology Engineering Group, Universidad Polit\u0013 ecnica de Madrid, Spain\nfcbadenes, ocorchog@\f.upm.es\nfborja.lozano.alvarezg@alumnos.upm.es\nAbstract: Information retrieval has traditionally been approached using vector\nmodels to describe texts. In large document collections, these models need to reduce\nthe dimensions of the vectors to make the operations manageable without compro-\nmising their performance. Probabilistic topic models (PTM) propose smaller vector\nspaces. Words are organized into topics and documents are related to each other\nfrom their topic distributions. As in many other AI techniques, the texts used to\ntrain the models have an impact on their performance. Particularly, we are inter-\nested on the impact that length of texts may have to create PTM. We have studied\nhow it in\ruences to semantically relate multilingual documents and to capture the\nknowledge derived from their relationships. The results suggest that the most ade-\nquate texts to train PTM should be of equal or greater length than those used to\nmake inferences later and documents should be related by hierarchy-based similarity\nmetrics at large-scale.\nKeywords: probabilistic topics, text similarity, hierarchical topics, document re-\ntrieval.\nResumen: La recuperaci\u0013 on de informaci\u0013 on ha utilizado tradicionalmente modelos\nvectoriales para describir los textos. A gran escala, estos modelos necesitan reducir\nlas dimensiones de los vectores para que las operaciones sean manejables sin com-\nprometer su rendimiento. Los modelos probabil\u0013 \u0010sticos de t\u0013 opicos (MPT) proponen\nespacios vectoriales m\u0013 as peque~ nos. Las palabras se organizan en t\u0013 opicos y los doc-\numentos se relacionan entre s\u0013 \u0010 a partir de sus distribuciones de t\u0013 opicos. Como en\nmuchas otras t\u0013 ecnicas de IA, los textos utilizados para entrenar los modelos in\ruyen\nen su rendimiento. En particular, nos interesa el impacto de la longitud de los textos\nal crear MPT. Hemos estudiado c\u0013 omo in\ruye al relacionar sem\u0013 anticamente docu-\nmentos multiling\u007f ues y al capturar el conocimiento derivado de sus relaciones. Los\nresultados sugieren que los textos m\u0013 as adecuados deben ser de igual o mayor longi-\ntud que los utilizados para hacer inferencias posteriormente y las relaciones deben\nbasarse en m\u0013 etricas de similitud jer\u0013 arquicas.\nPalabras clave: topicos probabil\u0013 \u0010sticos, semejanza de textos, jerarqu\u0013 \u0010a de t\u0013 opicos,\nrecuperaci\u0013 on de documentos.\n1 Introduction\nProbabilistic Topic Models (PTM) (Hof-\nmann, 2001) (Blei, Ng, and Jordan, 2003)\nare statistical methods based on bag-of-words\nthat analyze the words of the original texts\nto discover the themes that run through\nthem, how those themes are connected to\neach other, or how they change over time.\nPTM do not require any prior annotations\nor labeling of the documents. The topicsemerge, as hidden structures, from the anal-\nysis of the original texts. These structures\nare topic distributions, per-document topic\ndistributions or per-document per-word topic\nassignments. In turn, a topic is a distribu-\ntion over terms that is biased around those\nwords associated to a single theme. Figure 1\nshows some topics that have emerged when\ncreating a topic model with the collection of\nWikipedia articles to better understand what\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 27-36\nrecibido 30-04-2021 revisado 08-06-2021 aceptado 09-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-2\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalFigure 1: Topics discovered from the English edi-\ntion of Wikipedia.\ntopic means. Each topic is described, this ex-\nample, by its six most representative words,\ni.e., those words most present in the docu-\nments that mainly contain each topic.\nBag-of-words approach avoids the restric-\ntion of word sequences to relate documents\nbased on the use of the same words. PTMs\nrepresent texts by probability distributions\nover a vocabulary created from the whole cor-\npus. Each topic establishes di\u000berent levels\nof relevance for each word, and documents\nare described based on the presence of each\ntopic in their texts. Latent Semantic Index-\ning(Deerwester et al., 1990) (LSI) initially\nreduced the TF-IDF model using singular\nvalue decomposition to \fnd the linear sub-\nspace that capture most of the information\nin a collection. LSI has shown to yield high\ncorrelation with human perception of simi-\nlarity (Jung, Ruthru\u000b, and Goldsmith, 2017)\nand some authors argued that features de-\nrived from LSI are able to capture some basic\nlinguistic notions such as synonymy and pol-\nysemy. Probabilistic LSI (pLSI) (Hofmann,\n1999) improved LSI by introducing the con-\ncept of topic as a multinomial distribution\nover the vocabulary of a collection. In pLSI\neach document is described with a vector of\ntopic proportions, capturing the idea that\nthere is a \fxed number of common themes ex-\nhibited in a di\u000berent proportion by the docu-\nments in a collection. But it was not able\nto provide a generative process that infers\ntopic proportions for documents not used in\nthe training collection. Latent Dirichlet Al-\nlocation (LDA) (Blei, Ng, and Jordan, 2003)\nsolved the inferring problem of pLSI by plac-\ning a Dirichlet distribution over the topic pro-\nportions for the documents and allowing for\nthe discovery of the themes running through\nthe documents. It is considered the simplestgenerative Probabilistic Topic Model. LDA\nis one of the most widely-used methods when\nprocessing texts using NLP techniques and\nits functionality has been extended to multi-\nple domains (Jelodar et al., 2017).\nTopic models are trained with large cor-\npora of texts, which are generally from the\nsame domain for which we want to make in-\nferences. Documents can be related based on\ntheir topics, instead of sequences of words.\nTopic-based representations bring a lot of\npotential when applied over di\u000berent infor-\nmation retrieval (IR) tasks, as evidenced by\nworks in di\u000berent domains such as health\n(Nzali et al., 2017), legal (O'Neill et al.,\n2017), news (He, Li, and Wu, 2017), and hy-\nbrid proposals combining topic models and\nword embedding (Dieng, Ruiz, and Blei,\n2020). However, the ability of topics to ex-\npress the inherent knowledge on which the re-\nlationships between documents are built has\nnot been yet analyzed from the texts used\nto train the models. As far as we know,\nthere are no studies that evaluate how the\ntext length in\ruences on the probabilistic\nmodel created to represent and relate seman-\ntically documents from their topic distribu-\ntions by means of similarity functions. In\nthis work we have studied the impact that\nthe text length has, since it determines the\nspace where words can co-occur, to semanti-\ncally relate documents described in a proba-\nbilistic topic space.\nThis paper is structured as follows: Sec-\ntion 2 presents the state-of-the-art metrics\nused to compare documents represented by\nprobabilistic topics. The methodology that\nwe have used for the experimentation and\nhow the evaluation was performed is de-\nscribed in section 3. Finally, the results are\npresented and discussed in section 4, along\nwith the \fnal remarks and future work in sec-\ntion 5.\n2 Text Similarity based on\nProbabilistic Topics\nSome works have evaluated LDA models to\nsemantically relate documents. In (Syed\nand Spruit, 2017), the quality of the topics\nwas measured based on the abstract or the\nfull text of scienti\fc articles. It concluded\nthat full-text was less prone to noisy top-\nics in small datasets. Regarding the ability\nto relate similar papers, (Badenes-Olmedo,\nRedondo-Garc\u0013 \u0010a, and Corcho, 2017b) ana-\nCarlos Badenes-Olmedo, Borja Lozano-\u00c1lvarez, Oscar Corcho\n28lyzed similarity relations based on topic dis-\ntributions when using only sections of scien-\nti\fc papers to describe them (i.e. abstract,\nmethod, background, etc). It concluded that\nthe background section allows relating them\nin a more accurate way than using the ab-\nstract.\nDistance measures typically used in prob-\nabilistic topic models are not based on Eu-\nclidean spaces, e.g. cosine-similarity, but\nconsider the simplex space created by the\nDirichlet distribution to support the compar-\nisons.\n2.1 Density-based Similarity\nmetrics\nDocuments are represented as vectors of\ntopic distributions in the simplex space cre-\nated by probabilistic topic models, and dis-\ntance functions must take into account two\nconsiderations, namely none-negativity and\nsum-equal-one (Mao et al., 2017) to ensure\nthat the document representation is used\nas a probability distribution. Metrics such\nas Jensen-Shannon Divergence (JSD) (Eq.1)\n(also known as symmetric relative entropy)\nand Hellinger (He) distance (Eq.2) are com-\nmonly used in these spaces (Rus, Niraula,\nand Banjade, 2013):\nJSD (Q; D ) =X\ni=1qilog2qi\nqi+di+X\ni=1dilog2di\nqi+di(1)\nHe(Q; D ) =X\ni=1\u0010p\nq(xi)\u0000p\nd(xi)\u00112\n(2)\nHowever these metrics lack the interpreta-\ntive capacity o\u000bered by topics when compar-\ning documents. Furthermore, in real-world\nenvironments where computational cost has\nto be considered those metrics do not scale\nwell as they require complex operations be-\ntween all pairs of documents. To address\nboth issues, a set of metrics based on hierar-\nchical representation of topics were proposed,\nas described next.\n2.2 Hierarchy-based Similarity\nmetrics\nSimilarity metrics based on density functions\npresent four major problems when compar-\ning documents (Badenes-Olmedo, Redondo-\nGarc\u0013 \u0010a, and Corcho, 2019a):\n\u2022Pairwise computation of document sim-\nilarity is costly and grows linearly withthe size of the corpus.\n\u2022Simplex metrics do not o\u000ber a semantic\nexplanation for the similarity obtained.\n\u2022Documents that do not share any acti-\nvated topics (i.e. the bigger components\nof the topic proportion vector) can still\nhave high similarity due to the sum of\ndistances between the less representative\ntopics (i.e. the smaller components of\nthe topic proportion vector).\n\u2022These metrics cannot be extended to\nsupport semantic restrictions to enrich\nqueries in the corpus.\nTo alleviate these issues, a new approach\nto compare topic distributions was pro-\nposed (Badenes-Olmedo, Redondo-Garc\u0013 \u0010a,\nand Corcho, 2019a) that reduces the topic\ndistributions vector to a hierarchical set-type\nvector. Documents are described by sets of\ntopics grouped into three relevance levels.\nThey are compared using the Jaccard index,\na metric that compares how similar two sets\nare by how many objects they share. In our\nexperiments, a linear distribution of weights\n(i.ewi= 3\u0000i) has been used to add up the\nhierarchy levels (Eq.3):\nWJL (HA; HB) =LX\ni=0LX\nj=0wiwj\u0003jHA\ni\\HB\njj\njHA\ni[HB\njj(3)\n3 Experiments\nOur study is aimed at evaluating how text\nlength in\ruences the probabilistic topics that\nare created from a document corpus, and how\nthis in\ruences the calculations and results of\nstate-of-the-art similarity metrics to seman-\ntically relate documents. Several document\nretrieval tasks were designed from annotated\ndocument collections. The study considers\nboth multilingual and monolingual scenar-\nios. From a collection of documents man-\nually tagged with categories, we train topic\n(a) Before text processing.\n (b) After text processing.\nFigure 2: bag-of-words size.\nImpact of Text Length for Information Retrieval Tasks based on Probabilistic Topics\n29models to create representation spaces where\ntexts are projected and compared to identify\nsimilar documents. We divide the original\ncorpus into several datasets by grouping doc-\numents with similar length to measure how\ntext length in\ruences the relations obtained.\nA probabilistic topic model is trained for each\ndataset, and is used to make inferences across\nall datasets. In this way we evaluate the\nperformance of topic models to relate simi-\nlar documents when the length of the texts\nused in training and inferences vary (Fig.3).\n3.1 Corpora\nA multilingual corpora was created from the\nEnglish and the Spanish editions of the JRC-\nAcquis (Steinberger et al., 2006) and DGT-\nAcquis (Steinberger et al., 2014) datasets. It\ncontains 135.836 legislative texts of the Eu-\nropean Union (EU) from the 1950s to 2011.\nThe length of the texts is calculated using\nwhite spaces and punctuation marks to dis-\ntinguish terms. More advanced techniques\nbased on phrases or entities could have been\nused, but we want to avoid the noise they\nmight introduce in their inferences. The me-\ndian length of the texts, since Acquis is a\nparallel corpus, is 152 terms for English texts\nand 150 terms for Spanish texts with a high\nvariance from less than 7 terms in the short-\nest texts to more than 1.300 terms in the\nlongest texts (Table 1). The distribution of\ndocuments according to their number of to-\nkens is shown in Figure 3.\nDocuments are annotated with the Eu-\nroVoc taxonomy, which follows the Interna-\ntional Standards for processing the documen-\ntary information of the EU institutions ( ISO\n2788-1986 and ISO 5964-1985). It is a multi-\nlingual thesaurus with 7,193 concepts/labels\nfrom 21 domain areas such as politics, inter-\nnational relations, law, economics, etc. In\nour study we used the 452 root concepts iden-\nti\fed in (Badenes-Olmedo, Redondo-Garc\u0013 \u0010a,\nEnglish Spanish\n#Do cuments 67781 68055\n#TermsMedian 152 150\nMean 204.13 203.54\nVariance 36080.66 37074.97\nMin 7 6\nMax 1360 1411\nTable 1: Multilingual corpora created from the\nJRC and DGT Acquis datasets.and Corcho, 2019b) to categorize documents.\nIn this way we ensure independence between\nprobabilistic topics when creating the mod-\nels from these categories. This is a restric-\ntion imposed by topic models as they are de-\nscribed by density functions.\n3.2 Text Pre-Processing and Topic\nModel Training\nTexts were pre-processed to remove common\nstopwords and domain-speci\fc ones based on\ntopic distributions. Rare terms with ex-\ntremely low total document frequency were\nalso removed. Words were lemmatized and\ntransformed to lower-case. A lower and an\nupper limit on the number of words were de-\n\fned to homogenize the size of bag-of-words.\nThese bounds are based on the interquartile\nrange (Fig. 2) and are commonly applied\nin the state-of-the-art (Scho\feld, Magnusson,\nand Mimno, 2017).\nTopic models were created using the\nGibbs sampling implementation from li-\nbrAIry (Badenes-Olmedo, Redondo-Garc\u0013 \u0010a,\nand Corcho, 2017a) system. By default it\nonly uses verbs, nouns, proper nouns andad-\njectives to create the models. The Dirichlet\npriors\u000b= 0:1 and\f= 0:01 were set follow-\ning the conclusions from (Hu et al., 2014).\nModels with 50, 100, 300 and 500 topics were\nconsidered to analyze their ability to cap-\nture the knowledge needed to accurately re-\nlate similar documents.\n3.3 Experimental Scenarios\nOur task consists in searching for related doc-\numents to a given text, using representations\nbased on di\u000berent trained probabilistic top-\nics, and comparing this result with the set of\nrelated documents, based on their overlap in\nterms of Eurovoc categories (Figure 3). The\nability of probabilistic topics to capture the\ninherent knowledge of the corpus and allow\ndocuments to be related to each other from\ntheir vector representations is evaluated by\ncomparing the relationships obtained by this\nprocess with those obtained from the manual\nlabels they share.\nEach document in the original corpus\nis manually annotated with EuroVoc cate-\ngories. The original set of categories was\nreduced to 452 independently identi\fed ar-\neas. Documents that share the same cate-\ngories are considered to be semantically re-\nlated and serve as a ground-truth to validate\nCarlos Badenes-Olmedo, Borja Lozano-\u00c1lvarez, Oscar Corcho\n30Figure 3: Preparation of experiments by creating topic models for each subset of the original corpus and\ncross-validated with EuroVoc thesaurus.\nthe unsupervised approach based on proba-\nbilistic topics.\nThree evaluation scenarios were created,\neach of them dividing the initial corpora into\nsubsets of the same size with texts of simi-\nlar length. We have considered 3, 6 and 9\ndivisions of the original corpus in order to\nhave enough detail when analyzing the re-\nsults. The higher the number of divisions,\nthe greater the detail but the lower the num-\nber of documents in each subset and this may\na\u000bect the quality of the trained topic model.\nWith these three scenarios we have an ade-\nquate balance between detail and quality of\ntopic models.\nDocuments were pre-processed to \flter\nout verbs, proper names, nouns and adjec-\ntives (i.e tokens) and to create bag-of-words\nwith them. The inter-quartile index (\u00b11.5)\nwas taken into account to discard too short\nor long texts ( see Table 2).\nData was divided into a sample subset\n(5%) for testing and the rest (95%) was used\nto train a topic model. The test set was de-\nscribed by topic distributions based on the\ntrained model. State-of-the-art distance met-\nrics were used to compare them and to obtain\nthe most similar ones. The top10 most simi-\nlar documents are evaluated in terms of Mean\nAverage Precision (MAP) with the top10 ob-\ntained when comparing them from the Eu-\nroVoc labels. MAP allows evaluating on av-\nerage how good the results of a query are by\ntaking the mean of all average precisions for\nthe \frst 10 results when comparing a list of\nretrieved documents and the ground truth.#Divisions Partition #Do csMedian Mean\n31 22,594 43 49.75\n2 22,593 152 152.76\n3 22,594 337 409.88\n61 11,297 20 20.74\n2 11,297 84 78.75\n3 11,297 129 128.42\n4 11,296 175 177.11\n5 11,297 255 261.76\n6 11,297 517 558.03\n91 7,532 13 14.77\n2 7,531 43 45.49\n3 7,531 89 88.98\n4 7,531 121 120.56\n5 7,531 152 151.96\n6 7,531 185 185.76\n7 7,531 236 238.73\n8 7,531 337 346.14\n9 7,532 619 644.73\nTable 2: tokens per partition and division.\n4 Results\nAs expected, models trained with small doc-\numents perform worse than those with large\ndocuments specially for the supervised splits\nwhere groups had the same number of doc-\numents(see tables A.1 to A.6). The small-\nest document group still had the worst per-\nformance in the unsupervised split, but, due\nto groups not having the same number of\ndocuments, the biggest document group usu-\nally had the second worst P@k, specially for\nlarger texts.\nImpact of Text Length for Information Retrieval Tasks based on Probabilistic Topics\n314.1 Categorization based on\nTopics\nA topic-based similarity to all documents in\ncorpus is calculated according to density and\nhierarchy-based metrics (described in Section\n2) for each test document.\nSince several topic models have been cre-\nated for each dataset (with 50,100,300 and\n500 topics), the precision results for each\nmodel were averaged following the mean av-\nerage precision (MAP) metric. Thus, results\nre\rect the capacity of each topic model to au-\ntomatically capture the knowledge required\nto relate documents from their texts. As\nshown in Table 3, the use of probabilistic top-\nics to automatically relate documents o\u000bers\na performance with an accuracy above 0.8.\nThis performance is slightly higher for En-\nglish texts than for Spanish texts. We sus-\npect that this is due to the di\u000berence in qual-\nity of the text processing tools for each lan-\nguage (i.e. lemmatized, PoS, etc.).\nAmong the metrics used to relate docu-\nments, the JSD metric performs better than\nthe other density-based (e.g. Hellinger) and\nhierarchy-based (e.g. WJL) measures. How-\never, it seems that density-based metrics\nperform worse than hierarchy-based metrics\nwhen the number of topics is high. This\ncould be due to the fact that topics have dif-\nferent levels of speci\fcity and density-based\nmetrics assume that all topics are equally de-\nscriptive, since they all have the same weight\nwhen measuring distance. The sum of dis-\ntances of the less representative topics for\nJSD is higher as the number of topics diverge\nfrom its optimum number of topics (i.e be-\ntween 100 and 300 topics in Table 3). How-\never hierarchy-based metrics only take into\naccount the most relevant topics, and this\nbehavior not only makes them robust to di-\nmensional changes in the models, but also\nAcquis (MAP@10)\nLang Topics JSD HE WJL\nSpanish50 0.80060 0.79665 0.70583\n100 0.82741 0.77930 0.75555\n300 0.84261 0.58531 0.79036\n500 0.81238 0.68482 0.79336\nEnglish50 0.81421 0.80150 0.73367\n100 0.85510 0.74060 0.80315\n300 0.84005 0.52082 0.83277\n500 0.78874 0.43636 0.84555\nTable 3: Performance of density-based metrics.Acquis-3 (MAP@10)\nTraining Set\n1 2 3\nes en es en es enTest Set1JSD 0.85 0.83 0.86 0.85 0.87 0.87\nWJL 0.85 0.85 0.86 0.86 0.85 0.86\n2JSD 0.80 0.75 0.77 0.75 0.82 0.80\nWJL 0.73 0.77 0.81 0.83 0.82 0.83\n3JSD 0.72 0.62 0.68 0.65 0.69 0.68\nWJL 0.55 0.65 0.67 0.72 0.73 0.77\nTable 4: MAP of density- and hierarchical-based\ndistances from a corpus divided into three sub-\nsets.\nseems to improve its accuracy for higher di-\nmensions.\nWe can conclude that automatically gen-\nerated annotations from topic models o\u000ber\na knowledge close to that o\u000bered by cate-\ngories manually assigned from the EuroVoc\nthesaurus in the Acquis legal corpus to relate\ntexts. In the case of large and heterogeneous\ncollections, i.e. with a high number of dif-\nferent topics, it would be more appropriate\nto annotate documents by topic hierarchies\nthan using densities. In view of these results,\nthe knowledge o\u000bered by topics allows auto-\nmatically discovering what is being treated\nin a collection of documents, and the knowl-\nedge o\u000bered by its hierarchical representation\nallows understanding why documents are re-\nlated in a similar way as it would be done\nwith manually assigned labels.\n4.2 Text Length Impact\nTo better understand how the length of the\ntexts used for training a\u000bects the creation of\nprobabilistic topics, we evaluated three dif-\nferent scenarios where the original corpus is\ndivided into subsets with similar text sizes.\nIn the \frst scenario we have created three\nequal sets and compared the performance\nusing density-based metrics (i.e. JSD) and\nhierarchy-based metrics (i.e. WJL) for a doc-\nument retrieval task. Table 4 shows the mean\naverage precision when using a training set\n(columns) and a test set (rows) from among\nthe 3 subsets into which the initial corpus was\ndivided. The same experiment has been re-\npeated in an analogous way for the scenarios\nwith 6 (Table 5) and 9 (Table 6) subsets. Our\naim is to analyze if there is any behavior that\nis common in all of them.\nModels created from texts, i.e. train-\ning set, with greater or equal length to the\ntexts used in the inferences, i.e. test set,\nCarlos Badenes-Olmedo, Borja Lozano-\u00c1lvarez, Oscar Corcho\n32Acquis-6 (MAP@10)\nTraining Set\n1 2 3 4 5 6\nes en es en es en es en es en es enTest Set1jsd 0.79 0.76 0.79 0.77 0.79 0.78 0.79 0.77 0.79 0.77 0.77 0.73\nwjl 0.78 0.74 0.78 0.76 0.77 0.77 0.78 0.76 0.78 0.76 0.74 0.69\n2jsd 0.82 0.80 0.81 0.77 0.80 0.76 0.81 0.79 0.85 0.82 0.84 0.84\nwjl 0.81 0.82 0.85 0.86 0.84 0.86 0.85 0.85 0.85 0.85 0.83 0.84\n3jsd 0.76 0.73 0.78 0.73 0.72 0.68 0.78 0.71 0.81 0.75 0.81 0.76\nwjl 0.73 0.70 0.72 0.78 0.81 0.79 0.81 0.80 0.82 0.80 0.79 0.80\n4jsd 0.69 0.68 0.72 0.67 0.71 0.68 0.68 0.63 0.73 0.69 0.74 0.71\nwjl 0.63 0.69 0.66 0.72 0.74 0.76 0.77 0.78 0.77 0.79 0.75 0.79\n5jsd 0.62 0.57 0.69 0.61 0.66 0.62 0.67 0.59 0.63 0.59 0.70 0.65\nwjl 0.60 0.63 0.57 0.64 0.65 0.69 0.70 0.71 0.73 0.74 0.72 0.75\n6jsd 0.55 0.52 0.67 0.56 0.61 0.56 0.63 0.56 0.63 0.56 0.59 0.55\nwjl 0.51 0.57 0.51 0.60 0.58 0.63 0.63 0.67 0.66 0.70 0.69 0.71\nTable 5: MAP of density- and hierarchical-based distances from a corpus divided into six subsets.\no\u000bered better performance in document re-\ntrieval tasks regardless of the language used.\nThis behavior appears in the tables 4, 5 and\n6 in the cells whose column is greater than\nor equal to its row. This is evidenced by the\nfact that those models performed better for\nalmost all sets. Although for some evalua-\ntions of small documents models trained with\nlarge texts didn't yield the best performances\nthey were not signi\fcantly di\u000berent from the\nbest models. For small documents both met-\nrics performed similarly.\nOn the other hand, the performance\nof WJL signi\fcantly outperformed JSD for\nlonger documents (i.e. higher columns). A\nremarkable case is the table 6. The re-\nsults for the evaluation of the 9thset (group\nwith biggest document) with the 9thmodel\n(trained with the biggest document set) were\n13% better in the English case and 21%\nbetter, suggesting that, with enough text\ndata, PTM models produce small variations\nin topic proportion vectors from which WJL\nmetric bene\fts.\n4.3 Time Required for\nComparisons\nThe perception of e\u000eciency on hierarchy-\nbased metrics to calculate distances in topic\nmodels for large corpora was analyzed by\ncapturing the computational time (in sec-\nonds) that each metric used to compare the\ndocuments (Fig.4). For almost all number\nof topics, hierarchical-based metrics are so\nmuch faster than probabilistic ones. How-\never, for small number of dimensions (i.e.topics) the topic proportion vector is not\nsparse enough to identify any relevant topics\nfrom the uninformative ones, resulting in hi-\nerarchies containing all topics for every doc-\nument. In other words, all documents share\nat least one topic. Increasing the number of\ntopics alleviates this problem to the point of\narchiving an almost constant time for more\nthan 35 topics. Although density-based met-\nrics (e.g. JSD and HE) increased their com-\nputational time linearly with the representa-\ntion size, JSD calculation requires comput-\ning two logarithms for each dimension in the\ndocument representation, which is way more\ntime consuming than the HE metric square-\nroots. For the same reason, with a small\nnumber of dimensions, pairwise comparison\nis faster using the probabilistic metrics than\nthe hierarchical metrics.\nFigure 4: Time required to perform information\nretrieval tasks on a corpus of 100K documents\ndescribed with di\u000berent number of topics.\nImpact of Text Length for Information Retrieval Tasks based on Probabilistic Topics\n33Acquis-9 (MAP@10)\nTraining Set\n1 2 3 4 5 6 7 8 9\nes en es en es en es en es en es en es en es en es enTest Set1jsd 0.88 0.85 0.87 0.87 0.88 0.88 0.88 0.8 0.89 0.89 0.88 0.88 0.89 0.89 0.89 0.88 0.88 0.82\nwjl 0.89 0.79 0.89 0.82 0.87 0.86 0.88 0.86 0.89 0.87 0.88 0.87 0.89 0.87 0.89 0.87 0.87 0.77\n2jsd 0.70 0.66 0.70 0.63 0.71 0.64 0.69 0.63 0.71 0.66 0.72 0.68 0.73 0.70 0.74 0.73 0.71 0.71\nwjl 0.64 0.59 0.69 0.69 0.69 0.70 0.71 0.68 0.69 0.69 0.71 0.69 0.71 0.70 0.72 0.71 0.67 0.68\n3jsd 0.83 0.82 0.86 0.80 0.80 0.75 0.81 0.75 0.83 0.77 0.83 0.79 0.85 0.81 0.86 0.81 0.84 0.82\nwjl 0.80 0.78 0.84 0.83 0.87 0.86 0.88 0.86 0.88 0.85 0.87 0.85 0.86 0.85 0.87 0.84 0.84 0.83\n4jsd 0.74 0.72 0.77 0.70 0.72 0.67 0.65 0.63 0.69 0.63 0.73 0.66 0.76 0.70 0.78 0.72 0.77 0.73\nwjl 0.68 0.67 0.73 0.73 0.76 0.77 0.78 0.80 0.79 0.78 0.80 0.79 0.80 0.79 0.80 0.77 0.77 0.76\n5jsd 0.68 0.68 0.73 0.67 0.70 0.66 0.68 0.64 0.62 0.59 0.69 0.62 0.71 0.67 0.73 0.67 0.74 0.70\nwjl 0.60 0.65 0.64 0.72 0.67 0.73 0.72 0.76 0.75 0.77 0.77 0.78 0.73 0.78 0.75 0.77 0.74 0.77\n6jsd 0.61 0.61 0.68 0.59 0.64 0.58 0.63 0.59 0.63 0.56 0.57 0.54 0.65 0.59 0.68 0.60 0.68 0.62\nwjl 0.53 0.58 0.61 0.65 0.60 0.65 0.67 0.70 0.69 0.71 0.69 0.73 0.71 0.73 0.71 0.74 0.69 0.71\n7jsd 0.53 0.57 0.62 0.52 0.59 0.53 0.56 0.54 0.58 0.52 0.57 0.52 0.52 0.50 0.59 0.53 0.63 0.55\nwjl 0.47 0.55 0.53 0.63 0.52 0.64 0.58 0.66 0.62 0.66 0.65 0.69 0.65 0.70 0.66 0.71 0.63 0.68\n8jsd 0.52 0.48 0.60 0.47 0.59 0.48 0.56 0.47 0.56 0.47 0.57 0.48 0.58 0.47 0.53 0.45 0.60 0.50\nwjl 0.47 0.49 0.53 0.56 0.47 0.56 0.55 0.57 0.56 0.59 0.61 0.62 0.62 0.63 0.64 0.66 0.64 0.66\n9jsd 0.54 0.48 0.62 0.47 0.62 0.50 0.58 0.49 0.59 0.48 0.59 0.49 0.60 0.51 0.60 0.50 0.54 0.45\nwjl 0.51 0.48 0.55 0.55 0.54 0.57 0.59 0.59 0.61 0.59 0.64 0.62 0.63 0.65 0.66 0.65 0.67 0.67\nTable 6: MAP of density- and hierarchical-based distances from a corpus divided into nine subsets.\n5 Conclusions\nIn this paper we have studied the impact\nthat the length of texts has, since they de-\ntermine the space where words can co-occur,\nto semantically relate documents described\nin a probabilistic topic space. We have\nalso studied the ability of probabilistic top-\nics to automatically cluster related texts, and\nthe performance of density-based and topic\nhierarchy-based distance measures. Multi-\nple document retrieval tests were performed\non a collection of legal documents, compar-\ning the results obtained by this unsupervised\napproach, with the results obtained using\nmanual annotations. Representation meth-\nods based on probabilistic topics have proven\nto be reasonably accurate in annotating se-\nmantically related documents with the same\ncategories. State-of-the-art metrics based on\ndensities and hierarchical representations of\ntopics were evaluated to measure document\nsimilarity. Regardless of the approach used,\nthe knowledge captured by word distribu-\ntions (i.e topics) to automatically relate le-\ngal texts has shown an accuracy close to 0.8\ncompared to relations based on EuroVoc cat-\negories.\nThe results guide us in the use of proba-\nbilistic topic models to facilitate the explo-\nration of large collections of documents. The\nknowledge inferred by these models to au-\ntomatically group semantically related doc-\numents is highly sensitive to the texts used\nin their training. Their ability to general-\nize such knowledge only seems to make sense\nin one direction: with texts whose length isequal to or longer than those used during\ntraining. This allows us to conclude that,\nfor example, the knowledge extracted from\nthe topics inferred from a collection of tweets\n(texts of no more than 260 characters), can-\nnot be extended to automatically classify, for\nexample, blog posts (more than 300 charac-\nters). If we assume that the complexity of\na text increases as its length increases, the\nlogic used to infer topics is unable to capture\nmore complex knowledge than was proposed\nduring training.\nIf we consider that the complexity of a text\nis directly proportional to its length, proba-\nbilistic models are not able to generalize the\nknowledge they acquire during their train-\ning to process more complex texts. In other\nwords, the knowledge captured by probabilis-\ntic topics to group semantically related docu-\nments can only be applied to texts of equal or\nlesser length than those used during training.\nIn addition, the larger the corpus and the\nmore topics it contains (i.e. the more diverse\nthe content of its documents), the more ap-\npropriate it is to use similarity metrics based\non hierarchical representations of the topics\n(see Figures 5 and 6). The noise introduced\nby the less present topics in a text is adverse\nto density-based metrics. The relationships\nsuggested when manually annotating docu-\nments are therefore based on a small group\nof labels. Under these conditions, PTM can\nguide the corpus exploration by providing an\nunsupervised method to thematically anno-\ntate documents and potentially giving insight\nof the relations between documents.\nCarlos Badenes-Olmedo, Borja Lozano-\u00c1lvarez, Oscar Corcho\n34Figure 5: Test comparisons in 6 partitions based on JSD.\nFigure 6: Test comparisons in 6 partitions based on WJL.\nThere are still challenges and questions\nthat will have to be solved in future work,\nnamely \fnding the in\ruence of the weights in\nthe hierarchical-based metric; analyzing the\ncomplexity of texts beyond their lengths, tak-\ning into account the rhetoric of its discourse\nto represent scienti\fc texts (.e.g. using only\nthe paragraphs describing the approach or\nthe method to create the topic distributions);\nAnd even observing their behaviour in di\u000ber-\nent languages.\nAcknowledgments\nThis work is supported by the project\nKnowledgeSpaces with reference PID2020-\n118274RB-I00, \fnanced by the Spanish Min-\nistry of Science and Innovation.References\nBadenes-Olmedo, C., J. Redondo-Garc\u0013 \u0010a,\nand O. Corcho. 2019a. Large-Scale Se-\nmantic Exploration of Scienti\fc Literature\nusing Topic-based Hashing Algorithms.\nSemantic Web Journal.\nBadenes-Olmedo, C., J. Redondo-Garc\u0013 \u0010a,\nand O. Corcho. 2019b. Legal document\nretrieval across languages: topic hierar-\nchies based on synsets. Proceedings of the\n1st Workshop on Iberlegal co-located with\n32nd International Conference on Legal\nKnowledge and Information Systems orga-\nnized by the Foundation for Legal Knowl-\nedge Based Systems (JURIX).\nBadenes-Olmedo, C., J. L. Redondo-Garc\u0013 \u0010a,\nand O. Corcho. 2017a. Distributing text\nmining tasks with librairy. In DocEng\n2017 - Proceedings of the 2017 ACM Sym-\nImpact of Text Length for Information Retrieval Tasks based on Probabilistic Topics\n35posium on Document Engineering , pages\n63{66, August.\nBadenes-Olmedo, C., J. L. Redondo-Garc\u0013 \u0010a,\nand O. Corcho. 2017b. An initial anal-\nysis of topic-based similarity among sci-\nenti\fc documents based on their rhetori-\ncal discourse parts. In Proceedings of the\nFirst Workshop on Enabling Open Seman-\ntic Science (SemSci), pages 15{22.\nBlei, D., A. Ng, and M. Jordan. 2003. Latent\nDirichlet Allocation. Journal of Machine\nLearning Research , 3(4-5):993{1022.\nDeerwester, S., S. T. Dumais, G. W. Fur-\nnas, T. K. Landauer, and R. Harshman.\n1990. Indexing by latent semantic analy-\nsis. Journal of the American Society for\nInformation Science, 41(6):391{407.\nDieng, A. B., F. Ruiz, and D. Blei. 2020.\nTopic modeling in embedding spaces.\nTransactions of the Association for Com-\nputational Linguistics, 8:439{453.\nHe, J., L. Li, and X. Wu. 2017. A\nself-adaptive sliding window based topic\nmodel for non-uniform texts. In Pro-\nceedings - IEEE International Conference\non Data Mining, ICDM , volume 2017-\nNovem, pages 147{156.\nHofmann, T. 2001. Unsupervised Learning\nby Probabilistic Latent Semantic Analy-\nsis.Machine Learning, 42(1-2):177{196.\nHofmann, T. 1999. Probabilistic latent se-\nmantic indexing. In Proceedings of the\n22nd annual international ACM SIGIR\nconference on Research and development\nin information retrieval, pages 50{57.\nHu, Y., K. Zhai, V. Eidelman, and J. Boyd-\nGraber. 2014. Polylingual tree-based\ntopic models for translation domain adap-\ntation. In Proceedings of the 52nd Annual\nMeeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Pa-\npers), pages 1166{1176.\nJelodar, H., Y. Wang, C. Yuan, X. Feng,\nX. Jiang, Y. Li, and L. Zhao. 2017. Latent\ndirichlet allocation (lda) and topic model-\ning: models, applications, a survey.\nJung, K. H., E. Ruthru\u000b, and T. Goldsmith.\n2017. Document similarity misjudgment\nby lsa: Misses vs. false positives. Cogni-\ntive Science.Mao, X.-L., B.-S. Feng, Y.-J. Hao, L. Nie,\nH. Huang, and G. Wen. 2017. S2JSD-\nLSH: a locality-sensitive hashing schema\nfor probability distributions. In Thirty-\nFirst AAAI Conference on Arti\fcial In-\ntelligence .\nNzali, T., M. Donald, S. Bringay,\nC. Lavergne, C. Mollevi, and T. Opitz.\n2017. What Patients Can Tell Us: Topic\nAnalysis for Social Media on Breast\nCancer. JMIR medical informatics ,\n5(3):e23.\nO'Neill, J., C. Robin, L. O'Brien, and\nP. Buitelaar. 2017. An analysis of topic\nmodelling for legislative texts. CEUR\nWorkshop Proceedings, 2143.\nRus, V., N. Niraula, and R. Banjade.\n2013. Similarity measures based on la-\ntent dirichlet allocation. In International\nConference on Intelligent Text Process-\ning and Computational Linguistics, pages\n459{470. Springer.\nScho\feld, A., M. Magnusson, and D. Mimno.\n2017. Pulling out the stops: Rethink-\ning stopword removal for topic models.\nInProceedings of the 15th Conference of\nthe European Chapter of the Association\nfor Computational Linguistics: Volume 2,\nShort Papers , pages 432{436, Valencia,\nSpain. Association for Computational Lin-\nguistics.\nSteinberger, R., M. Ebrahim, A. Poulis,\nM. Carrasco-Benitez, P. Schl\u007f uter,\nM. Przybyszewski, and S. Gilbro. 2014.\nAn overview of the European Union's\nhighly multilingual parallel corpora.\nLanguage Resources and Evaluation ,\n48(4):679{707, November.\nSteinberger, R., B. Pouliquen, A. Widi-\nger, C. Ignat, T. Erjavec, D. Tu\f\u0018 s, and\nD. Varga. 2006. The JRC-Acquis: A mul-\ntilingual aligned parallel corpus with 20+\nlanguages. The 5th International Confer-\nence on Language Resources and Evalua-\ntion - Proceedings p. 2142-2147, May.\nSyed, S. and M. R. Spruit. 2017. Full-Text\nor Abstract? Examining Topic Coher-\nence Scores Using Latent Dirichlet Alloca-\ntion. 2017 IEEE International Conference\non Data Science and Advanced Analytics\n(DSAA), pages 165{174.\nCarlos Badenes-Olmedo, Borja Lozano-\u00c1lvarez, Oscar Corcho\n36Constructing Corpus and Word Embedding for Spanish  \nCovid- 19 Data \n Construcci \u00f3n de  corpus y  word e mbedding para datos de Covid -19 en  \nespa\u00f1ol  \nKyungjin Hwang \nKorea University, Seoul, Republic of Korea \nkjhwang0624@korea.ac.kr \nAbstract Severe acute respiratory syndrome coronavirus 2 ( COVID 19), colloquially \nreferred to as coronavirus, escalated into a global pandemic with severe transmission and \nmortality rates in 2019. Despite the escalation of the virus\u2019 worldwide impact in 2020, \nnumerous studies on Natural Language Processing  in Spanish have neglected corpus \nconstruction or word embedding, especially conspicuous in its absence being the corpora involving coron avirus or infectious diseases. Additionally, corpus construction or word \nembedding conducted in the medical field do not display efficacy in production pertaining \nto coronavirus or infectious diseases. To supplement this potentially detrimental \ninsufficien cy, this study collects Spanish Language data to build a relevant coronavirus \ncorpus through appropriate preprocessing and then obtains a word embedding. \nPerformance of the corpus and word embedding are then tested through word similarity \nevaluations, a co sine similarity evaluation,  and a visualization evaluation  with the existing \nSpanish corpus. After comparison, corpus and word  embedding suitable for coronavirus \nwill be suggested.   \n     Keywords:  corpus, word embedding, coronavirus . \nResumen La Enfermedad Inf ecciosa por Coronavirus -19 (en adelante Covid -19), que \ncomenz\u00f3 a extenderse globalmente en diciembre de 2019, mostr\u00f3 una  alta tasa de infecci\u00f3n \ny mortalidad, y tuvo un gran impacto en el mundo en 2020. Sin embargo, los estudios \nexistentes de procesamiento del lenguaje natural en espa\u00f1ol no han utilizado la \nconstrucci\u00f3n de corpus o  la incrustaci\u00f3n de palabras para enfermedad es infecciosas, \nincluido el coronavirus. La construcci\u00f3n de corpus y la incrustaci\u00f3n de palabras en el \ncampo biom\u00e9dico no han mostrado un rendimiento eficaz en la ayuda para luchar contra las \nenfermedades infecciosas, por lo tanto, este estudio recopila da tos en espa\u00f1ol relacionados \ncon el coronavirus para proceder despu\u00e9s a construir un corpus de coronavirus en espa\u00f1ol e \nincrustar palabra s a trav\u00e9s de un preprocesamiento  adecuado. Posteriormente, nos gustar\u00eda \npresentar un corpus e incrustaci\u00f3n de palabras adecuadas para coronavirus mediante la \ncomparaci\u00f3n de la similitud del coseno y la evaluaci\u00f3n de visualizaci\u00f3n con el corpus espa\u00f1ol existente.  \n          Palabras C lave:  corpus, word embedding, coronavirus . \n1 Introduction \nNatural Language Processing  is th e field by \nwhich computers understand and utilize human \nlanguage. Part -of-speech tagging for syntax \nanalysis, parsing, entity name recognition for semantic analysis, document classifi cation, and \nmachine translation are only some of the \napplications and components necessary for Natural Language Processing . Successful \nNatural Language Processing  requires that the \nhuman language be converted in such a way so \nthat it can be understood by a computer. Word embedding, which is the most representative \nmethod to do so, is defined  as transforming a \nword into a vector that can be calculated.  \nVarious field s in Natural Language \nProcessing  are often established by natural \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 37-44\nrecibido 01-05-2021 revisado 06-06-2021 aceptado 08-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-3\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturaldivisions of human life, developing specialty \nfields of study according to need.  A representative ex ample of this is Natural \nLanguage Processing  research in the field of \nbiomedical and medical science. Recently, research in biomedical fields  have increased \nexponentially. Accordingly, interest in an efficient approach to biological, chemical, and medical data, such as biomedical papers, patent \ndocuments, or electronic medical records are also increasing. However, according to Cohen et \nal. (2014), in the study of Natural Language \nProcessing  in the biomedical field, there are \nmany terms that are not part of the common \nlexic on, such as certain more obscure or \ncomplex ailments, symptoms, medications, and \nterms that are used with meanings that differ \nfrom those used in daily life or those used \nmainly as abbreviations. Existing corpora or \nword embedding methods h ave many limitat ions \ndue to specific medical terminology, and various \nstudies are being c onducted to overcome them . \nHowever, most Spanish cor pora and \nbiomedical word embeddings do not accurately reflect the worldwide coronavirus pandemic of 2020. The coronavirus, having  only begun in \n2019, was quickly transmitted across borders resulting in de vastating mortality rates.  \nCurrently existing Spani sh corpora lack terms \nrelated to coronavirus as well as terms related to \nother infectious diseases such as Middle E ast \nRespiratory Syndrome (MERS), which was \npreviously prevalent. Considering the fact tha t \ncoronavirus is affecting not only biomedical \nfields but also various social fields around the world, the necessity of constructing a corpus that \nreflects the terms of epidemiology , including \ncoronavirus, is significant.  \n Therefore, this study first attem pts to \ncompile a coronavirus -related Spanish corpus.  \nThis Spanish language coronavirus corpus aims \nto include data from biomedical and various \nother related fields. Second, word embedding is \nimplemented based on this corpus, and its \nperformance is verified  through comparison and \nvisualization evaluation of cosine similarity with \nthe existing Spanish corpora.  \n The structure of this paper is as follows:  \nSection 2 describe s previous stu dies relevant to \nthis study, Section 3 details the data and \nmethodology use d in this study, Section 4 \ndescribes the experiment and its results, and in \nSection 5 this author presents the conclusions of \nthis study.  2     Previous Studies \n2.1 Word Embedding \nWord embedding is the process by which text is \ntranslated into a representative vector that can be \ncalculated and understood by a computer. This \nprocess is widely used in Natural Language \nProcessing in combination with deep learning models by considering n ot only the meaning of a \nsingular word but also the information that can be understood through a phrase.  Word \nembedding is largely divided into sparse \nexpressions and dense expressions.  \nSparse expressions can be represented using \neither the Bag- of-Word mo del of expression \nclassification or the TF -IDF model. \nBag- of-Word is a method of information \nretrieval that considers only the freq uency of \noccurrence, whereas TF- IDF considers the \nfrequency of occurrence but then calculates \nrelevance to offset errors that  occur due to \nirrelevant but nevertheless commonly appearing \nwords. This is accomplished by calculating the \nterm frequency (TF) of a text but then \nincorporating an inverse document frequency (IDF) parameter that assigns low weight to \ngeneral but high frequ ency words and high \nweight to more meaningful terms.  \nOn the other hand, the most representative \nmethod of classifying dense express ions from \nword embedding processes is word2vec \n(Mikolov et al ., 2013). Word2vec is a deep \nlearning -based technique that reconstructs the \ncontext of words through dense vector expression. This technique is once again divided into Cbow and Skip -Gram models. C bow \npredicts intermediate words using surrounding words, while Skip -Gram predicts surrounding \nwords with interm ediate words (Mi kolov et al., \n2013).   \nAdditionally, there is the FastText extension  \n(Bojanowski et al., 2017) , which differs from \nword2vec in that it assumes that there ar e \nvarious sub -words within a single word. \nFastText has the advantage of being able t o \ncalculate the similarity of new words by its \nclassification of sub -words and then recognizing \nthose sub -words elsewhere. Consequently, even \nwords with low frequency can b e embedded \nefficiently as long as it contains previously classified or recognized su b-words.  \n  \n \n \nKyungjin Hwang\n38  2.2 Spanish Corpus and Biomedical Data \nTwo corpora used in Spanish Natural Language \nProcessing are CORPES XXI1 and SBWCE2 \n(Spanish Billion Word Corp us). Both include \nboth Peninsula /European Spanish and the \nvarieties of Spanish spoken in Latin America.  \nAdditionally,  there are v arious Spanish data \nsets available in the biomedical field. IBECS3 \n(The Spanish Bibliographical Index in Health Sciences ) is a dataset built by examining various \njournals in the health and medical fields. \nSciELO (Scientifi c Electronic Library  Online)\n4 \nis a dataset of various scientific journals \npublished in Latin America, South Africa, and Spain.  \n \n2.3 Spanish Biomedical Word Embedding \nSpanish language word embedding is applied in \nvarious fields, and research in biomedical f ields \nis also being actively cond ucted. Segura -Bedmar \nand Mart\u00ednez (2017) employed word embedding using the SBWCE corpus, a pre- existing corpus, \nto simplify drug description data , but there was a \nlimitation in that they did not build a special \nseparate cor pus for their result ing data set. \nVillegas et al. (2018) also collected biomedical data through text mining, but also experienced limitations due to not having obtained word \nembedding. Meanwhile, Santiso (2018) did \nobtain word embedding through Spanish medical records. Althou gh this word embedding \nin biomedical fields was attempted using \nElectronic Health Records (EHRs) of Spanish \nhospitals, there is a disadvantage in that no intrinsic evaluation was performed.  \n \n1 CORPES  is a corpus created by Real Academia \nEspa\u00f1ola, Spain, and contains more than 300,000 \ndocuments with approximately 312 million words derived from written texts and oral manuscripts. It is a corpus that in cludes literary works such as novels, \nfilms, scripts , and plays, as well as words from \nnon-literary books and periodicals, blogs, and \nInternet resources. Peninsular Spanish data accounts \nfor about 30% of the resources and Latin American \nSpanish data for abo ut 70% (Corpes XXI\u201d. Real \nAcademia Espa\u00f1ola.  \nhttps://www.rae.es/banco -de-datos/corpes -xxi, \nDecember 21, 2020)  \n2 SBWCE is a corpus created by the University of \nChile which contains approximately 1 billion words. It is a Spanish langua ge corpus made from var ious \nexisting Spanish corpora and is embedded usin g the \nword2vec algorithm.  \n3 http://ibecs.isciii.es  \n4 https://SciELO.org/es/  According to Sores et al. (2019), in the \nbiomedical field, terms a re often used in a \nmanner inconsistent with the typical dictionary \ndefinition, or with a different connotation, such \nas in the case of the word \u2018positive\u2019. Or , for \nexample, when two words are combined to \nrepresent a single meaning as th ey are in open \ncompound words, for example: \u2018brain dead\u2019. \nTherefore, in this case, the word2vec method is not suitable for producing an accurate word \nembedding in the biomedical field because it \nwould recognize \u201cbrain\u201d and \u201cdead\u201d separately. To solve this problem, Sores used FastText for \nbiomedical word embedding. FastText is effective in overcoming the above -mentioned \nlimitations as it analyzes the morphological and \nsemantic char acteristics of the words by \nbreaking them down and analyzing them in \nN-grams a s opposed to as a wh ole. This s tudy \nnot only accomplished the feat of obtaining a Spanish word embedding in the biomedical field, but also overcame the prior limitations of word embedding in the Spanish biomedical field by \nperforming both the intrinsic eva luation and the \nextrinsic evalu ation through the recognition of \nthe entity name, which had not been done before.  \nFollowing this, Rivera & Martineza (2020) \nimplemented a context -based word embedding \nand performed entity name recognition using a \ndeep learning neural network model  in Spani sh \nmedical records. Data was collected from the existing Spanish biomedical corpora and the biomedical corpora of other languages, and \nword embeddi ng was obtained using the vector \ncontextualizing Bidirectional Encoder \nRepresentations from Transfor mers (BER T) \nmodel, not the previously mentioned models word2vec and FastText, which are context -free \nand therefore may result in less precise word embedding s.  \nEver since this advance in technology it has \nbeen the objective of this study to overcome the limitations  of the currently existing context -free \nSpanish word embedding models by performing a Named Entity Recognition (NER) using the available word embedded data and deal with the \ncontext-based Spanish biomedical word \nembedding generation process in the future.  \nIn the case of Guti\u00e9rrez -Fandi\u00f1o et.al . (2021), \nthe Spanish biomedical corpus and the Spanish \nmedical record corpus were embedded using the \nFastText method. Since the former is larger than \nthe latter, embedding was performed separately, \nand for lower words,  embedding was performed \nusing Byte Pair Encoding. Corpus data was \nConstructing Corpus and Word Embedding for Spanish Covid-19 Data\n39collected from crawls, books, SciELO, Pubmed, \nand patent documents. This study i s meaningful \nin that it has created and embedded a corpus \nlarge enough to be used in the future in Natural \nLanguage Processing in the biomedical field. \nHowever, compared to the existing corpus, it is limited in that it has undergone no intrinsic or \nextrinsic evaluation.  \nIn short, research on word embedding in the \nfield of Spanish biomedical science is  indeed \nprogressing, despite the disadvantages of that have shadowed the many advances. However, research in the field of epidemiology, including \nthe all too t opical coronavi rus, has not been \nconducted well, and there is a constraint in  that, \nin many cases , the existing biomedical corpus \ndoes not contain terms related to infectious \ndiseases. Furthermore, although there are some corpora which reflect words related to \ncoronaviru s, an objective evaluation has not \nbeen conducted. \n3     Data and Method \nThe objective of this author\u2019s study is to \nconstruct a corpus and word embedding that \neffectively reveal s information related to \ninfectious dis eases, especially  in regard to the  \ncoronavirus. Therefore, data from various fields related to coronavirus and infectious diseases \nwas gathered, pre -processed, and then a corpus \nwas built, and word embedding was performed through FastText.   This study chose the FastText \nmodel as its embedding method due to its ability \nto effectively analyze and embed terms  whose \nbiomedical definitions are different from those \nin use in the common  lexicon . In addition, \nFastText can implement effective embedding \neven with a s mall number of words as i ts data set . \nThe performance of this word embedding  was \nthen tested through  a cosine similarity \nevaluation between the existing Spanish corpus \nand embedded SPWCE. Additionally, the corpus \nand embeddings will be tested for efficacy of related word construction through visualization \nevaluation. \n \n3.1 Constructing Corpus and \nPre-Processing Data \nFor the purp oses of this study , \ncoronavirus- related data was divided into  two \ncategories:  bio-medical data and social data.  \nThe reason this study included both bio -medical \ndata and social data in the corpus was to create a \ncorpus that reflects various issues which included, not only the coronavirus -related \nbiomedical domain, but also various social, economic, and cultural domains as well. Data in \nthe field of biomedical science was extracted \nfrom  medical journal articles containing \nkeywords related to coronavirus and other \ninfectious diseases such as \u201cC OVID 19\u201d, \n\u201cSARS\u201d , or \u201cMERS\u201d published in \nSpanish- speaking countries. In addition to this \ncollected data, hea lth science data from the \nIBEC and SciELO data sets was used as well.  \nSocial sector data w as obtained from onlin e \nmajor daily newspapers from  Spain, Mexico, \nChile, Peru, Colombia,  and Argentina. These \narticles contained the keywords \u201cCoronavirus\u201d, \n\u201cCovid-1 9\u201d, \u201cSARS\u201d, and \u201cMERS\u201d. To \nsupplement this data a web -crawler was used to \ncollect data fro m websites such as W ikipedia.   \nFollowing data collection,  the text was \nstandardized for processing.  Capital letters were \nchanged to lowercase letters , and special letters \nwere removed . Stop words were removed using \nthe NLTK Spanish Stop Word Corpus. In \naddition, a pre -processin g procedure was \nperformed to remove all English words using the \nNLTK English corpus so that English was not \nincluded  in the final results . Thus, the Spanish \ndata related to coronavirus was constructed as \nfollows.  \n \n Number of Token  \nAcadem ia Data 29,057,255 \nNews Paper  124,368,426 \nWikipedia  19,374,235 \nTotal  172,799,916 \nTable 1: Spanish Coronavirus Data Collection \nResults . \n3.2 Word Embedding and Training \nFastText is an opensource embedding method. It \nwas develo ped by Facebook as an alternative  \nway to turn words into vectors. Developed after \nword2vec, it exhibits  similar functions  to the \nskip- gram model of word2vec and the \nmechanism of CBOW. However, w ord2Vec sees \na word as an indivisible unit, whereas FastText \nsees that there are smaller sub- words within \nKyungjin Hwang\n40  \nwords. In FastText, each word can be \nrepresented by  a set of N- grams made of letters, \nand after the learning process o f the artificial \nneural network is complete , each N-gram  of all \nthe words in the dataset is al so embedded \n(Bojanowski et al., 2017). For example, in the \ncase of \u201cvirus\u201d, an example of FastText's \nsyllable N-gram function  is as follows ( where \nn=3). \n (1) G\nCoronav irus = {<vi, vir, iru, rus, us>, \n<virus>}  \n \n  The new score function of FastText is as \nfollows.  \n     \n(1) \n  \n   \nIn the above equation, z\ngT is the vector of \neach word in N-gram  and v c is the word vector \nincluded in the context.  \nIt is by virtue of this functio n that it is \npossible to calcu late a word\u2019s degree of \nsimilarity with other words , even in the case of \nunlearned words. The FastText Model of word \nembedding  is therefore superior to  Word2Vec , at \nleast in regard to the latter\u2019s  inability to cope \nwith unknow n words or to accurately embed  \nwords with low frequency within a word set.  \n \n3.3 Evaluation \n \nThe resulting corpus and word embedding of \nthis study were evaluated through w ord \nsimilarity and visual evaluations.  \n \n3.3.1 Word Similarity Test  \nThe word similari ty evaluation is a method to  \nevaluate the quality of word embeddings. First, a \nseries of word p airs is c reated . Then an \nevaluation of the similarity of the word pairs is conducted by human evaluators. After that the correlation between the scores obtained through \nthese evaluations  and the cosine similarity \nbetween words and vectors is calculated.  This \nstudy conducted the following two such \nevaluations of word similarity.  \nFirst, WordSim data created for general word \nembedding evaluation is select ed. WordSim data \nwas human evaluated  by dividing the degree of \nsimilarity between words by 0 -10 points, with a \ntotal of 322  evaluation sets. In this study, WordSim data written in English was first translate d using  Google Translator, and if the \nmeaning of the machine translated data was \ninaccurate when compared to the original \nWordSim data, the researcher direc tly translated \nthe W ordSim data set themselves . \nSecondly , MayoSRS (Pakhomov et al., 2011) \nwas used to evaluate the word embedding from  \nthe biomedical field. M ayoSRS is composed of \n101-word pairs, and the similarity was also \ndirectly human- evaluated with a parameter of \n0-10 points.  \n Third, cosine similarity comparison was \nperformed based on UMSRS -similarity \n(UMSRS -sim), which was widely used as a \nsimilarity comparison set in another biomedical \ndomain (Pakh omov et al., 2010). This  data \nselected consisted of 566-word pairs from a data \nset in which the similarity of UMLS (Unified Medical Language System) concepts w ere \nmanually evaluated. These two datasets were \nalso originally written in English, a nd, like \nWordSim, after an initial translation through \nGoogle Translator, if the resul ting word was  \ndifferent compared to the original data, it was \ndirectly translated and evaluated using the \nSpanish version of MayoSRS and UMSRS -sim. \n \n3.3.2 Visualization T est \n \nFollowing the word similarity evaluation , a \nvisualization evaluation was perfor med for the \ninternal evaluation of the word embedding. A \nvisualization evaluation is another method of \nevaluating word embeddings, and it is a techniqu e in which words with  similar meanings \nappear in close proximity to pictures so that \nhumans can easily understand them. Through this method , the quality of the embedding can be \nchecked indirectly for quality . Since word \nembedding is usually a hig h-dimensional vector, \nthis stud y applied t- SNE (t-Stochastic Neighbor \nEmbedding)\n5, reduced it t o two dimensions, and \nevaluated the visualization.  \n \n \n5 t-SNE expresses high dimensional data in a \ntwo-dimensional plane by use of an algorithm that \npreserves the structure of neighbor ing data and \ndistance as much as possibl e. In this way researchers \nmay visually analyze their word embeddings.  \nConstructing Corpus and Word Embedding for Spanish Covid-19 Data\n414    Result \n4.1 Cosine Similarity T est \nThe word embedding underwent a cosine \nsimilarity evaluation with SBWCE, an existing \nSpanish language corpus. The results of this \nevaluation  were conducted mainly using  the \nEnglish language data sets for cosine similarity \nevaluation, Word -sim data and MayoSRS , the \nresults of which are shown in <Table  2>. \n  SCC (Spanish Co rona Corpus), the corpus \ncreated by  this study, showed higher cosine \nsimilarity than the existi ng SBWCE corona. In \nthe case of Word -Sim, which is composed of \npairs of common words, the cosine similarity of SCC was 0.4796 . This is m uch higher than that \nof SBWCE w hich is 0.2674. In the case o f the \nMayoSRS  evaluation, which was composed of \nmedical terms, the cosine similarity o f SCC was \n0.2025. Compare this again with  the cosine \nsimilarity of SBWCE which was 0.1174 . Once \nagain SCC proves higher . Similarly, in the case \nof UMSRS- sim composed of biomedical terms , \nwhile the similarity of SBWCE was 0.4280,  the \ncosine similarity of SCC was 0.4873, \ndefinitively higher in the corpus of this study.   Finally, similarity evaluation was performed \nusing the Multi -SimLex dataset\n6. Although this \nevaluation resulted in much lower scores than than other similarity evaluation test sets, SCC, \nthe c orpus of this study, showed better results \nthan the existing SBW CE. \n \n SCC \n(our corpus)  SBWCE \nWord -Sim 0.4796 0.2674 \nMayoSRS 0.2025 0.1174 \nUMSRS- sim 0.4873 0.4280 \nmultisimlex  0.0981 0.0521 \nTable 2:  Cosine similarity test between SCC and \nSBWCE. \n \n6 Multi -SimLex ( https: //multisimlex.com/ ) is a \ndata set created to evaluate semantic similarity, and in \nthe case of Spanish, it consists of about 1900 pairs of   \nwords. This evaluation wa s conducted by ten \nindividuals.  \nFigure 1: Visualization test result.  \nKyungjin Hwang\n42  These results conclusively show that the \ncoronavirus d ata-based corpus conducted in this \nstudy demonstra te better performance than the \nexisting Spanish  language corpus. Particularly \nnoteworthy is that despite th e SCC\u2019s  fewer \nembedded words than SBWCE, it  has shown a \nconsistently  higher performance, strong \nevidence that the corpus and word embedding in \nthis study are greatly  efficien t. \n \n \n4.2 Visualization Test \nAs a result of the word emb edding visualization  \nof the  Spanish C orona Corpus in this study \n(Figure 1 & Figure 2) , many epidemiological \nterms that do not exist in the existing Spanish \ncorpus (SBWCE, etc.) such as \u201cMERS\u201d, \n\u201cSARS\u201d, and \u201cCorona virus\u201d appear. \nAdditionally, va rious medication termino logy \nrelated to the treatment of illne ss in patien ts \nappear. For example, terms such as , \n\u201cepidemiol\u00f3gica\u201d, \u201ccontagiado\u201d, and \u201cpositivo\u201d \nare located close to each other. Words such as , \n\u201ccuarentena\u201d, \u201c crisis\u201d, and \u201c econom\u00eda\u201d , which \nare related to the economi c problem s of the \ncoronavirus are also  concentrated in close \nproximity . Terms related to s ocial issues in \nconnection  to coronavirus are easily displayed . \nIt seems to have been reflected  in the results of \nthe visualization .  Additionally , other various words are derived \nthrough the visualization evaluation s uch as \n\u201cmarzo\u201d, \u201cb rote\u201d, \u201c online\u201d, and \u201cc olegio\u201d \n(elementary school), etc. In shor t, is possible to \nunderstand the impact of coronavi rus on people \nin disparate area s of society, fr om economic \nsectors to education. From the results of the \nvisualization, i t can easily  be seen that the \npositions bet ween common similar words are \nvisually near each other . \n5     Conclusion \nThe coronavirus infection (Covid -19) has \ngreatly  impacted the world, and the need to \nprocess coronavirus -related data has grown ever \nmore ur gent. Due to the absence of a dedicated \nSpanish language coronavirus corpus with an implemented word embedding, this  study is \nessential to the future to the field of Natural \nLanguage Processing, specificall y in the field o f \nepidemiology and bio medicine, in  that it can be \nused for future related studies by presenting coronavirus- related corpora and word \nembeddings. \nThe coronavirus corpus constructed through \nthis study contains various epidemiological terms that are not included in the existing \nbiomedical corpus and word embedding. This \ndedicated coronavirus corpus has a high cosine similarity between similar words compared to \nthe existing Spanish language corpus making \nFigure 2: Zoom in overlapping part of Figure 1.  \nConstructing Corpus and Word Embedding for Spanish Covid-19 Data\n43effective word embedding possible. From the \nevidence shown here word embedding has been successfully implemented. In addition, \naccording to the results of the visualization \nevaluation, words on topics rela ted to the fields \nof medicine, economics, society, education, and \nmiscellany are clustered, providing researchers \nwith the opportunity to unde rstand what the \ncoronavirus specifically suggests in each field. Most importantly, the word embedding created \nin this study has shown a consistently superior \nperformance alongside the SCC despite its \nsmaller size and serve s as proof of its efficacy.  \nHoweve r, the data in this study limited to \nbiomedical academic papers and major daily newspapers sourced from Spain, Mexico, Chile, \nPeru, and Argentina. It is necessary to secure \nadditional data such as various online journals, blogs, SNS, and books in the futur e. \nFurthermore, there is a need for supplementary experiments for more effective word embedding implementation. First, by running several word \nembeddings using the same model consecutively \nbut redefining the parameters and  dimensions of \nits requirements. Secondly, by obtaining word \nembeddings using several different models \naltogether.\n  \nReferences \nBojanowski, P., E. Grave, A. Joulin,  and T.    \nMikolov . 2017. Enriching word vectors with \nsubword i nformation. Transactions of th e \nAssociation for Computational Linguistics , \n5:135- 146. \nCohen, K. B., and D. Demner -Fushman. 2014. \nBiomedical Natural Language Processing , \n11. John Benjamins Publishing Company .  \nGuti\u00e9rrez-Fandi\u00f1o, A.,  J. Armengol- Estap \u00e9, C. P. \nCarrino, O. De Gibert, A.  Gonzalez- Agirre, \nand M.  Villegas. 2021. Spanish Biomedical \nand Clinical Language Embeddings. arXiv \npreprint arXiv:2102.12843.  \nMikolov, T., K. Chen, G.  Corrado, and J. Dean.  \n2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.  \nPakhomov, S., B. McInnes, T. Adam, Y.  Liu, T.  \nPedersen, and G.  B. Melton . 2010. Semantic \nsimilarity and relatedness between clinical terms: an experimental study. AMIA annual \nsymposium proceedings , 2010:572. Pakhomov, S. V., T. Pedersen,  B. McInnes, G. B.  \nMelton, A. Ruggieri, and C. G.  Chute. 2011. \nTowards a framework for developing \nsemantic relatedness reference standards. \nJournal of biomedical informatics , \n44(2):251- 265. \nRivera- Zavalaa, R. , and  P. Martineza. 2020. \nDeep Neural Mode l wit h \nContextualized- word Embeddings for \nNamed Entity Recognition in Spanish Clinical Text. Proceedings of the Iberian \nLanguages Evaluation Fo rum (IberLEF \n2020), CEUR Workshop Proceedings.  \nSantiso, S ., A. Casillas,  A. P\u00e9rez, and M. Oronoz.  \n2019. Word embeddi ngs for negation \ndetection in health records written in Spanish. \nSoft Computing , 23:10969- 10975. \nSegura- Bedmar, I. , and  P. Mart\u00ednez. 2017. \nSimplifying drug package leaflets written in \nSpanish by using word embedding. Journal \nof biomedical semantics , 8(1):1- 9. \nSoares, F.,  M. Villegas, A. Gonzalez -Agirre,  M.  \nKrallinger, and J.  Armengol- Estap\u00e9,  2019. \nMedical word embeddings for Spanish: \nDevelopment and evaluation.  Proceedings of \nthe 2nd Clinical Natural Language Processing Workshop , pages 123- 133. \nVillegas, M., A. Intxaur rondo, A. \nGonzalez -Agirre, M. Marimon, and M.  \nKrallinger. 2018. The MeSpEN resource for \nEnglish- Spanish medical machine translation \nand terminologies: census of parallel corpora, glossaries and term translations. LREC \nMultilingualBIO: Multilingual Biomed ical \nText Processi ng, pages 32- 39. \nKyungjin Hwang\n44Procesamiento de Expresiones Multipalabra en\ng\nallego mediante Aprendizaje Profundo\nMultiword expressions processing in Galician using Deep\nLearning\nV\u00b4 \u0131ctor Darriba1, Yerai Doval1, Elmurod Kuriyozov2\n1Universidad de Vigo, Depto. de Inform\u00b4 atica\n2Universidad de A Coru\u02dc na, CITIC\n{darriba,yerai.doval }@uvigo.es, e.kuriyozov@udc.es\nResumen: El tratamiento de Expresiones Multipalabra es todav\u00b4 \u0131a una tarea pen-\ndiente en el Procesamiento del Lenguaje Natural. En este trabajo pretendemos de-\nterminar experimentalmente la utilidad de los modelos de Aprendizaje Autom\u00b4 atico\npara el procesamiento de Expresiones Multipalabra en gallego. Para ello usamos\nCORGA, un corpus con 40 millones de palabras, con el cual entrenamos modelos\ntransformer de Aprendizaje Profundo, y comparamos su rendimiento con el de mo-\ndelos m\u00b4 as tradicionales de campo aleatorio condicional.\nPalabras clave: Expresiones multipalabra, aprendizaje autom\u00b4 atico, transformers,\ngallego.\nAbstract: Treatment of Multiword Expressions is still a pending task in Natural\nLanguage Processing. In this work, we want to experimentally determine the useful-\nness of Machine Learning models for Multiword Expression processing in Galician.\nWith that aim, we use CORGA, a 40 million word corpus, with which we train Deep\nLearning-based transformers , comparing their performances with those of more tra-\nditional conditional random \ufb01elds.\nKeywords: Multiword expressions, machine learning, transformers, Galician.\n1 Introducci\u00b4 on\nAunquenoexisteuna \u00b4 unicade\ufb01nici\u00b4 onuniver-\nsalmente aceptada, se suelen considerar como\nExpresiones Multipalabra (EMs) las \u201ccombi-\nnaciones de palabras habituales y recurrentes\ndel lenguaje com\u00b4 un\u201d (Firth, 1957). Hay mu-\nchos tipos diferentes: frases hechas (\u201ccantar\nlas cuarenta\u201d), colocaciones (\u201cderecho de ve-\nto\u201d), nombres propios (\u201cMiguel P\u00b4 erez\u201d), etc.\nLasEMssoncomunesentodoslosidiomas\ny dominios (Jackendo\ufb00, 1997). Por ejemplo,\nRamisch (2015) informa que el 51,4% de los\nnombres y el 25,5% de los verbos de la ver-\nsi\u00b4 on inglesa de WordNet son multipalabra.\nDesde el punto de vista l\u00b4 exico, muchas EMs\ntienden a comportarse como palabras indivi-\nduales, y su sem\u00b4 antica no tiene por qu\u00b4 e resul-\ntar de la simple composici\u00b4 on del signi\ufb01cado\nde sus palabras constituyentes. Por lo tanto,\nes aconsejable incorporar el tratamiento de\nEMs en tareas basadas en Procesamiento del\nLenguaje Natural (PLN) (Ramisch, 2015).\nEn la literatura se distinguen dos tareas\nen el procesamiento de EMs (Constant et al.,\n2017): detecci\u00b4 on e identi\ufb01caci\u00b4 on. La detec-ci\u00b4 on se centra en encontrar EMs no vistas\nanteriormente en corpora textuales, con el \ufb01n\nde almacenarlas en alg\u00b4 un tipo de reposito-\nrio (como un lexic\u00b4 on) para su uso futuro: por\nejemplo, detectar como EM el nombre propio\n\u201cOseja de Sajambre\u201d cuando aparezca en un\ncorpus. Por su parte, la identi\ufb01caci\u00b4 on consis-\nte en anotar autom\u00b4 aticamente las EMs pre-\nsentes en un texto. Ambas tareas est\u00b4 an re-\nlacionadas. Una lista de EMs obtenidas me-\ndiante detecci\u00b4 on pueden ser utilizadas como\nun recurso externo por herramientas de iden-\nti\ufb01caci\u00b4 on. Por otra parte, una herramienta\nde identi\ufb01caci\u00b4 on con capacidad de generali-\nzaci\u00b4 on a partir de ejemplos conocidos puede\nusarse para detectar nuevas EMs.\nEste trabajo se centra en el uso de herra-\nmientasdeAprendizajeAutom\u00b4 atico(AA)su-\npervisadoparaelprocesamientodeEMsenel\nidioma gallego. Para ello, tratamos la detec-\nci\u00b4 on e identi\ufb01caci\u00b4 on de EMs como tareas de\netiquetaci\u00b4 on de secuencias. Cada palabra in-\ndividual en el corpus de entrenamiento recibe\nuna etiqueta que indica si forma parte de una\nEM o no, de modo que el modelo entrenado\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 45-57\nrecibido 29-04-2021 revisado 04-06-2021 aceptado 07-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-4\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalcon dicho corpus aplica el mismo esquema de\nanot\naci\u00b4 on a cualquier nuevo texto que reciba\ncomo entrada, intentando asignar la secuen-\ncia de etiquetas m\u00b4 as id\u00b4 onea a cada frase de\ndicho texto.\nEn este sentido, nuestro prop\u00b4 osito es tes-\ntear la viabilidad de dos modelos modernos\nde Aprendizaje Profundo (AP): BERT mul-\ntilingue (mBERT) (Devlin et al., 2019) y\nXLM-RoBERTa (XLM-R) (Conneau et al.,\n2020). Se trata de dos arquitecturas trans-\nformer, basadas en la generaci\u00b4 on de modelos\nde lenguaje para m\u00b4 ultiples idiomas, que des-\npu\u00b4 es pueden ser reentrenados para una tarea\nconcreta. Para estimar la posible mejora in-\ntroducida por estos modelos respecto a apro-\nximaciones previas, utilizaremos como refe-\nrencia mwetoolkit (Ramisch, 2015), una he-\nrramienta de que permite entrenar modelos\nde campo aleatorio condicional ( Conditional\nRandom Fields o CRF) (La\ufb00erty, McCallum,\ny Pereira, 2001) para la anotaci\u00b4 on autom\u00b4 ati-\nca de EMs en textos.\nPara entrenar y probar dichos modelos\nusamos el Corpus de Referencia do Galego\nActual(CORGA)(CentroRam\u00b4 onPi\u02dc neiropa-\nra a Investigaci\u00b4 on en Humanidades, 2019a),\nque cuenta con una versi\u00b4 on con anotaci\u00b4 on\nmorfosint\u00b4 actica y lematizaci\u00b4 on realizadas au-\ntom\u00b4 aticamente, en la que se analizan como\nuna sola unidad, en forma de \u201cpalabras con\nespacios\u201d, dos tipos de EMs: Entidades Nom-\nbradas (Named Entities o NEs) y locuciones.\nLa estructura del resto de este art\u00b4 \u0131culo es\nla siguiente. La Secci\u00b4 on 2 presenta trabajos\nanteriores sobre procesamiento de EMs usan-\ndoAA,yenlaSecci\u00b4 on3sedescribenlameto-\ndolog\u00b4 \u0131a, modelos y datos empleados en nues-\ntros experimentos. En la Secci\u00b4 on 4 se pre-\nsentan los resultados de los mismos, y en la\nSecci\u00b4 on 5 se detallan nuestras conclusiones.\n2 Trabajos relacionados\nCentraremos nuestro an\u00b4 alisis principalmente\nen los trabajos basados en t\u00b4 ecnicas de AA\nsupervisado, m\u00b4 as populares en la identi\ufb01ca-\nci\u00b4 on de EMs, aunque tambi\u00b4 en veremos como\nse han usado en la tarea de detecci\u00b4 on.\n2.1 Identi\ufb01caci\u00b4 on\nEn esta tarea es habitual utilizar alg\u00b4 un ti-\npo de modelo de etiquetaci\u00b4 on de secuencias.\nUn ejemplo es Blunsom y Baldwin (2006),\nque asignan tipos l\u00b4 exicos a las palabras de\nla secuencia de entrada usando un modeloCRF, mientras que Vincze, Nagy T., y Be-\nrend (2011) entrenan un CRF con un corpus\npreviamente anotado con EMs, junto con le-\nxicones externos, y Diab y Bhutada (2009)\nentrenan M\u00b4 aquinas de Vectores de Soporte\n(SVMs) sobre un corpus anotado con cons-\ntrucciones nombre-verbo, con el objetivo de\ndistinguir aquellas que son idiom\u00b4 aticas.\nEs frecuente integrar informaci\u00b4 on de dic-\ncionarios externos de EMs para calcular ca-\nracter\u00b4 \u0131sticas de las entradas usadas en el pro-\nceso de entrenamiento. As\u00b4 \u0131, Constant y Si-\ngogne (2011) entrenan un modelo CRF pa-\nra la realizaci\u00b4 on conjunta del an\u00b4 alisis l\u00b4 exi-\nco y etiquetaci\u00b4 on morfosint\u00b4 actica partiendo\nde un banco de \u00b4 arboles. Por su parte, Cons-\ntant, Sigogne, y Watrin (2012) comparan el\nrendimiento de un etiquetador de secuencias\n(un CRF) frente a un analizador sint\u00b4 actico\ncon reordenaci\u00b4 on de \u00b4 arboles. Las caracter\u00b4 \u0131sti-\ncas obtenidas a partir de lexicones usadas en\neste trabajo son re\ufb01nadas por Schneider et\nal. (2014) para el entrenamiento de un per-\nceptr\u00b4 onestructurado, usandoun juegode eti-\nquetas que permite anotar EMs no contiguas\ny EMs anidadas. Finalmente, Riedl y Bie-\nmann (2016) intentan determinar el impacto\nen el rendimiento de caracter\u00b4 \u0131sticas obteni-\ndas a partir de anotaci\u00b4 on manual y mediante\nanotaci\u00b4 on autom\u00b4 atica, partiendo de las usa-\ndas en los trabajos anteriores.\nOtra aproximaci\u00b4 on es usar informa-\nci\u00b4 on sint\u00b4 actica en el proceso de identi\ufb01ca-\nci\u00b4 on (Constant, Sigogne, y Watrin, 2012). Di-\ncha informaci\u00b4 on puede integrarse como ca-\nracter\u00b4 \u0131sticas de un modelo de etiquetaci\u00b4 on\nde secuencias (Maldonado et al., 2017), pe-\nro es m\u00b4 as habitual identi\ufb01car EMs durante\nel an\u00b4 alisis sint\u00b4 actico, entrenando un mode-\nlo a partir de un banco de \u00b4 arboles anotado\ncon EMs. Se pueden usar diferentes formalis-\nmos gramaticales: Green et al. (2011) traba-\njan con gram\u00b4 aticas de sustituci\u00b4 on de \u00b4 arbo-\nles, anotando cada EM como un sub\u00b4 arbol\nde estructura plana, aproximaci\u00b4 on tambi\u00b4 en\nusada por Green, de Marne\ufb00e, y Manning\n(2013) junto con gram\u00b4 aticas independientes\ndel contexto probabil\u00b4 \u0131sticas. Otra posibilidad\nes el an\u00b4 alisis sint\u00b4 actico de dependencias, em-\npleando arcos espec\u00b4 \u0131\ufb01cos para denotar los\ncomponentes de una EM (Vincze, Zsibrita,\ny Nagy T., 2013; Simk\u00b4 o, Kov\u00b4 acs, y Vincze,\n2017). Tambi\u00b4 en se pueden escoger diferentes\nrepresentaciones seg\u00b4 un el tipo de EM: Can-\ndito y Constant (2014) y Constant y Nivre\nV\u00edctor Darriba, Yerai Doval, Elmurod Kuriyozov\n46(2016) usan relaciones de dependencia pa-\nra\nlas expresiones sint\u00b4 acticamente regulares\ny una estructura plana para las irregulares.\nLa popularidad creciente de las Redes de\nNeuronas ha llevado a su empleo en la identi-\n\ufb01caci\u00b4 on de EMs (Legrand y Collobert, 2016),\nhaciendo uso de representaciones vectoriales\ncontinuas ( word embeddings ). M\u00b4 ultiples ar-\nquitecturas de red han sido propuestas pa-\nra esta tarea: Klyueva, Doucet, y Straka\n(2017) y Zampieri et al. (2018) usan Gated\nRecurrent Units (GRUs), mientras que Tas-\nlimipoor y Rohanian (2018) combinan Re-\ndes Convolucionales y Long Short-Term Me-\nmory(LSTMs). Recientemente, se ha em-\npezado a hacer uso de arquitecturas trans-\nformer: Taslimipoor, Bahaadini, y Kochmar\n(2020) usan BERT como modelo de lengua-\nje para el aprendizaje conjunto de EMs y\nan\u00b4 alisis sint\u00b4 actico de dependencias, mientras\nque Kurfal\u0131 (2020) emplea BERT y mBERT\npara la identi\ufb01caci\u00b4 on de EMs verbales.\n2.2 Detecci\u00b4 on\nLo m\u00b4 as habitual en esta tarea es utilizar\nm\u00b4 etodos no supervisados, distinguiendo EMs\nen base a sus puntuaciones en m\u00b4 etricas de\nasociaci\u00b4 on,sustitubilidaddepalabrascompo-\nnentes o similitud sem\u00b4 antica. Sin embargo,\ntambi\u00b4 en existen trabajos basados en t\u00b4 ecni-\ncas supervisadas de AA, usando a menudo\nlas m\u00b4 etricas mencionadas como caracter\u00b4 \u0131sti-\ncas en el aprendizaje junto con informaci\u00b4 on\nling\u00a8 u\u00b4 \u0131stica. Por ejemplo, Pecina (2009) uti-\nliza modelos basados en Regresi\u00b4 on Log\u00b4 \u0131sti-\nca, An\u00b4 alisis Discriminante Lineal y SVMs\npara el descubrimiento de colocaciones. Los\nSVMs son tambi\u00b4 en usados por Farahmand\ny Martins (2014) en conjunci\u00b4 on con medi-\ndas estad\u00b4 \u0131sticas y caracter\u00b4 \u0131sticas contextua-\nles de las EMs, como pre\ufb01jos y su\ufb01jos. Por su\nparte, Lapata y Lascarides (2003) comparan\nel rendimiento de \u00b4 arboles de decisi\u00b4 on y cla-\nsi\ufb01cadores bayesianos para extraer nombres\ncompuestos,mientrasqueDubremetzyNivre\n(2014) prueban varios algoritmos de aprendi-\nzaje para extraer grupos nombre-nombre y\nnombre-adjetivo, obteniendo los mejores re-\nsultados con redes bayesianas. Los modelos\nas\u00b4 \u0131 entrenados pueden utilizarse para deter-\nminar cuales de las caracter\u00b4 \u0131sticas usadas en\nel entrenamiento son m\u00b4 as \u00b4 utiles (Ramisch et\nal., 2008), o para intentar reducir su depen-\ndencia sobre recursos de entrenamiento, co-\nmo Rondon, Caseli, y Ramisch (2015), queimplementan un sistema iterativo basado en\nSVMs que, partiendo de un lexic\u00b4 on y mode-\nlo iniciales, mina texto de la web, anota sus\nEMs y usa el nuevo texto anotado para en-\ntrenar una nueva versi\u00b4 on del modelo.\n2.3 Nuestra aportaci\u00b4 on\nEste trabajo usa un enfoque similar al\nde Kurfal\u0131 (2020), estudiando la viabilidad de\nlas arquitecturas transformer para el trata-\nmiento de EMs, si bien hay grandes diferen-\ncias entre ambas aproximaciones. En primer\nlugar, nos centraremos en un \u00b4 unico idioma,\nel gallego, minoritario y pobre en recursos\npara PLN, y tratamos de determinar la me-\njor con\ufb01guraci\u00b4 on de entrenamiento en base a\nla informaci\u00b4 on l\u00b4 exica contenida en CORGA.\nAdem\u00b4 as, probamos dos arquitecturas distin-\ntas, mBERT y XLM-R, y usamos un modelo\nCRFparadeterminarsiofrecenunmejorren-\ndimiento que aproximaciones m\u00b4 as tradiciona-\nles. Finalmente, en lugar de centrarnos sola-\nmente en la tarea de identi\ufb01caci\u00b4 on, tambi\u00b4 en\ntratamos de medir el rendimiento de estas ar-\nquitecturas en el reconocimiento de nuevas\nEMs para la tarea de detecci\u00b4 on.\n3 Enfoque\nEn esta secci\u00b4 on vamos a examinar los recur-\nsos usados en nuestros experimentos, as\u00b4 \u0131 co-\nmo el dise\u02dc no de los mismos.\n3.1 CORGA\nElCorpus de Referencia do Galego Ac-\ntual(Centro Ram\u00b4 on Pi\u02dc neiro para a Investi-\ngaci\u00b4 on en Humanidades, 2019a) es una co-\nlecci\u00b4 on documental, abierta y representativa\nque recoge textos y transcripciones de habla\nen gallego, desde 1975 hasta la actualidad,\ncon el objetivo de proporcionar datos para el\nestudio ling\u00a8 u\u00b4 \u0131stico de dicho idioma. Contiene\n40.178.271 millones de palabras y 48.184.012\nmillones de elementos gramaticales (palabras\noentidadesl\u00b4 exicasamalgamadasenpalabras,\nsignos de puntuaci\u00b4 on, etc) procedentes de\ndiversas fuentes: peri\u00b4 odicos, revistas, libros,\nguiones televisivos, blogs y transcripciones\ndeprogramasradiof\u00b4 onicos(Dom\u00b4 \u0131nguezNoya,\nL\u00b4 opez Mart\u00b4 \u0131nez, y Barcala Rodr\u00b4 \u0131guez, 2019).\nPara posibilitar el estudio del idioma des-\nde m\u00b4 ultiples perspectivas, CORGA est\u00b4 a eti-\nquetado morfosint\u00b4 acticamente y lematizado\nde manera autom\u00b4 atica. Para ello, se ha em-\npleado el Etiquetador Lematizador do Galego\nActual(XIADA) (Centro Ram\u00b4 on Pi\u02dc neiro pa-\nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo\n47ra a Investigaci\u00b4 on en Humanidades, 2019b),\nuna\nherramienta basada en Modelos de Mar-\nkoventrenadasobreunsubconjuntodeCOR-\nGA con 744.530 elementos gramaticales, con\nanotaci\u00b4 onautom\u00b4 aticacorregidaporexpertos.\nPara recoger la rica morfolog\u00b4 \u0131a de la lengua\ngallega, se usa un juego de 453 etiquetas.1\nAunque no se ha medido la exactitud de la\nversi\u00b4 on actual del etiquetador sobre COR-\nGA,unaversi\u00b4 onpreviaalcanz\u00b4 oun96%sobre\ntextos period\u00b4 \u0131sticos (Dom\u00b4 \u0131nguez Noya, Bar-\ncala Rodr\u00b4 \u0131guez, y Molinero \u00b4Alvarez, 2009).\nEl proceso de etiquetaci\u00b4 on/lematizaci\u00b4 on\nde XIADA incluye la identi\ufb01caci\u00b4 on de dos\ntipos de EMs: a) NEs, como \u201cUniversida-\nde de Santiago\u201d, \u201cMinisterio de Agricultu-\nra\u201d, \u201cFondo Monetario Internacional\u201d, etc; y\nb) Locuciones adverbiales, preposicionales y\nconjuntivas, como \u201cpor tanto\u201d, \u201cpor se aca-\nso\u201d (\u201cpor si acaso\u201d), \u201csen d\u00b4 ubida\u201d(\u201csin du-\nda\u201d),\u201csi ben\u201d (\u201csi bien\u201d), etc. Para el pro-\nceso de identi\ufb01caci\u00b4 on de estas \u00b4 ultimas, XIA-\nDA hace uso de un lexic\u00b4 on extra\u00b4 \u0131do de varias\nfuentes, con un total de 628 locuciones.\nDicha tarea de identi\ufb01caci\u00b4 on es una carac-\nter\u00b4 \u0131stica opcional del etiquetador, por lo que\nhemos dispuesto de dos versiones de CORGA\ncon etiquetaci\u00b4 on y lematizaci\u00b4 on autom\u00b4 aticas:\nuna con cada EM etiquetada y lematizada\ncomo una unidad, y otra en donde cada en-\ntidad constituyente de una EM es etiquetada\ny lematizada por separado. Ello permite re-\ncuperar la informaci\u00b4 on morfosint\u00b4 actica de los\nconstituyentes de las EMs, combinando la in-\nformaci\u00b4 on de ambas versiones del corpus.\nOtra caracter\u00b4 \u0131stica importante es la se-\nparaci\u00b4 on de amalgamas. El idioma gallego\npresenta un gran n\u00b4 umero de palabras gra-\nmaticales formadas por amalgamas de en-\ntidades l\u00b4 exicas, como verbos con cl\u00b4 \u0131ticos o\ncontracciones de preposiciones, art\u00b4 \u0131culos o\ndeterminantes: \u201cenc\u00b4 ollese\u201d = \u201cencolle\u201d+\u201cse\u201d\n(\u201cse encoge\u201d), \u201ccoa\u201d=\u201ccon\u201d+\u201ca\u201d (\u201ccon la\u201d),\n\u201cnaqueloutro\u201d= \u201cen\u201d+\u201caquel\u201d+\u201coutro\u201d(\u201cen\naquel otro\u201d), etc. XIADA separa y etique-\nta/lematiza esos constituyentes l\u00b4 exicos, lo\nque hace posible el tratamiento de EMs di-\nrectamente sobre las formas amalgamadas o\nsobre las entidades l\u00b4 exicas que las integran.\n3.2 Modelos de aprendizaje\nEn nuestros experimentos empleamos mode-\nlos de AP bien conocidos en el estado del\n1Verhttp://corpus.cirp.gal/xiada/etiquetario/taboa/\npara una descripci\u00b4 on de las mismas.arte en PLN y basados en la arquitectura\ntransformer : mBERT (Devlin et al., 2019) y\nXLM-R(Conneauetal.,2020).Elcomponen-\nte principal en estos modelos es el mecanis-\nmo de atenci\u00b4 on, mediante el cual se pueden\nrealizar c\u00b4 alculos para un elemento de la se-\ncuencia de entrada ponderando las contribu-\nciones del resto de dicha secuencia. Ello viene\na sustituir al mecanismo de recurrencia de las\nredes basadas en celdas LSTMs (Hochreiter\ny Schmidhuber, 1997) o GRUs (Cho et al.,\n2014) que s\u00b4 olo consideraban para sus c\u00b4 alculos\nel estado anterior al procesar en secuencia el\nelemento actual. Sin embargo, esta indudable\nventaja se obtiene a costa de un aumento sig-\nni\ufb01cativo de la complejidad computacional.\nOtro elemento caracter\u00b4 \u0131stico de estos mo-\ndelos es su entrenamiento en dos etapas: una\nprimera de preentrenamiento en una tarea no\nsupervisada, normalmente una variaci\u00b4 on del\nmodelado del lenguaje, y una segunda de en-\ntrenamiento (semi-)supervisado en la tarea\nespec\u00b4 \u0131\ufb01ca que se desea resolver, como el pro-\ncesamiento de EMs en nuestro caso.\nTanto mBERT como XLM-R est\u00b4 an preen-\ntrenados sobre grandes cantidades de texto\nen varios idiomas: volcados de la Wikipedia\nen cada idioma en el primer caso y una ver-\nsi\u00b4 on \ufb01ltrada de Common Crawl particionada\npor idioma en el segundo. El gallego es uno\nde los idiomas incluidos, si bien la propor-\nci\u00b4 on del mismo en los datos de entrenamien-\nto de estas arquitecturas es muy peque\u02dc na en\ncomparaci\u00b4 on con idiomas m\u00b4 as mayoritarios\ncomo el ingl\u00b4 es.2Sin embargo, dado el alto\ncoste de reentrenar y optimizar estos mode-\nlos desde el principio, consideramos a priori\nestos recursos multiling\u00a8 ues como adecuados\npara nuestros experimentos. La principal di-\nferencia entre ambas arquitecturas radica en\nla escala del corpus usado durante sus respec-\ntivos preentrenamientos, mucho mayor en el\ncaso de XLM-R, que adem\u00b4 as posee un mayor\nn\u00b4 umerodepar\u00b4 ametros.Adem\u00b4 as,enelcasode\nmBERT utilizamos dos variantes: una preen-\ntrenada con la totalidad del texto pasado a\nmin\u00b4 usculas, y otra con el texto original.\nA nivel de implementaci\u00b4 on, usamos la li-\nbrer\u00b4 \u0131a Python Transformers de HuggingFa-\nce (Wolf et al., 2020) junto con los modelos\npreentrenados y distribuidos desde sus cana-\nles. Dichos modelos poseen un tama\u02dc no con-\nsiderable: 179 millones de par\u00b4 ametros distri-\n2Tal y como se puede ver en la Figura 1 de Con-\nneau et al. (2020).\nV\u00edctor Darriba, Yerai Doval, Elmurod Kuriyozov\n48buidos en 12 capas para mBERT (168 millo-\nne\ns de par\u00b4 ametros en la variante preentrena-\ndaentextoenmin\u00b4 usculas)yalrededorde270\nmillones de par\u00b4 ametros distribuidos de forma\nsimilar en el caso de XLM-R. Para el entre-\nnamiento, mantenemos los valores por defec-\nto en la mayor parte de los hiperpar\u00b4 ametros\ny especi\ufb01camos un tama\u02dc no de batchy una\nlongitud m\u00b4 axima de secuencia de 64 elemen-\ntos, sin realizar ning\u00b4 un proceso de optimiza-\nci\u00b4 on de estos valores. El proceso se desarrolla\ndurante una sola iteraci\u00b4 on sobre el conjunto\ncompleto de datos de entrenamiento.\nPara comparar los resultados de las t\u00b4 ecni-\ncas de AP con aproximaciones m\u00b4 as tradicio-\nnales, usamos mwetoolkit (Ramisch, 2015).\nEsta herramienta permite entrenar un mo-\ndelo CRF (La\ufb00erty, McCallum, y Pereira,\n2001), implementado con CRFSuite (Okaza-\nki, 2007). El usuario puede determinar las\ncaracter\u00b4 \u0131sticas extra\u00b4 \u0131das de las entradas del\nmodelo, que en nuestro caso son las sugeri-\ndas por los autores de mwetoolkit (unigra-\nmas, bigramas y trigramas de etiquetas mor-\nfosint\u00b4 acticas y lemas, alrededor de la palabra\nactual), y que est\u00b4 an inspiradas por los traba-\njos de Constant y Sigogne (2011), Schneider\net al. (2014) y Riedl y Biemann (2016). Con\nrespecto a los par\u00b4 ametros de la arquitectura,\nel \u00b4 unico con\ufb01gurable desde mwetoolkit es el\ncoe\ufb01ciente \u03bbde regularizaci\u00b4 on L2, teniendo el\nresto los valores por defecto \ufb01jados por CRF-\nSuite. Tras algunas pruebas preliminares, he-\nmos elegido \u03bb=0,1 para todos los tests.\nFinalmente, tambi\u00b4 en hemos implementa-\ndo un modelo simple de referencia ( Baseli-\nne), consistente en extraer del corpus de en-\ntrenamiento las secuencias de etiquetas mor-\nfosint\u00b4 acticas correspondientes a EMs, elimi-\nnar aquellas con un n\u00b4 umero de ocurrencias\ninferior a 500 y anotar como EMs en el con-\njunto de prueba las entidades l\u00b4 exicas con esas\nsecuencias de etiquetas. En caso de que ha-\nya dos o m\u00b4 as posibles EMs que empiezan o\nterminan en la misma entidad l\u00b4 exica, selec-\ncionamos la m\u00b4 as larga; y si hay dos posibles\nEMs que se solapan, nos quedamos con la co-\nrrespondiente a la secuencia de etiquetas m\u00b4 as\nfrecuente en el conjunto de entrenamiento.\n3.3 Dise\u02dc no experimental\nPara poder entrenar los modelos de apren-\ndizaje es necesario disponer de un esque-\nma de etiquetaci\u00b4 on para EMs. Hemos elegidoIOB2 (Ramshaw y Marcus, 1995),3que cons-\nta de tres etiquetas: B para marcar el elemen-\nto gramatical en donde comienza una EM, I\npara el resto de constituyentes de una expre-\nsi\u00b4 on,yOparaloselementosfueradeunaEM.\nEn el caso particular de mwetoolkit se usa\nel formato DiMSUM (Schneider et al., 2016),\nquepermiteincluirinformaci\u00b4 onadicional,co-\nmo el lema y etiqueta morfosint\u00b4 actica. En el\ncaso de usar palabras amalgamadas, sus eti-\nquetas morfosint\u00b4 acticas se generan autom\u00b4 ati-\ncamente como la concatenaci\u00b4 on de las etique-\ntas de sus entidades l\u00b4 exicas constituyentes\nPara encontrar el formato de CORGA que\nofrezca el mejor rendimiento para el trata-\nmiento de EMs, consideramos 8 con\ufb01guracio-\nnes de prueba para cada arquitectura, combi-\nnando las siguientes opciones: a) usando pa-\nlabras ortogr\u00b4 a\ufb01cas amalgamadas, o sus cons-\ntituyentes separados; b) usando todas las fra-\nsesdelcorpusenelaprendizaje,os\u00b4 ololasque\ncontienen EMs; y c) usando las palabras o le-\nmas tal y como aparecen en el corpus, o con-\nvirti\u00b4 endolas a min\u00b4 usculas. Inicialmente, esta\n\u00b4 ultima modalidad se incluy\u00b4 o para probar la\nversi\u00b4 on de mBERT preentrenada con texto\nen min\u00b4 usculas, pero la hemos extendido a las\notras arquitecturas.\nAlprobarelrendimientoendiferentescon-\n\ufb01guraciones de prueba, usamos la misma par-\ntici\u00b4 on del conjunto de datos inicial en tres\nsubconjuntos de entrenamiento, validaci\u00b4 on y\nprueba, con una proporci\u00b4 on aproximada del\n80%/10%/10% de las frases. El conjunto de\nentrenamiento se utiliza en el aprendizaje del\nmodelo. Una vez entrenado, su rendimiento\nen la predicci\u00b4 on de EMs se mide sobre el con-\njunto de prueba con las m\u00b4 etricas Accuracy ,\nPrecision, Recall4y valor F1. Los conjuntos\nde validaci\u00b4 on se han empleado para seleccio-\nnar el coe\ufb01ciente \u03bben mwetoolkit y el um-\nbral de ocurrencias de Baseline, adoptando\nlos valores asociados a los mejores resultados\nde F1 sobre dichos conjuntos. Tambi\u00b4 en que-\nremos emplearlos en el futuro para optimizar\nlos hiperpar\u00b4 ametros de mBERT y XLM-R.\nIdenti\ufb01caci\u00b4 on. Primero hemos \ufb01ltrado de\nCORGA fragmentos en idiomas extranjeros\ny frases cuya diferente segmentaci\u00b4 on en las\nversiones con o sin EMs hace imposible incor-\nporar los lemas y etiquetas a los constituyen-\n3DeInside-Outside-B eginning.\n4En espa\u02dc nol, Exactitud, Precisi\u00b4 on y Exhaustivi-\ndad, respectivamente, pero usaremos los nombres en\ningl\u00b4 es por su mayor difusi\u00b4 on.\nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo\n49Entrenamiento Validaci\u00b4 on Prueba\nTodas las EMs 36.183.656 (11.159.908) 3.647.604 3.474.748\nS\u00b4 olo NEs 31.619.486 (6.595.738) 3.379.553 3.333.069\nS\u00b4 olo Locuciones 28.431.686 (3.407.938) 2.970.140 2.976.18 1\nTabla 1: N\u00b4 umero de elementos gramaticales en los conjuntos de entrenamiento usados para\nlas pruebas de detecci\u00b4 on de EMs. Indicamos entre par\u00b4 entesis el tama\u02dc no de los conjuntos de\nentrenamiento incluyendo s\u00b4 olo las frases sin EMs.\ntesdelasEMsautom\u00b4 aticamente.Acontinua-\nci\u00b4 on, particionamos las secciones de CORGA\n(peri\u00b4 odicos, revistas, libros, guiones televisi-\nvos, blogs y transcripciones de programas ra-\ndiof\u00b4 onicos) de modo que el primer 80% de\nlas frases de cada secci\u00b4 on se asigna al conjun-\nto de entrenamiento, el siguiente 10% al de\nvalidaci\u00b4 on, y el 10% \ufb01nal al de prueba. De\neste modo, el n\u00b4 umero de elementos gramati-\ncales es de 38.871.031 para el conjunto de en-\ntrenamiento (14.173.449 si s\u00b4 olo incluimos las\nfrases con EMs), 4.955.264 para el de valida-\nci\u00b4 on y 4.573.942 para el de prueba, incluyen-\ndo 738.118 EMs.5. Con este particionamien-\nto, nos aseguramos de que s\u00b4 olo el 75,22% de\nlas EMs presentes en los conjuntos de prueba\nsean conocidas durante el entrenamiento.\nDetecci\u00b4 on. El particionamiento anterior\ntiene el problema de que el reducido n\u00b4 umero\nde locuciones distintas identi\ufb01cadas en COR-\nGA hace que la pr\u00b4 actica totalidad de las mis-\nmas sean conocidas durante el aprendizaje,\nlo que no permite estimar la capacidad de\ngeneralizaci\u00b4 on de las arquitecturas considera-\ndas para identi\ufb01car nuevas locuciones. Dicha\ncapacidad de generalizaci\u00b4 on puede ser \u00b4 util,\nadem\u00b4 as, en la tarea de detecci\u00b4 on, aplicando\nmodelos ya entrenados a nuevo texto y extra-\nyendo las EMs desconocidas encontradas.\nPor lo tanto, pretendemos evaluar el ren-\ndimientodelasarquitecturasconsideradasen\nla detecci\u00b4 on de EMs, poni\u00b4 endonos en el caso\nm\u00b4 as desfavorable: una partici\u00b4 on del corpus en\nla que no haya solapamiento entre las EMs\npresentes en los conjuntos de entrenamiento,\nvalidaci\u00b4 on y test. Para ello, seleccionamos un\nsubconjunto de las EMs presentes en COR-\nGA,loordenamosaleatoriamenteya\u02dc nadimos\nlas frases conteniendo el primer 80% de las\nexpresiones en el conjunto de entrenamiento,\nlas correspondientes a otro 10% en el conjun-\nto de validaci\u00b4 on y las del 10% \ufb01nal en el con-\n5Eltotal de elementos gramaticales supera a la\ncifra dada por Dom\u00b4 \u0131nguez Noya, Barcala Rodr\u00b4 \u0131guez,\ny Molinero \u00b4Alvarez (2009) porque nosotros contamos\nseparadamente los constituyentes de las EMs.junto de prueba. Completamos los conjuntos\na\u02dc nadiendo frases sin EMs con la misma pro-\nporci\u00b4 onde80%/10%/10%,aunquesinllegar\na usar CORGA en su totalidad.\nAdem\u00b4 as, realizamos el mismo proceso de\nevaluaci\u00b4 on de arquitecturas sobre las NEs y\nlocuciones, respectivamente. En el primer ca-\nso, generamos los conjuntos de entrenamien-\nto, validaci\u00b4 on y prueba como se detalla en\nel p\u00b4 arrafo anterior, pero usando s\u00b4 olo las fra-\nses con NEs, y cambiando a O las etiquetas\nIOB2 de las locuciones que aparezcan en di-\nchasfrases.Tambi\u00b4 ena\u02dc nadimosfrasessinEMs\npara las con\ufb01guraciones de prueba que las re-\nquieran. El mismo proceso se realiza para los\ntests sobre locuciones, pero prescindiendo en\nese caso de las frases con NEs.\nLa Tabla 1 incluye los tama\u02dc nos de los con-\njuntos de entrenamiento, validaci\u00b4 on y prueba\nusados para la detecci\u00b4 on de EMs, NEs multi-\npalabra y locuciones, y el n\u00b4 umero de estos.\nLos n\u00b4 umeros entre par\u00b4 entesis corresponden\nal tama\u02dc no de los conjuntos de entrenamien-\nto cuando incluimos s\u00b4 olo las frases sin EMs.\nEl n\u00b4 umero total de EMs en estos tests es de\n488.035,siendoel70,53%delasmismasNEs,\ny el 29.47% locuciones.\n4 Resultados\nEn esta secci\u00b4 on vamos a presentar los resul-\ntados de nuestros experimentos, para todas\nlas arquitecturas de aprendizaje y con\ufb01gura-\nciones de prueba, tanto en el caso de la iden-\nti\ufb01caci\u00b4 on como de la detecci\u00b4 on de EMs.\n4.1 Identi\ufb01caci\u00b4 on\nLos resultados de los experimentos sobre\nidenti\ufb01caci\u00b4 on de EMs se presentan en la Ta-\nbla2.Comoseespeci\ufb01c\u00b4 oenlaSubsecci\u00b4 on3.3,\ntenemos 8 con\ufb01guraciones de prueba, con re-\nsultados para las cuatro arquitecturas de en-\ntrenamiento, para un total de 32 tests. En\ncada uno de ellos, medimos Accuracy (Acc),\nPrecision (P),Recall(R) y valor F1.\nA la izquierda de la tabla se muestran los\nresultados para las con\ufb01guraciones en la que\nV\u00edctor Darriba, Yerai Doval, Elmurod Kuriyozov\n50Con amalgamas Sin amalgamas\nAcc P R F1 Acc P R F1Todo\nMaysmBERT 99,83 96,32 96,81 96,57 99,84 96,38 96,78 96,58\nXLM\n-R 99,82 96,32 96,54 96,43 99,83 96,13 96,52 96,32\nmwetoolkit 99,84 97,3896,48 96,92 99,8597,49 96,3996,94\nBaseline 91 ,52 19,32 53,23 28,35 91,06 17,61 51,12 26,19MinsmBERT 99,28 88,24 87,13 87,68 99,32 87,81 87,19 87,50\nXLM-R 99,25 87,18 87,15 87,16 99,31 87,48 86,85 87,16\nmwetoolkit 99,84 97,3196,4296,86 99,8597,4196,4296,91\nBaseline 91 ,52 19,32 53,23 28,35 91,06 17,61 51,12 26,19Solo EMs\nMaysmBERT 97,05 46,32 98,26 62,96 99,16 76,39 98,30 85,97\nXLM\n-R 98,50 62,80 98,08 76,57 99,34 80,48 98,04 88,39\nmwetoolkit 99,78 93,1597,13 95,09 99,7993,1997,13 95,12\nBaseline 91 ,52 19,32 53,23 28,35 91,06 17,61 51,12 26,19MinsmBERT 97,03 49,78 92,15 64,64 97,41 50,84 91,81 65,44\nXLM-R 96,80 47,62 91,77 62,71 97,48 52,77 91,60 66,96\nmwetoolkit 99,77 92,9997,0794,99 99,7993,0897,1695,07\nBaseline 91 ,52 19,32 53,23 28,35 91,06 17,61 51,12 26,19\nTabla 2: Resultados de los tests de identi\ufb01caci\u00b4 on de EMs.\nse\nha usado texto con palabras amalgama-\ndas, mientras que a la derecha aparecen los\nresultados para el entrenamiento con entida-\ndes l\u00b4 exicas sin amalgamas. \u201cTodo\u201d identi\ufb01ca\na las con\ufb01guraciones en las que se ha usado\ntodo el corpus en el proceso de aprendizaje y\nprueba, mientras que \u201cSolo EMs\u201d correspon-\nde a los casos en los que s\u00b4 olo hemos usado las\nfrases que incluyen EMs en el conjunto de en-\ntrenamiento. Finalmente, \u201cMins\u201d correspon-\nde a los tests en los que hemos usado texto\nen min\u00b4 usculas y \u201cMays\u201d a aquellos en los que\nhemos mantenido las may\u00b4 usculas y min\u00b4 uscu-\nlas tal y como aparecen en CORGA.\nRespecto a los valores de las m\u00b4 etricas, re-\npresentanporcentajesy,pormotivosdeespa-\ncio, se representan con dos cifras decimales.\nSe destaca en negrita el mejor resultado de\ncada m\u00b4 etrica para todos los tests, y se mues-\ntran subrayados los mejores resultados en ca-\nda con\ufb01guraci\u00b4 on de prueba. Para determinar\nlos mejores valores se han usado m\u00b4 as de dos\ncifras decimales.\nTambi\u00b4 en hemos usado la prueba de los\nrangos con signo de Wilcoxon para compro-\nbar la signi\ufb01cancia estad\u00b4 \u0131stica de las conclu-\nsiones obtenidas analizando la tabla. Para\nahorrar espacio, s\u00b4 olo citaremos el valor de p\nen la minor\u00b4 \u0131a de casos en los que p\u22650,05.\nLo primero que salta a la vista en la\ntabla 2 es el liderazgo en rendimiento de\nmwetoolkit. Obtiene los mejores valores pa-\nraAccuracy (99,85%), Precision (97,49%),\ny F1 (96,94%), mientras que mBERT s\u00b4 olo\nes capaz de aventajarlo en Recall(98,30%).\nAdem\u00b4 as, mwetoolkit obtiene los mejores va-\nloresparatodaslasm\u00b4 etricasentodaslascon-\ufb01guraciones de prueba, excepto Recall, para\nla que mBERT es el mejor en las cuatro con-\n\ufb01guraciones que usan texto con may\u00b4 usculas\ny min\u00b4 usculas ( p= 0,055).\nCon respecto a las con\ufb01guraciones de\nprueba, los mejores valores de las m\u00b4 etricas\nse agrupan en la con\ufb01guraci\u00b4 on \u201cSin amal-\ngamas\u201d+\u201cTodo\u201d+\u201cMays\u201d, excepto en el ca-\nso deRecall, cuyo mejor caso es \u201cSin amal-\ngamas\u201d+\u201cSolo EMs\u201d+\u201cMays\u201d. Comparando\ncada opci\u00b4 on de con\ufb01guraci\u00b4 on, entrenar sin\namalgamas mejora los valores de Accuracy ,\nPrecision y F1, respectivamente, en un 75%\n(p= 0,137), 62,5% (p = 0,353) y 62,5%\n(p= 0,372) de los casos, aunque entrenar\ncon amalgamas consigue un mejor Recallen\nel 75% de los tests.\nPara comparar los resultados de \u201cTodo\u201d\nvs \u201cSolo EMs\u201d y \u201cMays\u201d vs \u201cMin\u201d, hemos\nexclu\u00b4 \u0131do Baseline, dado que obtiene el mis-\nmo rendimiento en cada par de opciones.6\nEl entrenamiento con \u201cTodo\u201d obtiene mejo-\nres o iguales valores de Accuracy ,Precision\ny F1 en todos los tests, mientras que \u201cSolo\nEMs\u201d siempre genera un mejor Recall. Por\nsu parte, usar may\u00b4 usculas y min\u00b4 usculas tam-\nbi\u00b4 en mejora los valores de Accuracy (100%\nde los tests), Precision (91,67% de los tests),\nRecall(83,33%) y F1 (91,67%), lo que no\nresulta sorprendente, dado que buena parte\nde las EMs que estamos intentando identi\ufb01-\ncar son nombres propios. Hasta la versi\u00b4 on de\nmBERT preentrenada con texto en min\u00b4 uscu-\nlas(queusamosenelcaso\u201cMins\u201d)pareceob-\ntener peores resultados que la versi\u00b4 on preen-\n6Haremos lo mismo en los tests de detecci\u00b4 on.\nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo\n51Con amalgamas Sin amalgamas\nAcc P R F1 Acc P R F1Todo\nMaysmBERT 99,58 90,06 83,50 86,65 99,64 93,6881,58 87,22\nXLM\n-R 99,57 90,18 83,10 86,49 99,70 93,23 84,13 88,44\nmwetoolkit 99,58 92,9481,37 86,77 99,60 92,64 79,27 85,43\nBa\nseline 91,70 9,41 53,91 16,03 91,10 8,36 54,30 14,49MinsmBERT 99,76 43,73 68,42 53,36 99,79 41,74 63,29 50,31\nXLM\n-R 99,76 43,36 68,25 53,03 99,78 38,64 62,54 47,77\nmwetoolkit 99,58 92,79 81,2086,61 99,59 92,35 78,9185,11\nBaseline 91 ,70 9,41 53,91 16,03 91,10 8,36 54,30 14,49Solo EMs\nMaysmBERT 97,53 34,67 94,9450,79 99,44 76,59 83,56 79,92\nXLM\n-R 97,88 39,57 94,25 55,74 99,54 79,53 87,92 83,51\nmwetoolkit 99,52 87,6783,76 85,67 99,52 85,90 80,91 83,33\nBa\nseline 91,70 9,41 53,91 16,03 91,10 8,36 54,30 14,49MinsmBERT 96,72 31,87 86,68 46,61 97,14 30,20 83,06 44,30\nXLM\n-R 96,44 29,67 85,06 43,99 96,84 27,76 79,00 41,08\nmwetoolkit 99,52 87,5983,81 85,66 99,5285,6280,99 83,24\nBaseline 91 ,70 9,41 53,91 16,03 91,10 8,36 54,30 14,49\nTabla 3: Resultados de los tests de detecci\u00b4 on de EMs.\ntr\nenada con may\u00b4 usculas y min\u00b4 usculas (aun-\nque con p= 0,063, p= 0,125, p= 0,063,\np= 0,125 para Accuracy ,Precision, Recall\ny F1, respectivamente), lo que se repetir\u00b4 a en\nlos tests de detecci\u00b4 on de EMs.\nEnresumen,losvaloresdetodaslasm\u00b4 etri-\ncas son bastante altos, lo que prueba la viabi-\nlidad de las arquitecturas empleadas, si bien\nno son tan signi\ufb01cativos estad\u00b4 \u0131sticamente en\naquelloscasosenlosquenohay unaarquitec-\ntura/con\ufb01guraci\u00b4 on netamente superior. Esto\nes esperable y se repetir\u00b4 a en el resto de tests.\n4.2 Detecci\u00b4 on\nEn esta tarea, para la que intentamos prede-\ncirEMsdesconocidasenelentrenamiento,se-\nguimos las mismas con\ufb01guraciones de prueba\ny tests de signi\ufb01cancia que en la subsecci\u00b4 on\nanterior, pero realizamos tres juegos de tests\ndistintos, seg\u00b4 un las EMs que se procesan: to-\ndas, s\u00b4 olo NEs y s\u00b4 olo locuciones.\nTodas las EMs. En la Tabla 3 presenta-\nmos los resultados para el primer caso, en el\nque mBERT obtiene los mejores valores de\nAccuracy (99,79%), Precision (93,68%)y Re-\ncall(94,94%),XLM-RlideraenF1(88,44%),\ny mwetoolkit sigue obteniendo los mejores re-\nsultados en la mayor\u00b4 \u0131a de las con\ufb01guracio-\nnes de prueba: mejor Accuracy en 4 de las 8,\nmejorPrecision en 7 y mejor F1 en 6. Por\nsu parte, mBERT obtiene los mejores valo-\nres deRecallen 4 de dichas con\ufb01guraciones\n(p= 0,334).\nRespecto a las opciones de con\ufb01guraci\u00b4 on,\n\u201cSin amalgamas\u201d+\u201cTodo\u201d+\u201cMays\u201d obtiene\nla mejor Precision y F1, mientras que \u201cSinamalgamas\u201d+\u201cTodo\u201d+\u201cMins\u201dobtienelame-\njorAccuracy , y el mejor Recallcorresponde\na \u201cCon amalgamas\u201d+\u201cSolo EMs\u201d+\u201cMins\u201d.\nEntrenar con amalgamas mejora los valores\ndePrecision, Recally F1 en el 75% ( p=\n0,281), 68,75% y 75% de los casos, respec-\ntivamente. Sin embargo, empeora los valores\ndeAccuracy en un 78,75% de los tests. Por\nsu parte, \u201cTodo\u201d da mejores valores de Accu-\nracy,Precision y F1 en todos los tests, aun-\nque \u201cSolo EMs\u201d siempre da lugar a mejores\nvalores de Recall, una situaci\u00b4 on que ya en-\ncontr\u00b4 abamosenlostestsdeidenti\ufb01caci\u00b4 on.Fi-\nnalmente, usar may\u00b4 usculas y min\u00b4 usculas tal\ny como aparecen en CORGA tambi\u00b4 en mejora\nlos resultados de Accuracy ,Precision, Recall\nyF1enel58,33%(p = 0,190),100%,83,33%\ny 100% de los tests.\nEn general, observamos un empeoramien-\nto de los resultados con respecto a la tarea\nde identi\ufb01caci\u00b4 on, lo que es razonable dado\nque estamos asegurando que todas las EMs\npresentes en el conjunto de prueba sean des-\nconocidas durante el entrenamiento.\nS\u00b4 olo NEs multipalabra. Presentamos los\ntests correspondientes a este caso en la Ta-\nbla 4, con mBERT liderando en Accuracy\n(99,98%), Precision (98,14%) y valor F1\n(98,16%), mientras que XLM-R obtiene los\nmejores resultados en Recall(98,47%). Sin\nembargo, mwetoolkit obtiene mejores valores\nque las arquitecturas transformer enAccu-\nracy,Precision y F1 para 6 de las 8 con\ufb01gu-\nraciones posibles, y mejor Recallen 4. A mo-\ndo de comparaci\u00b4 on, mBERT obtiene el mejor\nRecallen 3 con\ufb01guraciones, y lidera en el res-\nV\u00edctor Darriba, Yerai Doval, Elmurod Kuriyozov\n52Con amalgamas Sin amalgamas\nAcc P R F1 Acc P R F1Todo\nMaysmBERT 99,89 93,08 96,38 94,70 99,9898,14 98,1898,16\nXLM-R 99,90 94,04 96,14 95,08 99,97 98,08 98,13 98,11\nmw\netoolkit 99,90 95,5095,23 95,36 99,92 95,76 95,63 95,69\nBa\nseline 99,48 72,96 86,70 79,24 99,52 72,97 86,65 79,22MinsmBERT 99,85 50,57 75,83 60,68 99,87 49,77 75,38 59,96\nXLM-R 99,84 47,09 76,05 58,17 99,86 48,55 75,32 59,04\nmwetoolkit 99,90 95,4095,0995,24 99,9195,6395,3695,49\nBaseline 99 ,48 72,96 86,70 79,24 99,52 72,97 86,65 79,22Solo EMs\nMaysmBERT 99,08 50,87 97,89 66,95 99,96 95,9598,40 97,16\nXLM-R 99,63 73,78 97,82 84,12 99,94 92,57 98,47 95,43\nmw\netoolkit 99,88 92,4696,92 94,64 99,89 91,89 97,13 94,44\nBa\nseline 99,48 72,96 86,70 79,24 99,52 72,97 86,65 79,22MinsmBERT 96,22 23,82 90,52 37,72 96,82 25,50 90,91 39,83\nXLM-R 96,45 25,41 90,47 39,68 96,96 26,73 90,10 41,22\nmwetoolkit 99,87 91,8096,6294,15 99,8891,8597,0294,36\nBaseline 99 ,48 72,96 86,70 79,24 99,52 72,97 86,65 79,22\nTabla 4: Resultados de los tests de detecci\u00b4 on de NEs multipalabr a.\nto de m\u00b4 etricas en s\u00b4 olo 2 casos.\nDe manera similar a lo que ocurr\u00b4 \u0131a en los\ntests de identi\ufb01caci\u00b4 on, los mejores valores de\nlasm\u00b4 etricasseencuentranenlacon\ufb01guraci\u00b4 on\n\u201cSin amalgamas\u201d+\u201cTodo\u201d+\u201cMays\u201d, excepto\nen el caso de Recall, cuyo mejor caso es \u201cSin\namalgamas\u201d+\u201cSolo EMs\u201d+\u201cMays\u201d. Compa-\nrando cada opci\u00b4 on de con\ufb01guraci\u00b4 on, entrenar\nsin amalgamas mejora los valores de Accu-\nracy,Recall,Precision y F1, respectivamen-\nte, en un 100%, 87,5%, 66,25% y 72,5% de\nlos tests, mientras que entrenar con \u201cTodo\u201d\npermite obtener mejores valores de Accuracy ,\nPrecision y F1 en todos los tests, y entrenar\ncon \u201cSolo EMs\u201d siempre genera mejores valo-\nres deRecall. Entrenar con \u201cMays\u201d da lugar\na mejores valores para todas las m\u00b4 etricas res-\npecto a usar s\u00b4 olo min\u00b4 usculas. En general, los\nvalores de las m\u00b4 etricas tienden a ser mejores\nque en los tests previos.\nS\u00b4 olo locuciones. Este caso es, sin duda, el\nque arroja peores resultados, como se puede\nobservar en la Tabla 5. La mejor Accuracy co-\nrresponde a mBERT (99,92%), la mejor Pre-\ncision(82,5%) a mwetoolkit, y los mejores\nvalores de Recall(73,88%) y F1 (55,41%) a\nXML-R. Todos los valores anteriores, excep-\ntoAccuracy , son sustancialmente m\u00b4 as bajos\nque sus hom\u00b4 ologos en los juegos de tests an-\nteriores, y el mejor resultado de Precision se\nobtiene a costa de un Recallmuy bajo (y\nviceversa). Adem\u00b4 as, no hay una arquitectu-\nra m\u00b4 as consistente que las dem\u00b4 as: mwetool-\nkit obtiene los mejores valores de Accuracy\n(p= 0,390) y Precision (p= 0,232) en 4 de\nlas 8 con\ufb01guraciones de test, XLM-R obtie-ne mejor Recall(p= 0,106) en 6, y mBERT\n(p= 0,390) y XML-R ( p= 0,232) obtienen\nlos mejores valores de F1 3 veces cada uno.\nCon respecto a las con\ufb01guraciones de\nprueba, entrenar con amalgamas mejora Ac-\ncuracy,Recall,Precision y F1, respectiva-\nmente, en un 56,25% ( p= 0,798), 93,75%,\n100% y 93,75% de los tests. Esto es comple-\ntamente opuesto a lo visto las pruebas ante-\nriores, en las que usar entidades l\u00b4 exicas no\namalgamadas mejoraba sustancialmente los\nresultados. Con respecto a las frases usadas\nen el entrenamiento, usar todas s\u00b4 olo ofre-\nce mejoras en Accuracy (en todos los tests),\nmientras que usar s\u00b4 olo las frases con EMs\nmejoraPrecision, Recally F1 en un 66,33%\n(p= 0,545), 100% y 87,33% de los casos, res-\npectivamente. De nuevo, ello contrasta fuer-\ntemente con lo que hab\u00b4 \u0131amos visto en tests\nanteriores.Finalmente,\u201cMays\u201dmejora Preci-\nsionyRecall(en ambos casos, en un 58,33%\nde los tests, con p= 0,238 y p= 0,193,\nrespectivamente), pero \u201cMins\u201d ofrece mejo-\nresAccuracy y F1 (ambas en un 58,33% de\nlos tests, con p= 0,361 para la segunda).\nLos resultados con locucionesson pobres y\ndiferentes al resto de los tests, con menos sig-\nni\ufb01cancia estadistica. Ello puede deberse a la\nmayor heterogeneidad de estas construccio-\nnes en relaci\u00b4 on con las NEs, al menor n\u00b4 umero\nde las mismas, y a que la lista de locuciones\nen gallego anotadas en CORGA no es nece-\nsariamentecompleta.Tampocopodemosdes-\ncartarquelosposibleserroresdeetiquetaci\u00b4 on\nmorfosint\u00b4 actica tambi\u00b4 en tengan un efecto en\nel rendimiento.\nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo\n53Con amalgamas Sin amalgamas\nAcc P R F1 Acc P R F1Todo\nMaysmBERT 99,48 78,55 41,78 54,55 99,43 18,53 2,70 4,72\nXLM\n-R 99,48 78,54 38,76 51,90 99,44 17,31 2,74 4,73\nmwetoolkit 99,43 82,05 32,41 46,47 99,43 3,59 0,34 0,62\nBa\nseline 91,55 2,84 28,03 5,15 90,46 1,22 19,09 2,29MinsmBERT 99,90 33,99 32,70 33,33 99,92 6,454,31 5,17\nXLM-R 99,91 37,87 38,71 38,28 99,92 5,90 4,49 5,10\nmw\netoolkit 99,43 82,00 32,41 46,46 99,43 3,59 0,34 0,62\nBa\nseline 91,55 2,84 28,03 5,15 90,46 1,22 19,09 2,29Solo EMs\nMaysmBERT 98,25 22,38 71,57 34,09 99,24 27,65 45,1034,28\nXLM-R 98,52 25,27 73,88 37,65 99,26 24,32 27,16 25,66\nmw\netoolkit 99,36 63,4339,53 48,70 99,3513,61 4,43 6,69\nBa\nseline 91,55 2,84 28,03 5,15 90,46 1,22 19,09 2,29MinsmBERT 99,12 39,61 64,91 49,20 99,21 24,71 38,14 29,99\nXLM-R 99,29 47,74 66,02 55,41 99,28 28,25 39,0932,80\nmwetoolkit 99,36 63,5839,49 48,72 99,35 13,52 4,39 6,63\nBa\nseline 91,55 2,84 28,03 5,15 90,46 1,22 19,09 2,29\nTabla 5: Resultados de los tests de detecci\u00b4 on de locuciones.\n5 C\nonclusiones y trabajo futuro\nPodemos extraer algunas conclusiones preli-\nminares de este trabajo. La primera de ellas\nes que las arquitecturas de AA consideradas\nparecen funcionar bastante bien en la tarea\nde identi\ufb01caci\u00b4 on, si bien los resultados son\nm\u00b4 as relevantes para NEs multipalabra que\npara locuciones, dado que estas \u00b4 ultimas son\nconocidas durante el entrenamiento casi en\nsu totalidad. En la tarea de detecci\u00b4 on, los re-\nsultados son prometedores para NEs, y algo\ndecepcionantes para las locuciones.\nEnloreferentealasarquitecturasconside-\nradas, mwetoolkit ha funcionado mejor de lo\nesperado en relaci\u00b4 on a las modelos transfor-\nmer, si bien se bene\ufb01cia de una informaci\u00b4 on\n(etiquetas morfosint\u00b4 acticas y lemas) no siem-\npre disponible. Adem\u00b4 as, el gallego s\u00b4 olo repre-\nsenta una peque\u02dc na parte de los recursos usa-\ndos para entrenar dichos modelos transfor-\nmer. Por otra parte, en la tarea de detecci\u00b4 on,\nmBERT tiende a obtener los valores m\u00b4 as al-\ntos en las m\u00b4 etricas consideradas m\u00b4 as veces\nque XLM-R, a pesar de usar un modelo m\u00b4 as\npeque\u02dc no y menos complejo.\nCon respecto a las posibles con\ufb01guracio-\nnes de CORGA para el entrenamiento, ex-\ncluyendo la detecci\u00b4 on de locuciones, los re-\nsultados de los tests indican que: a) se tiende\na obtener los mejores resultados entrenando\ncon entidades l\u00b4 exicas sin amalgamar, aunque\nen la mayor parte de las con\ufb01guraciones usar\namalgamas parece bene\ufb01cioso; b) incluir tex-\nto de entrenamiento sin EMs parece mejorar\nlos resultados en Precision y F1, pero em-\npeora los de Recall; c) por lo tanto, si nosinteresa recuperar el mayor n\u00b4 umero posible\nde EM a expensas de la Precision, entrenar\ns\u00b4 olo con frases que contengan EMs puede ser\nbuena idea; y d) pasar a min\u00b4 usculas el texto\nno parece mejorar los resultados, incluso en\nuna arquitectura (mBERT) espec\u00b4 \u0131\ufb01camente\npreentrenada con texto en min\u00b4 usculas.\nEn lo referente a la identi\ufb01caci\u00b4 on de locu-\nciones, nuestra intenci\u00b4 on es estudiar por qu\u00b4 e\nlosresultadostienenasertaninferioresenes-\ntos tests. Tambi\u00b4 en queremos comprobar has-\ntaqu\u00b4 epuntolaoptimizaci\u00b4 ondehiperpar\u00b4 ame-\ntros puede mejorar el rendimiento en los mo-\ndelostransformer . Para ello, nos proponemos\nhacer dicha optimizaci\u00b4 on en los tests en los\nque mBERT y XLM-R hayan sacado peores\nresultados.\nFinalmente, hasta ahora s\u00b4 olo hemos tra-\nbajado con la versi\u00b4 on de CORGA etiquetada\nautom\u00b4 aticamente. Nos proponemos hacer ex-\nperimentos con la porci\u00b4 on del mismo que ha\nsido corregida de manera manual y utiliza-\nda para entrenar XIADA, para comprobar si\nusar un corpus corregido manualmente tiene\nalg\u00b4 un impacto en los resultados.\nAgradecimientos\nEste trabajo ha sido parcialmente \ufb01nanciado\npor la Xunta de Galicia, a trav\u00b4 es del Conve-\nnio de colaboraci\u00b4 on plurianual entre el Cen-\ntro Ram\u00b4 on Pi\u02dc neiro para la Investigaci\u00b4 on en\nHumanidades y la Universidad de Vigo, y la\nAyuda para la Consolidaci\u00b4 on y Estructura-\nci\u00b4 on de Unidades de Investigaci\u00b4 on Competi-\ntivas ED431C 2018/50, y por el Ministerio\nde Econom\u00b4 \u0131a, Industria y Competitividad a\ntrav\u00b4 es del proyecto TIN2017-85160-C2-2-R.\nV\u00edctor Darriba, Yerai Doval, Elmurod Kuriyozov\n54Bibliograf\u00b4 \u0131a\nBl\nunsom, P. y T. Baldwin. 2006. Multilin-\ngual deep lexical acquisition for HPSGs\nvia supertagging. En Proceedings of the\n2006 Conference on Empirical Methods\nin Natural Language Processing, p\u00b4 aginas\n164\u2013171, Sydney, Australia, Julio. Asso-\nciation for Computational Linguistics.\nCandito, M. y M. Constant. 2014. Stra-\ntegies for Contiguous Multiword Expres-\nsion Analysis and Dependency Parsing.\nEnACL\u201914 - The 52nd Annual Meeting\nof the Association for Computational Lin-\nguistics, Baltimore, United States. ACL.\nCentro Ram\u00b4 on Pi\u02dc neiro para a Investigaci\u00b4 on\nen Humanidades. 2019a. Corpus de Refe-\nrencia do Galego Actual (CORGA) [v3.2].\nhttp://corpus.cirp.gal/corga/.\nCentro Ram\u00b4 on Pi\u02dc neiro para a Inves-\ntigaci\u00b4 on en Humanidades. 2019b.\nEtiquetador/Lematizador do Ga-\nlego Actual (XIADA) [v2.7].\nhttp://corpus.cirp.gal/xiada/.\nCho, K., B. van Merri\u00a8 enboer, C. Gulcehre,\nD. Bahdanau, F. Bougares, H. Schwenk,\ny Y. Bengio. 2014. Learning phrase re-\npresentationsusingRNNencoder\u2013decoder\nfor statistical machine translation. En\nProceedings of the 2014 Conference on\nEmpirical Methods in Natural Language\nProcessing, p\u00b4 aginas 1724\u20131734, Doha, Qa-\ntar, Octubre. Association for Computatio-\nnal Linguistics.\nConneau, A., K. Khandelwal, N. Goyal,\nV. Chaudhary, G. Wenzek, F. Guzm\u00b4 an,\nE. Grave, M. Ott, L. Zettlemoyer, y\nV. Stoyanov. 2020. Unsupervised cross-\nlingual representation learning at scale.\nEnProceedings of the 58th Annual Mee-\nting of the Association for Computatio-\nnal Linguistics , p\u00b4 aginas 8440\u20138451, Onli-\nne, Julio. Association for Computational\nLinguistics.\nConstant,M.,G.Eryi\u02d8 git,J.Monti,L.vander\nPlas, C. Ramisch, M. Rosner, y A. Todi-\nrascu. 2017. Multiword Expression Pro-\ncessing: A Survey. Computational Lin-\nguistics, 43(4):837\u2013892.\nConstant, M. y J. Nivre. 2016. A transition-\nbased system for joint lexical and syn-\ntactic analysis. En Proceedings of the\n54th Annual Meeting of the Associationfor Computational Linguistics (Volume 1:\nLong Papers) , p\u00b4 aginas 161\u2013171, Berlin,\nGermany,Agosto.AssociationforCompu-\ntational Linguistics.\nConstant, M. y A. Sigogne. 2011. MWU-\naware part-of-speech tagging with a CRF\nmodel and lexical resources. En Pro-\nceedings of the Workshop on Multiword\nExpressions: from Parsing and Genera-\ntion to the Real World, p\u00b4 aginas 49\u201356,\nPortland, Oregon, USA, Junio. Associa-\ntion for Computational Linguistics.\nConstant, M., A. Sigogne, y P. Watrin. 2012.\nDiscriminative strategies to integrate mul-\ntiword expressionrecognitionand parsing.\nEnProceedings of the 50th Annual Mee-\nting of the Association for Computatio-\nnal Linguistics (Volume 1: Long Papers) ,\np\u00b4 aginas 204\u2013212, Jeju Island, Korea, Ju-\nlio. Association for Computational Lin-\nguistics.\nDevlin, J., M.-W. Chang, K. Lee, y K. Tou-\ntanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for lan-\nguage understanding. En Proceedings\nof the 2019 Conference of the North\nAmerican Chapter of the Association for\nComputational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and\nShort Papers) , p\u00b4 aginas 4171\u20134186, Min-\nneapolis, Minnesota, Junio. Association\nfor Computational Linguistics.\nDiab, M. y P. Bhutada. 2009. Verb noun\nconstruction MWE token classi\ufb01cation.\nEnProceedings of the Workshop on Multi-\nword Expressions: Identi\ufb01cation, Interpre-\ntation, Disambiguation and Applications ,\np\u00b4 aginas 17\u201322, Singapore, Agosto. Asso-\nciation for Computational Linguistics.\nDom\u00b4 \u0131nguez Noya, E. M., F. M. Barca-\nla Rodr\u00b4 \u0131guez, y M. \u00b4A. Molinero \u00b4Alva-\nrez. 2009. Avaliaci\u00b4 on dun etiquetador au-\ntom\u00b4 atico estat\u00b4 \u0131stico para o galego actual:\nXiada. Cadernos de Lingua, 30-31:151\u2013\n193.\nDom\u00b4 \u0131nguez Noya, E. M., M. S.\nL\u00b4 opez Mart\u00b4 \u0131nez, y F. M. Barca-\nla Rodr\u00b4 \u0131guez. 2019. O Corpus de\nReferencia do Galego actual (CORGA):\ncomposici\u00b4 on, codi\ufb01caci\u00b4 on, etiquetaxe e\nexplotaci\u00b4 on. Verba: Anuario Galego de\nFilolox\u00b4 \u0131a , Anexo 74:179\u2013219.\nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo\n55Dubremetz, M. y J. Nivre. 2014. Ex-\ntr\naction of nominal multiword expres-\nsions in French. En Proceedings of the\n10th Workshop on Multiword Expressions\n(MWE),p\u00b4 aginas72\u201376,Gothenburg,Swe-\nden, Abril. Association for Computational\nLinguistics.\nFarahmand, M. y R. Martins. 2014. A\nsupervised model for extraction of mul-\ntiword expressions, based on statistical\ncontext features. En Proceedings of the\n10th Workshop on Multiword Expressions\n(MWE),p\u00b4 aginas10\u201316,Gothenburg,Swe-\nden, Abril. Association for Computational\nLinguistics.\nFirth, J. R. 1957. Papers in Linguistics,\n1934-1951. Oxford University Press, Lon-\ndon.\nGreen, S., M.-C. de Marne\ufb00e, J. Bauer, y\nC. D. Manning. 2011. Multiword ex-\npression identi\ufb01cation with tree substitu-\ntion grammars: A parsing tour de force\nwith French. En Proceedings of the 2011\nConference on Empirical Methods in Na-\ntural Language Processing, p\u00b4 aginas 725\u2013\n735, Edinburgh, Scotland, UK., Julio. As-\nsociation for Computational Linguistics.\nGreen, S., M.-C. de Marne\ufb00e, y C. D. Man-\nning. 2013. Parsing models for identif-\nying multiword expressions. Computatio-\nnal Linguistics , 39(1):195\u2013227.\nHochreiter, S. y J. Schmidhuber. 1997. Long\nshort-term memory. Neural Computation,\n9(8):1735\u20131780, 11.\nJackendo\ufb00,R. 1997. Twistin\u2019thenightaway.\nLanguage, 73(3):534\u2013559, Septiembre.\nKlyueva, N., A. Doucet, y M. Straka. 2017.\nNeural networks for multi-word expres-\nsion detection. En Proceedings of the\n13th Workshop on Multiword Expressions ,\np\u00b4 aginas 60\u201365, Valencia, Spain, Abril. As-\nsociation for Computational Linguistics.\nKurfal\u0131, M. 2020. TRAVIS at PARSEME\nshared task 2020: How good is (m)BERT\nat seeing the unseen? En Proceedings\nof the Joint Workshop on Multiword Ex-\npressions and Electronic Lexicons , p\u00b4 agi-\nnas 136\u2013141, online, Diciembre. Associa-\ntion for Computational Linguistics.\nLa\ufb00erty, J. D., A. McCallum, y F. C. N. Pe-\nreira. 2001. Conditional Random Fields:Probabilistic Models for Segmenting and\nLabeling Sequence Data. En Proceedings\nof the Eighteenth International Conferen-\nce on Machine Learning, ICML \u201901, p\u00b4 agi-\nnas 282\u2013289, San Francisco, CA, USA.\nMorgan Kaufmann Publishers Inc.\nLapata, M. y A. Lascarides. 2003. Detec-\nting novel compounds: The role of distri-\nbutional evidence. En 10th Conference of\nthe European Chapter of the Association\nfor Computational Linguistics , Budapest,\nHungary, Abril. Association for Compu-\ntational Linguistics.\nLegrand, J. y R. Collobert. 2016. Phrase\nrepresentations for multiword expressions.\nEnProceedings of the 12th Workshop\non Multiword Expressions , p\u00b4 aginas 67\u201371,\nBerlin, Germany, Agosto. Association for\nComputational Linguistics.\nMaldonado, A., L. Han, E. Moreau, A. Al-\nsulaimani, K. D. Chowdhury, C. Vogel, y\nQ. Liu. 2017. Detection of verbal multi-\nword expressions via conditional random\n\ufb01elds with syntactic dependency featu-\nres and semantic re-ranking. En Procee-\ndings of the 13th Workshop on Multiword\nExpressions , p\u00b4 aginas 114\u2013120, Valencia,\nSpain, Abril. Association for Computatio-\nnal Linguistics.\nOkazaki, N. 2007. CRFsuite: a\nfast implementation of Condi-\ntional Random Fields (CRFs).\nhttp://www.chokkan.org/software/crfsuite/.\nPecina, P. 2009. Lexical Association Mea-\nsures: Collocation Extraction. \u00b4UFAL,\nPraha, Czechia.\nRamisch, C. 2015. Multiword Expressions\nAcquisition: A Generic and Open Frame-\nwork, volumen XIV de Theory and Ap-\nplications of Natural Language Processing.\nSpringer.\nRamisch, C., A. Villavicencio, L. Moura, y\nM. Idiart. 2008. Picking them up and\n\ufb01guring them out: Verb-particle construc-\ntions, noise and idiomaticity. En A. Clark\ny K. Toutanova, editores, Proceedings of\nthe Twelfth Conference on Natural Lan-\nguage Learning, p\u00b4 aginas 49\u201356, Manches-\nter, UK. ACL.\nRamshaw, L. A. y M. Marcus. 1995. Text\nchunking using transformation-based lear-\nning. En D. Yarowsky y K. Church, edito-\nV\u00edctor Darriba, Yerai Doval, Elmurod Kuriyozov\n56res,Third Workshop on Very Large Cor-\np\nora, VLC@ACL 1995, Cambridge, Mas-\nsachusetts, USA, June 30, 1995.\nRiedl, M. y C. Biemann. 2016. Impact of\nMWEresourcesonmultiwordrecognition.\nEnProceedings of the 12th Workshop on\nMultiword Expressions , p\u00b4 aginas 107\u2013111,\nBerlin, Germany, Agosto. Association for\nComputational Linguistics.\nRondon, A., H. Caseli, y C. Ramisch.\n2015. Never-ending multiword expres-\nsions learning. En Proceedings of the 11th\nWorkshop on MWEs , p\u00b4 aginas 45\u201353, Den-\nver, CO, USA. ACL.\nSchneider, N., E. Danchik, C. Dyer, y N. A.\nSmith. 2014. Discriminative lexical se-\nmantic segmentation with gaps: Running\nthe MWE gamut. Transactions of the As-\nsociation for Computational Linguistics ,\n2:193\u2013206.\nSchneider, N., D. Hovy, A. Johannsen, y\nM. Carpuat. 2016. SemEval-2016 task\n10: Detecting minimal semantic units and\ntheir meanings (DiMSUM). En Procee-\ndings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016) ,\np\u00b4 aginas 546\u2013559, San Diego, California,\nJunio.AssociationforComputationalLin-\nguistics.\nSimk\u00b4 o, K. I., V. Kov\u00b4 acs, y V. Vincze. 2017.\nUSzeged: Identifying verbal multiword ex-\npressions with POS tagging and par-\nsing techniques. En Proceedings of the\n13th Workshop on Multiword Expressions ,\np\u00b4 aginas 48\u201353, Valencia, Spain, Abril. As-\nsociation for Computational Linguistics.\nTaslimipoor, S., S. Bahaadini, y E. Kochmar.\n2020. MTLB-STRUCT @Parseme 2020:\nCapturing unseen multiword expressions\nusing multi-task learning and pre-trained\nmasked language models. En Proceedings\nof the Joint Workshop on Multiword Ex-\npressions and Electronic Lexicons , p\u00b4 agi-\nnas 142\u2013148, online, Diciembre. Associa-\ntion for Computational Linguistics.\nTaslimipoor, S. y O. Rohanian. 2018. SHO-\nMA at parseme shared task on automatic\nidenti\ufb01cation of vmwes: Neural multiword\nexpression tagging with high generalisa-\ntion.CoRR, abs/1809.03056.\nVincze, V., I. Nagy T., y G. Berend. 2011.\nMultiword expressions and named entitiesin the wiki50 corpus. En Proceedings of\nthe International Conference Recent Ad-\nvances in Natural Language Processing\n2011, p\u00b4 aginas 289\u2013295, Hissar, Bulgaria,\nSeptiembre. Association for Computatio-\nnal Linguistics.\nVincze, V., J. Zsibrita, y I. Nagy T. 2013.\nDependency parsing for identifying Hun-\ngarian light verb constructions. En Pro-\nceedings of the Sixth International Joint\nConference on Natural Language Proces-\nsing,p\u00b4 aginas207\u2013215,Nagoya,Japan,Oc-\ntubre. Asian Federation of Natural Lan-\nguage Processing.\nWolf, T., L. Debut, V. Sanh, J. Chaumond,\nC. Delangue, A. Moi, P. Cistac, T. Rault,\nR. Louf, M. Funtowicz, J. Davison, S. Sh-\nleifer, P. von Platen, C. Ma, Y. Jerni-\nte, J. Plu, C. Xu, T. L. Scao, S. Gug-\nger, M. Drame, Q. Lhoest, y A. M. Rush.\n2020. Transformers: State-of-the-art na-\ntural language processing. En Procee-\ndings of the 2020 Conference on Empi-\nrical Methods in Natural Language Pro-\ncessing: System Demonstrations , p\u00b4 aginas\n38\u201345, Online, Octubre. Association for\nComputational Linguistics.\nZampieri, N., M. Scholivet, C. Ramisch, y\nB. Favre. 2018. Veyn at PARSEME sha-\nred task 2018: Recurrent neural networks\nfor VMWE identi\ufb01cation. En Procee-\ndings of the Joint Workshop on Linguistic\nAnnotation, Multiword Expressions and\nConstructions , p\u00b4 aginas 290\u2013296, Santa Fe,\nNew Mexico, USA, Agosto. Association\nfor Computational Linguistics.\nProcesamiento de Expresiones Multipalabra en gallego mediante Aprendizaje Profundo\n57A\nutoPunct: A BER T-based Automatic Punctuation andCapitalisation System for Spanish and Basque\nAutoPunct: Sistema de Puntuaci\u00f3n y Mayusculizaci\u00f3n\nAutom\u00e1tico basado en BER T para Castel lano y Euskera\nAnder Gonz\u00e1lez-Docasal1,2, Aitor Garc\u00eda-Pablos1,\nHaritz Arzelus1, Aitor \u00c1lvarez1\n1Vicomtech F oundation, Basque Research and T echnology Alliance (BR T A),\nMikeletegi 57, 20009 Donostia-San Sebasti\u00e1n (Spain)\n2University of Zaragoza, Pedro Cerbuna 12, 50009 Zaragoza (Spain)\n{agonzalezd, agarciap, harzelus, aalvarez}@vicomtech.org\nAbstract: The raw output of an Automatic Speech Recognition system usually con-\nsists in a stream of words without any casing nor punctuation. In order to improve\nthe readability and enable further uses of this output, punctuation and capitalisation\nhave to be included. In this context, we present AutoPunct, a T ransformers-based\nautomatic punctuation and capitalisation model that combines both acoustic (i.e.\nsilences duration) and lexical information (the words themselves). W e compared\nits performance with a system based on Bidirectional Recurrent Neural Networks\n(BRNN) on Basque (a low-resource language) and Spanish, both individually and\nsimultaneously . The result is a system that achieves high accuracy for punctuation\nand capitalisation in both languages at the same time, with a throughput of several\nthousand words per second using a standard GPU.\nKeywords: punctuation, capitalisation, low-resource languages.\nResumen: La salida en bruto de un sistema de Reconocimiento Autom\u00e1tico del\nHabla generalmente consiste en una secuencia de palabras sin may\u00fasculas ni si-\ngnos de puntuaci\u00f3n. Para mejorar la legibilidad y posibilitar posteriores usos de\nesta salida es necesario incluir la puntuaci\u00f3n y las may\u00fasculas. En este contexto,\npresentamos AutoPunct, un modelo para puntuaci\u00f3n y mayusculizaci\u00f3n basado\nen arquitecturas de T ransformers que combina tanto informaci\u00f3n ac\u00fastica (silencios)\ncomo l\u00e9xica (palabras). Hemos comparado su desempe\u00f1o con un sistema basado en\nredes neuronales recursivas bidireccionales (BRNN) en euskera (un idioma de po-\ncos recursos) y castellano, as\u00ed como combinando ambos idiomas. El resultado es\nun sistema que obtiene buenos resultados aplicando mayusculizaci\u00f3n y puntuaci\u00f3n\nde manera simult\u00e1nea en dos idiomas diferentes, con una velocidad de proceso que\nalcanza varios miles de palabras por segundo en una GPU est\u00e1ndar.\nPalabras clave: puntuaci\u00f3n, mayusculizaci\u00f3n, lenguas con pocos recursos.\n1 Introduction\nAutomatic Speech Recognition (ASR) sys-\ntems are increasingly more integrated in\nour daily lives and workflows through differ-\nent solutions such as voice assistants, nat-\nural interfaces, speech-to-text applications or\nbiometrics, among others. The growth of\nthis technology has been mainly due to the\nevolution of Deep Learning techniques and\ntheir integration to develop neural models\nfor speech recognition (Nassif et al., 2019),\nin addition to the continuous release of more\nand more training data and the availability of\npowerful hardware devices for high perform-ance computing.\nThe aim of an ASR system is to trans-\nform an audio input into text that may be\nexploited for further Natural Language Pro-\ncessing applications. However, the output\nstring is usually composed by a raw sequence\nof words which does not include casing nor\npunctuation marks, which noticeably reduces\nits readability (Jones et al., 2003) and its pos-\nsibility of being employed as input to other\nmodules that require a well-segmented and\ncorrectly punctuated text (Peitz et al., 2011).\nTherefore, the ASR module is usually concat-\nenated to other technological modules which\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 59-68\nrecibido 07-05-2021 revisado 01-06-2021 aceptado 06-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-5\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalare in\ncharge of enriching the initial raw tran-\nscriptions, as it is described in Figure 1.\nFigure 1:\nExample of Automatic Speech\nRecognition output before and after adding\npunctuation and capitalisation.\nIn this work, we compared the perform-\nance of two neural architectures for the task\nof automatically recovering the punctuation\nmarks for Spanish and Basque languages,\nboth individually and simultaneously . The\nfirst system was considered as a reference\nbaseline. It is inspired on the architecture\nproposed by Tilk and Alum\u00e4e (2016), which\nincludes a bidirectional recurrent neural net-\nwork (BRNN) model (Schuster and Paliwal,\n1997) that takes advantage of Gated Recur-\nrent Units (GRU) as recurrent layers and an\nattention mechanism to further increase its\ncapacity of finding relevant parts of the con-\ntext for punctuation decisions.\nAs an alternative to this initial system,\nwe present AutoPunct , which is composed\nby an architecture based on Bidirectional\nEncoder Representations from T ransformers\n(BER T) (Devlin et al., 2019), that com-\nbines the text representation obtained by a\nT ransformer model with the acoustic inform-\nation related to the duration of the silences\nbetween words. W e tested this second system\nusing several architectural variations in order\nto evaluate different ways of combining and\nexploiting the acoustic and lexical informa-\ntion. In addition, this model variations were\nalso evaluated with the integration of an ex-\ntra head to predict whether a word should be\ncapitalised or not.\nThe rest of the paper is structured as it fol-\nlows. Section 2 introduces related work in the\nfield. Section 3 presents AutoPunct along\nwith its main architecture. Section 4 illus-\ntrates the initial experimental set-up, whilst\nthe datasets used to train both neural ap-proaches are described in Section 5. Section 6\ndisplays the experiments and the obtained\nresults. Finally , Section 7 concludes the pa-\nper and presents future work.\n2 Related work\nThe challenge of automatically recovering\ncapitalisation and punctuation marks has\nbeen extensively studied through many sys-\ntems presented in the literature. These sys-\ntems can be divided into three main cat-\negories (Yi et al., 2020a): those using lex-\nical features, derived from the text; those us-\ning prosodic features, derived from acoustic\ninformation; and those using a combination\nof both. These prosodic feature-based archi-\ntectures show that this type of information\nis indeed useful for the task, although they\ntend to fail in places where the speaker does\nunnatural pauses (Christensen, Gotoh, and\nRenals, 2001; Kim and W oodland, 2003).\nIn the last years, the problem of recov-\nering punctuation marks have been faced by\nthe use of Deep Learning algorithms, such as\nConvolutional Networks (Che et al., 2016),\nBidirectional RNN with attention (Tilk and\nAlum\u00e4e, 2016), the use of word- and speech-\nembeddings (Yi and T ao, 2019), and more\nrecently , T ransformers based on BER T-like\narchitectures (Devlin et al., 2019), which\nhave been shown to obtain values as high as\n83.9% (Courtland, F aulkner, and McElvain,\n2020) on F1score in the well-known and\nreference IWSL T 2012 data set (F ederico et\nal., 2012). Different architectures show the\nuse of a pre-trained BER T model such as\nRoBER T a (Liu et al., 2019) in order to ob-\ntain the word-embeddings, which are fed to\nvarious networks based on BiLSTM (Alam,\nKhan, and Alam, 2020) or focal loss (Yi\net al., 2020b), or aggregated across over-\nlapping context windows for each individual\ntoken (Courtland, F aulkner, and McElvain,\n2020). Another BER T-based architecture\nthat performs both punctuation and capital-\nisation simultaneously can be found in (Sunk-\nara et al., 2020), where the word- or subword-\nembeddings obtained from a pre-trained or a\ncustom BER T are fed to two Softmax layers\nfor punctuation and capitalisation respect-\nively . These experiments, however, are fo-\ncused on the specific domain of medical texts.\nNevertheless, the variety of literature ded-\nicated to solve the problem of punctuation\nfocuses only on the following three marks:\nAnder Gonz\u00e1lez-Docasal, Aitor Garc\u00eda-Pablos, Haritz Arzelus, Aitor \u00c1lvarez\n60period (.), comma (,) and question (\n?).\nIn addition, these results have been mostly\nevaluated over a single language: English.\nThe system presented in this paper\naddresses 13 different punctuation marks,\nwhich are described in the section 5.\nMoreover, the architecture of the proposed\nmodel features a multi-label output, which\nmeans that it can predict more than a single\npunctuation mark per word.\n3 Description of the systems\nAs it was initially introduced, this work\npresents a comparison between two neural\narchitectures for capitalisation and punctu-\nation restoration for Spanish and Basque,\nboth individually and simultaneously . These\nsystems are described in more detail in the\nfollowing subsections.\n3.1 BRNN-based system\nThe architecture proposed for the first system\nis based on Punctuator (Tilk and Alum\u00e4e,\n2016). It integrates a Bidirectional Recurrent\nNeural Network (BRNN) with an attention\nmechanism for restoring punctuation marks\nin unsegmented transcribed speech. This ar-\nchitecture allows the use of both GRU or\nLSTM layers as recurrent layers, whilst the\nattention mechanism increases the capacity\nof the model of detecting relevant context\nsegments to improve punctuation decisions.\nFinally , the recurrent layers and the atten-\ntion mechanism are coupled by a late fusion\napproach, which allows the output of the at-\ntention model to directly interact with the\nstate of the recurrent layer while not interfer-\ning with its memory . A simplified diagram of\nthis architecture is shown in Figure 2.\nF or this work, we trained the model in\ntwo different stages. In the first stage, only\nannotated text is used to train an initial\nmodel, so it learns to restore punctuation\nmarks based on textual features only . In\nthe optional second stage, a new model is\nestimated adding acoustic information as in-\nput, therefore learning to combine pause dur-\nations between words with textual features.\nThe pause durations are obtained by using\nthe start and end time-codes given by the\nspeech recognition system at word level.\n3.2 BER T-based system\nOur BER T-based system, called Auto-\nPunct, combines the lexical information ob-ASR outputGRU GRU GRUAtten\ntionGRU GRULate fusionPunctuation lab\nels\nFigure 2:\nSimplified architecture of BRNN-\nbased Punctuator system.\ntained from a pre-trained BER T model with\nthe acoustic information coming from silence\nduration between words. These two sources\nof information are further combined using\nan additional T ransformer model (a custom\nBER T trained from scratch). This system is\ntrained to predict both punctuation and cap-\nitalisation at the same time. A high-level dia-\ngram of the architecture of AutoPunct is\nshown in Figure 3.\nThis particular architecture was care-\nfully constructed considering the following\nguidelines:\n1. Speed and e\ufb00iciency should be priorit-\nised so the system can work in real-time\nif needed.\n2. Capitalisation should be performed\nalongside punctuation to avoid adding a\nstandalone module just for casing.\n3. The system should be able to exploit si-\nlence duration between words as an ad-\nditional source of prosodic information.\n4. The architecture should be language-\nindependent. It should serve as a basis\nto train models for different languages\nby changing the training data and the\npre-trained T ransformer model.\nIn the following subsections, the main\ncomponents of the AutoPunct architecture\nare explained in more detail.\n3.2.1 Input embeddings\nThe input to the AutoPunct system cor-\nresponds to the output of a given ASR sys-\nAutoPunct: A BERT-based Automatic Punctuation and Capitalisation System for Spanish and Basque\n61ASR outp\nutSilence v\nalues W ordsSilence\nemb\neddingPre-trained\nBER T+Custom\nBER TPunctuation\nlay\nerCapitalisation\nlay\nerPunctuation\nlabelsCapitalisation\nlabels\nFigure 3:\nMain architecture of the BER T-\nbased AutoPunct system.\ntem; this is, a sequence of lowercase words\npaired with their silence duration value. A\npre-trained BER T (Devlin et al., 2019) model\nis used to encode words as they are out-\nput by the speech recognition system into\nthe so-called contextual embeddings. These\nembeddings are vectors of continuous values\nthat encode relevant linguistic information\nabout the words, according to the language\nmodel represented by BER T. More informa-\ntion about the pre-trained models used in the\nexperiments can be found in Section 6.\nThis lexical and semantic information ob-\ntained by the pre-trained BER T is then com-\nbined with the silence duration for each in-\ndividual word. Given that the silence val-\nues between word are scalars, they cannot\nbe combined with the word-embeddings in\nthis state. In order to solve this problem,\nwe evaluated two different approaches of in-\njecting the silences information to the sys-\ntem: as continuous values and as discrete\nvalues. The former involves repeating the\nsilence value as many times as the size of\neach word embedding (768 for the BER T-\nbase models). The latter consists of parti-\ntioning a ten-seconds-time range into buck-\nets of 10 milliseconds, leading to 1000 dis-crete silence values. These discrete values\nare used as indexes over a silence-embedding\nlayer that provides, for each silence value, a\ntrainable vector of the same size as the word-\nembeddings.\nNevertheless, using discrete silence val-\nues with such a fine granularity has a major\ndrawback: most of the discrete values will\nprobably never appear in the training data;\ntherefore their embeddings will not be used\nand they will not capture any valuable in-\nformation. T o prevent this, all the embed-\ndings from a window of 1 second centred on\nthe original value are averaged into a single\nembedding. W e evaluated the system with\ntwo types of averaging: uniform averaging, in\nwhich each embedding weights the same, and\nGaussian averaging, in which a Gaussian dis-\ntribution centred on the original embedding\nwas used to compute a weighted average of\nthe embeddings. This final silence embed-\nding is added to the word-embedding com-\nputed by the pre-trained BER T in order to\nobtain a combined representation.\n3.2.2 Custom BER T\nAs an additional step, we evaluated the sys-\ntem adding an extra T ransformer module\nlayer after the word and silence combin-\nation. This custom T ransformer, (hence-\nforth Custom BER T) is much smaller than\na base BER T (4 hidden layers with 4 atten-\ntion heads each), and it is initialised from\nscratch. The rationale for adding this inter-\nmediate T ransformer is to endow the model\nwith the ability to attend to the whole se-\nquence, instead of focusing on isolated com-\nbinations of word and silence pairs, through\nthe self-attention among inputs to the T rans-\nformer.\nThe impact of this layer is also evaluated\nin Section 6.\n3.2.3 Punctuation and Capitalisation\nlayers\nFinally , the representation of each word is fed\ninto two classification heads. These heads\nare composed of a dense layer, followed by\na non-linearity , a dropout layer and a final\nlinear layer that maps the input into the cor-\nresponding label space, namely , the different\npunctuation marks for the punctuation head\nand the capitalisation labels for the capital-\nisation head.\nSince the punctuation marks in a sentence\nmay influence the capitalisation of the fol-\nAnder Gonz\u00e1lez-Docasal, Aitor Garc\u00eda-Pablos, Haritz Arzelus, Aitor \u00c1lvarez\n62lowing\nwords, the output of the punctuation\nhead is fed into the capitalisation head along\nwith the word embedding generated by the\npre-trained BER T at the beginning of the\nnetwork.\nThe punctuation head is treated as a\nmulti-label classification head to cope with\nthe fact that some words may bear more than\none punctuation mark attached, e.g. closing\nquotation mark and period at the end of a\nsentence.\n4 T raining setup\nEach neural architecture was initialised using\nspecific components and resources, while the\nfinal models were constructed over the same\nmain dataset employed in this work.\nWith regard to the BRNN-based system,\nwe followed the training stages described in\nsubsection 3.1. At a first stage, an initial\nmodel was estimated using text features only .\nThese features were obtained from a text cor-\npus of generic domain consisting of web news\ncrawled from digital newspapers in Basque\nand Spanish. This generic text corpus is\ndescribed in more detail in subsection 5.1.\nThen, the initial model was fine-tuned in\na second stage of training with acoustic in-\nformation, exploiting the principal dataset\nemployed in this work, known as mintzai-\nST (Etchegoyhen et al., 2021). This corpus\nis also described in more detail in subsection\n5.2.\nThe models for all the languages were es-\ntimated using the same training configura-\ntion. During the first stage, the training fin-\nished when the validation perplexity was not\nimproved at the first time, with a maximum\nepochs of 50 and a minibatch size of 128. The\nhidden layers consist of 256 units and we em-\nployed a learning rate of 0.02. Regarding the\nsecond stage where the acoustic information\nwas integrated, the training process was set\nto be finished when the validation perplexity\nwas not improved in the last 3epochs with\na maximum epochs of 50 and a minibatch\nsize of 128. The hidden layers consist of 256\nunits and the learning rate was fixed to 0.02.\nThe input vocabularies had a maximum size\nof 100,000 words, composed by the most fre-\nquent words that occur at least two times in\nthe training corpus. The output vocabulary\nwas composed by the predicted punctuation\nmarks in addition to a non-punctuation sym-\nbol defined as O. The trainings were perfomedon a 12 GB Nvidia Titan X GPU card.\nRegarding the BER T-based system\nAutoPunct, we employed a specific pre-\ntrained BER T model for each language:\nBETO for Spanish (Ca\u00f1ete et al., 2020),\nBER T eus for Basque (Agerri et al., 2020),\nand IXAmBER T for Spanish+Basque (Otegi\net al., 2020). As in the previous architecture,\nthe final models of the BER T-based systems\nwere estimated on the mintzai-ST dataset\nand using the same hyper-parameters to\nmake them comparable. The learning-rate\nwas set to 2\u00b710\u22125with a warm-up period of\n5 epochs and using Adam W (Loshchilov and\nHutter, 2019) as the optimizer. The training\nmini-batches were of size 8. The training of\neach model was performed using an Nvidia\nGeF orce R TX 2080ti GPU with \u223c11GB of\nmemory for a maximum of 50 epochs with\nan early-stopping patience of 20 epochs,\nmonitoring the punctuation F1metric on the\ncorresponding development set.\n5 Main datasets\nIn this section, we first describe the text cor-\npus used to estimate the initial models of the\nBRNN-based models, and then we present\nthe main dataset employed to generate the\nfinal models of both neural architectures. Fi-\nnally , we detail the punctuation and capital-\nisation labels as well.\n5.1 Generic text corpus\nThis corpus is composed by news of generic\ndomain crawled from digital newspapers from\n2012 to 2019. The number of words for each\npartition is shown in T able 1.\nEU ES ES+EU\ntrain 14,010,067 11,609,170\n25,619,237\ndev 683,230 1,323,545\n2,006,775\nT able 1: Number of words in each partition\nof the generic text corpus per language. The\nES+EU corpus is a concatenation of the data\nfrom EU and ES.\n5.2 Mintzai-ST corpus\nAs it was previously mentioned, the final\nmodels of both BRNN-based and BER T-\nbased systems were trained and evaluated\nwith the mintzai-ST corpus, which incorpor-\nates both textual and acoustic features. This\ncorpus is composed by a collection of manual\ntranscriptions of proceedings of the Basque\nAutoPunct: A BERT-based Automatic Punctuation and Capitalisation System for Spanish and Basque\n63Parliamen\nt from 2011 to 2018, which com-\nprises content in Basque and Spanish. The\noriginal train, development and test parti-\ntions of the corpus were maintained in both\nlanguages as they are described in its related\npaper. T able 2 presents the amount of words\nin each partition.\nEU ES ES+EU\ntrain 1,137,727 4,468,041\n5,605,768\ndev 13,672 25,139\n38,811\ntest 40,644 75,470\n116,114\nT able 2: Number of words in each partition\nof the corpus mintzai-ST per language. The\nES+EU corpus is a concatenation of the data\nfrom EU and ES.\nThe original mintzai-ST corpus was pro-\ncessed in order to represent the information\nneeded to train models. This information is\nrelated to each word and consists of four ele-\nments: the lowercase word itself, the punc-\ntuation label, a float value representing the\nsilence duration, and the capitalisation label.\nThe word and the silence value act as inputs,\nwhile the other two are the outcomes the sys-\ntem should learn to predict. An example of a\ncompletely annotated sentence from the cor-\npus mintzai-ST can be found in Figure 4.\nprimer O 0.00 FIRST_CAP\npunto O 0.00 O\ndel O 0.00 O\norden O 0.00 O\ndel O 0.00 O\nd\u00eda COLON 0.00 O\npregunta OPEN_QUOTE 0.00 FIRST_CAP\nformulada O 0.00 O\npor O 0.00 O\ndon O 0.00 O\nandoni O 0.00 FIRST_CAP\nortuzar O 0.00 FIRST_CAP\narruabarrena COMMA 0.36 FIRST_CAP\nFigure 4: An example of the training cor-\npus. Each word has its corresponding punc-\ntuation, acoustic and capitalisation labels re-\nspectively .\n5.3 Punctuation labels\nThe punctuation labels represent the punctu-\nation marks that should go attached to each\nword. The current punctuation labels invent-\nory is the following: COLON (:), COMMA (,),\nDASH (W\u2013), ELLIPSIS (\u2026), EXCLAMATION (!),\nOPEN_DASH (\u2013W), OPEN_EXCL (\u00a1),\nOPEN_QUES (\u00bf), OPEN_QUOTE (\u201c), PERIOD (.),\nQUESTION (?), QUOTE (\u201d) and SEMICOLON (;).There is also an Olabel to indicate words\nthat bear no punctuation. A single word\ncan have more than one punctuation label\nattached to it.\nThe punctuation labels were derived from\nthe different Unicode code-points present in\nthe original transcriptions of the dataset, e.g.\nthe code-points U+00BB (\u00bb) and U+201D (\u201d)\nare both labelled as QUOTE.\nT able 3 shows the distribution in percent-\nages of the punctuation labels for Basque\n(EU), Spanish (ES) and Spanish+Basque\n(ES+EU). The distributions shows a label\nunbalance. This is to be expected since some\npunctuation marks, such as periods or com-\nmas, are much more frequent than the others.\nlabel EU ES\nES+EU\nCOMMA 54.62% 59\n.89% 58 .35%\nPERIOD 36.25% 28\n.02% 30 .41%\nQUESTION 1.87% 1\n.72% 1 .76%\nCOLON 1.88% 1\n.49% 1 .61%\nDASH 1.21% 1\n.54% 1 .44%\nOPEN_QUES 0.02% 1\n.68% 1 .20%\nSEMICOLON 1.08% 1\n.20% 1 .16%\nOPEN_QUOTE 1.15% 1\n.09% 1 .11%\nQUOTE 1.02% 1\n.05% 1 .04%\nELLIPSIS 0.69% 1\n.08% 0 .97%\nEXCLAMATION 0.17% 0\n.62% 0 .49%\nOPEN_EXCL 0.01% 0 .61%\n0.43%\nOPEN_DASH 0.03% 0\n.00% 0 .01%\nT able 3: Percentage of appearance of each\npunctuation label in each language in the cor-\npus mintzai-ST.\n5.4 Capitalisation labels\nThe capitalisation labels consist in two dif-\nferent options: FIRST_CAP if the first letter\nof the word is a capital letter, and ALL_CAPS\nif the whole word is written in capital letters.\nSimilarly to the punctuation, the label Oin-\ndicates that the word is not capitalised. F or\nwords that do not fall into any of these cat-\negories (e.g. EiTB), these same criteria are\nsimilarly applied: if the first letter is cased\nit would carry the label FIRST_CAP, and O\notherwise.\n6 Evaluation and discussion\nIn this section, we present the results ob-\ntained by each neural architecture proposed\nin this work on the test partition of the\nmintzai-ST corpus. In the case of our BER T-\nbased system, it is composed of several ele-\nments and parameters that can be enabled\nAnder Gonz\u00e1lez-Docasal, Aitor Garc\u00eda-Pablos, Haritz Arzelus, Aitor \u00c1lvarez\n64or disabled\nto build the models. During our\nexperiments, we constructed several models\nwith different combinations in order to eval-\nuate the impact of these elements:\n\u2022 Using or ignoring silences duration.\n\u2022 Modelling silences as continuous or dis-\ncrete values.\n\u2022 W eighting silence embeddings uniformly\nor using a Gaussian distribution (only\napplies to experiments with discrete si-\nlences).\n\u2022 Using or skipping the additional T rans-\nformer layer (Custom BER T).\n6.1 Punctuation results\nT able 4 shows the micro-averaged F1score\nfor each experiment with AutoPunct, and\nthe comparison with Punctuator.\nS D f B EU ES ES+EU\n\u00d7 \u2013\n\u2013 \u00d7 76.2 78.2 75.4\n\u00d7 \u2013 \u2013 \u2713 76.9 78.5 75.6\n\u2713 \u00d7 \u2013 \u00d7 76.8 78.4 75.9\n\u2713 \u00d7 \u2013\u2713 76.6 77.7 75.2\n\u2713 \u2713 G \u00d7 76.8 78.4 76.0\n\u2713 \u2713 G\u2713 76.7 78.9 76.4\n\u2713 \u2713 U \u00d7 76.8 78.5 76.3\n\u2713 \u2713 U\u2713 76.9 78.9 76.3\nBRNN 74.6\n68.7 72.9\nT able 4: Micro-averaged F1scores for Auto-\nPunct and Punctuator in each language.\nS: Using information of silences. D: Using\ndiscrete silences. f: Gaussian (G) or uni-\nform (U) distribution for contiguous buck-\nets. B: Adding a custom BER T. The results\nachieved by the BRNN-based system are also\ndisplayed.\nAs it can be observed in T able 4, Auto-\nPunct obtains better aggregated scores for\nthe three language scenarios. The scores\nshow a moderate improvement towards the\nlower part of the table, where discrete silences\nare used in combination with the intermedi-\nate custom BER T.\nIn T able 5, the evaluation results for each\nindividual punctuation label are displayed.\nAs it can observed, the BRNN-based sys-\ntem obtains a higher F1score than Auto-\nPunct in the label PERIOD for both Basque\nand Spanish+Basque. Nevertheless, the rest\nof punctuation marks are better modelled\nby the BER T-based architectures. Moreover,the score for some of these labels using the\nBRNN-based system is 0.00%, such as in\nQUOTE or OPEN_QUOTE in Basque, in contrast\nwith AutoPunct reaching F1scores higher\nthan 50%. This can be due to the fact that\nin the first corpus used to train the BRNN-\nbased system (Section 5.1) does not contain\nthese labels, as well as to their low appear-\nance rate in the corpus of mintzai-ST.\nAs it can be appreciated, for the\nBasque language, the absence of predicted\nOPEN_QUES and OPEN_EXCL indicates a cor-\nrect behaviour, since such punctuation marks\nare not used in this language. F or Spanish,\nin contrast, OPEN_QUES reaches a 64.8%. In\nthe case of the labels with lower F1scores, it\nseems that the number of occurrences in this\ndataset are not enough for a proper training\nand evaluation.\nFinally , in the case of the Spanish+Basque\nmodel there is a small performance loss, but\nit can be considered a reasonable trade-off for\nperforming punctuation and capitalisation in\ntwo languages simultaneously using a single\nmodule. F urthermore, it is not uncommon in\nBasque to interleave Spanish words or sen-\ntences spontaneously in casual conversations,\nso using a model that deals with both lan-\nguages at the same time can be advantageous\non these scenarios.\n6.2 Capitalisation results\nThe capitalisation results are presented just\nfor AutoPunct, since the BRNN-based\nmodel does not perform this task. The micro-\naveraged F1scores for automatic capitalisa-\ntion evaluation are shown in T able 6, follow-\ning the same experiment breakdown.\nAs it can be seen in T able 6, the obtained\nmicro-averaged F1values are very high in\nall the cases and for every language scen-\nario. The variations in the architecture do\nnot show a high impact in the final scores.\n6.3 Inference speed\nIn addition to evaluating the quality of the\nsystems in the automatic punctuation and\ncapitalisation tasks, parameters like speed\nand e\ufb00iciency are also desirable properties.\nT o assess if the proposed system would be\nsuitable for a scenario requiring real-time pre-\ndictions, we measured the rate of words per\nsecond at inference time. This measure has\nbeen carried on during the evaluation, using\nthe test set for each language. The eval-\nAutoPunct: A BERT-based Automatic Punctuation and Capitalisation System for Spanish and Basque\n65Basque (EU)\nS\nDf B COM PER QUES COL DASH O_QS SCOL O_QUO QUO ELL EXC O_EX\n\u00d7 \u2013\n\u2013 \u00d7 74.6 84.8 58.8 53.8 13.6 0.0 17.9 59.3 47.6 0.0 0.0 0.0\n\u00d7 \u2013 \u2013 \u2713 75.7 85.5 60.4 50.9 20.3 0.0 15.7 55.3 54.5 8.3 0.0 0.0\n\u2713 \u00d7 \u2013 \u00d7 75.3 85.8 61.0 51.2 0.0 0.0 0.0 54.4 40.0 0.0 0.0 0.0\n\u2713 \u00d7 \u2013\u2713 75.5 85.0 63.5 47.1 3.6 0.0 0.0 50.7 43.9 0.0 0.0 0.0\n\u2713 \u2713 G \u00d7 75.4 85.6 65.5 49.8 14.5 0.0 10.9 58.1 51.2 0.0 0.0 0.0\n\u2713 \u2713 G\u2713 75.2 85.9 60.8 48.5 14.5 0.0 12.9 55.1 51.5 11.6 0.0 0.0\n\u2713 \u2713 U \u00d7 75.2 85.5 64.5 50.6 5.4 0.0 6.0 56.4 46.7 0.0 0.0 0.0\n\u2713 \u2713 U\u2713 75.5 86.1 62.0 50.0 13.6 0.0 10.3 59.9 47.3 0.0 0.0 0.0\nBRNN 70.1 87.2 52.2\n23.4 0.0 0.0 12.4 0.0 0.0 0.0 0.0 0.0\nSpanish (ES)\nS D f B COM PER QUES COL DASH O_QS SCOL O_QUO QUO ELL EXC O_EX\n\u00d7 \u2013\n\u2013 \u00d7 79.8 85.7 58.5 49.8 17.6 64.2 30.6 48.9 26.9 5.4 5.6 10.7\n\u00d7 \u2013 \u2013 \u2713 80.1 86.1 56.5 48.1 21.7 63.8 33.3 50.4 32.7 16.3 13.3 24.1\n\u2713 \u00d7 \u2013 \u00d7 79.8 86.8 56.3 51.6 11.9 61.3 23.8 48.4 25.3 0.0 2.9 0.0\n\u2713 \u00d7 \u2013\u2713 79.7 85.1 56.6 46.1 12.3 63.3 25.5 50.0 20.2 0.0 0.0 5.9\n\u2713 \u2713 G \u00d7 80.1 86.5 54.1 45.0 17.9 62.0 27.1 47.7 31.1 0.0 2.9 5.7\n\u2713 \u2713 G\u2713 80.2 87.0 59.7 46.8 20.4 64.8 31.3 51.9 33.0 0.0 2.9 16.0\n\u2713 \u2713 U \u00d7 80.2 86.3 57.0 48.4 16.9 63.4 30.5 48.2 29.4 1.8 2.9 5.7\n\u2713 \u2713 U\u2713 80.0 87.3 59.0 48.2 21.4 64.0 33.9 52.2 34.2 0.0 5.7 16.2\nBRNN 65.4\n85.6 46.0 11.9 1.8 23.0 1.7 13.0 0.0 5.2 11.9 0.0\nSpanish+Basque (ES+EU)\nS D f B COM PER QUES COL DASH O_QS SCOL O_QUO QUO ELL EXC O_EX\n\u00d7 \u2013\n\u2013 \u00d7 75.6 83.7 56.9 48.6 14.7 57.5 19.3 53.6 41.7 13.2 14.9 16.7\n\u00d7 \u2013 \u2013 \u2713 75.9 83.7 55.7 48.6 21.1 59.9 20.8 55.2 44.8 23.1 24.6 33.0\n\u2713 \u00d7 \u2013 \u00d7 75.9 84.6 54.1 51.7 10.9 57.0 18.3 54.7 37.7 12.7 11.4 10.8\n\u2713 \u00d7 \u2013\u2713 75.3 83.7 56.9 50.2 21.6 60.3 20.3 54.1 43.8 20.1 15.7 21.3\n\u2713 \u2713 G \u00d7 76.1 84.5 55.4 48.6 7.6 57.1 19.8 52.4 36.4 10.4 9.3 13.9\n\u2713 \u2713 G\u2713 76.5 85.4 56.2 51.5 18.1 58.3 21.3 55.3 40.8 18.5 19.0 24.0\n\u2713 \u2713 U \u00d7 76.3 85.0 55.8 51.3 16.4 57.9 22.1 54.4 40.7 17.4 13.3 24.1\n\u2713 \u2713 U\u2713 76.2 85.6 56.2 50.2 23.7 58.8 24.8 55.5 41.7 19.0 21.4 32.7\nBRNN 69.7 87.4 49.9\n16.1 2.0 3.3 9.1 6.2 0.0 13.6 5.2 0.0\nT able 5: Class-wise F1scores for AutoPunct and the BRNN-based system in each language.\nLabels with a F1score of 0.0 in the three language scenarios were omitted. S: Using information\nof silences. D: Using discrete silences. f: Gaussian (G) or uniform (U) distribution for contiguous\nbuckets. B: Adding a custom BER T.\nS D f B EU ES ES+EU\n\u00d7 \u2013\n\u2013 \u00d7 91.7 92.1 90.6\n\u00d7 \u2013 \u2013 \u2713 91.6 92.1 90.9\n\u2713 \u00d7 \u2013 \u00d7 91.9 92.2 91.4\n\u2713 \u00d7 \u2013\u2713 91.6 91.7 90.7\n\u2713 \u2713 G \u00d7 91.7 92.1 91.3\n\u2713 \u2713 G\u2713 91.9 92.3 91.5\n\u2713 \u2713 U \u00d7 91.6 92.1 91.4\n\u2713 \u2713 U\u2713 92.2 92.4 91.7\nT able 6: Capitalisation micro-averaged F1\nscores. S: Using information of silences. D:\nUsing discrete silences. f: Gaussian (G) or\nuniform (U) distribution for contiguous buck-\nets. B: Adding a custom BER T.\nuations have been run using a Nvidia Ge-F orce R TX 2080ti GPU1. Again, we compare\nAutoPunct with the BRNN-based system.\nThese results are shown in T able 7.\nF rom the results of the table 7, it can be\nobserved that the computation speed is fast\nenough to enable a real-time processing. The\ntrend in the numbers show that the use of\ndiscrete silences requires more time, but this\nis not surprising due to the extra amount of\ncomputation to select and average the silence\nembeddings. The same reasoning applies to\nthe intermediate Custom BER T. Compared\nto the Punctuator baseline, all the architec-\ntural variations of the proposed system are\nfaster, in special taking into account that\n1These n\numbers should be taken as approxima-\ntions, since different hardware or different workloads\nmay lead to slightly different results.\nAnder Gonz\u00e1lez-Docasal, Aitor Garc\u00eda-Pablos, Haritz Arzelus, Aitor \u00c1lvarez\n66S D f B\nEU ES ES+EU\n\u00d7 \u2013\n\u2013 \u00d7 6221 7379 7572\n\u00d7 \u2013 \u2013 \u2713 6454 7038 6576\n\u2713 \u00d7 \u2013 \u00d7 6802 7112 7537\n\u2713 \u00d7 \u2013\u2713 5749 7186 6749\n\u2713 \u2713 G \u00d7 3097 3315 3128\n\u2713 \u2713 G\u2713 3071 3322 3068\n\u2713 \u2713 U \u00d7 2877 3124 3206\n\u2713 \u2713 U\u2713 3017 3174 3169\nBRNN 1729\n2126 2524\nT able 7: Processing speed in words per\nsecond. S: Using information of silences. D:\nUsing discrete silences. f: Gaussian (G) or\nuniform (U) distribution for contiguous buck-\nets. B: Adding a custom BER T.\nAutoPunct is also adding capitalisation to\nthe output in the same process.\n7 Conclusions\nIn this work we present AutoPunct , an\nautomatic punctuation and capitalisation\nsystem based on BER T that also makes use\nof the silence duration between words. The\nsystem was trained for 13 different punc-\ntuation labels and two types of capitalisa-\ntion. It works as a multilabel classifier, so\nit can predict several punctuation marks at-\ntached to a single word. The model is lan-\nguage agnostic and only depends on training\ndata, it can be even trained on several lan-\nguages at the same time. Due to its infer-\nence speed, ranging from 3000 to 7000 words\nper second depending on the chosen archi-\ntectural variation, it can be used in real-time\nscenarios. The system was tested in Span-\nish and Basque, both individually and simul-\ntaneously , using the mintzai-ST dataset. W e\ncarried on experiments with several architec-\ntural variations to assess their impact in the\nfinal result. W e also compared the proposed\nsystem with another well known automatic\npunctuation system, Punctuator from (Tilk\nand Alum\u00e4e, 2016), showing not only better\nresults but also faster inference times. As fu-\nture work, we would like to test additional ar-\nchitectural variations and hyper-parameters,\nand also train and evaluate on datasets of\nmore varied styles and application domains.\nAckowledgments\nThis work was supported by the Department\nof Economic Development and Competitive-\nness of the Basque Government under pro-jects GAMES (ZL-2020/00074) and Deep-\nT ext (KK-2020-00088).\nReferences\nAgerri, R., I. S. Vicente, J. A. Campos,\nA. Barrena, X. Saralegi, A. Soroa, and\nE. Agirre. 2020. Give your text repres-\nentation models some love: the case for\nbasque. arXiv preprint arXiv:2004.00033 .\nAlam, T., A. Khan, and F. Alam. 2020.\nPunctuation restoration using transformer\nmodels for resource-rich and-poor lan-\nguages. pages 132\u2013142.\nCa\u00f1ete, J., G. Chaperon, R. F uentes, J.-H.\nHo, H. Kang, and J. P\u00e9rez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. In PML4DC at ICLR 2020.\nChe, X., C. W ang, H. Y ang, and C. Meinel.\n2016. Punctuation prediction for unseg-\nmented transcript based on word vector.\npages 654\u2013658.\nChristensen, H., Y. Gotoh, and S. Renals.\n2001. Punctuation annotation using stat-\nistical prosody models.\nCourtland, M., A. F aulkner, and\nG. McElvain. 2020. E\ufb00icient auto-\nmatic punctuation restoration using\nbidirectional transformers with robust\ninference. pages 272\u2013279.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. T outanova. 2019. BER T: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. pages 4171\u20134186,\nJune.\nEtchegoyhen, T., H. Arzelus, H. Gete Ugarte,\nA. Alvarez, A. Gonz\u00e1lez-Docasal, and\nE. Benites F ernandez. 2021. mintzai-\nST: Corpus and Baselines for Basque-\nSpanish Speech T ranslation. In Proc.\nIberSPEECH 2021, pages 190\u2013194.\nF ederico, M., M. Cettolo, L. Bentivogli,\nP . Michael, and S. Sebastian. 2012. Over-\nview of the iwslt 2012 evaluation cam-\npaign. In IWSL T-International W orkshop\non Spoken Language T ranslation, pages\n12\u201333.\nJones, D. A., F. W olf, E. Gibson, E. Willi-\nams, E. F edorenko, D. A. Reynolds, and\nM. Zissman. 2003. Measuring the read-\nability of automatic speech-to-text tran-\nscripts. In Eighth European Conference on\nSpeech Communication and T echnology .\nAutoPunct: A BERT-based Automatic Punctuation and Capitalisation System for Spanish and Basque\n67Kim, J.-H.\nand P . C. W oodland. 2003.\nA combined punctuation generation and\nspeech recognition system and its per-\nformance enhancement using prosody .\nSpeech Communication , 41(4):563\u2013577.\nLiu, Y., M. Ott, N. Goyal, J. Du,\nM. Joshi, D. Chen, O. Levy , M. Lewis,\nL. Zettlemoyer, and V. Stoyanov. 2019.\nRoberta: A robustly optimized bert\npretraining approach. arXiv preprint\narXiv:1907.11692.\nLoshchilov, I. and F. Hutter. 2019. De-\ncoupled W eight Decay Regularization. In\nProceedings of the Seventh International\nConference on Learning Representations\n(ICLR 2019), pages 1\u201318.\nNassif, A. B., I. Shahin, I. Attili, M. Azzeh,\nand K. Shaalan. 2019. Speech recognition\nusing deep neural networks: A systematic\nreview. IEEE Access , 7:19143\u201319165.\nOtegi, A., A. Agirre, J. A. Campos, A. Soroa,\nand E. Agirre. 2020. Conversational ques-\ntion answering in low resource scenarios:\nA dataset and case study for basque. In\nProceedings of The 12th Language Re-\nsources and Evaluation Conference, pages\n436\u2013442.\nPeitz, S., M. F reitag, A. Mauser, and H. Ney .\n2011. Modeling punctuation prediction\nas machine translation. In International\nW orkshop on Spoken Language T ransla-\ntion (IWSL T) 2011.\nSchuster, M. and K. K. Paliwal. 1997.\nBidirectional recurrent neural networks.\nIEEE transactions on Signal Processing ,\n45(11):2673\u20132681.\nSunkara, M., S. Ronanki, K. Dixit,\nS. Bodapati, and K. Kirchhoff. 2020.\nRobust prediction of punctuation and\ntruecasing for medical asr. pages 53\u201362.\nTilk, O. and T. Alum\u00e4e. 2016. Bidirec-\ntional recurrent neural network with at-\ntention mechanism for punctuation restor-\nation. pages 3047\u20133051.\nYi, J. and J. T ao. 2019. Self-attention\nbased model for punctuation prediction\nusing word and speech embeddings. pages\n7270\u20137274.\nYi, J., J. T ao, Y. Bai, Z. Tian, and C. F an.\n2020a. Adversarial transfer learning forpunctuation restoration. arXiv preprint\narXiv:2004.00248.\nYi, J., J. T ao, Z. Tian, Y. Bai, and C. F an.\n2020b. F ocal loss for punctuation predic-\ntion. Proc. Interspeech 2020, pages 721\u2013\n725.\nAnder Gonz\u00e1lez-Docasal, Aitor Garc\u00eda-Pablos, Haritz Arzelus, Aitor \u00c1lvarez\n68Unimo\ndal F eature-level improvement on Multimodal\nCMU-MOSEI Dataset: Uncorrelated and ConvolvedF eature Sets\nMejora unimodal a nivel de caracter\u00edsticas en el dataset\nmultimodal CMU-MOSEI: Caracter\u00edsticas no correlacionadas y\nconvolucionadas\nDaniel Mora Melanchthon\nGrupo T ecling.com\nPontificia Universidad Cat\u00f3lica de V alpara\u00edso, Chile\ndaniel.mm91@gmail.com\nAbstract: This study investigates unimodal features \u2013BER T embeddings (text),\neGeMAPs (acoustic), and openF ace set (visual)\u2013 used on the multimodal CMU-\nMOSEI dataset for Emotion Recognition in order to seek unimodal feature-level\nimprovements. T wo approaches are investigated: feature selection by hierarchically\nclustering each set according to their Spearman correlation value, and the use of\nConvolutional Neural Network (CNN) models to act as emotion feature extractors.\nExperiments are performed with Random F orest (RF). Main results show, firstly ,\nthat the use of uncorrelated feature sets tend to not change model\u2019s performance,\nallowing for trainable parameters, training time, and storage requirements reduction.\nSecondly , the direct use of CNN-embeddings with RF models yields improvements\nfor acoustic modality , which suggests that major improvements could be sought\nthrough embedding acoustic features.\nKeywords: CMU-MOSEI, Emotion Recognition, Convolved F eatures, Spearman\nCorrelation.\nResumen: Este estudio investiga dos caminos con el fin de mejorar las caracter\u00edsti-\ncas unimodales que son utilizadas para el reconocimiento de emociones en el dataset\nmultimodal CMU-MOSEI. El primer camino es la selecci\u00f3n de caracter\u00edsticas basado\nen la correlaci\u00f3n de Spearman al interior de cada modalidad (textual, ac\u00fastica, vi-\nsual). El segundo camino es utilizando una Red Neuronal Convolucional (CNN)\npara extraer caracter\u00edsticas unimodales que sean relevantes para el reconocimiento\nde emociones. Los experimentos comparan los distintos sets de caracter\u00edsticas uti-\nlizando un Bosque Aleatorio (Random F orest). Los resultados muestran, primero,\nque el uso de caracter\u00edsticas unimodales no correlacionadas no modifican el resul-\ntado del modelo, lo que permite reducir la cantidad de par\u00e1metros, tiempo de en-\ntrenamiento y almacenamiento computacional. Segundo, el uso de caracter\u00edsticas\ngeneradas por el modelo de Redes Neuronales Convolucionadas utilizadas en un\nBosque Aleatorio s\u00ed genera mejoras para la modalidad ac\u00fastica, lo que sugiere que\nfuturas mejoras puedan desarrollarse en esta l\u00ednea.\nPalabras clave: CMU-MOSEI, reconocimiento de emociones, correlaci\u00f3n de Spear-\nman, extracci\u00f3n de caracter\u00edsticas.\n1 Introduction\nEmotion Recognition is one of the goals of\nAffective Computing: to build machines able\nto recognize and behave in response to hu-\nman emotions (Picard, 1995). How Machine\nLearning models perform on these tasks re-\nlies mainly on available datasets and features\nextracted from them. There has been a ma-\njor improvement in accomplishing these tasks\nfrom a multimodal perspective recently , how-ever, which modalities to use, how to repre-\nsent multimodal information, when and how\nto fuse it (Atrey et al., 2010), and even how\nto build consistent and appropriate datasets,\nare still active research questions.\nThis study focuses on Emotion Recogni-\ntion on CMU-MOSEI dataset (Bagher Zadeh\net al., 2018). Most recent state-of-the-art re-\nsults can be attributed to model\u2019s improve-\nments, but there have not been much consid-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 69-81\nrecibido 01-05-2021 revisado 29-05-2021 aceptado 04-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-6\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalerations regarding\nwhat features have been\nused or if they could be improved. This re-\nsearch shows how unimodal features could be\nimproved by selecting uncorrelated features\nusing Spearman correlation and using Con-\nvolutional Neural Network (CNN) models as\nfeature extractors. A Random F orest model\nwas used to evaluate the performance of un-\ncorrelated feature sets and CNN-embeddings\ngiven that it is a model that directly uses fea-\ntures.\nIn what follows, section 2 defines the\ntask performed, highlights the latest stud-\nies on CMU-MOSEI dataset and the features\nused. Section 3 introduces the CMU-MOSEI\ndataset. Section 4 details the information\nconveyed in each modality . Section 5 details\nhow subsets of features were built, and mod-\nels used. Section 6 shows the results of the\nexperiments, and finally section 7 discusses\nthe main findings and proposes future areas\nof investigation.\n2 Background\nEmotion Recognition is the task of recogniz-\ning specific affective states or feelings that are\nclassified given certain emotion typology . In\nthis case, emotion recognition is considered\nas a categorical classification for each emo-\ntion: present or absent.\n2.1 F eature Engineering\nResearchers have focused on how specific\nmodalities (text, speech, face gestures,\nbody movements, biosignals) convey affec-\ntive states, and have related the behaviour\nof specific-domain features to changes in sen-\ntiment or emotions. Cowie et al. (2001) sum-\nmarize results that identify relevant acoustic\nand visual features for predicting emotions\nunder controlled scenarios.\nF or example, from an acoustic perspec-\ntive, they observed that there is a pitch in-\ncrease when feeling fear, but a decrease when\nfeeling sadness; falling and rise-fall tones\nwhen feeling excitement; faster speech rate\nwhen feeling disgust compared to when feel-\ning happy , and much slower when feeling sad-\nness. Regarding visual cues, emotions are\nexpressed by the movement of face muscles:\nwhen compared to a neutral face, a happy\nstate moves the cheek muscles upwards and\nshrink the eyes; a surprised state is character-\nized by rising eyebrows, and opening of the\neyes and mouth.This line of research leads to specific\nknowledge-based features that are highly dis-\ncriminative of affective states. This is the\ncase of The Extended Geneva Minimalistic\nAcoustic Parameters Set (eGeMAPs) (Eyben\net al., 2016) which were derived from decades\nof research on how acoustic features relate\nto emotional states. This effort also avoids\nbrute-force feature sets such as the Computa-\ntional Paralinguistic Set (ComPare) that has\nmore than 6,000 features and demands high\ncomputer resources for storage and manipu-\nlation.\nAlong with selecting highly discriminative\nfeatures based on theoretical and empirical\nstudies, the increase in available data and\nlack of model\u2019s capabilities to learn represen-\ntations from the input data has led to neces-\nsary pre-processing steps (Guyon and Elisse-\neff, 2003). In this sense, it is common prac-\ntice to select a subset of features following a\nfeature-knowledge or data criteria (Liu and\nY u, 2005). In this study , Spearman correla-\ntion as a dependency measure \u2013feature Y can\nbe obtained by knowing feature X, thus Y is\nredundant\u2013 was performed to select a subset\nof features (section 5).\nOne example of knowledge-based features\nfor emotion recognition is a Bag-of-W ords,\nwhich weights how much each word is in-\ndicative of a given emotion. Jin et al.\n(2015) proposed a Support V ector Machine\nthat achieves better generalization compared\nto Artificial Neural Networks on IEMOCAP\ndataset (Busso et al., 2008).\nEven though this approach helps in un-\nderstanding the relation between modalities\nand sentiments/emotions, this link is usu-\nally not a one-to-one straightforward map-\nping between features and outputs. This is\nmade even more evident when affective states\nare spontaneous and not elicited. Kim et al.\n(2005) used a Fisher\u2019s Linear Discriminant\nClassifier with acoustic and physical (electro-\ncardiogram and respiration signals) modal-\nities, and their results were not as high as\nprevious studies on scripted environments.\nAs they concluded, biosignals are more\nconsistent when emotions are elicited com-\npared to spontaneous scenarios. This finding\nagrees with Tian, Moore, and Lai (2017) con-\nclusion for acoustic modality: pitch, inten-\nsity and voice quality have a higher standard\ndeviation in acted dialogues than in sponta-\nneous conversations when comparing IEMO-\nDaniel Mora Melanchthon\n70CAP and\nA VEC2012 (Schuller et al., 2012)\ndatasets.\nNoticeable differences between affective\nstates and consistency across samples help\nsimple models to achieve higher performance,\nwhich suggests that the manner in which\nthe dataset is collected highly influences the\nmodel\u2019s results. On spontaneous datasets\nsuch as CMU-MOSEI, Neural Network mod-\nels achieve state-of-the-art results, as the\nnext section depicts.\n2.2 F eatures Used on\nCMU-MOSEI Dataset\nRecent developments have shown that Neural\nNetwork models achieve state-of-the-art per-\nformance on Emotion Recognition for com-\nmon benchmarks: RECOLA (Ringeval et al.,\n2013) , CMU-MOSI (Zadeh et al., 2016),\nCMU-MOSEI (Bagher Zadeh et al., 2018).\nOne of the reasons for this advancement is\nnot better extraction of features or a deeper\nunderstanding of how they relate to affec-\ntive states, but rather the model\u2019s capabil-\nities to select, discriminate and learn feature\nrepresentations that are relevant for the task.\nHowever, this improvement comes with the\ndrawback of a lack of understanding and in-\nterpretation of the model\u2019s output, especially\nwhen using raw input data.\nEnd-to-end Neural Network models have\nbeen proposed to build acoustic feature rep-\nresentations. T rigeorgis et al. (2016)\nproposed a CNN model to capture emo-\ntional content from raw input waveforms that\nfeeds a Long Short-T erm Memory (LSTM)\nmodel that captures contextual information.\nAs they demonstrated, their approach per-\nformed better than using eGeMAPs and\nComPare sets on the RECOLA dataset.\nOn the same dataset, T zirakis, Zhang, and\nSchuller (2018) used a CNN model to learn\nfeature representations from raw waveforms,\nand added a Bidirectional LSTM (BiLSTM)\non top to model contextual information for\nemotion recognition task. Their experiments\nconfirm that an end-to-end model is capa-\nble of performing better than using selected\nfeatures sets, such as eGeMAPs, log mel-\nfilter bank, and Mel-frequency cepstral coef-\nficients.\nBoth studies used a deep CNN architec-\nture with several convolution layers to model\nraw input data, but simpler architectures are\nalso able to achieve state-of-the-art results onCMU-MOSEI dataset. Williams et al. (2018)\nproposed a multimodal approach using text,\nvisual, and acoustic modalities on an early-\nfusion architecture. The three modalities are\nconcatenated and serve as input for CNN,\nLSTM, or BiLSTM models. The BiLSTM\nmodel yielded the best performance winning\nthe Grand Challenge and W orkshop on Hu-\nman Multimodal Language (ACL, 2018).\nOn CMU-MOSI and CMU-MOSEI\ndatasets, several Neural Network models\nhave been proposed with a focus on model\u2019s\narchitecture \u2013mainly LSTM and CNN\u2013,\nfusion of modalities, and interpretation of\nmodalities\u2019 contribution rather than the\nfeatures used.\nThe fusion and interaction between\nmodalities have been approached by using\na bimodal attention mechanism (Ghosal et\nal., 2018), dynamically fusing modalities\n(Bagher Zadeh et al., 2018), and even on\nunaligned modalities (T sai et al., 2019). The\nouter-product between bimodal representa-\ntions was proposed as an alternative (Sun\net al., 2019), and a gating mechanism to\nlearn bimodal interactions (Kumar and V epa,\n2020).\nThe use of a wider context has been ex-\nplored to account for temporal variation of\nemotion in conversational setups. Shenoy\nand Sardana (2020) made use of the model\u2019s\noutputs for previous sentences as additional\ninput, and Poria et al. (2017) considered sur-\nrounding utterances to model context with\nan RNN model.\nPham et al. (2019) aimed to learn\njoint representations by translating modali-\nties, thus the model is more robust if any\nmodality is missing (e.g. there are no vi-\nsual cues), and T sai et al. (2020) sought\nto interpret different modalities\u2019 contribu-\ntions by adding a routing mechanism that\nmeasured the contribution that each modal-\nity makes towards a correctly classified out-\nput. They suggested that acoustic fea-\ntures are better at predicting negative sen-\ntiments/emotions, and that the interaction\nbetween acoustic-visual modalities help pre-\ndict emotion \u2018Anger\u2019 .\nRegarding the best results on CMU-\nMOSEI, T sai et al. (2019) achieved state-of-\nthe-art on Sentiment Analysis with an F1-\nscore of 82.3 on CMU-MOSEI. On Emo-\ntion Recognition, Shenoy and Sardana (2020)\nachieved best results at recognizing emo-\nUnimodal Feature-level improvement on Multimodal CMU-MOSEI Dataset: Uncorrelated and Convolved Feature Sets\n71tions using\naccuracy as metric: Happiness\n(70.01%), Sadness (76.19%), Anger (83.09%)\nSurprise (87.48%), Disgust (90.37%), and\nF ear (89.75%).\nOverall, all models use text, visual, and\nacoustic modalities. Each modality provides\ninformation from a different channel of ex-\npression: text features are the words pro-\nnounced, acoustic features relate to speech\nsignal properties, and visual features refer to\nfacial expressions and muscle movements.\nSpecifically , for text modality , GloV e em-\nbeddings (Pennington, Socher, and Manning,\n2014), output of CNN (Karpathy et al.,\n2014), and lately BER T embeddings (Devlin\net al., 2019) are used. F or acoustic modality ,\nopenSMILE (Eyben, W\u00f6llmer, and Schuller,\n2010) or COV AREP (Degottex et al., 2014)\ntoolkits are used to extract acoustic features.\nF or visual modality , output of 3D CNN (Ji\net al., 2013), F acet (iMotions), or openF ace\n(Baltrusaitis et al., 2018) toolkits are used to\nextract visual features.\nEven though Neural Networks tend to\nachieve best results on these datasets, their\nperformance is at the expense of the model\u2019s\ninterpretability (Buhrmester, M\u00fcnch, and\nArens, 2019), especially when it comes to\nunderstanding the performance at a fea-\nture level. Knowing which features better\nmodel sentiment and emotion allows us to\nreduce the amount of trainable parameters,\nthe model\u2019s training time, and storage re-\nquirements (Louppe, 2015). It also helps us\nin interpreting results and in understanding\nthe relationship between the model\u2019s perfor-\nmance and specific variables.\nThe focus, therefore, is on gaining in-\nsight regarding which multimodal features\nare more relevant or redundant on CMU-\nMOSEI dataset in order to reduce unimodal\nfeature sets, and if a CNN model is able\nto better generate feature representations\nfor Emotion Recognition task to improve\nmodel\u2019s performance.\nThe multimodal features used are BER T\nembeddings as text modality due to their\nhigh performance across different tasks;\nopenSMILE toolkit to extract eGeMAPs be-\ncause it is highly selected (88 features);\nand openF ace toolkit to extract visual fea-\ntures. The next section presents CMU-\nMOSEI dataset and how this multimodal in-\nformation is conveyed.3 Dataset: CMU-MOSEI\nZadeh et al. (2018) introduced the CMU-\nMOSEI dataset and at the time of writ-\ning it is the largest multimodal spontaneous\nsentiment and emotion-related dataset (49.1\nhours of labelled segments) publicly avail-\nable1. It groups 3,837 videos from 1,000\nspeakers that are segmented in 23,259 la-\nbelled segments (train set: 16,327; valid set:\n2,270; test set: 4,662). On average each video\nhas 6 segments and each segment lasts 7.6\nseconds.\nAccording to Zadeh et al. (2018),\nvideos were automatically extracted from the\nY ouT ube platform and then manually filtered\nconsidering a set of criteria: real-life situa-\ntions (monologue style) in which the camera\nstood still in front of the speaker; presence\nof text, acoustic, and visual modalities; gen-\nder balance (57% male, 43% female); mini-\nmum quality of video and transcription us-\ning a threshold confidence interval for fa-\ncial feature extraction and forced alignment;\nwide variety of topics, but mainly product re-\nviews (16.2%), debates (2.9%), and consult-\ning (1.8%); and reliable transcriptions writ-\nten by the uploader.\nRecording equipment ranged from com-\nmon built-in cameras and microphones\n(living-room setup) to more professional stu-\ndios (new\u2019s setup). T able 1 shows an excerpt\nof one segment.\n3.1 Labels: Emotions\nEach video-segment is annotated on a [0, +3]\nLikert scale by 0.33 intervals meaning [0: ab-\nsence of emotion and +3 highly present] re-\ngarding each emotion {happiness, sadness,\nanger, fear, disgust, surprise} \u2013following Ek-\nman, F reisen, and Ancoli (1980) typology\u2013\n. Out of all segments, 28% have no emo-\ntions, and 72% present 1 or 2: \u2018Happiness\u2019 is\nthe most common occurring approximately\non 50% of all video segments, and \u2018F ear\u2019 is\nthe least common with an appearance of just\n0.08% on the entire set, followed by \u2018Surprise\u2019\nwith 0.09%. Figure 1 summarizes the relation\nbetween presence and absence per emotion on\nthe dataset.\nWith respect to arousal values, emotions\nare either not present (arousal = 0) or weakly\npresent (arousal = 0.33). The emotion \u2018Hap-\npiness\u2019 is the only one with higher number\n1https://gith\nub.com/A2Zadeh/CMU-\nMultimodalSDK/\nDaniel Mora Melanchthon\n72Modalit\ny Description\nT ext sp hello\neveryone sp\nand welcome to a sp\ncustomer\nAcoustic Neutral sp\neech\n[2.8]\nVisual 98%\nT able\n1: \u2018T ext\u2019 is the transcription provided\n(sp stands for short pause ). \u2018Acoustic\u2019 is a\ndescription of speech and its duration in sec-\nonds. \u2018Visual\u2019 depicts openF ace\u2019s recognition\nconfidence interval (i.e. how sure the toolkit\nis about recognizing a face).\nof annotations than the rest of the emo-\ntions, and shows a decreasing curve towards\na stronger presence (arousal = 3.0), whereas\n\u2018F ear\u2019 is the least common and is only anno-\ntated up to 1.67.\nFigure 1:\nRelation between presence (arousal\nvalue >0) and absence (arousal value = 0) of\neach emotion for CMU-MOSEI dataset.\n4 Multimodal F eatures\nF eatures were extracted from raw videos2.\nF or text features, huggingface repository3was\nused to load BER T embeddings (Devlin et\n2http://immortal.m\nulticomp.cs.cmu.edu/raw_datasets/\n3https://github.com/huggingface/transformersal., 2019). F or acoustic features, openS-\nMILE toolkit (Eyben, W\u00f6llmer, and Schuller,\n2010) was utilized to extract the Extended\nGeneva Minimalistic Acoustic Parameter Set\n(eGeMAPs). F or visual features, the open-\nF ace toolkit (Baltrusaitis et al., 2018) was\nused.\n4.1 Acoustic F eatures\nAcoustic features aim to grasp speech prop-\nerties that are related to affective states. In-\ntuitively , we know that we vary loudness,\nspeech rate, tone, as our mood changes, in-\ndependently of the exact words pronounced.\nAcoustic sets contain frequency , energy ,\nspectral, and temporal parameters.\nAs a result of multidisciplinary research\non affecting speech, Eyben et al. (2016) pro-\nposed a carefully selected set known as the\nGeneva Minimalistic Acoustic Parameter set\n(GeMAPs) that has 62 features as well as its\nextended version eGeMAPs with 88 features.\nThe selection of acoustic features followed\nthree main criteria: its relation to affective\nspeech, its relevance in previous studies, and\nits theoretical importance. F or a detailed de-\nscription, refer to Eyben (2016), section 3.5.\n4.2 Visual features\nVisual features are based on previous studies\nthat identify the location and movement (de-\nformation) of facial muscles to affective states\n\u2013known as F acial Action Units (F AU)\u2013 (Ek-\nman, F reisen, and Ancoli, 1980), in addition\nwith other cues, such as how the head moves\nand what direction the eyes are looking to-\nwards. The general working process considers\ndetecting a face, and then computing static\nparameters that are used to obtain tempo-\nral parameters resembling movement or de-\nformation.\nopenF ace starts with face detection to ex-\ntract 3D facial landmark location, and based\non this it is able to extract head pose, eye\ngaze, face alignment, and F AU. It outputs in\ntotal 709 features every 0.333 seconds. T o\nensure reliability and real-time speed, face\ndetection starts with up to 11 initialization\nhypotheses. However, if a confidence thresh-\nold is not achieved, then it outputs 0 values\nfor all features.\n4.3 T ext F eatures\nW e used pre-trained-uncased BER T base-\nmodel (Devlin et al., 2019) to extract contex-\ntualized word representations. It generates\nUnimodal Feature-level improvement on Multimodal CMU-MOSEI Dataset: Uncorrelated and Convolved Feature Sets\n73word\nembeddings by tokenizing at sub-word\nlevel \u2013W ordPiece (W u et al., 2016)\u2013 and us-\ning information regarding the position of the\nword in the sentence as contextualized infor-\nmation. The result is a dynamic representa-\ntion of each word. It outputs 12 layers of 768\ndimensional vectors per word.\nThis contrasts with the widely used Glove\nembeddings (Pennington, Socher, and Man-\nning, 2014), which do not take into account\nthe position of words, thus word embeddings\nregardless of their surrounding context are\ngenerated and do not account for homonyms.\nIn addition, entire words are used as tokens,\nwhich results in an out-of-vocabulary issue:\nfollowing Zipf\u2019s Law, a majority of words are\nseen only once in a corpus.\nSentences were extracted using the tran-\nscriptions provided in the dataset. Accord-\ning to Devlin et al. (2019), the last 4 layers\nwithout fine-tuning show competitive results,\nthus these are used in the experiments.\n5 Methodology\nT aking the initial feature sets as a baseline,\ntwo feature sets were derived: the first one\nwith uncorrelated features and the second\none with convolved features. Experiments\nwere tried out using Random F orest model.\n5.1 Random F orest\nRandom F orest (Breiman, 2001) is an ensem-\nble method that combines multiple Decision\nT rees (Quinlan, 1986) by taking the average\nof all T rees\u2019 output. The main motivation of\ndoing this was the ease of interpretation of\nthe model: a tree-based model that predicts\na target value based on decision rules applied\ndirectly to features. The decision tree-based\nalgorithm used is CAR T (binary-only splits),\nand the purity criterion is Gini index.\n5.2 CNN as F eature Extractor\nBesides their high performance across com-\nmon benchmarks on 2D and 1D signal pro-\ncessing (Khan et al., 2020), Convolutional\nNeural Network (LeCun et al., 1989) was cho-\nsen due to its ability to work as a feature ex-\ntractor and achieve competitive performance,\neven when using raw input data (see section\n2.2).\nConvolutional Neural Networks are sim-\nilar to F eed F orward Neural Networks, but\ninstead of a matrix multiplication, they use\na linear operation called convolution (Good-fellow, Bengio, and Courville, 2016). In its\ngeneral form, the convolution layer has a ker-\nnel that traverses the input data applying\nthe convolution over a portion of the input\ndata and outputting features maps. Because\nthe kernel has learnable parameters that will\nbe adjusted according to an objective func-\ntion through back-propagation, the output is\nconsidered to be an intermediate representa-\ntion tuned for the specific task, hence intrin-\nsically acting as a feature extractor (LeCun,\nKavukvuoglu, and F arabet, 2010).\nIt is common practice to apply a pooling\nlayer to extract highly-discriminative dimen-\nsions only and reduce parameters along the\nnet (Khan et al., 2020), and regularize the\nlearning process applying batch normaliza-\ntion (Ioffe and Szegedy , 2015) and dropout\n(Hinton et al., 2012). Figure 2 depicts the\nCNN\u2019s architecture: it is fed with word-level\nfeatures that act as input and six convolu-\ntion layers are applied, each followed by a\nnon-linear activation function (ReLU); then\nmax-pooling of size 2 and dropout (0.3) are\napplied before the fully-connected layer that\nreduces dimensionality to the desired output.\n5.3 Uncorrelated F eatures Set\nThe Spearman rank-correlation between pair\nof features was measured to hierarchically\ncluster each feature set. W ard\u2019s minimum\nvariance clustering method was used to build\na dendrogram. Then, each dendrogram was\nflattened selecting one feature per cluster\nthat were grouped within a cophenetic dis-\ntance.\n5.3.1 Spearman Correlation\nSpearman rank-order correlation coe\ufb00icient\nmeasures the monotonic relationship of two\nvariables by ranking the samples from each\nvariable (Zwillinger and Kokoska, 1999). Its\nvalue ranges from -1 (as one variable in-\ncreases, the other decreases), to +1 (as one\nvariable increases, the other increases). 0\nvalue means no correlation.\nConsidering a pair of features, uandv, ob-\nservations from each feature are ranked from\nsmallest to largest, and observations with\nequal value are assigned the mean rank of\ntheir position. T o obtain the correlation, the\nfollowing formula is applied:\nrs= 1\u22126\u03a3n\ni=1d2\ni\nn(n2\u22121)(1)\nDaniel Mora Melanchthon\n74Figure 2:\nArchitecture of CNN model used to act as feature extractor. The star symbol rep-\nresents the convolution operation. The \u2019F eature Representation\u2019 layer before the final fully-\nconnected is used as input for Random F orest model. The output are six values for emotion\nrecognition (one value per emotion).\nWhere n=total _samples , and d2\niis the\nsquared difference of the rank of the ithsam-\nple from feature uand feature v. This mea-\nsure yields a squared correlation matrix of\ndimension equal to the amount of features.\n5.3.2 Hierarchical Clustering\nW ard\u2019s linkage is performed on the correla-\ntion matrix, and its result is a dendrogram\nthat groups features according to their corre-\nlation value (Romesburg, 2004).\nW ard\u2019s linkage is an agglomerative hier-\narchical clustering procedure that groups a\nset of input values into a binary tree (Bar-\nJoseph, Gifford, and Jaakkola, 2001). It\nstarts from each sample being a cluster on\nits own, and iteratively merging two clusters\nat each time-step using the minimum vari-\nance criterion. This process is repeated until\nall input values are grouped into one cluster.\nAt each time-step, the variance of all pos-\nsible mergers are compared. First, the vari-\nance of each cluster is measured by summing\nthe squared Euclidean distance between all\nelements from a cluster and its mean. Then,\nthe variance of each cluster is summed up\nand obtained the variance value of the merge.\nThe merge that results in the lowest increase\nof variance is chosen.\nFinally , the dendrogram is flattened and\none feature per cluster is retrieved. T o flatten\nthe dendrogram, a threshold tindicates what\nthe maximum distance will be considered to\ncluster. If tis low, then only nearby clusters\nwill be merged. If tis high, then further apart\nclusters will be merged.\n5.3.3 Baseline Correlation\nFigure 3 shows correlation heat-maps for each\ninitial set of features and after selecting fea-\ntures following hierarchical clustering based\non Spearman correlation. Figure 4 showsthe reduction per group of parameters for\neGeMAPs and openF ace feature sets.\nAs evidenced by the figures, openF ace ini-\ntial set has high correlation, especially for\n\u2019eye gaze\u2019 and \u2019landmarks\u2019 groups (see fig-\nure 4) that have similar measures but with\ndifferent metrics (e.g. 2D and 3D), and it\nis considerably reduced when grouping clus-\nters at a maximum cophenetic distance of\n1. eGeMAPs shows slight correlation (green\noverall). Certain degree of correlation is ex-\npected given that it is made of related func-\ntionals (e.g. arithmetic mean and max value)\nthat compute over related LLDs (e.g. loud-\nness and energy).\n6 Experiments\nRandom F orest was used to compare three\nfeature sets on CMU-MOSEI dataset: the\nbaseline feature set, the uncorrelated feature\nset, and the convolved feature set. The train,\nvalid, and test set are the ones predefined by\nthe dataset and commonly used by previous\nstudies. The metric used for Emotion Recog-\nnition is F1-score.\nT wo hypotheses were tested. The first one\nis that uncorrelated feature sets will leave\nmodel\u2019s performance unchanged, given that\nthe same information is conveyed by multi-\nple features.\nThe second one is that convolved-features\nare expected to better model emotion com-\npared to initial feature sets. This difference\nshould be more notable for eGeMAPs and\nopenF ace feature sets compared to BER T\nbecause the former are handcrafted features\nwhereas BER T are already a learned repre-\nsentation. However, a slight improvement\nmight occur as a consequence of finetuning\npre-trained BER T embeddings for these spe-\nUnimodal Feature-level improvement on Multimodal CMU-MOSEI Dataset: Uncorrelated and Convolved Feature Sets\n75Figure 3:\nF eature correlation heat-map per\nfeature set. Overall, the selection feature pro-\ncess applied selects features that are less cor-\nrelated within the set. Figures on the left\nare the initial feature set, and on the right\nthe reduced set with a cophenetic distance of\n1. Y ellow means high correlation and dark\nblue means low correlation. The amount of\nfeatures is in parentheses.\ncific tasks.\nResults on table 2 show that, compared\nto baseline feature sets, uncorrelated feature\nsets remain the same for Emotion Recogni-\ntion. Overall, all modalities achieve almost\nthe same performance at classifying \u2018Happi-\nness\u2019 (roughly F1-score of 0.70) which is the\nonly balanced emotion. A huge difference\nbetween emotions suggests that the model\nmainly uses \u2018Happiness\u2019 when seeking impu-\nrity decrease, which is a consequence of be-\ning the only emotion that is present in almost\n50% of the dataset.\nResults per emotion indicate that \u2018Sad-\nness\u2019 is better predicted with acoustic modal-\nity \u2013in line with T sai et al. (2020) inter-\npretation of acoustic modality\u2013, \u2018Anger\u2019 has\na prominent rise when using visual features,\nand \u2018Disgust\u2019 is better predicted with text\nmodality . F or recognizing \u2018Surprise\u2019 and\n\u2018F ear\u2019, which are the least represented emo-\ntions, all modalities fail, except for BER T\nand eGeMAPs that show an imperceptible\nimprovement. The improvement per emotion\nregarding each feature set can be interpreted\nas how each modality better conveys specific\nemotion information, as well as how easily\nFigure 4:\nNumber of features per group of pa-\nrameters according to each subset of features\nfor acoustic and visual sets. eGeMAPs fea-\ntures gradually decrease in line with a slight\ncorrelation overall, whereas openF ace feature\nset dramatically decreases for \u2018eye gaze\u2019 and\n\u2018landmarks\u2019 that have hundreds of features\nhighly correlated. t[n] means the cophenetic\ndistance used. F or this study , t = 1.\nperceived these emotions are for label anno-\ntators through these modalities.\nA closer look at confusion matrices (figure\n5) when using openF ace-t1 feature set con-\nfirms that the model is able to predict the\nemotion \u2018Happiness\u2019 whether it is present or\nabsent. However, as there are fewer samples\nof each emotion, the model tends to predict\nabsence of the emotion (i.e. negative predic-\ntion) most of the time. As can be seen, for\n\u2018Anger\u2019 the model is able to correctly classify\nalmost 10% when the emotion is present, and\nfor \u2018Surprise\u2019 it never predicts the presence of\nthat emotion, which yields an F1-score of 0.0.\nRegarding how Random F orest performed\nwith the convolved features sets, two conclu-\nsions are derived. The first one is that for\ntext modality it was not useful. The intuition\nthat fine-tuning BER T features according to\nthe specific task could lead to a model\u2019s im-\nprovement is rejected. This is also the case\nfor visual modality that did not see any im-\nprovement but decreased performance, spe-\ncially for \u2018Anger\u2019 that went down from 0.19\nDaniel Mora Melanchthon\n76F eature\nSet F1-macro Happiness Sadness Anger Surprise Disgust F ear\neGeMAPs 0.15 0.68 0.10 0.08 0.0 0.06 0.005\neGeMAPs-t1 0.15 0.67 0.10 0.07 0.0 0.06 0.005\neGeMAPs-CNN 0.21 0.65 0.23 0.19 0.004 0.20 0.0\nopenF\nace 0.17 0.71 0.08 0.19 0.0 0.07 0.0\nopenF\nace-t1 0.17 0.71 0.07 0.20 0.0 0.07 0.0\nopenF\nace-CNN 0.11 0.69 0.0 0.0 0.0 0.0 0.0\nBER T 0.17 0.70 0.05 0.02 0.0 0.23 0.005\nBER T-CNN 0.11 0.59 0.04 0.02 0.0 0.01 0.0\nT able\n2: Results for Emotion Recognition using Random F orest. Unimodal\u2019s baseline feature\nset is compared against the uncorrelated and the convolved feature set. Bold numbers reflect\nhow acoustic, visual, and text modality relate to \u2018Sadness\u2019, \u2018Anger\u2019, and \u2018Disgust\u2019, respectively .\nFigure 5:\nConfusion matrices for \u2018Happiness\u2019, \u2018Anger\u2019, and \u2018Surprise\u2019 when using openF ace-t1\nfeature set.\nto 0.0.\nHowever, the opposite is true for the\nacoustic modality . There is a huge in-\ncrease between the uncorrelated feature\nset (eGeMAPs-t1) and the convolved set\n(eGeMAPs-CNN), specially for \u00b4Sadness\u2019,\n\u2018Anger\u2019, and \u00b4Disgust\u2019 that raised 13, 12 and\n14 points, respectively . Along all feature\nsets, eGeMAPs-CNN is the best at recogniz-\ning emotions with a F1-macro score of 0.21\nwhich is even better than BER T.\nThis difference in how the CNN model be-\nhaves as feature extractor regarding differ-\nent modalities highlights that specific archi-\ntectures suit a given modality , or that the\nnumber of features within the set is rele-\nvant. F or instance, eGeMAPs has 88 fea-\ntures against openF ace with 709 and BER T\nwith 768. Even though eGeMAPs-CNN de-\ncreased 3 points for \u2018Happiness\u2019 compared to\nthe acoustic baseline, it was able to model\nless represented emotions, such as \u00b4Sadness\u2019,\n\u2018Anger\u2019, and \u00b4Disgust\u2019 .\n7 Conclusions\nEmotion Recognition from a multimodal per-\nspective is still an active research topic, espe-cially on spontaneous datasets such as CMU-\nMOSEI. As the experiment showed, there are\nredundant features that could lead to uni-\nmodal feature set reduction without losing\nsignificant performance. Therefore, reducing\ncorrelated features could be considered when\nreducing computational efforts.\nThe use of features that depict the same\ninformation, as the case of openF ace feature\nset, yields highly correlated features that are\nnot crucial for the Emotion Recognition task.\nF or example, the initial set could be reduced\nfrom 709 features to 129 by dropping \u2019eye\ngaze\u2019 and \u2019landmarks\u2019 correlated information.\nThis approach could be complemented with\nthe interaction between modalities to recog-\nnize, for example, noisy , redundant, absent\nor mismatch of modalities. This will help in\nreducing training time, using less storage re-\nsources, and helping in interpreting how spe-\ncific features might be responsible for correct\noutputs.\nIn general, all three modalities perform\nsimilar on this task. T wo reasons might ex-\nplain this behaviour. One reason is that the\ndataset is unbalanced towards one particu-\nlar emotion which is \u2018Happiness\u2019 . The sec-\nUnimodal Feature-level improvement on Multimodal CMU-MOSEI Dataset: Uncorrelated and Convolved Feature Sets\n77ond reason\nis that modalities could not be\nconsistent throughout the dataset. F or ex-\nample, when modalities are not present and\nconvey misleading affective cues (background\nmusic, no recognition of faces, unrelated im-\nages, etc.). One way to address this issue\ncould be assigning more weight to confidence\nlevels to tell if features are reliable.\nWhen considering the capability of a deep\nCNN model as feature extractor, results show\nthat text and visual modality substantially\ndecreased performance. However, acoustic\nmodality performed better, which suggests\nthat there is room for improving feature rep-\nresentations with the knowledge that hand-\ncrafted features could be improved by Neural\nNetwork models.\nOverall, these experiments show that fea-\nture engineering is useful, but how to imple-\nment it is an open question. F eature reduc-\ntion and convolved embeddings were effective\nfor specific modalities. Finally , it is worth\ninvestigating what kind of information the\nmodalities are in fact conveying and to what\nextent this has an impact on the model\u2019s re-\nsults.\nAcknowledgment\nThis work was supported by the Government\nof Chile through \u201dProyecto F ondecyt Regu-\nlar 1191481: Inducci\u00f3n autom\u00e1tica de tax-\nonom\u00edas de marcadores discursivos a partir\nde corpus multiling\u00fces (2019-2021)\u201d, lead in-\nvestigator Rogelio Nazar.\nReferences\nACL. 2018. Grand Challenge and W orkshop\non Human Multimodal Language - Multi-\nComp.\nAtrey , P . K., M. A. Hossain, A. El Saddik,\nand M. S. Kankanhalli. 2010. Multi-\nmodal fusion for multimedia analysis: a\nsurvey . Multimedia Systems, 16(6):345\u2013\n379, November.\nBagher Zadeh, A., P . P . Liang, S. Poria,\nE. Cambria, and L.-P . Morency . 2018.\nMultimodal Language Analysis in the\nWild: CMU-MOSEI Dataset and Inter-\npretable Dynamic F usion Graph. In Pro-\nceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics\n(V olume 1: Long Papers) , pages 2236\u2013\n2246, Melbourne, Australia. Association\nfor Computational Linguistics.Baltrusaitis, T., A. Zadeh, Y. C. Lim, and\nL.-P . Morency . 2018. OpenF ace 2.0: F a-\ncial Behavior Analysis T oolkit. In 2018\n13th IEEE International Conference on\nAutomatic F ace Gesture Recognition (FG\n2018), pages 59\u201366, May .\nBar-Joseph, Z., D. K. Gifford, and T. S.\nJaakkola. 2001. F ast optimal leaf order-\ning for hierarchical clustering. Bioinfor-\nmatics, 17(Suppl 1):S22\u2013S29, June.\nBreiman, L. 2001. Random F orests. Ma-\nchine Learning , 45(1):5\u201332, October.\nBuhrmester, V., D. M\u00fcnch, and M. Arens.\n2019. Analysis of Explainers of Black Box\nDeep Neural Networks for Computer Vi-\nsion: A Survey . arXiv:1911.12116 [cs],\nNovember. arXiv: 1911.12116.\nBusso, C., M. Bulut, C.-c. Lee,\nA. Kazemzadeh, E. Mower, S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan.\n2008. IEMOCAP: interactive emo-\ntional dyadic motion capture database.\nLanguage Resources and Evaluation;\nDordrect , 42(4):335\u2013359, December. Num\nPages: 25 Place: Dordrect, Netherlands,\nDordrect Publisher: Springer Nature\nB.V.\nCowie, R., E. Douglas-Cowie, N. T sapat-\nsoulis, G. V otsis, S. Kollias, W. F ellenz,\nand J. T aylor. 2001. Emotion recognition\nin human-computer interaction. IEEE\nSignal Processing Magazine, 18(1):32\u201380,\nJanuary . Conference Name: IEEE Signal\nProcessing Magazine.\nDegottex, G., J. Kane, T. Drugman,\nT. Raitio, and S. Scherer. 2014. CO-\nV AREP &#x2014; A collaborative voice\nanalysis repository for speech technolo-\ngies. In 2014 IEEE International Con-\nference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 960\u2013964, Flo-\nrence, Italy , May . IEEE.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. T outanova. 2019. BER T: Pre-\ntraining of Deep Bidirectional T rans-\nformers for Language Understanding.\narXiv:1810.04805 [cs], May . arXiv:\n1810.04805.\nEkman, P ., W. V. F reisen, and S. Ancoli.\n1980. F acial signs of emotional experi-\nence. Journal of Personality and Social\nPsychology , 39(6):1125\u20131134.\nDaniel Mora Melanchthon\n78Eyben,\nF. 2016. Real-time Speech and\nMusic Classification by Large Audio F ea-\nture Space Extraction. Springer Theses.\nSpringer International Publishing, Cham.\nEyben, F., K. R. Scherer, B. W. Schuller,\nJ. Sundberg, E. Andre, C. Busso, L. Y.\nDevillers, J. Epps, P . Laukka, S. S.\nNarayanan, and K. P . T ruong. 2016. The\nGeneva Minimalistic Acoustic Parameter\nSet (GeMAPS) for V oice Research and Af-\nfective Computing. IEEE T ransactions on\nAffective Computing, 7(2):190\u2013202, April.\nEyben, F., M. W\u00f6llmer, and B. Schuller.\n2010. Opensmile: the munich versatile\nand fast open-source audio feature extrac-\ntor. In Proceedings of the international\nconference on Multimedia - MM \u201910, page\n1459, Firenze, Italy . ACM Press.\nGhosal, D., M. S. Akhtar, D. Chauhan,\nS. Poria, A. Ekbal, and P . Bhattacharyya.\n2018. Contextual Inter-modal Attention\nfor Multi-modal Sentiment Analysis. In\nProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing , pages 3454\u20133466, Brussels,\nBelgium. Association for Computational\nLinguistics.\nGoodfellow, I., Y. Bengio, and A. Courville.\n2016. Deep Learning. MIT Press.\nGuyon, I. and A. Elisseeff. 2003. An Intro-\nduction to V ariable and F eature Selection.\npage 26.\nHinton, G., L. Deng, D. Y u, G. E. Dahl, A.-r.\nMohamed, N. Jaitly , A. Senior, V. V an-\nhoucke, P . Nguyen, T. N. Sainath, and\nB. Kingsbury . 2012. Deep Neural Net-\nworks for Acoustic Modeling in Speech\nRecognition: The Shared Views of F our\nResearch Groups. IEEE Signal Processing\nMagazine, 29(6):82\u201397, November. Con-\nference Name: IEEE Signal Processing\nMagazine.\niMotions. iMotions: Unpack Human Behav-\nior.\nIoffe, S. and C. Szegedy . 2015. Batch Nor-\nmalization: Accelerating Deep Network\nT raining by Reducing Internal Covari-\nate Shift. arXiv:1502.03167 [cs] , March.\narXiv: 1502.03167.\nJi, S., W. Xu, M. Y ang, and K. Y u. 2013. 3D\nConvolutional Neural Networks for Hu-man Action Recognition. IEEE T ransac-\ntions on Pattern Analysis and Machine In-\ntel ligence , 35(1):221\u2013231, January .\nJin, Q., C. Li, S. Chen, and H. W u. 2015.\nSpeech emotion recognition with acoustic\nand lexical features. In 2015 IEEE Inter-\nnational Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages\n4749\u20134753, April. ISSN: 2379-190X.\nKarpathy , A., G. T oderici, S. Shetty , T. Le-\nung, R. Sukthankar, and L. F ei-F ei.\n2014. Large-Scale Video Classification\nwith Convolutional Neural Networks. In\n2014 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pages 1725\u2013\n1732, June. ISSN: 1063-6919.\nKhan, A., A. Sohail, U. Zahoora, and A. S.\nQureshi. 2020. A survey of the recent\narchitectures of deep convolutional neural\nnetworks. Artificial Intel ligence Review ,\nApril.\nKim, J., E. Andre, M. Rehm, T. V ogt, and\nJ. W agner. 2005. Integrating information\nfrom speech and physiological signals to\nachieve emotional sensitivity . pages 809\u2013\n812.\nKumar, A. and J. V epa. 2020. Gated Mech-\nanism for Attention Based Multimodal\nSentiment Analysis. arXiv:2003.01043\n[cs, stat], F ebruary . arXiv: 2003.01043.\nLeCun, Y., B. Boser, J. S. Denker, D. Hen-\nderson, R. E. Howard, W. Hubbard, and\nL. D. Jackel. 1989. Backpropagation Ap-\nplied to Handwritten Zip Code Recogni-\ntion. Neural Computation, 1(4):541\u2013551.\nLeCun, Y., K. Kavukvuoglu, and C. F arabet.\n2010. Convolutional Networks and Appli-\ncations in Vision. In Proc. International\nSymposium on Circuits and Systems (IS-\nCAS\u201910). IEEE.\nLiu, H. and L. Y u. 2005. T oward integrat-\ning feature selection algorithms for clas-\nsification and clustering. IEEE T rans-\nactions on Knowledge and Data Engi-\nneering , 17(4):491\u2013502, April. Conference\nName: IEEE T ransactions on Knowledge\nand Data Engineering.\nLouppe, G. 2015. Understanding Ran-\ndom F orests: F rom Theory to Prac-\ntice. arXiv:1407.7502 [stat] , June. arXiv:\n1407.7502.\nUnimodal Feature-level improvement on Multimodal CMU-MOSEI Dataset: Uncorrelated and Convolved Feature Sets\n79Pennington,\nJ., R. Socher, and C. Manning.\n2014. Glove: Global V ectors for W ord\nRepresentation. In Proceedings of the\n2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP),\npages 1532\u20131543, Doha, Qatar. Associa-\ntion for Computational Linguistics.\nPham, H., P . P . Liang, T. Manzini, L.-P .\nMorency , and B. P\u00f3czos. 2019. F ound in\nT ranslation: Learning Robust Joint Rep-\nresentations by Cyclic T ranslations be-\ntween Modalities. Proceedings of the\nAAAI Conference on Artificial Intel li-\ngence , 33:6892\u20136899, July .\nPicard, R. W. 1995. Affective Computing.\nMIT Media Laboratory(Perceptual Com-\nputing):16.\nPoria, S., E. Cambria, D. Hazarika, N. Ma-\njumder, A. Zadeh, and L.-P . Morency .\n2017. Context-Dependent Sentiment\nAnalysis in User-Generated Videos. In\nProceedings of the 55th Annual Meeting\nof the Association for Computational Lin-\nguistics (V olume 1: Long Papers), pages\n873\u2013883, V ancouver, Canada. Association\nfor Computational Linguistics.\nQuinlan, J. R. 1986. Induction of deci-\nsion trees. Machine Learning, 1(1):81\u2013\n106, March.\nRingeval, F., A. Sonderegger, J. Sauer, and\nD. Lalanne. 2013. Introducing the\nRECOLA multimodal corpus of remote\ncollaborative and affective interactions. In\n2013 10th IEEE International Confer-\nence and W orkshops on Automatic F ace\nand Gesture Recognition (FG) , pages 1\u20138,\nApril.\nRomesburg, C. 2004. Cluster Analysis for\nResearchers . Lulu.com. Google-Books-\nID: ZuIPv7OKm10C.\nSchuller, B., M. V alster, F. Eyben, R. Cowie,\nand M. Pantic. 2012. A VEC 2012:\nthe continuous audio/visual emotion chal-\nlenge. In Proceedings of the 14th ACM in-\nternational conference on Multimodal in-\nteraction - ICMI \u201912, page 449, Santa\nMonica, California, USA. ACM Press.\nShenoy , A. and A. Sardana. 2020.\nMultilogue-Net: A Context A ware RNN\nfor Multi-modal Emotion Detection and\nSentiment Analysis in Conversation. Sec-\nond Grand-Chal lenge and W orkshop onMultimodal Language (Chal lenge-HML),\npages 19\u201328. arXiv: 2002.08267.\nSun, Z., P . Sarma, W. Sethares, and\nY. Liang. 2019. Learning Relationships\nbetween T ext, Audio, and Video via Deep\nCanonical Correlation for Multimodal\nLanguage Analysis. arXiv:1911.05544 [cs,\nstat], November. arXiv: 1911.05544.\nTian, L., J. D. Moore, and C. Lai. 2017.\nRecognizing emotions in spoken dialogue\nwith acoustic and lexical cues. In Proceed-\nings of the 1st ACM SIGCHI International\nW orkshop on Investigating Social Interac-\ntions with Artificial Agents - ISIAA 2017,\npages 45\u201346, Glasgow, UK. ACM Press.\nT rigeorgis, G., F. Ringeval, R. Brueckner,\nE. Marchi, M. A. Nicolaou, B. Schuller,\nand S. Zafeiriou. 2016. Adieu features?\nEnd-to-end speech emotion recognition\nusing a deep convolutional recurrent net-\nwork. In 2016 IEEE International Con-\nference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5200\u20135204,\nMarch. ISSN: 2379-190X.\nT sai, Y.-H. H., S. Bai, P . P . Liang, J. Z.\nKolter, L.-P . Morency , and R. Salakhut-\ndinov. 2019. Multimodal T ransformer\nfor Unaligned Multimodal Language Se-\nquences. arXiv:1906.00295 [cs], June.\narXiv: 1906.00295.\nT sai, Y.-H. H., M. Q. Ma, M. Y ang,\nR. Salakhutdinov, and L.-P . Morency .\n2020. Interpretable Multimodal Rout-\ning for Human Multimodal Language.\narXiv:2004.14198 [cs], April. arXiv:\n2004.14198.\nT zirakis, P ., J. Zhang, and B. W. Schuller.\n2018. End-to-End Speech Emotion Recog-\nnition Using Deep Neural Networks. In\n2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing\n(ICASSP), pages 5089\u20135093, April. ISSN:\n2379-190X.\nWilliams, J., S. Kleinegesse, R. Comanescu,\nand O. Radu. 2018. Recognizing Emo-\ntions in Video Using Multimodal DNN\nF eature F usion. In Proceedings of Grand\nChal lenge and W orkshop on Human Multi-\nmodal Language (Chal lenge-HML) , pages\n11\u201319, Melbourne, Australia. Association\nfor Computational Linguistics.\nDaniel Mora Melanchthon\n80W u\n, Y., M. Schuster, Z. Chen, Q. V. Le,\nM. Norouzi, W. Macherey , M. Krikun,\nY. Cao, Q. Gao, K. Macherey , J. Klingner,\nA. Shah, M. Johnson, X. Liu, \uffff. Kaiser,\nS. Gouws, Y. Kato, T. Kudo, H. Kazawa,\nK. Stevens, G. Kurian, N. Patil, W. W ang,\nC. Y oung, J. Smith, J. Riesa, A. Rudnick,\nO. Vinyals, G. Corrado, M. Hughes, and\nJ. Dean. 2016. Google\u2019s Neural Machine\nT ranslation System: Bridging the Gap be-\ntween Human and Machine T ranslation.\narXiv:1609.08144 [cs], October. arXiv:\n1609.08144.\nZadeh, A., P . P . Liang, N. Mazumder, S. Po-\nria, E. Cambria, and L.-P . Morency . 2018.\nMemory F usion Network for Multi-view\nSequential Learning. arXiv:1802.00927\n[cs], F ebruary . arXiv: 1802.00927.\nZadeh, A., R. Zellers, E. Pincus, and L.-P .\nMorency . 2016. MOSI: Multimodal Cor-\npus of Sentiment Intensity and Subjec-\ntivity Analysis in Online Opinion Videos.\narXiv:1606.06259 [cs], August. arXiv:\n1606.06259.\nZwillinger, D. and S. Kokoska. 1999. CRC\nStandard Probability and Statistics T ables\nand F ormulae . CRC Press, December.\nUnimodal Feature-level improvement on Multimodal CMU-MOSEI Dataset: Uncorrelated and Convolved Feature Sets\n81Masking and BERT-based Models for\nStereotype Identi\fcation\nModelos Basados en Enmascaramiento y en BERT para la\nIdenti\fcaci\u0013 on de Estereotipos\nJavier S\u0013 anchez-Junquera1, Paolo Rosso1, Manuel Montes-y-G\u0013 omez2, Berta Chulvi1\n1PRHLT Research Center, Universitat Polit\u0012 ecnica de Val\u0012 encia, Val\u0012 encia, Spain;\n2Laboratorio de Tecnolog\u0013 \u0010as del Lenguaje, Instituto Nacional de Astrof\u0013 \u0010sica,\n\u0013Optica y Electr\u0013 onica, Puebla, Mexico\nfjuasanj3@doctor.; prosso@dsic.; berta.chulvi@gupv.es; mmontesg@inaoep.mx\nAbstract: Stereotypes about immigrants are a type of social bias increasingly\npresent in the human interaction in social networks and political speeches. This\nchallenging task is being studied by computational linguistics because of the rise of\nhate messages, o\u000bensive language, and discrimination that many people receive. In\nthis work, we propose to identify stereotypes about immigrants using two di\u000berent\nexplainable approaches: a deep learning model based on Transformers; and a text\nmasking technique that has been recognized by its capabilities to deliver good and\nhuman-understandable results. Finally, we show the suitability of the two models\nfor the task and o\u000ber some examples of their advantages in terms of explainability.\nKeywords: social bias, immigrant stereotypes, BETO, masking technique.\nResumen: Los estereotipos sobre inmigrantes son un tipo de sesgo social cada vez\nm\u0013 as presente en la interacci\u0013 on humana en redes sociales y en los discursos pol\u0013 \u0010ticos.\nEsta desa\fante tarea est\u0013 a siendo estudiada por la ling\u007f u\u0013 \u0010stica computacional debido al\naumento de los mensajes de odio, el lenguaje ofensivo, y la discriminaci\u0013 on que reciben\nmuchas personas. En este trabajo, nos proponemos identi\fcar estereotipos sobre\ninmigrantes utilizando dos enfoques diametralmente opuestos prestando atenci\u0013 on\na la explicabilidad de los mismos: un modelo de aprendizaje profundo basado en\nTransformers; y una t\u0013 ecnica de enmascaramiento de texto que ha sido reconocida\npor su capacidad para ofrecer buenos resultados a la vez que comprensibles para los\nhumanos. Finalmente, mostramos la idoneidad de los dos modelos para la tarea, y\nofrecemos algunos ejemplos de sus ventajas en t\u0013 erminos de explicabilidad.\nPalabras clave: sesgo social, estereotipos hacia inmigrantes, BETO, t\u0013 ecnica de\nenmascaramiento.\n1 Introduction\nNowadays, social media, political speeches,\nnewspapers, among others, have a strong im-\npact on how people perceive reality. Very of-\nten, the information consumers are not aware\nof how biased is what they are exposed to.\nTo mitigate this situation, many computa-\ntional linguistics e\u000borts have been made to\ndetect social bias such as gender and racial\nbiases (Bolukbasi et al., 2016; Garg et al.,2018; Liang et al., 2020; Dev et al., 2020).\nThe immigrant stereotype is another type of\nsocial bias that is present when a message\nabout immigrants disregards the great diver-\nsity of this group of people and highlights a\nsmall set of their characteristics. This pro-\ncess of homogenization of a whole group of\npeople is at the very heart of the stereotype\nconcept (Tajfel, Sheikh, and Gardner, 1964).\nAs (Lipmann, 1922) said in his seminal work\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 83-94\nrecibido 06-05-2021 revisado 08-06-2021 aceptado 10-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-7\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalabout stereotypes, stereotyping, as a cogni-\ntive process, occurs because \\we do not \frst\nsee and then de\fne, we de\fne \frst and then\nsee\". In short, we can say that a stereotype is\nbeing used in language when a whole group\nof people, itself very diverse, is represented\nby appealing to a few characteristics.\nUnfortunately, the use of stereotypes pro-\nmotes undesirable behaviors among people\nfrom di\u000berent nationalities; an example is the\nviolence against Asian Americans that have\ntaken place recently (Tessler, Choi, and Kao,\n2020). Moreover, political analysts have as-\nsociated the success of anti-immigration par-\nties with the even more negative attitudes\nto the immigration phenomenon (Dennison\nand Geddes, 2019). These stereotypes have\nreceived little attention to be automatically\nidenti\fed, despite the harmful consequences\nthat prejudices and attitudes, in many cases\nnegative, may have.\nThere have been some works related to\nthe problem of immigrant stereotypes iden-\nti\fcation (Sanguinetti et al., 2018), but they\nare mainly focused on the expressions of hate\nspeech; or social bias in general that involves\nracism (Nadeem, Bethke, and Reddy, 2020;\nFokkens et al., 2018). However, it is nec-\nessary to have a whole view of immigrant\nstereotypes, taking into account both posi-\ntive and negative beliefs, and also the vari-\nability in which stereotypes are re\rected in\ntexts. This more re\fned analysis of stereo-\ntypes would make it possible to detect them\nnot only in clearly dogmatic or violent mes-\nsages, but also in other more formal and sub-\ntle texts such as news, institutional state-\nments, or political representatives' speeches\nin parliamentary debates.\nSimilar to applications of healthcare, se-\ncurity, and social analysis, in this task is\nnot enough to achieve high results, but it\nis also mandatory that results could be un-\nderstood or interpreted by human experts on\nthe domain of study (e.g., social psycholo-\ngists). Taking into account these two as-\npects, performance and explainability, the\nobjective of this work is to compare two ap-\nproaches diametrically opposite to each other\nin the text classi\fcation state of the art. On\nthe one hand, a transformer-based model,\nwhich has shown outstanding performance,\nbut high complexity and poor explainabil-\nity; and, on the other hand, a masking-based\nmodel, which requires fewer computational-resources and showed a good performance in\nrelated tasks like pro\fling.\nWe aim to \fnd explainable predictions of\nimmigrant stereotypes with BETO by using\nits attention mechanism. In this way, we de-\nrive the explanation by investigating the im-\nportance scores of di\u000berent features used to\noutput the \fnal prediction. With the other\napproach that we use, the masking technique\n(Stamatatos, 2017; S\u0013 anchez-Junquera et al.,\n2020), it is possible to know what are the\nmost important words that the model pre-\nferred to highlight. We compare these ap-\nproaches using a dataset of texts in Span-\nish, which contains annotated fragments of\npolitical speeches from Spanish Congress of\nDeputies.\nThe research questions aim to answer in\nthis work are:\nRQ1: Is the transformer more e\u000bective than\nthe masking technique at identifying\nstereotypes about immigrants?\nRQ2: Is it possible to obtain local explana-\ntions on the predictions of the models, to\nallow human interpretability about the\nimmigrant stereotypes?\nThe rest of the paper is organized as fol-\nlow: Section 2 presents related work concern-\ning the immigration stereotype detection and\nthe models that we propose. Section 3 de-\nscribes the two models, and Section 4 the\ndataset used in the experiments. Sections 5\nand 6 contain the experimental settings and\nthe discussion about the results. Finally, we\nconclude the work in Section 7 where we men-\ntion also future directions.\n2 Related Work\n2.1 Immigrant Stereotype\nDetection\nThere have been attempts to study stereo-\ntypes from a computational point of view,\nsuch as gender, racial, religion, and ethnic\nbias detection do (Garg et al., 2018; Boluk-\nbasi et al., 2016; Liang et al., 2020). Those\nworks prede\fne two opposite categories (e.g.,\nmen vs. women) and use word embeddings\nto detect the words that tend to be more\nassociated with one of the categories than\nwith the other. In (Nadeem, Bethke, and\nReddy, 2020), the authors propose two dif-\nferent level tests for measuring bias. First,\nJavier S\u00e1nchez-Junquera, Paolo Rosso, Manuel Montes-y-G\u00f3mez, Berta Chulvi\n84the intra-sentence test, with a sentence de-\nscribing the target group and a set of three\nattributes that correspond to a stereotype, an\nanti-stereotype, and a neutral option. Sec-\nond, the inter-sentence test, with a sentence\ncontaining the target group; a sentence con-\ntaining a stereotypical attribute of the tar-\nget group; another sentence with an anti-\nstereotypical attribute; and lastly, a neutral\nsentence. These tests are similar to the idea\nof (Dev et al., 2020) that consist in using\nnatural language inference to measure entail-\nment, contradiction, or neutral inferences to\nquantify the bias. To evaluate their proposal,\nin (Nadeem, Bethke, and Reddy, 2020), the\nauthors collected a dataset (StereoSet) for\nmeasuring bias related to gender, profession,\nrace, and religion domains.\nOn the other hand, stereotypes are not al-\nways the (explicit) association of words (seen\nas attributes or characteristics) from two op-\nposite social groups, like women vs. men in\nthe context of gender bias. Such is the case\nof immigrant stereotypes, in which sentences\nlike>Por qu\u0013 e ha muerto una persona joven?\n(Why did a young person die?) do not con-\ntain an attribute of the immigrant group al-\nthough from its context1it is possible to con-\nclude that here immigrants are placed as vic-\ntims of su\u000bering. Also, it is not clear the\nrepresentative word of the social group, since\npersona joven (young person) is neutral to\nimmigrants and non-immigrants.\nOther works have built annotated data\nto foster the development of supervised ap-\nproaches. In (Sanguinetti et al., 2018), was\npresented an Italian corpus focused on hate\nspeech against immigrants, which includes\nannotations about whether a tweet is a\nstereotype or not. This corpus was used\nin the HaSpeeDe shared task at EVALITA\n2020 (Sanguinetti et al., 2020). Most partic-\nipant teams only adapted their hate speech\nmodels to the stereotype identi\fcation task,\nthus, representing (and reducing) stereo-\ntypes to characteristics of hate speech. One\nof the conclusions was that the immigra-\ntion stereotype appeared as a more subtle\nphenomenon, which also needs to be ap-\nproached as non-hurtful text. Additionally,\nin (Nadeem, Bethke, and Reddy, 2020), it\n1Fragment of a political speech from a Popular\nParliamentary Group politician in 2006. The speaker\nis mentioning some of the conditions of immigrants\nin Spain in that period.was proposed a dataset that includes the do-\nmain of racism (additionally to gender, reli-\ngion, and profession). Although this dataset\ndoes not focus on the study of stereotypes\nabout immigrants, its authors reported the\nword \\immigrate\" as one of the most rele-\nvant keywords that characterized the racism\ndomain.\n2.2 On the explainability of AI\nmodels\nSince eXplainable Arti\fcial Intelligence\n(XAI) systems have become an integral part\nof many real-world applications, there is an\nincreasing number of XAI approaches (Islam\net al., 2021) including white and black boxes.\nThe \frst group, which includes decision trees,\nhidden Markov models, logistic regressions,\nand other machine learning algorithms, are\ninherently explainable; whereas, the second\ngroup, which includes deep learning models,\nare less explainable (Danilevsky et al., 2020).\nXAI has been characterized according to\ndi\u000berent aspects, for example, (i) by the\nthe level of the explainability, for each\nsingle prediction (local explanation ) or the\nmodel's prediction process as a whole (global\nexplanation ); (ii) and if the explanation\nrequires post-processing ( post-hoc ) or not\n(self-explaining ).\nXAI has also been characterized in accor-\ndance to the source of the explanations, for\nexample: (i) surrogate models, in which the\nmodel predictions are explained by learning\na second model as a proxy, such is the case of\nLIME (Ribeiro, Singh, and Guestrin, 2016);\n(ii)example-driven, in which the prediction\nof an input instance is explained by identify-\ning other (labeled) instances that are seman-\ntically similar (Croce, Rossini, and Basili,\n2019); (iii) attention layers, which appeal to\nhuman intuition and help to indicate where\nthe neural network model is \\focusing\"; and\n(iv) feature importance , in which the rele-\nvance scores of di\u000berent features are used to\noutput the \fnal prediction (Danilevsky et al.,\n2020).\nTaking into account this characterization,\nwe frame our approach in the self-explaining\nscope, and consider two di\u000berent models to\nobtain local explanations of the predicted\ntexts. In this sense, we use the attention lay-\nerswhich have been commonly applied by lo-\ncal self-explaining models (Mullenbach et al.,\n2018; Bodria et al., 2020). For example, in\nMasking and BERT-based Models for Stereotype Identication\n85(Mathew et al., 2020) the attention weights\nwere used to compare the posts' segments on\nwhich the labeling decision was based, high-\nlighting the tokens that the models found\nthe most relevant. Similarly, in (Clark et\nal., 2019) the authors used datasets for tasks\nlike dependency parsing, to evaluate atten-\ntion heads of BERT, and found relevant lin-\nguistic knowledge in the hidden states and at-\ntention maps, such as direct objects of verbs,\ndeterminers of nouns, and objects of preposi-\ntions. Finally, in (Jarqu\u0013 \u0010n-V\u0013 asquez, Montes-\ny-G\u0013 omez, and Villase~ nor-Pineda, 2020) at-\ntention was used to prove that some swear\nwords are inherently o\u000bensive, whilst others\nare not, since their interpretation depends on\ntheir context.\nThe other self-explaining model that we\nuse to obtain the local explanations, is a\nmasking technique which can be described\nas a white box. In this case, the explain-\nable strategy is based on the feature impor-\ntance idea, by measuring and observing the\nrelevant words used in its masking process.\nThe masking technique used in this work in-\ncorporates an additional way to explain de-\ncisions (Stamatatos, 2017; Granados et al.,\n2011; S\u0013 anchez-Junquera et al., 2020). It al-\nlows highlighting content and style informa-\ntion from texts, by masking a prede\fned and\ntask-oriented set of irrelevant words.\n3 Models\nIn this section, we brie\ry describe the two\nmodels that we use in our experiments.\nBETO: it is based on BERT, but it\nwas pre-trained exclusively on a big Spanish\ndataset (Ca~ nete et al., 2020). The framework\nof BETO consists of two steps: pre-training\nand \fne-tuning, similar to BERT (Devlin et\nal., 2018). For the pre-training, the collected\ndata included Wikipedia and other Spanish\nsources such as United Nations and Govern-\nment journals, TED Talks, Subtitles, News\nStories among others. The model has 12 self-\nattention layers with 16 attention-heads each,\nand uses 1024 as hidden size, with a total of\n110M parameters. The vocabulary contains\n32K tokens.\nFor \fne-tuning, the model is \frst initial-\nized with the pre-trained parameters, and all\nof the parameters are \fne-tuned using la-\nbeled data from the downstream task, which\nin our case is a stereotype-annotated dataset\n(see Section 4). The \frst token of every se-quence is always a special classi\fcation token\n([CLS]), which is used as the aggregate se-\nquence representation for classi\fcation tasks.\nIn our work, we add to the [CLS] representa-\ntion two dense layers and a Softmax function\nto obtain the binary classi\fcation.\nThe masking technique: it consists of\ntransforming the original texts to a distorted\nform where the textual structure is main-\ntained while irrelevant words are masked, i.e.,\nreplaced by a neutral symbol. The irrelevant\nterms are task-dependent and have to be de-\n\fned in advance, following some frequency\ncriteria or the expert's intuition.\nThe masking technique replaces each term\ntof the original text by a sequence of \\*\".\nThe length of the sequence is determined\nby the number of characters that tcontains.\nOne example of this is shown in Figure 1\nconsidering the Spanish stopwords. In Sec-\ntion 5.1, we explain which are the relevant\nwords that we considered better to mask.\nAfter all texts are distorted by the mask-\ning technique, we use a traditional classi\fer\nto be compared with BETO. In our experi-\nments, we use Logistic Regression (LR) clas-\nsi\fer which has been used before to be com-\npared with BERT (Alaparthi and Mishra,\n2021).\n4 Dataset\nWe use the StereoImmigrants dataset2for\nidentifying stereotypes about immigrants\n(S\u0013 anchez-Junquera et al., 2021). In this\nprevious work we collected texts on immi-\ngrant stereotypes from the political speeches\nof the ParlSpeech V2 dataset (Rauh and\nSchwalbach, 2020); from where we also ex-\ntracted the negative examples (labeled as\nNon-stereotypes). These texts are extracted\nfrom the speeches of the Spanish Congress of\nDeputies (Congreso de los Diputados ), and\nare written in Spanish.\nIn the construction of StereoImmigrants\n(S\u0013 anchez-Junquera et al., 2021), we proposed\na new approach to the study of immigrant\nstereotyping elaborating a taxonomy to an-\nnotate the corpus that covers the whole spec-\ntrum of beliefs that make up the immigrant\nstereotype. The novelty of this taxonomy and\nthis annotation process is that the work has\nnot focused on the characteristics attributed\nto the group but on the narrative contexts in\n2https://github.com/jjsjunquera/StereoImmigrants.\nJavier S\u00e1nchez-Junquera, Paolo Rosso, Manuel Montes-y-G\u00f3mez, Berta Chulvi\n86Original text\nla\ninmigraci\u0013 on sigue siendo hoy - lo con\frman los \u0013 ultimos sondeos del CIS -\nel principal problema que preocupa a los ciudadanos del estado\n(Immigration is still today - con\frmed by the latest CIS polls -\nthe main problem that worries the citizens of the state)\nMasking stop\nwords\n** inmigraci\u0013 on sigue siendo hoy ** con\frman *** \u0013 ultimos sondeos *** cis\n** principal problema *** preocupa * *** ciudadanos ** *** ******\nMasking the\nnon-stopwords\nla *********** ***** ****** *** lo ********* los ******* ******* del ***\nel ********* ******** que ******** los ********** del estado\nFigure 1: Example of masking the stopwords, or keeping only the stopwords unmasked.\nwhich the immigrant group is repetitively sit-\nuated in the public discourses of politicians.\nTo do this, the authors applied the frame the-\nory {a social psychology theory{ to the study\nof stereotypes. The frame theory allows us\nto show that politicians in their speeches cre-\nate and recreate di\u000berent frames (Scheufele,\n2006), i.e. di\u000berent scenarios, where they\nplace the group. The result of this rhetori-\ncal activity of framing ends with the creation\nof a stereotype: a diverse group is seen only\nwith the characteristics of the main actor in\na particular scenario.\nIn (S\u0013 anchez-Junquera et al., 2021), we\nidentify di\u000berent frames used to speak about\nimmigrants that could be classi\fed in one of\nthe following categories: (i) present the im-\nmigrants as equals to the majority but the\ntarget of xenophobia (i.e., they must have the\nsame rights and same duties but are discrim-\ninated), (ii) as victims (e.g., they are people\nsu\u000bering from poverty or labor exploitation),\n(iii) as an economic resource (i.e., they are\nworkers that contribute to economic devel-\nopment), (iv) as a threat for the group (i.e.,\nthey are the cause of disorder because they\nare illegal, too many, and introduce unbal-\nances in societies), or (v) as a threat for the\nindividual (i.e., they are competitors for lim-\nited resources or a danger to personal wel-\nfare and safety). In the construction of the\nStereoImmigrants dataset, an expert in prej-\nudice from the social psychology area anno-\ntated manually the sentences at the \fnest\ngranularity of the taxonomy and selected also\nnegatives examples where politicians speak\nabout immigration but do not refer, explic-\nitly or implicitly, to the people that integrates\nthe group \\immigrants\". After this expert\nannotation, \fve non-experts annotators read\nthe label assigned by the expert to each sen-\ntence and decided if they agreed with it or\nconsidered that another label from the tax-Label Length Texts\nStere\notype 45.62\u000624.69 16733635Non-ster e\notype 36.00\u000621.17 1962\nVictims 48.93\u000627.5 7431479Thre\nat 45.84\u000624.42 736\nTable 1: Distribution of texts per label and\nthe average length (with standard deviation)\nof their instances. The texts labeled as Vic-\ntims orThreat are a subset of the texts la-\nbeled as Stereotype.\nonomy was better suited for this sentence.\nThe dataset only contains sentences where\nat least three annotators agreed on the same\ncategory.\nIn (S\u0013 anchez-Junquera et al., 2021), at-\ntending to a second annotation of the atti-\ntudes that each sentence expresses, we pro-\nposed two supra-categories of the stereotypes\nannotated as Victims orThreat, where the\ncategories (i) and (ii) belong to the Victims\nsupra-category, and (iv) and (v) belong to\ntheThreat supra-category. Table 1 shows the\ndistribution per label of the dataset.\nTable 2 shows examples of Non-\nstereotypes and Stereotypes labels. The\nStereotypes examples specify if they were\nlabeled as Victims orThreat. From these ex-\namples, it is possible to see that the dataset\ncontains stereotypes that are not merely the\nassociation of attributes or characteristics\nto the group, but texts which re\rect biased\nrepresentations of the group (i.e., how\nthe immigrants are indirectly perceived or\nassociated with speci\fc situations and social\nissues).\n5 Experimental Settings\nWe applied a 10-fold cross-validation proce-\ndure and reported our results in terms of F-\nmeasure.\nFor BETO, we searched the following hy-\nMasking and BERT-based Models for Stereotype Identication\n87Non-stereot yp\ne\nNonos\nvale que se contabilice todo lo que se dedica a inmigraci\u0013 on porque no estamos hablando de lo mismo.\n(We are not worth accounting for everything that is dedicated to immigration because we are not talking about the same thing.)\nElGobierno\nest\u0013 a desbordado por la inmigraci\u0013 on, por su pol\u0013 \u0010tica improvisada, irresponsable, descoordinada y unilateral.\n(The Government is overwhelmed by immigration, by its improvised, irresponsible, uncoordinated and unilateral policy.)\nStereot yp\ne: Victims\n>Por qu\n\u0013 e ha muerto una persona joven?\n(Why did a young person die?)\nHay una\nsituaci\u0013 on de desamparo en muchas personas a la que necesitamos dar una soluci\u0013 on.\n(There is a helplessness situation in many people to which we need to provide a solution.)\nStereot yp\ne: Threat\nEspa\n~ na hoy est\u0013 a desbordada con la inmigraci\u0013 on ilegal.\n(Spain today is overwhelmed with illegal immigration.)\nEsta alarmante\nsituaci\u0013 on, agravada por la incapacidad del Gobierno socialista, ha producido el colapso, el desbordamiento\nde los servicios humanitarios, judiciales y policiales que han generado una gran alarma social.\n(This alarming situation, aggravated by the incapacity of the socialist government, has produced the collapse, the over\row\nof humanitarian, judicial and police services that have generated great social alarm.)\nTable 2: Examples from each label of the dataset.\nperparameter grid to obtain the results of\nBETO: learning rate2 f0:01;3e-5g; the\nbatch size2 f16; 34g; and the optimizer2\nfadam; rmspropg (in bold we highlighted\nthe optimal hyperparameter values). More-\nover, we applied a dropout value of 0.3 to the\nlast dense layer. We have selected a value\nof 180 for the max length hyperparameter\naccording to the maximum length of all the\ntexts in the dataset. The model was \fne-\ntuned for 10 epochs on the training data for\neach task.\nFor the masking-based approach, we used\nthesklearn implementation of the LR classi-\n\fer. All the parameters were taken by de-\nfault, except for the optimization method:\nwe selected newton-cg. The model used the\nbag of words representation, using the t\fdf\nterm weighting. We tested with unigrams,\nbigrams, and trigrams of words; and with\ncharacters n-grams (n2f 3;4;5;6g) obtaining\nthe better results with character 4-grams.\nWhen we used LR with the original texts, un-\nigrams of words achieved better results than\ncharacter n-grams.\n5.1 Unmasking Stereotypes\nRelated works on social bias detection have\nfound a list of words that tend to be associ-\nated with one of two opposite social groups\n(e.g., female vs. male, Asian vs Hispanic\npeople) (Bolukbasi et al., 2016; Garg et al.,\n2018). In the immigrant stereotypes case, it\nis particularly di\u000ecult to de\fne two opposite\ngroups and consequently to \fnd such biased\nwords. In this paper, we use the dataset de-scribed in Section 4 to \fnd which could be\nthe most relevant terms to be used in the\nmasking process.\nIntuitively, in the immigrant stereotypes'\ncontext, the relevant words could be content-\nrelated, although style-related terms like\nfunction words could play also an interest-\ning role. After preliminary experiments, we\nfound higher results by masking the words\nout of the following lists: (i) the words with\nhigher relative frequency ( RelFreq ), i.e., the\nkwords with a frequency in one class remark-\nably higher than its frequency in the opposite\nclass; and (ii) the kwords with the high-\nest absolute frequency (AbsFreq ) in all the\ncollection, excluding stopwords (i.e., stop-\nwords were masked). In our experiments we\nachieved better results with k= 1000.\nEach list was computed using the corre-\nsponding set of texts depending on the classi-\n\fcation task: Stereotype vs. Non-stereotype,\nor Victims vs. Threat. The information that\nis kept unmasked corresponds to the content-\nrelated words.\n6 Results and Discussion\nWe report the results of the models in Ta-\nble 3. It is possible to see high results of\nLR with the original texts. However, we\nobserve that masking the terms out of the\nlistRelFreq is slightly better than using the\noriginal text. These results suggest that the\nmasking technique improves the quality of\nthe stereotype detection and its dimensions.\nIn comparison with AbsFreq , maintain-\nJavier S\u00e1nchez-Junquera, Paolo Rosso, Manuel Montes-y-G\u00f3mez, Berta Chulvi\n88S/N V/T\nOriginal text 0.82 0.79\nMasking T\nechnique with AbsFreq 0.79 0.75\nMasking T\nechnique with RelFreq 0.84 0.81\nBETO 0.86 0.83\nTable 3: F-measure in both classi\fcation\ntasks: Stereotype vs. Non-stereotype (S/N),\nand Victims vs. Threat (V/T).\ning unmasked the RelFreq words helps to\nignore more words that are less discrimina-\ntive for classi\fcation tasks. This could be ex-\nplained because AbsFreq includes words sim-\nilarly frequent in both classes, which could\nnot help at predicting immigrant stereotypes:\npa\u0013 \u0010ses (countries), gobierno (government),\nse~ nor (mister), partido (party); or at iden-\ntifying the immigrant-stereotype dimension:\nfronteras (frontiers), pol\u0013 \u0010tica (politic), se-\nguridad (security), grupo (group). Table 4\nshows examples of words included in RelFreq\nthat are indeed re\recting some bias accord-\ning to the category. For example, it is\nnot surprising to \fnd words like derechos\n(rights), humanos (human3or humans), po-\nbreza (poverty), muerto (dead), and ham-\nbre(hunger) more associated to immigrants\nseen as victims; and words like irregular (ir-\nregular), ilegal (illegal), regularizaci\u0013 on (reg-\nularization), masiva (massive), and problema\n(problem), more used in speeches where im-\nmigrants are seen as collective or personal\nthreat.\nBETO achieves the highest results in both\nclassi\fcation tasks (RQ1 ). This is not sur-\nprising because the transformer-based mod-\nels are known for their properties at captur-\ning semantic and syntactic information, and\nricher patterns in which the context of the\nwords are taken into account. However, we\ndo not observe a signi\fcant di\u000berence be-\ntween the results of such a resource-hungry\nmodel, and the combination of the mask-\ning technique with the traditional LR classi-\n\fer. Considering the computational capabil-\nities that BETO demands, and the less com-\nplexity of the masking technique, the latter\nshows a better trade-o\u000b between e\u000bectiveness\nand e\u000eciency than the latter.\n6.1 Discriminating Words\nMotivated by the similar results of BETO\nand the masking technique, we wanted to ob-\n3It refers to the adjective: human rights.Stereot yp\neNon-stereot yp\ne Victims Threat\npersonas pol\n\u0013 \u0010tica derec hos inmigraci\u0013 on\ncanarias europ ea personas canarias\nderec hos uni\u0013on humanos gobierno\nproblema materia derec ho irregular\npa\u0013\n\u0010s grupo mujeres ilegales\nirregular pol\n\u0013 \u0010ticas pa\u0013\n\u0010ses irregulares\nsituaci\u0013 on consejo pobreza regularizaci\u0013 on\nregularizaci\u0013 on coop\neraci\u0013 on integraci\u0013\non ilegal\nilegales gobierno mundo espa~\nna\nhumanos moci\u0013\non vida problema\nciudadanos europ eo solidaridad proceso\nirregulares ley asilo masiv a\nefecto parlamen tario condiciones llegado\norigen comisi\u0013 on millones aeropuertos\ncentros c\u0013amara muerto ministro\nilegal desarrollo refugiados control\ndrama consenso social efecto\nacogida subcomisi\u0013\non miseria pateras\nllamada socialista internacional llamada\nmenores com\u0013\nun ciudadanos medidas\nvida temas xenofobia llegada\nma\fas tema hambre inmigran tes\nllegada asuntos viven marruecos\nmasiv a grupos emigran tes cayucos\nextranjeros emigran tes muerte presi\u0013on\nTable 4: Examples of the relevant words\nthat were not masked, considering the list\nRelFreq in each classi\fcation task.\nserve and compare what portions of the texts\nthey could be focusing on. For this purpose,\nwe looked at the last layer of BETO and\ncomputed the average of the attention heads.\nTherefore, for each text, we had an atten-\ntion matrix from which we could compute the\nattention that the transformer gave to each\nword in that texts. Figure 2 shows examples\nof texts where the two models agreed on the\nright label.\nFrom the \fgure, it is possible to see what\nwords were relevant for both approaches. Al-\nthough some of the relevant words are func-\ntion words (e.g., para, muy) and are not too\ninformative at \frst glance for human inter-\npretation, we can observe that some content-\nrelated words can be helpful for expert's\nanalysis. For instance, the text labeled as\nStereotype has as relevant words fen\u0013 omeno\n(phenomenon), inmigraci\u0013 on (immigration),\nproblema (problem), terrorismo (terrorism),\nparo (unemployment), among others. The\ntext labeled as Victims contains desamparo\n(abandonment), personas (people), necesita-\nmos dar una soluci\u0013 on (we need to give a so-\nlution), re\recting how immigrants were seen\nas people more than their illegal status (e.g.,\nsee Tables 4), and the target of problems that\nneed solutions. Moreover, in the example of\nThreat, some of the words and phrases re-\nceiving more importance (such as, problema\nmuy serio, problema muy importante ) re\rect\nhow immigrants were seen as a problem to\nMasking and BERT-based Models for Stereotype Identication\n89Figure 2: Examples of attention visualization and masking transformation over the same texts.\nThese examples were correctly classi\fed by both models. The more intense the color, the greater\nis the weight of attention given by the model.\nthe continent and the country, but not the\ncountry where immigrants come from.\nTable 5 presents some of the words with\nthe highest attention scores in only the true\npositive predictions of each class. Therefore,\nthese words could be among the most dis-\ncriminative for stereotype identi\fcation.\nWe contrasted the list of words with more\nattention on the BETO true positive predic-\ntions, with the RelFreq words used by the\nmasking technique as more discriminative for\neach class. Table 6 shows the percentage of\nRelFreq words (which were not masked) that\nwere present in the top of the ranking as more\ndiscriminative from BETO. In the top 30 of\nthe ranking, we found the vast majority of\nthe not masked words. This suggests that\nthe two approaches have seen similar cues.\nFor now, we have seen that BETO and themasking technique achieved similar results\nand have an intersection in the discrimina-\ntive words they focused on in the texts (which\nin fact answer RQ2), despite one of them is\na resource-hungry model and the other re-\nquires less computational resources. We do\nnot think that BETO should not be used be-\ncause of its complexity: one of the di\u000berences\nwe should highlight is that for the masking\ntechnique the list of words should be pre-\nde\fned with some limitations and algorithm\nbias that this could imply. However, BETO\nlearns by itself to score the words gradu-\nally, instead of giving a binary score like in\nthe masking technique (to mask or keep un-\nmasked). Therefore, we can apply from the\ntransformer a comparison of the importance\nof the di\u000berent words (like it was visualized\nin Figure 2), which was automatically learned\nJavier S\u00e1nchez-Junquera, Paolo Rosso, Manuel Montes-y-G\u00f3mez, Berta Chulvi\n90Stereot yp\ne Non-stereot yp\neVictims Threat\ndrama\nl\nlegada\nilegales\nefecto\nirregulares\nllamada\ncostas\nexpulsiones\ntrabajadores\npateras\nxenofobia\nma\fa\ncondiciones\nlegalidad\ndinero\navalancha\nvienen\npeninsula\nmuertes\nmiles\nhumanitaria\ncoladero\npreocupaconsejo\nasuntos\ntemas\nc\nomparecen\nproducen\nrecibir\nacuerdos\nesfuerzo\ndi\u0013 alogo\npacto\ncongreso\nnecesidad\ncumbre\nproyecto\nzapatero\nenmiendas\nmiembros\ncolabora\nconferencia\ngobiernos\nexterior\nimportantes\nacci\u0013 ondere\ncho\nesclavitud\nmujeres\nasilo\nrefugiados\npobreza\nxenofobia\nmuerto\nsistema\ndevoluciones\nmiseria\ndesgracia\ndif\u0013 \u0010cil\ngrupos\nracismo\nhambre\nrefugio\npersona\nsituaciones\nexplotaci\u0013 on\ndenuncia\nmuerte\ndemocraciamasiva\npater\nas\nllamada\navalancha\naeropuertos\ntrasladar\nllegada\nzapatero\ncaldera\nalarma\nayudas\nilegalmente\nafrontar\njudiciales\ncapacidad\narchipi\u0013 elago\ndelincuencia\ngrave\ncongreso\ntropicales\nfallecido\ncoladero\noleadaTable 5: Words whose attention scores are\nthe highest only on the true positive predic-\ntions of BETO in each class.\nAtten\ntion\nRanking%from\na total of\nunmasked words\nS/N V/T\ntop10 48.96% 35.89%\ntop20 78.92% 65.11%\ntop30 95.84% 82.98%\nTable 6: The percentage of RelFreq words\nthat are in the top of words with the highest\nattention.\nfrom the context of the words in the texts. In\nthe next sections, we con\frm the advantages\nof both models by analyzing the results of an\nideal ensemble and other utilities of the at-\ntention mechanisms.\n6.2 An Ideal Ensemble\nWe have seen the results achieved by the\nproposed models and the intersection set of\nwords they focus on in the texts. Therefore,\none could think that these models are clas-\nsifying correctly the same texts. In this sec-\ntion, we report that the models are misclas-\nsifying di\u000berent instances in general.\nTable 7 shows the misclassi\fed instances\nof LR with the masked texts and BETO in\neach classi\fcation task. The models have\ngood performances, so it is licit to think in\nan ideal ensemble that could wisely combine\ntheir predictions. The resultant ensemble will\nmiss only the texts where both models are\nwrong: 272 texts at distinguishing Stereo-\ntype vs. Non-stereotype, which means thatS/N V/T\nTotal\nof instances 3630 1477\nmisclassi\fed b\ny LR 624 263\nmisclassi\fed b\ny BETO 518 259\nmisclassi\fed b\ny both 272 115\nWell\npredicted by\nan ideal ensemble92.5% 92.2%\nTable 7: Misclassi\fed instances and the per-\nformance of an ideal ensemble for Stereotype\nvs. Non-stereotype and Victims vs. Threat\ntasks.\n92.5% of the 3630 texts will be correctly clas-\nsi\fed. A similar analysis can be done in the\nVictims vs. Threat classi\fcation task, which\nwill result in 92.2% of the 1477 texts that will\nbe potentially correctly classi\fed.\n6.3 Relations with the Highest\nAttention Scores\nAnother advantage of the attention mech-\nanism is the relations between the non-\ndiscriminative words and other words from\neach class. We could \fnd noisy features sim-\nilarly present in the opposite classes. One of\nthe words with the highest attention scores\nin our dataset is inmigraci\u0013 on (immigration);\nsince we found its scores high in the two op-\nposite classes, we did not count it as discrim-\ninative by BETO. However, we think that as\nthe heads have the attention that each word\ngives to the others in the texts, we can ob-\nserve how the \\noisy\" words are used in the\nopposite classes, by looking at the relations\nwith their context. We hypothesize that the\nimmigration-related words are used in di\u000ber-\nent contexts in the opposite classes.\nTable 8 shows an example of the words\nwhose relation with inmigraci\u0013 on are the most\nscored in each class. We omitted the ones in\nthe Non-stereotype class due to they are not\ninformative. Interestingly, the words associ-\nated with inmigraci\u0013 on are also describing dif-\nferently the Stereotype, Victims, and Threat\nclasses. For example, with this strategy we\nobserve that words like criminal, and enfer-\nmedades (diseases) are now in the top of dis-\ncriminating words of the Threat category (in\ncontrast to Table 5). We conclude that the\nattention mechanism should be exploited in\nthe future in this sense. Probably the atten-\ntion scores could be a source of interesting\ncues not only in terms of biased words from\nRelFreq list or the ones shown in Table 5,\nMasking and BERT-based Models for Stereotype Identication\n91Stereot yp\ne Victim Threat\nmuertes\nsaturado\nmiseria\np\nobres\npoliciales\ninternamiento\ndramaticos\ndescontrol\nempresarios\nhumanitario\ncostas\navalancha\ngarant\u0013 \u0010as\ndelincuencia\ntr\u0013 a\fco\ndevueltos\nexplotaci\u0013 on\nllamada\nalarman\nilegales\npateras\nexpulsi\u0013 ondiscriminaci\u0013 on\ncolectiv\nos\nmujeres\nconsenso\ndentro\nrefugiados\nfamilias\neducativo\nplanteamos\nmiseria\nrefugiados\npobreza\nreto\nmujeres\nvoto\nvoluntad\nespeci\fcas\npobreza\niniciativa\nenmienda\npod\u0013 \u0010an\nsabenllega\nnuev\no\ndelincuencia\naeropuertos\nprocedente\nzapatero\nsaturado\npoliciales\nretenci\u0013 on\nmadrid\nenfermedades\ncongreso\nevoluci\u0013 on\nentran\nfrancia\naeropuertos\ntropicales\ncriminal\naeropuerto\nintentos\nllegaron\ncoladeroTable 8: Words with the highest attention\nscores in relation to inmigraci\u0013 on (immigra-\ntion).\nbut also concerning the forms in which neu-\ntral terms are contextualized.\n7 Conclusion and Future Work\nThis work is a contribution to the im-\nmigrant stereotype identi\fcation problem.\nThe particularities of the immigration phe-\nnomenon make this bias detection task dif-\nfers from other kinds of bias that have re-\nceived much more attention (e.g., gender\nbias). We addressed two classi\fcation tasks,\nthe Stereotype vs. Non-stereotype detec-\ntion, and Victims vs. Threat dimensions\nidenti\fcation using an annotated dataset in\nSpanish. We proposed two di\u000berent mod-\nels: BETO, a resource-hungry model which\ndemands strong computational capabilities;\nand a masking technique, a less complex ap-\nproach that transforms the texts to be used\nby a traditional classi\fer. We demonstrate\nthat both approaches are suitable for immi-\ngrant stereotype identi\fcation; and interest-\ningly, the masking technique achieves almost\nthe same results of BETO, despite its sim-\nplicity (RQ1).\nWe developed a comparison between the\nattention mechanism of BETO, and the list\nof relevant terms that the masking technique\nuses. These two di\u000berent approaches focused\non similar portions of the texts. Speci\fcally,\nthe majority of the relevant words main-\ntained unmasked are at the top of the words\nthat BETO gave the highest attention. Fur-thermore, with these models it is possible to\nhighlight some stereotype cues that could be\nconsidered as local explanations for further\nstudies about immigrant stereotypes (RQ2).\nOn the basis of the reported results, we\nconclude that both models are e\u000bective at\nidentifying the immigrant stereotypes, and\ncould be combined to build an ideal ensem-\nble that overcomes the results of each one.\nWe also point out that BETO can help to in-\nvestigate with more detail the bias towards\nimmigrants with the attention mechanisms.\nFor these reasons, we think we cannot rule\nout the use of either model.\nTo our knowledge, this is the \frst work\non immigrant stereotypes identi\fcation that\ncompares deep learning with traditional ma-\nchine learning approaches paying special at-\ntention to the explicability of the models in\nthis task. However, more work is necessary\nto explore more deeply the advantages of the\nattention mechanisms in this sense. In future\nwork, we plan to combine the two approaches\nto increase the performance; and to use dis-\ncriminative words to \fnd debiasing strategies\nto mitigate the immigrant stereotypes in so-\ncial media and political speeches.\nAgradecimientos\nThe work of the authors from the Univer-\nsitat Polit\u0012 ecnica of Val\u0012 encia was funded by\nthe Spanish Ministry of Science and Inno-\nvation under the research project MISMIS-\nFAKEnHATE on MISinformation and MIS-\ncommunication in social media: FAKE\nnews and HATE speech (PGC2018-096212-\nB-C31). Experiments were carried out on\nthe GPU cluster at PRHLT thanks to the\nPROMETEO/2019/121 (DeepPattern) re-\nsearch project funded by the Generalitat Va-\nlenciana.\nReferences\nAlaparthi, S. and M. Mishra. 2021. Bert:\na sentiment analysis odyssey. Journal of\nMarketing Analytics, pages 1{9.\nBodria, F., A. Panisson, A. Perotti, and\nS. Piaggesi. 2020. Explainability methods\nfor natural language processing: Applica-\ntions to sentiment analysis. In SEBD.\nBolukbasi, T., K.-W. Chang, J. Y. Zou,\nV. Saligrama, and A. T. Kalai. 2016. Man\nis to computer programmer as woman is to\nhomemaker? debiasing word embeddings.\nJavier S\u00e1nchez-Junquera, Paolo Rosso, Manuel Montes-y-G\u00f3mez, Berta Chulvi\n92Advances in neural information processing\nsystems, 29:4349{4357.\nCa~ nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u0013 erez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. In PML4DC at ICLR 2020 .\nClark, K., U. Khandelwal, O. Levy, and C. D.\nManning. 2019. What does BERT look\nat? an analysis of BERT's attention. In\nProceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP , pages 276{286,\nFlorence, Italy, August. Association for\nComputational Linguistics.\nCroce, D., D. Rossini, and R. Basili. 2019.\nAuditing deep learning processes through\nkernel-based explanatory models. In Pro-\nceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Pro-\ncessing and the 9th International Joint\nConference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 4037{4046,\nHong Kong, China, November. Associa-\ntion for Computational Linguistics.\nDanilevsky, M., K. Qian, R. Aharonov,\nY. Katsis, B. Kawas, and P. Sen. 2020.\nA survey of the state of explainable ai\nfor natural language processing. arXiv\npreprint arXiv:2010.00711.\nDennison, J. and A. Geddes. 2019. A ris-\ning tide? the salience of immigration and\nthe rise of anti-immigration political par-\nties in western europe. The political quar-\nterly, 90(1):107{116.\nDev, S., T. Li, J. M. Phillips, and V. Sriku-\nmar. 2020. On measuring and mitigating\nbiased inferences of word embeddings. In\nAAAI, pages 7659{7666.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. arXiv preprint\narXiv:1810.04805.\nFokkens, A., N. Ruigrok, C. Beukeboom,\nG. Sarah, and W. Van Atteveldt. 2018.\nStudying muslim stereotyping through mi-\ncroportrait extraction. In Proceedings\nof the Eleventh International Conference\non Language Resources and Evaluation\n(LREC 2018) .Garg, N., L. Schiebinger, D. Jurafsky, and\nJ. Zou. 2018. Word embeddings quan-\ntify 100 years of gender and ethnic\nstereotypes. Proceedings of the National\nAcademy of Sciences, 115(16):E3635{\nE3644.\nGranados, A., M. Cebri\u0013 an, D. Camacho, and\nF. De Borja Rodr\u0013 \u0010guez. 2011. Reduc-\ning the loss of information through an-\nnealing text distortion. IEEE Transac-\ntions on Knowledge and Data Engineer-\ning, 23(7):1090{1102. cited By 19.\nIslam, S. R., W. Eberle, S. K. Ghafoor, and\nM. Ahmed. 2021. Explainable arti\fcial\nintelligence approaches: A survey. arXiv\npreprint arXiv:2101.09429.\nJarqu\u0013 \u0010n-V\u0013 asquez, H. J., M. Montes-y-G\u0013 omez,\nand L. Villase~ nor-Pineda. 2020. Not all\nswear words are used equal: Attention\nover word n-grams for abusive language\nidenti\fcation. In K. M. Figueroa Mora,\nJ. Anzurez Mar\u0013 \u0010n, J. Cerda, J. A.\nCarrasco-Ochoa, J. F. Mart\u0013 \u0010nez-Trinidad,\nand J. A. Olvera-L\u0013 opez, editors, Pat-\ntern Recognition , pages 282{292, Cham.\nSpringer International Publishing.\nLiang, P. P., I. M. Li, E. Zheng,\nY. C. Lim, R. Salakhutdinov, and L.-\nP. Morency. 2020. Towards debiasing\nsentence representations. arXiv preprint\narXiv:2007.08100.\nLipmann, W. 1922. Public Opinion. New\nYork:Harcourt Brace.\nMathew, B., P. Saha, S. Muhie Yimam,\nC. Biemann, P. Goyal, and A. Mukherjee.\n2020. HateXplain: A Benchmark Dataset\nfor Explainable Hate Speech Detection.\narXiv e-prints , page arXiv:2012.10289,\nDecember.\nMullenbach, J., S. Wiegre\u000be, J. Duke, J. Sun,\nand J. Eisenstein. 2018. Explainable pre-\ndiction of medical codes from clinical text.\nInProceedings of the 2018 Conference of\nthe North American Chapter of the As-\nsociation for Computational Linguistics:\nHuman Language Technologies, Volume\n1 (Long Papers) , pages 1101{1111, New\nOrleans, Louisiana, June. Association for\nComputational Linguistics.\nNadeem, M., A. Bethke, and S. Reddy. 2020.\nStereoset: Measuring stereotypical bias\nMasking and BERT-based Models for Stereotype Identication\n93in pretrained language models. arXiv\npreprint arXiv:2004.09456.\nRauh, C. and J. Schwalbach. 2020. The parl-\nspeech v2 data set: Full-text corpora of\n6.3 million parliamentary speeches in the\nkey legislative chambers of nine represen-\ntative democracies. Harvard Dataverse.\nRibeiro, M. T., S. Singh, and C. Guestrin.\n2016. \"why should i trust you?\": Explain-\ning the predictions of any classi\fer. In\nProceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge\nDiscovery and Data Mining, KDD '16,\npage 1135{1144, New York, NY, USA. As-\nsociation for Computing Machinery.\nSanguinetti, M., G. Comandini, E. Di Nuovo,\nS. Frenda, M. A. Stranisci, C. Bosco,\nC. Tommaso, V. Patti, R. Irene, et al.\n2020. Haspeede 2@ evalita2020: Overview\nof the evalita 2020 hate speech detection\ntask. In EVALITA 2020 Seventh Evalua-\ntion Campaign of Natural Language Pro-\ncessing and Speech Tools for Italian , pages\n1{9. CEUR.\nSanguinetti, M., F. Poletto, C. Bosco,\nV. Patti, and M. Stranisci. 2018. An\nitalian twitter corpus of hate speech\nagainst immigrants. In Proceedings of\nthe Eleventh International Conference\non Language Resources and Evaluation\n(LREC 2018) .\nScheufele, D. A. 2006. Framing as a Theory\nof Media E\u000bects. Journal of Communica-\ntion, 49(1):103{122, 02.\nStamatatos, E. 2017. Authorship attribu-\ntion using text distortion. 15th Confer-\nence of the European Chapter of the As-\nsociation for Computational Linguistics,\nEACL 2017 - Proceedings of Conference,\n1:1138{1149. cited By 31.\nS\u0013 anchez-Junquera, J., B. Chulvi, P. Rosso,\nand S. P. Ponzetto. 2021. How do you\nspeak about immigrants? taxonomy and\nstereoimmigrants dataset for identifying\nstereotypes about immigrants. Applied\nSciences, 11(8).\nS\u0013 anchez-Junquera, J., L. Villase~ nor-Pineda,\nM. Montes-y-G\u0013 omez, P. Rosso, and\nE. Stamatatos. 2020. Masking domain-\nspeci\fc information for cross-domain de-\nception detection. Pattern Recognition\nLetters, 135:122{130.Tajfel, H., A. A. Sheikh, and R. C. Gardner.\n1964. Content of stereotypes and the in-\nference of similarity between members of\nstereotyped groups. Acta Psychologica,,\n22(3):191{201.\nTessler, H., M. Choi, and G. Kao. 2020.\nThe anxiety of being asian american: Hate\ncrimes and negative biases during the\ncovid-19 pandemic. American Journal of\nCriminal Justice, 45(4):636{646.\nJavier S\u00e1nchez-Junquera, Paolo Rosso, Manuel Montes-y-G\u00f3mez, Berta Chulvi\n94El sentimient o de las letras de las canciones y su relaci\u00f3n con las \ncaracter\u00edsticas musicales  \nThe sentiment  of the lyrics of the songs and their relationship w ith the musical \ncharacteristics  \nMarco Palomeque , Juan de Lucio ,  \nUniversidad de Alcal\u00e1, Madrid, Espa\u00f1a  \nmarco.palomeque@uah.es , juan.de lucio@uah.es  \nResumen:  El trabajo analiza el  sentimiento de las letras de las canciones m\u00e1s exitosas \nsemanalmente durante el periodo 1958 -2020 con el prop\u00f3sito de  captar el sentir social a lo \nlargo del tiempo . Observamos que el conjunto de informaci\u00f3n recogido por las letras difiere \nde aqu\u00e9l que se refiere a la m\u00fasica, lo que nos indica que las letras aportan informaci\u00f3n \ncomplementaria a la extra\u00edda a partir de las caracter\u00edsticas musicales.  \nPalabras cla ve: Letra, canci \u00f3n, m\u00fasica , sentimiento.  \nAbstract: The work analyzes the sentiment of the lyrics of the most successful songs on a \nweekly basis during the period 1958 -2020 to capture the social sentiment over time. We \nobserve that the set of information co llected by the lyrics differs from that which refers to \nmusic, which indicates that the lyrics provide information that is complementary to that \nextracted from the musical characteristics.  \nKeywords:  Lyric, song, music , sentiment.  \n1 Introducci\u00f3n  \nLas letras de las canciones reflejan el sentir de \nsus oyentes , sus preocupaciones y sus intereses.  \nPor lo tanto, a nivel agregado, las canciones m\u00e1s \nconsumidas reflejan el sentir mayoritario del \nconjunto de la sociedad. El Billboard Hot 100 es \nel ranking semanal de las 100 canciones m\u00e1s \nconsumidas en Estados Unidos  tanto en ventas \nde discos , como escuchas en radio  y, m\u00e1s \nrecientemente,  en fuentes digitales como \nYoutube o Spotify. El Billboard Hot 100 \ncomenz\u00f3 a elaborarse en 1958. Es el registro m\u00e1s \nantiguo y constituye una  referencia para otras \nlistas de \u00e9xitos. Billboard Hot 100 configura as\u00ed \nun registro del sentir social a lo largo de l tiempo.  \nLas personas  usan la m\u00fasica para aliviar el \nestr\u00e9s y la ansiedad y para mejorar su bienestar \nemocional y mental. Park  et al. ( 2019 ) y Heggli  \net al. ( 2021 ) muestran que se producen \nfluctuaciones en las preferencias musicales a lo \nlargo  del d\u00eda y seg\u00fan el d\u00eda de la semana. La \nm\u00fasica es particularmente efectiva para apoyar \nel bienestar emocional y regular las emociones \ndurante las fluctuaciones relacionadas con eventos con consecuencias emocionales durante \nciertos per\u00edodos de la vida (Saarikallio , 2011), \nsuponiendo un elemento de autorregulaci\u00f3n . \nHanser  et al. ( 2016 ) muestra que la m\u00fasica es la \nfuente de consuelo m\u00e1s importante en \ncomparaci\u00f3n con otros comportamientos \nrelajantes ofrec iendo  alivio  en situaciones de \np\u00e9rdida y tristeza. La m\u00fasica , en s\u00ed  misma , y la \nletra, en particular, son los aspectos m\u00e1s \nimportantes de una canci\u00f3n.   \nLas letras de las canciones pueden  generar \ndatos que completen y valid en resultados a partir \nde m\u00e9todos puramente ac\u00fasticos  (Mahedero et \nal., 2005). Mihalcea y Strapparava (2012) \nproponen una clasificaci\u00f3n de las emociones de \n100 canciones anotadas, utilizando la m\u00fasica y \nlas letras de las canciones. Pyrovolakis  et al. \n(2020) muestran que, para l a detecci\u00f3n del \nestado de \u00e1nimo en el marco de la m\u00fasica, tanto \nla letra como el audio, contienen informaci\u00f3n \n\u00fatil. \nEste trabajo pretende medir el sentimiento en \nlas letras y compararlo  con indicadores de las \ncaracter\u00edsticas musicales.  El objetivo principal \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 95-102\nrecibido 13-05-2021 revisado 29-05-2021 aceptado 04-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-8\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalde este art\u00edculo consiste en determinar si el sentir \nde la canci\u00f3n reflejado en las letras contiene \ninformaci\u00f3n propia o se acomoda  perfectamente \na las caracter\u00edsticas de  la m\u00fasica que le \nacompa\u00f1a.   \n \nEl resto del trabajo se organiza de la siguiente \nmanera. La pr\u00f3xima secci\u00f3n introduce las \nt\u00e9cnicas utilizadas . La secci\u00f3n 3 presenta la base \nde datos construida, mientras que en la secci\u00f3n 4 \nse ofrece evidencia de que el contenido \ninformativo sobre el sentimiento de las \ncanciones de la le tra y la m\u00fasica no es \nexactamente el mismo,  aunque est\u00e1n \nrelacionados . La secci\u00f3n 5 concluye con algunas \nreflexiones adicionales.  \n \n2 T\u00e9cnicas  utilizadas  \nPara realizar el an\u00e1lisis ser\u00e1n necesarias varias \nt\u00e9cnicas. La primera, el llamado \u201cweb \nscrapping\u201d, con e l cual extraeremos las letras de \nlas canciones del Billboard Hot 100. La segunda \ntarea ser\u00e1 unificar el idioma de las canciones por \nmedio de la librer\u00eda para Python de Google \nTranslate. La tercera ser\u00e1 el an\u00e1lisis de \nsentimiento  de cada texto, una t\u00e9cnica \nproveniente del campo del Procesamiento del \nLenguaje Natural (PLN). Este an\u00e1lisis se har\u00e1 a \ntrav\u00e9s de la herramienta conocida como \nVADER, la cual analiza cada texto en t\u00e9rminos \nde positividad, negatividad o neutralidad. \nTambi\u00e9n se ha descargado un indicado r de \n\u201cvalencia\u201d, a trav\u00e9s de la API de Spotify. La \nvalencia mide c\u00f3mo de feliz o triste es una \ncanci\u00f3n de acuerdo con sus caracter\u00edsticas \nmusicales (recabadas tambi\u00e9n por Spotify), sin \ntener en cuenta la letra de la canci\u00f3n . Adem\u00e1s, se \nhan utilizado otras herramientas que permiten \nanalizar la positividad en textos para contrastar \nque la clasificaci\u00f3n de VADER es correcta.  \n \n2.1 Web  scrapping  \nPara realizar el an\u00e1lisis de sentimiento  de las \nletras de las canciones m\u00e1s consumidas de cada \nsema na lo primero que necesitaremos ser\u00e1 \nconstruir nuestra base de datos. Como ya hemos \nexplicado en la introducci\u00f3n, el Billboard Hot \n100 es un ranking semanal que incluye las 100 \ncanciones m\u00e1s consumidas de cada semana, \ndesde agosto de 1958 hasta hoy en d\u00eda,  en \nEstados Unidos (existen rankings de otros \npa\u00edses, pero su periodo temporal es mucho m\u00e1s \ncorto). La medici\u00f3n del consumo de una canci\u00f3n ha variado a lo largo del tiempo, ya que si bien \ncuando comenz\u00f3 el registro la venta de discos y \nla emisi\u00f3n en la rad io eran las dos formas m\u00e1s \nhabituales de consumir canciones, actualmente \nesto ha cambiado, siendo Spotify y YouTube, \nentre otras plataformas, dos de los medios m\u00e1s \ncomunes para escuchar una canci\u00f3n. De esta \nforma, para adaptarse a los cambios en el \nconsumo , Billboard ha ido cambiando su forma \nde medir cada unidad de consumo, dando una \nponderaci\u00f3n distinta a cada forma de consumirla, \ndado que los beneficios  econ\u00f3micos  que genera \nson diferentes. As\u00ed mismo, cabe destacar que una \ncanci\u00f3n puede aparecer en difer entes semanas en \nel ranking, siempre y cuando siga siendo una de \nlas 100 canciones m\u00e1s consumidas. No obstante, \nexisten ciertas reglas para que una canci\u00f3n no se \nrepita un n\u00famero excesivo de veces, \nendureciendo sus condiciones para llegar al top \n(normalmen te a partir de la semana 20). A partir \nde este ranking, obtenemos el nombre de la \ncanci\u00f3n y el de su int\u00e9rprete para todas las \ncanciones que han aparecido en el mismo. Para \nbuscar la letra de las canciones necesitaremos de \nambos, dado que existen muchas ca nciones con \nel mismo nombre, aunque sean distintas. Con \nesta informaci\u00f3n, dise\u00f1amos nuestro c\u00f3digo de \nweb scrapping, con el cual buscamos la letra en \ndistintas p\u00e1ginas web (Google,  AZ Lyrics y \nSongs Lyrics).  Cada web requiere un c\u00f3digo \ndistinto que se adap te a la forma de escribir el \nenlace web  en el que se almacena la letra de cada \ncanci\u00f3n, as\u00ed como para identificar dentro del \nc\u00f3digo html la parte de texto que corresponde a \nla letra para poder extraerla , si bien es cierto que \nen el caso de Google el enlace  admite peque\u00f1as \nvariaciones y sigue extrayendo  el resultado  \nadecuado . De esta forma obtenemos la letra de la \ngran mayor\u00eda de las canciones que han llegado \nhasta el Billboard Hot 100, como veremos en la \nsecci\u00f3n 3. \n \n2.2 Traducci\u00f3n de letras  \nAunque, trat\u00e1ndose de las canciones m\u00e1s \nconsumidas en Estados Unidos, la gran mayor\u00eda \nde canciones est\u00e1n en ingl\u00e9s, lo cierto es que no \ntodas lo est\u00e1n. Para realizar el an\u00e1lisis de \nsentimiento de las letras  armonizamos todas \nellas al ingl\u00e9s. Para esto, uti lizamos la librer\u00eda de \nGoogle Translate para Python. Esta librer\u00eda nos \npermite : primero, identificar el idioma de cada \ntexto, con lo que encontramos que 143 \ncanciones, un 0. 5% del total, est\u00e1n en idiomas \ndiferentes al ingl\u00e9s. En concreto, contamos con \nMarco Palomeque, Juan de Lucio\n96 \n  119 canciones en castellano, 20 en coreano y 1 \nen portugu\u00e9s, italiano, franc\u00e9s y baeggu. Para la \ntraducci\u00f3n de las canciones  al ingl\u00e9s , utilizamos \ntambi\u00e9n Google Translate. Aunque las \ncaracter\u00edsticas y connotaciones de los idiomas no \nson iguales y la traducci\u00f3n autom\u00e1tica  no es \nperfecta , dado que el volumen de letras en otros \nidiomas es menor  y un porcentaje elevado de \noyentes estadounidense pueden entender el \nespa\u00f1ol , entendemos que esta falta de precisi\u00f3n \nno es preocupa nte en exceso . \n \n2.3 2.3 An\u00e1lisis de sentimiento  V ADER  \nEl an\u00e1lisis de sentimiento lo llevamos a cabo con \nVADER  (Hutto  y Gilbert , 2015 ). VADER utiliza \nuna combinaci\u00f3n de m\u00e9todos cualitativos y \ncuantitativos para construir una lista de \ncaracter\u00edsticas l\u00e9xicas junto con sus medidas de \nintensida d de sentimiento asociadas. Luego, \ncombina estas caracter\u00edsticas con algunas reglas \nque incorporan convenciones gramaticales y \nsint\u00e1cticas para expresar la intensidad del \nsentimiento. Esta herramienta se ha utilizado \npara diferentes prop\u00f3sitos, como analiz ar los \nsentimientos expresados en Twitter  (Elbagir  y \nYang , 2020 ), o qu\u00e9 tan positivas o negativas son \nlas evaluaciones de los estudiantes sobre la \ndocencia  (Newman , 2018 ).  \n \nEn este trabajo  estudiaremos qu\u00e9 tan \npositivas o negativas son las letras de las \ncanciones que han llegado al Billboard Hot 100. \nVADER proporciona tres coeficientes , con \nvalores comprendidos  entre 0 y 1 : uno de \npositividad, otro de negatividad y otro de \nneutralidad; sumando  1 entre los tres . Los \nvalores cercanos a 1 indican que el texto  est\u00e1 \nprincipalmente influenciado por el sentimiento \nque mide el coeficiente, y los valores cercanos a \n0 indican lo contrario. Ejecutamos el an\u00e1lisis de \nsentimiento para todas las canciones de nuestro \nconjunto de datos . Los resultados se pueden \nobservar en  la secci\u00f3n 3.  \n \n2.4 2.4. Caracter\u00edsticas musicales  \nPara analizar si la emotividad que transmiten las \ncanciones a nivel musical coincide con el \nsentimiento que transmite su letra  necesitamos \ndescargar esta informaci\u00f3n de Spotify . Para esto, \nutilizaremos la valencia de cada canci\u00f3n, se trata \nde un coeficiente entre 0 y 1 que mide c\u00f3mo de \nalegre es una canci\u00f3n, teniendo en cuenta \ncaracter\u00edsticas musicales como pueden ser el \nmodo, el  ritmo o el tempo. De esta forma, si una canci\u00f3n tiene una valencia de 0.5 se considerar\u00e1 \nuna canci\u00f3n neutra, mientras que si tiene un \nvalor mayor ser\u00e1 considerada alegre y si es \nmenor triste. Tambi\u00e9n consideramos las \nsiguientes caracter\u00edsticas de las comp osiciones \nmusicales: clave, duraci\u00f3n,  energ\u00eda y tempo de \nlas canciones.  \n \n2.5 2.5. Robustez  \nPara contrastar los resultados obtenidos por \nVADER, hemos utilizado tres m\u00e9todos \nalternativos para medir la positividad en textos.  \nEn primer lugar , se utiliza el an\u00e1li sis de \nsentimiento de Textblob  (Loria et al , 2014), que \nfunciona de un modo similar a VADER. \nTextblob entrega un \u00fanico coeficiente, entre -1 y \n1, siendo un texto negativo cuanto m\u00e1s cercano \na -1 y positivo cuanto m\u00e1s cercano a 1. El \nsegundo m\u00e9todo ha consi stido en calcular el \nporcentaje de palabras positivas  contenidas en \ncada letra, para lo cual se ha utilizado una lista \nde 265 palabras positivas en ingl\u00e9s.  El tercer \nm\u00e9todo ha sido la herramienta de an\u00e1lisis de \nsentimiento de Pytorch, la cual utiliza \ntrans formadores pre -entrenados  (Cheng ,  2020). \nEste m\u00e9todo no admite textos con m\u00e1s de 512 \ntokens , por lo que hemos excluido de este \nm\u00e9todo las canciones con m\u00e1s tokens . \n \nLas correlaciones entre los indicadores \nextra\u00eddos de las distintas t\u00e9cnicas tienen los \nsignos esperados  y validan el uso de VADER, \nver figura 1.  \n \n \nFigura 1: Coeficientes de correlaci\u00f3n de los \nindicadores de sentimiento extra\u00eddos por \ndistintas t\u00e9cnicas.  \n3 Base de datos  \nUna canci\u00f3n estar\u00e1 en la base de datos si se \nencuentra entre las 100 canciones m\u00e1s \nconsumidas durante una determinada semana de \nEl sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales\n97referencia. Disponemos de informaci\u00f3n desde la \nprimera semana publicada, el 4 de agosto de \n1958, hasta la \u00faltima semana de 20 20. Durante \nlas 32 58 semanas  que transcurrieron en dicho \nperiodo,  29663  canciones diferentes alcanzaron \nun puesto en la lista de canciones m\u00e1s \ndemandadas. Una misma canci\u00f3n puede \naparecer en el top varias semanas distintas , si \nbien Billboard  endurece las condiciones para \npermanecer en el ranking cuando se alcanza la \nsemana 20 semana  de presencia en el ranking \n(estas condiciones no son siempre las mismas, \nactualmente solo se mantienen si est\u00e1n por \nencima del puesto 50) . Por esta raz\u00f3n, \nobserva mos un escal\u00f3n en la semana 20  en el \nhistograma de la Figura 2 (n\u00f3tese que el gr\u00e1fico \nmuestra el n\u00famero m\u00e1ximo de semanas que \nestuvo cada canci\u00f3n, por lo que muchas \ncanciones que podr\u00edan haber estado 21 o m\u00e1s \nsemanas se acumulan  en la semana 20 ). En \npromed io, una canci\u00f3n permanece en la lista \ndurante 9 semanas.  \n \n \nFigura 2: N\u00famero de canciones en funci\u00f3n de las \nsemanas que se mantiene n en el ranking.  \n \nA trav\u00e9s del web scrapping, hemos podido  \nrecopilar la letra de 2 6250 canciones , un 88.5% \ndel total de canc iones que han aparecido en la \nlista. El porcentaje medio de canciones que se \ndispone en cada semana es del 94%. Esto supone \nque las canciones con mayor permanencia tienen \nuna probabilidad mayor de estar recogidas en la \nbase de datos de letras. El porcentaj e de \ncanciones para las cuales hemos obtenido letra \nen relaci\u00f3n con el total de letras posibles \naumenta cada a\u00f1o, tal y como se puede observar \nen la Figura 3 en la que se presenta el porcentaje \nde canciones que contiene de la base de datos \ntanto para el top 50 como para el top 100 durante \nel periodo de publicaci\u00f3n de la clasificaci\u00f3n de \ncanciones . Esto se debe a que algunas canciones \nantiguas son m\u00e1s dif\u00edciles  de encontrarse \nactualmente  y no se encuentran en ninguna de las \np\u00e1ginas web que hemos utilizado. Otra raz\u00f3n que explica la no identificaci\u00f3n de la letra se deriva \nde la formulaci\u00f3n espec\u00edfica del t\u00edtulo de la \ncanci\u00f3n o de los artistas responsables de la \nmisma . En el periodo m\u00e1s reciente se observa \nuna mayor frecuencia de canciones con \ncolaboraciones de diversos artistas, lo cual \ndificulta la obtenci\u00f3n de la letra  por medios \nautom\u00e1ticos . \n \nEl \u00e9xito de una canci\u00f3n tambi\u00e9n determina la \nfacilidad para encontrar  su letra en la web. Si nos \nce\u00f1imos al top 50 de cada a\u00f1o, nos encontramos \ncon que el porcentaje obtenido anual nunca baja \ndel 87.5%, mientras que en el caso del top 100 \ncompleto hay alg\u00fan a\u00f1o en el que solo \nobtenemos un 80% del total , ver Figura 3.  \n \n \nFigura 3: Porcentaje de canciones obtenidas por \na\u00f1o. \n \nPara observar de forma m\u00e1s clara el efecto de \nla posici\u00f3n del ranking en la trascendencia a \nlargo plazo de la canci\u00f3n (considerando que una \ncanci\u00f3n ha sido m\u00e1s transcendente si es f\u00e1cil \nencontrar su letra  en la web), en la Figura 4 \nvemos c\u00f3mo el porcentaje de canciones para las \ncuales hemos encontrado letra va disminuyendo \nde forma clara conforme baja la posici\u00f3n de la \ncanci\u00f3n en el ranking, desde un m\u00e1ximo cercano \nal 99% para aquellas canciones que han es tado \nen los primeros puestos .  \n \nFigura 4: Porcentaje de canciones con letra \nobtenida por posici\u00f3n.  \n \nEn cuanto a la medici\u00f3n de consumo de letras \npositivas y negativas, utilizaremos dos m\u00e9todos. \nMarco Palomeque, Juan de Lucio\n98 \n  El primero se trata simplemente del promedio \nsemanal de los c oeficientes positivo y negativo  \nobtenidos mediante VADER para las canciones \nque se encuentran en el Billboard Hot 100 . \nPodemos observar la distribuci\u00f3n de estos \npromedios en la Figura 5, en la que se representa \ntanto el valor medio por semana y una media \nm\u00f3vil de anual (52 semanas) . Nos encontramos \ncon que la mayor parte de las letras de las \ncanciones tienen un mensaje principalmente \nneutral, dado que  las suma de  los coeficientes \npositivo y negativo suele estar por debajo de los \n0.3 juntos, por lo que el coeficiente neutro de las \nletras gira entorno al 0.7.  \n \n \nFigura 5: Evoluci\u00f3n del sentimiento positivo y \nnegativo por semana  y media m\u00f3vil de 52 \nsemanas . \n \nEn la Figura 5 se observa que el consumo de \nletras positivas est\u00e1 disminuyendo y el consumo \nde letras negativas est\u00e1 creciendo , aunque no es \nuna tendencia exenta de fluctuaciones.   \n \nOtro descriptiv o relevante es el an\u00e1lisis de la \npositividad  / negatividad  de las letras en funci\u00f3n \nde su  posici\u00f3n. En la figura 6 se observa que las \ncanciones positivas alcanzan posiciones m\u00e1s \nelevadas en los rankings . \n \n \n \nFigura 6: Evoluci\u00f3n del sentimiento positivo y \nposici\u00f3n en el ranking.  Proponemos un segundo m\u00e9todo para medir \nel consumo de letras positivas y negativas . \nConsideraremos que una canci\u00f3n es positiva si el \ncoeficiente positivo es superior al doble del \nnegativo, mientras que clasificaremos negativas \na aquellas en las que ocurra lo contrario. De esta \nforma, el valor seman al ahora ser\u00e1 el porcentaje \nde canciones clasificadas como positivas o \nnegativas respecto al total.  En la Figura 7 \npodemos observar los resultados de esta medida . \nVemos c\u00f3mo , de nuevo , las canciones positivas \nest\u00e1n disminuyendo y las negativas aumentando, \naunque ahora se observa de manera m\u00e1s clara la \ndiferencia entre el consumo de canciones \npositiv as y negati vas, ya que hay a\u00f1os en los que \ncasi el 80% de las canciones tienen un mensaje \nel doble de positivo que de negativo mientras \nque las que tienen un men saje el doble de \nnegativo que de positivo apenas superan el 10% \nen su pico.  \n \n \nFigura 7: Evoluci\u00f3n de l porcentaje de canciones \npositivas y negativas , indicador alternativo . \n \nLa Figura 8 presenta la positividad de las \ncanciones en relaci\u00f3n con la posici\u00f3n ordinal en \nla clasificaci\u00f3n. Se observa que las canciones \ncon un coeficiente de positividad m\u00e1s alto llegan \na puestos m\u00e1s altos.  \n \nFigura 8: Coeficiente medio de positividad por \nposici\u00f3n.  \n \nA continuaci\u00f3n , en la base de datos \nintroducimos  el indicador de va lencia  facilitado \nEl sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales\n99por Spotify . Como se dijo anteriormente , la \nvalencia toma valores entre 0 y 1, a mayor valor \nmayor alegr\u00eda de la canci\u00f3n seg\u00fan sus \ncaracter\u00edsticas musicales. En la Figura 9 \nobserv amos  que el valor de valencia est\u00e1 \ndisminuyendo, al igual q ue en el caso de las \nletras positivas, por lo que la clasificaci\u00f3n \nmusical del estado de \u00e1nimo de Spotify parece \ncoincid ir parcialmente con nuestro an\u00e1lisis de \ntexto ( hay que tener en cuenta  que Spotify no usa \nletras para estimar la valencia).  \n \nFigura 9: Evoluci\u00f3n de  la valencia por semana . \n \nDe la misma forma, observamos en la Figura \n10 c\u00f3mo la valencia es m\u00e1s alta en los puestos \nm\u00e1s altos del ranking.  \n \n \nFigura 10: Coeficiente medio de valencia por \nposici\u00f3n.  \n \nTambi\u00e9n hemos obtenido  otras variables \nmusicales, procedentes de Spotify, que ser\u00e1n \nobjeto de l an\u00e1lisis de  correlaci\u00f3n con nuest ros \ncoeficientes positivo y negativo. Estas variables: \nla \u201cduraci\u00f3n\u201d (cu\u00e1nto tiempo dura la canci\u00f3n, en \nmilisegundos), el \u201ctempo\u201d ( la velocidad a la que \nest\u00e1 tocada la canci\u00f3n, medido en beats -per-\nminute , BPM), la \u201cclave\u201d (indica la tonalidad en \nla que est \u00e1 compuesta la canci\u00f3n, tomando el \nvalor \u201c1\u201d si est\u00e1 en modo mayor y \u201c0\u201d si est\u00e1 en \nmodo menor) y la \u201cenerg\u00eda\u201d (un coeficiente entre \n0 y 1 que mide la intensidad de la canci\u00f3n, tomando valores altos las canciones r\u00e1pidas y \nruidosas).  La evoluci\u00f3n temporal de estos \nindicadores se presenta en la Figura 11.  \n \n \nFigura 11: Otras caracter\u00edsticas ac\u00fasticas de las \ncanciones . \n \n4 Relaciones entre indicadores  \nEn la Figura 12 se pueden observar las \ncorrelaciones entre las variables musicales \nprocedentes de Spotify y los coeficientes \npositivo y negativo  calculados por VADER  para \ncada canci\u00f3n . La primera observaci\u00f3n es que la \ncorrelaci\u00f3n entre el coeficiente positivo y la \nvalenc ia es positiva, mientras que entre  el \nnegativo y la valencia es negativo, siendo ambas \nsignificativas al 1%.  Esto es una demonstraci\u00f3n \nde que, de acuerdo con lo que Spotify considera \nuna canci\u00f3n positiva seg\u00fan sus caracter\u00edsticas \nmusicales y lo que VADER c onsidera como un \ntexto positivo, los compositores tienden a \ncomponer m\u00fasica positiva para acompa\u00f1ar a \nletras positivas. Esta afirmaci\u00f3n, aunque pueda \nparecer obvia, hasta donde conocemos, es la \nprimera vez que se analiza cuantitativamente. Es \nun primer pas o para realizar an\u00e1lisis basados en \nel comportamiento del consumidor de m\u00fasica y  \nde qu\u00e9 contextos  provocan que prefieran \nconsumir canciones que expresan un sentimiento \nu otro, pudiendo analizarse de manera \nindependiente tanto la letra como la m\u00fasica.  \n  \nMarco Palomeque, Juan de Lucio\n100 \n   \nFigura 12: Coeficientes de correlaci\u00f3n por \ncanci\u00f3n entre las distintas variables . \n \nTeniendo en cuenta el resto de las variables , \npodemos destacar las siguientes correlaciones \nentre las letras positivas y negativas y las \ncaracter\u00edsticas musicales:  \n \n\u2022 Las canciones positivas son \nsignificativamente m\u00e1s cortas  (correlaci\u00f3n \ndel -4.2%) . Podr\u00eda estar reflejando que la \nexpres i\u00f3n musical de problemas requiere de \nm\u00e1s tiempo que transmitir ideas alegres . Por \nsu parte la valencia tambi\u00e9n tiene una \ncorrelaci\u00f3n negativ a con la duraci\u00f3n \n(correlaci\u00f3n del -14%). \n \n\u2022 La energ\u00eda se relaciona de manera clara con \nla valencia (correlaci\u00f3n del 3 5%). Sin \nembargo, l as canciones negativas son algo \nm\u00e1s en\u00e9rgicas  (correlaci\u00f3n del 2 .6%) que las \npositivas  (correlaci\u00f3n de l -12%) que parece n \nser algo m\u00e1s relajadas . Las canciones \nen\u00e9rgicas son canciones  m\u00e1s ruidosas, por lo \nque pueden ser canciones m\u00e1s enfocadas a la \nprotesta o a g\u00e9neros que tratan tem\u00e1ticas  m\u00e1s \npesimistas u oscuras como el heavy metal.  \n \n\u2022 Que una canci\u00f3n est\u00e9 compuesta en una  \ntonalidad mayor o menor no indica que su \nmensaje sea positivo o negativo  (correlaci\u00f3n \nnula) . La relaci\u00f3n tampoco es significativa \ncon la valencia, por lo que es algo que \ncoincide tanto a nivel de letra como musical.  \n \n\u2022 El tempo es algo m\u00e1s lento en cancione s \npositivas,  pero sobre todo el tempo se \nincrementa con la energ\u00eda de la canci\u00f3n . Esto \nse puede deber a que un mensaje positivo casa con m\u00fasica relajada, mientras que uno \nnegativo puede requerir de m\u00e1s potencia.  \n \n5 Conclusiones  \nEl an\u00e1lisis de las letras de l as canciones contiene \ninformaci\u00f3n sobre las preferencias de sus \noyentes. A escala agregada refleja las \npreferencias musicales de la sociedad y de su \nestado de \u00e1nimo. Los rankings musicales,  como \nel Billboard Hot 100, acumulan el sentir social \nreflejado en la m\u00fasica , que se recoge tanto en las \ncaracter\u00edsticas musicales como en las letras que \nincorporan.  \n \nLas t\u00e9cnicas de an\u00e1lisis de texto nos han \npermitido construir un indicador del sentimiento \npositivo / negativo de las canciones.  Con \u00e9l, \nhemos podido comprobar c\u00f3mo el consumo de \nmensajes positivos en las canciones est\u00e1 \ndecayendo con el paso de los a\u00f1os, lo cual es la \nprimera conclusi\u00f3n del estudio.  \n \n Este indicador de sentimiento se \ncorrelaciona con indicadores construidos a parti r \nde las caracter\u00edsticas musicales de las canciones \ncomo es el indicador de valencia de Spotify. Esta \ncorrelaci\u00f3n nos indica una concordancia limitada \nentre la m\u00fasica y la letra . Aunque se observa una \nrelaci\u00f3n  positiva con las letras positivas y \nnegativa c on las negativas , los coeficientes de \ncorrelaci\u00f3n son reducidos .  \n \nTambi\u00e9n hemos mostrado la existencia de \nuna relaci\u00f3n entre el sentimiento expresado en \nlas letras con otras caracter\u00edsticas musicales. Los \nresultados muestran que el an\u00e1lisis de los textos \nde las canciones proporciona indicadores \nadicionales a los que ya se elaboran enfoc\u00e1ndose \nen las caracter\u00edsticas de la m\u00fasica . Este tipo de \nan\u00e1lisis puede ayudar, por ejemplo, a los \nsistemas de recomendaci\u00f3n . \n \nPor \u00faltimo, el objetivo de este estudio es \nservir de primer paso para estudios futuros. \nAhora sabemos que el consumo de canciones \ncon mensajes positivos est\u00e1 disminuyendo, pero \nnos falta un por qu\u00e9. Es aqu\u00ed donde entran \nestudios centrados en el bienestar del individuo \nbasado en la situaci\u00f3n socioecon\u00f3 mica general, \nya que este cambio en el consumo musical es \nagregado. Esperamos pues que la \nimplementaci\u00f3n de herramientas de NLP en \naspectos como la industria musical pueda no solo \nEl sentimiento de las letras de las canciones y su relaci\u00f3n con las caracter\u00edsticas musicales\n101ayudar a la propia industria musical o a  la \ninvestigaci\u00f3n centrada en el len guaje, sino \ntambi\u00e9n a otros campos como pueden ser la \npsicolog\u00eda o la econom\u00eda.  \n \nAgradecimientos  \nLos autores  agradece n la financiaci\u00f3n \nrecibida por la Comunidad de Madrid y la UAH \n(ref: EPU -INV/2020/006) . \n \n \nBibliograf\u00eda  \n \nCheng, R.  2020. Sentiment Analysis with \nPretrained Transformers Using Pytorch . \nTowards Data Science.   \nElbagir, S., y J. Yang . 2020 . Sentiment Analysis \non Twitter with Python\u2019s Natural Language \nToolkit and VADER Sentiment Analyzer.  \nEn Iaeng Transactions on Engineering \nSciences: Special Issue For The International \nAssociation Of Engineers Conferences \n2019  (p. 63). World Scientific.  \nHanser, W. E., T. F.  ter Bogt,  A. J. Van den Tol,  \nR. E.  Mark  y A. J.  Vingerhoets . 2016. \nConsolation through music: A survey \nstudy.  Musicae Scientiae , 20(1), 122 -137. \nHeggli, O. A., J. Stupacher, y P. Vuust.  2021. \nDiurnal fluctuations in musical preference. \nPsyArXiv.   \nHutto, C. y E. Gilbert. 2015. Vader: A \nparsimonious rule -based model for sentiment \nanalysis of social media text. Conference: \nProceedings of the Eighth International \nAAAI Conference on Weblogs and Social  \nMedia . \nLoria, S.,  P. Keen, M. Honnibal, R. Yankovsky, \nD. Karesh, y E. Dempsey. 2014. Textblob: \nsimplified text processing.  Secondary \nTextBlob: simplified text processing , 3. \nMahedero, J. P.,  A. Mart \u00ednez, P. Cano, M. \nKoppenberger, y F. Gouyon. 2005. Natural \nlanguage processing of lyrics. \nEn Proceedings of the 13th annual ACM \ninternational conference on Multimedia , pp. \n475-478. \nMihalcea, R., y C. Strapparava. 2012. Lyrics, \nmusic, and emotions. In  Proceedings of the \n2012 Joint Conference on Empirical Methods \nin Natural Language Processing and Computational Natural Language Learning , \npp. 590 -599. \nNewman, H. y D. Joyner. 2018. Sentiment \nanalysis of student evaluations of teaching. \nEn Lecture Notes in Computer Science,  pages \n246\u2013250. Springer International Publishing.  \nPark, M., J. Thom, S. Mennicken, H. Cramer, y \nM. Macy. 2019. Global music streaming data \nreveal diurnal and seasonal patterns of \naffective preference.  Nature human \nbehaviour , 3(3), 230-236. \nPyrovolakis,  K., P. Tzouveli , y G. Stamou . 2020 . \n\"Mood detection analyzing lyrics and audio \nsignal based on deep learning \narchitectures,\"  2020 25th International \nConference on Pattern Recognition (ICPR) , \n2021, pp. 9363 -9370, doi: \n10.1109/ICPR48806 .2021.9412361.  \nSaarikallio, S. 2011. Music as emotional self -\nregulation throughout adulthood.  Psychology \nof music , 39(3), 307 -327. \nMarco Palomeque, Juan de Lucio\n102 Reconocimiento  y clasificaci\u00f3n de entidades n ombradas en textos \nlegales  en espa\u00f1ol  \nNamed Entities  Recognition and Classification  in Spanish  Legal Texts  \nDoaa Samy   \nCairo University , Giza, Egypt  \nInstituto de Ingenier\u00eda del Conocimie nto (IIC), Madrid, Spain  \ndoaasamy@cu.edu.eg  \nResumen:  El reconocimiento y la clasificaci\u00f3n de las entidades nombradas (NER/NERC) \nes una tarea principal en las \u00e1reas del Procesamiento del Lenguaje Natural (PLN) y la \nExtracci\u00f3n de la Informaci\u00f3n. El papel de NERC en el dominio legal es imprescindible en \nel desarrollo de sistemas legales inteligentes. El presente trabajo pretende dar un primer \npaso hacia establecer un \"baseline\" para la tarea NERC en el espa\u00f1ol jur\u00eddico. El objetivo \nprincipal consiste en  proporcionar un recurso ling\u00fc\u00eds tico anotando  cinco tipos b\u00e1sicos de \nentidades nombradas en los textos legislativos en espa\u00f1ol peninsular. Los cinco tipos de \nentidades nombradas son: Personas, Organizaciones, Lugares, Fechas absolutas y \nReferencias a leyes, decretos, \u00f3rdenes, normativas y art\u00edculos. Se adopta una metodolog\u00eda \nh\u00edbrida que re\u00fane tres t\u00e9cnicas principales: Patrones de expresiones regulares, listas de \nfuentes externas y el entrenamiento de tres modelos NERC utili zando la librer\u00eda abierta \nspaCy  v3. De los tres modelos entrenado s, el mejor ha obtenido un f -score de 0.93 \nalcanzando en  algunos tipos como las menciones a leyes o fechas valores de 0.9 8 y 0.97 \nrespectivamente . El peor de los modelos ha alcanzado una media de f-score de 0.85 que \nsigue siendo un resultado satisfactorio comparado con el estado de la cuesti\u00f3n . \nPalabras clave:  Entidades Nombradas , Procesamiento de t extos legales, Procesamiento \ndel espa\u00f1ol jur\u00eddico , Extracci\u00f3n de la informaci\u00f3n en textos legales.  \nAbstract: Named Entity Recognition and Classification (NER/NER C) is a major task in \nNatural Language Processing (NLP) and Information Extraction (IE).  In the legal domain, \nNERC is indispensable in developing legal intelligent systems . This study pretends to \ntake a first step towards a baseline for Spanish NERC in the  legal domain. The main \nobjective is  to provide a linguistic resource by  annotating five basic categories of Named \nEntities in Spanish legislative texts. The se five categories are Person, Organization, \nLocation, Dates (absolute expressio ns) and, finally Re ferences to aws, decrees, \nregulations, etc. To achieve this goal, we adopt a hybrid approach by combining three \ntechniques: hand-crafted patterns through regular expressions, l ook-up lists and training \nof three NERC model s using the architecture of spaCy . The best model achieved a \ngeneral f -score of 0.93 with some types of entities such as Legal entities and Dates \nreaching up to 0.98 and 0.97 respectively. The worst model achieved a general f -score of \n0.85, which is still satisfactory given the state of the  art.  \nKeywords:  Named Entities , Legal Text Processing , Information Extraction in Legal \nTexts, Spanish Legal Text Processing.  \n1 Introducci\u00f3 n \nEl reconocimiento y la clasificaci\u00f3n de las \nentidades nombradas (en adelante NER/NERC \npor las siglas en ingl\u00e9s: Named Entity \nRecoginition and Classification ) es una tarea \nprincipal en las \u00e1reas del Procesamiento del Lenguaje Natural (PLN) y la Extracci\u00f3n de la \nInformaci\u00f3n. El t\u00e9rmino de entidades \nnombradas fue acu\u00f1ado por primera vez en la \nserie de los congresos MUC ( Message \nUnderstanding Conference ) en el a\u00f1o 1995 \npara referirse al proceso de extraer unidades \nrelevantes de informaci\u00f3n a partir de textos no -\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 103-114\nrecibido 10-05-2021 revisado 07-06-2021 aceptado 08-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-9\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural estructurados (Sekine, 2004) (Nadeau y \nSekine, 2007) . Estas unidades incluyen \nnombres propios de personas, organiz aciones, \nlugares o expresiones num\u00e9ricas como fechas o \ncantidades , etc.  \nDada su relevancia en el an\u00e1lisis sem\u00e1ntico, \nla tarea NERC se ha convertido en una piedra \nangular para aplicaciones inteligentes como los \nsistemas de Pregunta -Respuesta (QA), la \ngener aci\u00f3n de res\u00famenes autom\u00e1ticos, la \nmejora de los sistemas de recuperaci\u00f3n de la \ninformaci\u00f3n, la traducci\u00f3n autom\u00e1tica, la \nanonimizaci\u00f3n de textos, la generaci\u00f3n de \ngrafos de conocimientos, etc.  \nEn cuanto a las metodolog\u00edas y t\u00e9cnicas, los \nm\u00e9todos empleados  para abordar la tarea de \nNERC han ido desarrollando desde modelos \nbasados en reglas con patrones de expresiones \nregulares, listas o gazetteers  hacia modelos de \naprendizaje autom\u00e1tico supervisado y semi -\nsupervisado como  Hidden Markov  Models  \n(HMM), Support Vector Machine  (SVM) y \nConditional Random Field  (CRF) siendo este \n\u00faltimo de los m\u00e1s eficientes en NERC (Roy, \n2021). En los \u00faltimos a\u00f1os, el uso de las redes \nneuronales con el aprendizaje profundo y la \nintegraci\u00f3n de  modelos del lenguaje con  los \nWordEmbeddi ngs ha supuesto un cambio en el \nparadigma del PLN en general y en las tareas \nespec\u00edficas como NERC (Roy, 2021).  \nEl papel de NERC es imprescindible en el \ndesarrollo d e sistemas legales inteligentes . \nDado  el gran volumen de textos que se suele \nmanejar  en est e dominio , ha surgido  un inter\u00e9s , \ncada vez mayor , por el procesamiento de textos \nlegales , en general y por la tarea NERC, en \nparticular.  \nEste inter\u00e9s se fundamenta en el gran \npotencial  de las t\u00e9cnicas de PLN y su \ncapacidad de ofrecer soluciones inteligent es \nque benefici en a usuarios  claves  del sector \ncomo los abogados, los jueces, los juristas, los \ndocumentalistas jur\u00eddicos, adem\u00e1s del sector de \nla adminis traci\u00f3n p\u00fablica  que, aunque  no trate \ntextos estrictamente jur\u00eddicos, s\u00ed maneja  textos \nadmin istrativos  con un alto contenido legal \ncomo es  el caso de la contrataci\u00f3n p\u00fablica o \nlos convenios.  \nPor tanto, los avances en el procesamiento \nde textos legales constituyen un gran potencial \npara agilizar procesos internos de la \nadministraci\u00f3n p\u00fablica, simplificar lo s \nprocedimientos y mejorar el acceso de la \nciudadan\u00eda a la informaci\u00f3n legal  y \nadministrativa.  Para impulsar  la apertura de datos p\u00fablicos, la transformaci\u00f3n digital \ninteligente y  la agilizaci\u00f3n d e procesos  \nadministrativos,  legales y judiciales, existen  \niniciativas y programas a nivel  europeo como \nel portal de  e-Justice . Adem\u00e1s, el programa de \nEuropa Digital \u201c Digital Europe Programme \u201d \npone \u00e9nfasis en el papel  de la inteligencia \nartificial en la administraci\u00f3n p\u00fablica para \nmejor ar la  interacci\u00f3n digital entre ciudadanos \ny administraci\u00f3n p\u00fablica.   \nSon numerosas las  soluciones inteligentes \nque puede ofrecer el PLN , en general, y la \ntarea NERC , en particular,  al \u00e1mbito legal y \nadministrativo. Identificar sentencias parecidas \npara fundamentar un caso , enlazar documentos \na trav\u00e9s de las entidades, construir grafos de \ndocumentos, anonimizar datos personales o \ngenera r una l\u00ednea temporal de las leyes y  los \nhechos en un documento legal son solo \nalgunos ejemplos de c\u00f3mo el PLN y NERC \npueden asistir  tanto a abogados, juristas como \na jueces  en realizar sus tareas .   \nPese a las oportunidades que supone el \ndominio legal, son  pocos los estudio s, recursos \ny herramientas de PLN en este dominio, sobre \ntodo en espa\u00f1ol . Sin embargo, en los \u00faltimos \na\u00f1os, han aparecido algunas iniciativas . En \ndiciembre de 2019 y c on el fin de impulsar el \ndesarrollo de recursos y herramientas de PLN  \nen el dominio legal en espa\u00f1ol, catal\u00e1n, vasco \ny gallego, se organiz\u00f3 la jornada \u201cIberLegal\u201d \ndentro del marco de las a ctividades del Plan \nespa\u00f1ol de Tecnolog\u00edas del L enguaje. Las \nactas de la jornada ofrecen un abanico de \ntemas de  inter\u00e9s como  la extracci\u00f3n de \nterminolog\u00eda legal, b\u00fasquedas inteligen tes en \ndocumentos y  recuperaci\u00f3n de informaci\u00f3n  \nlegal, herramientas para asistir a la ciudadan\u00eda \nen la redacci\u00f3n de textos para la \nadministraci\u00f3n p\u00fablica y, por \u00faltimo, \nexpresione s temporales en textos legales \n(PlanTL, 2019).   \nOtra iniciativa es el corpus Legal -ES (Samy \net al. 2020) , considerado  como un meta corpus \nque re\u00fane varias fuentes del dominio en lengua \nespa\u00f1ola con m\u00e1s de dos mil millones de \npalabras recopiladas a partir de fuentes de \ndatos abiertos espa\u00f1ol as, europeas, \nhispanoamericanas e internacionales. Estas \nfuentes  representan una variedad de textos \njur\u00eddicos que incluyen textos legislativos, \njurisprudenciales ( sentencias ) y textos \nadministrativos.  Adem\u00e1s, el estudio presenta \nresultados preliminares so bre c\u00e1lculos de \nEmbeddings  del espa\u00f1ol jur\u00eddico y un modelo \nDoaa Samy\n104  de t\u00f3pico entrenado sobre el conjunto de \nlegislaci\u00f3n.  \nNo obstante, los trabajos en esta \u00e1rea se \nenfrentan con retos como: 1) El n\u00famero \nlimitado de recursos y herramientas de PLN \nadaptados al domin io en general; 2) La \npredominancia  del ingl\u00e9s, ya que la mayor\u00eda de \nlos recursos y las herramientas disponibles se \ndesarrollan para el  tratamiento de textos en  \ningl\u00e9s; 3) Una adopci\u00f3n ralentizada de las \ntecnolog\u00edas inteligentes en el sector legal y \nadminis trativo en comparaci\u00f3n con otros \nsectores como el sector biom\u00e9dico o \nfinanciero.  \nEstos retos han influido en que la \nconsolidaci\u00f3n de la tarea NERC en el dominio \nlegal ha tardado unos a\u00f1os  en comparaci\u00f3n con \notros dominios.  De ah\u00ed, el presente estudio \nprete nde afrontar la tarea en los textos legales \nespa\u00f1oles teniendo como objetivo  principal  el \nreconocimiento y la clasificaci\u00f3n de cinco \ntipos b\u00e1sicos  de entidades nombradas en textos \nlegislativos  espa\u00f1oles.  \nEl trabaj o se estructura  en ocho secciones \nadem\u00e1s d e la introducci\u00f3n y las conclusiones. \nLas primeras secciones presentan un enfoque \nte\u00f3rico con an\u00e1lisis  del estado de la cuesti\u00f3n de \nla tarea de  NERC legal , en general y la NERC \nlegal en espa\u00f1ol. El resto de las secciones se \ncentra n en aspectos pr\u00e1cticos do nde se describe  \nel trabajo y los experimentos realizados  \ndetallando  el alcance, la metodolog\u00eda, los datos \nutilizado s y las fases del estudio desde la \nextracci\u00f3n de los datos , pasando por la pre -\nanotaci\u00f3n, la validaci\u00f3n, el entrenamiento \nhasta la  evaluaci\u00f3n  y la visualizaci\u00f3n final de \nlos resultados obtenidos por los modelos \nentrenados . \n2 Estado de l arte : NERC en el dominio \nlegal  \nPara ofrecer una visi\u00f3n panor\u00e1mica acerca del \ndesarrollo de los estudios de NERC en el \ndominio legal, subrayamos algunas inic iativas  \ny estudios en esta \u00e1rea  teniendo en cuenta tres \ncriterios: a) La lengua  objeto de an\u00e1lisis,  b) el \ntipo de texto legal  (textos legislativos, textos \njurisprudenciales (sentencias), resoluciones, \ncontratos,  convenios, registros legales, etc .) y \nc) la evoluci \u00f3n de las t\u00e9cnicas . \nEn primer lugar, destacamos los estudios \nque han tratado el tema en otras lenguas y , en \nsegundo lugar, nos centramos en los estudios \nque han abordado la tarea en espa\u00f1ol.   \n2.1 NERC de textos legal es en otras \nlenguas  \nEl a\u00f1o  2006 marca la cel ebraci\u00f3n de  la primera \ntarea de evaluaci\u00f3n dedicada a la recuperaci\u00f3n \nde textos en el dominio legal \u201c TREC Legal\u201d, \norganizada por el Instituto Estadounidense de \nEst\u00e1ndares y Tecnolog\u00eda (NIST ) (Cormack et \nal., 2010 ). Desde esa fecha, e l inter\u00e9s por las \nentidades nombradas en los textos legales ha \nseguido cobrando mayor importancia  por su \nrelevancia en la extracci\u00f3n y la recuperaci\u00f3n \nde la informaci\u00f3n .  \nEn 2010 se public\u00f3 el volumen titulado \n\u201cSemantic Processing of Legal Texts\u201d \n(Francesconi et al., 2010)  incluyendo  uno de \nlos estudios pioneros sobre  la identificaci\u00f3n y \nresoluci\u00f3n de las entidades nombradas en \ntextos legales en ingl\u00e9s  (Dozier et al., 2010).   \nEn cuanto a t\u00e9cnicas, en esos a\u00f1os \ndominaban los modelos de aprendizaje \nautom\u00e1tico cl\u00e1sico y se combinab an con \nreglas. Dozier et al. (2010) empleaban reglas, \nlistas y modelos estad\u00edsticos (SVM) para \nreconocer jueces, abogados, empresas, \ntribunales y \u00e1reas de jurisdicci\u00f3n alcanzando \nvalores de f-score  por encima del 0 ,90. El \nestudio se centra en textos de jur isprudencia \nbasada en casos ( case law ), \ndeposiciones/declaraciones, juicios y alegatos.  \nSiguiendo la misma aproximaci\u00f3n , \nQuaresma y Gon\u00e7alves (2010) propon en \nutilizar rasgos sint\u00e1cticos combinando el uso \nde un analizador  sint\u00e1ctico (parser) con un \nmodelo S VM para reconocer nombres, lugares, \nfechas y referencias a documentos en textos de \nconvenios internacionales  del corpus europeo \nEur-lex en cuatro lenguas : ingl\u00e9s, alem\u00e1n, \nportugu\u00e9s e italiano . \nPor otro lado, en esos a\u00f1os, se observa el \nauge de las ontolog\u00ed as y los datos enlazadas. \nLandthaleret al. , (2016) generan redes de \ngrafos de textos legislativos  del c\u00f3digo civil \nalem\u00e1n  bas\u00e1ndose en entidades nombradas . \nCordellini et al. (2017) extraen las entidades \nnombradas a partir de un conjunto de textos \nformados por juicios  de la Corte Europea de \nDerechos Humanos y la Wikipedia en ingl\u00e9s  \ncon el fin de enriquecer una ontolog\u00eda.   \nLos modelos  de aprendizaje autom\u00e1tico \ncl\u00e1sico  y las aproximaciones h\u00edbridas siguen \ndominando el panorama en el tratamiento de \ntextos legal es, Chalkidis et al., 2017 aplican  \nclasificadores lineales ( Logistic Regression  y \nReconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol\n105 SVM) junto a reglas  para el reconocimiento de \nentidades en contratos en ingl\u00e9s . Asimismo, \nGlaser et al. (2018) integra n diferentes \naproximaciones para el reconocimiento y la \ndesambiguaci\u00f3n de entidades nombradas en \ncontratos  en alem\u00e1n . Por \u00faltimo, Andrew y \nTannier (2018) combinan un modelo \nestad\u00edstico (CRF) y reglas ling\u00fc\u00edsticas para \nidentificar entidades nombradas y enlazarlas en \ntextos de registros legal es en franc\u00e9s .  \nEl paso de los modelos estad\u00edsticos y el \naprendizaje autom\u00e1tico cl\u00e1sico al aprendizaje \nprofundo  fue inminente en el panorama del \nPLN . Por eso, siguiendo e sta l\u00ednea,  Chalkidis \nand Androutsopoulos  (2017)  aplican modelos \nde aprendizaje profundo utilizando un model o \nde BiLSTM ( Bidirectional Short Term \nMemory ) para identificar y clasificar 11 tipos \nde elementos en contratos  en ingl\u00e9s  \nobteniendo resultados que superan los estudios \nanteriores que aplicaban SVM.  Los modelos de \naprendizaje profundo ya se consolidan y \nsuperan los resultados de los modelos cl\u00e1sicos. \nLeitner et al. (2019) proponen una metodolog\u00eda \npara el reconocimiento de un total de \ndiecinueve tipos de entidades nombradas  \nagrupadas en siete clases generales.  La tarea \nNERC fue aplicada a un conjunto de  \nsente ncias  alemanas  con Conditional Random \nFields  (CRFs) y BiLSTMS . Los modelos de \nBiLSTM han alcanzado mejores resultados con \nun f-score de 0,9546 para el conjunto de 19 \ntipos y 0.9595 para el conju nto de las 7 clases \ngenerales.  \n \n2.2 NERC  de textos legal es en esp a\u00f1ol \nSon pocos los estudios que  han abordado  la \ntarea NERC  en el dominio legal en espa\u00f1ol. \nSin embrago, u no de los trabajo s pioneros es el \nestudio de Mart\u00ednez -Gonz\u00e1lez et al.  (2005) que \ntiene com o objetivo automatizar la extracci\u00f3n \nde referencias en textos  legales, su resoluci\u00f3n y \nsu indexaci\u00f3n mediante reglas y gram\u00e1ticas \npara mejorar la recuperaci\u00f3n de la  informaci\u00f3n \nen este dominio. El estudio se ha realiz ado \nsobre  colecciones de documentos de una \neditorial de textos legales, pero no queda claro \nqu\u00e9 tipo  de documentos son  si son legislativos \no de otra categor\u00eda.   \nPasan muchos a\u00f1os hasta que empiece n a \naparecer otros estudios como Badji (2018) , en \nel que se presenta  una aproximaci\u00f3n basada en \nreglas y patrones para reconocer y enlazar  \nentidades legales (re ferencias a leyes, decretos, sentencias, cortes o fases judiciales) en fuentes \nlegislativas  espa\u00f1olas  y fuentes no oficiales \ncomo las noticias y redes sociales  donde \naparecen con otras denominaciones populares \n(Ej. El caso de la \u201cLey Cela\u00e1\u201d) .  \nEn la misma l\u00ednea,  Rodr\u00edguez -Doncel et al. \n(2018) han transformado un conjunto de la  \nlegislaci\u00f3n espa\u00f1ola en \u201cLinked Data\u201d  en \nformato RDF bas\u00e1ndose en identificar \nentidades ( leyes, organismos/empresas y \nlugares ) en los textos del Bolet\u00edn Oficial del \nEstado (BOE) y enl azarlas con fuentes \nexternas  como la Wikipedia, Wikidata,  el \nRegist ro Europeo de Autoridades, el Directorio \nCom\u00fan de Unidades Org\u00e1nicas y Oficinas \nespa\u00f1olas  o la base terminol\u00f3gica europea \nEurovoc.  \nPor otro lado , Haag (2019) adopta una \nmetodolog\u00eda h\u00edbrida combinando modelos \nestad\u00edsticos y reglas para el reconocimiento de  \nlas entidades legales en las fuentes legislativas \nargentinas utilizando el corpus d e legislaci\u00f3n \nargentina InfoLEG, en el que  se recogen los \ntextos del Bolet\u00edn Oficial de la Rep\u00fablica \nArgen tina. \nPor \u00faltimo, Navas -Loro ( 2020) y (Navas -\nLoro y Rodr\u00edguez -Doncel, 2019) , bas\u00e1ndose en \nreglas y gram\u00e1ticas, han abordado las \nexpresiones temporales en textos legales \ndesarrollando \u201cTimeLex\u201d para la detecci\u00f3n , la \nnormalizaci\u00f3n en TimeML  y la resoluci\u00f3n d e \nlas expresiones temporales en textos legales . \nTras este an\u00e1lisis del estado del arte, se \npuede observar la relevancia de la tarea NERC \nen el dominio legal  y la evoluci\u00f3n de las \nt\u00e9cnicas empleadas a lo largo de los a\u00f1os . Sin \nembargo, los esfuerzos y los r ecursos \ndesarrollados en espa\u00f1ol se centran en tipos  \nconcretos  de entidades nombradas como las \nreferencias a leyes o las expresiones \ntemporales desde una perspectiva  vertical  \nenfocada en unas entidades espec\u00edficas . La \n\u00fanica excepci\u00f3n, a este respecto, es el estudio \nsobre  la conversi\u00f3n de la legislaci\u00f3n espa\u00f1ola a \n\u201cLinked Data\u201d (Rodr\u00edguez -Doncel et al., 2018) \ndonde se  ha tratado varios tipos de entidades \nen los textos legislativos. No obstante, las \nentidades nombradas no constituyen el \nobjetivo principal, da do que el enfoque se \ncentra en enlazar los datos.   \nPartiendo de estas observaciones , se echa \nen falta una aproximaci\u00f3n transversal para las \nentidades nombradas en  el dominio legal \nespa\u00f1ol que aborda la tarea a gran escala. Este \nplanteamiento integral  es im prescindible como \nDoaa Samy\n106  punto de partida para  establecer un baseline  de \nla tarea NERC en el espa\u00f1ol jur\u00eddico. Adem\u00e1s, \nayudar\u00e1 a analizar  aspectos principales como:  \nlos tipos  de entidades  m\u00e1s frecuentes, su \ndistribuci\u00f3n a trav\u00e9s de los distintos tipos de \ntexto le gal, por ejemplo , entre textos \nlegislativos y textos jurisprudenciales, etc.  \nDe ah\u00ed, el presente  trabajo pretende dar este \nprimer  paso hacia la tarea  NERC en el espa\u00f1ol \njur\u00eddico . No obstante, para conseguir un \nbaseline  objetivo  es necesario unir esfuerzos  y \ncontrastar aproximaciones mediante  tareas de \nevaluaci\u00f3n  (nuestro pr\u00f3ximo  objetivo)  o \niniciativas y proyectos interdiscipl inarios e \ninter-institucionales como el proyecto europeo \nLYNX (Rehm et al., 2019) o los recursos \ndesarrollados por Stanford Codex \u201cStanford \nCenter for Legal Informatics \u201d (Waltl y Vogl, \n2018)  (Rios, 2019) . \n3 Alcance y metodolog\u00eda  \nEl objetivo de este estudio es ano tar cinco tipos \nb\u00e1sicos de entidades n ombradas en los textos \nlegislativos en espa\u00f1ol peninsular. Los cinco \ntipos de entidades nombradas  son:   \n\uf0b7 Personas   \n\uf0b7 Organizaciones  \n\uf0b7 Lugares  \n\uf0b7 Referencias  a leyes, decretos, \u00f3rdenes, \nnormativas y art\u00edculos . \n\uf0b7 Fechas absolutas  entendidas como \nexpresiones temporale s referentes a \nfechas y d\u00edas concretos como por \nejemplo \u201cel 3 de mayo de 2021\u201d . \nComo  primer acercamiento a la anotaci\u00f3n \nde NE en el \u00e1mbito legal, se ha decantado por \nlas categor\u00edas b\u00e1sicas de NE (Organizaciones , \nPersonas y Lugares) junto a dos categor\u00edas \nrelevantes del dominio legal ; las fechas y las \nreferencias a leyes.  La selecci\u00f3n de las dos \n\u00faltimas categor\u00edas ha teniendo en cuenta tres \ncriterios: 1) La  utilidad de cara a futuras \naplicaciones (Ej. Enlazar las leyes, las l\u00edneas \ntemporales, detecci\u00f3n de pe riodos de vigencia, \netc.); 2) La alta frecuencia  de estos tipos de \nentidades en el dominio legal como se \ndemostrar\u00e1 en secci\u00f3n 5 y, por \u00faltimo, 3) La \nnaturaleza de los textos legales donde las \nreferencias a leyes, art\u00edculos, etc. es el rasgo \ndistintivo, po r excelencia del dominio objeto \nde estudio . No se han ampliado las categor\u00edas a \ncantidades u otras categor\u00edas porque este \nestudio se considera como un primer paso y para futuros trabajos, s\u00ed se valora incluir \nnuevos tipos de entidades.  \nEl previo an\u00e1lisis d el estado del arte \ndemuestra que los mejores resultados en \ncuanto a  acierto y cobertura , se han logrado  \ncombinando diferentes estrategias para afrontar \nla tarea de anotaci\u00f3n, especialmente si se trata \nde distintos tipos de entidades. Por este \nmotivo, hemos  optado por una metodolog\u00eda \nh\u00edbrida que re\u00fane tres t\u00e9cnicas principales  y \nque emplea las \u00faltimas t\u00e9cnicas en PLN y en el \ntratamiento de textos legales : \n\uf0b7 Patrones de expresiones regulares  \n\uf0b7 Listas de fuentes externas  \n\uf0b7 Entrenamiento de tres modelo s NERC \nutilizan do la librer\u00eda abierta spaCy, v3  \nEl trabajo se estructura  en seis fases  \nprincipales:  Extracci\u00f3n de datos, p re-\nanotaci\u00f3n , validaci\u00f3n parcial , entrenamiento , \nevaluaci\u00f3n  y visualizaci\u00f3n  \n \nFigura 1:Estructura del trabajo . \n4 Datos  \nPara el presente  estudio , se ha utilizado  el \nconjunto de la legislaci\u00f3n del BOE  incluido  \njunto a otros conjuntos  en el meta -corpus \nLegal -ES (Samy et al., 2020) . Cabe se\u00f1alar que \nel portal del BOE incluye m\u00e1s conjuntos como \nanuncios o c\u00f3digo electr\u00f3nicos, etc. Sin \nembargo, aqu\u00ed nos limitamos al conjunto de \nLegislaci\u00f3n.  \n \n4.1 Estructura del conjunto  \nEl conjunto consiste en  un total de 21587 0 \nficheros  legislativos  desde el a\u00f1o 1 661 hasta \nseptiembre de 2019. Los ficheros est\u00e1n  en \nformato XML  seg\u00fan el est\u00e1ndar de ELI \n(European Lesgislation Identifier ). Al analizar \nla estructura y la nomenclatura del conjunto, se \nhan identificado  las siguientes categor\u00edas:  \n \n \n \n \nReconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol\n107 Categor\u00eda  N\u00ba \nfichero s E xplicaci\u00f3n  \nBOE-A 144599 Leyes -Decretos  \nBOE-T 1135 Sentencias \nconstitucionales  \nDOUE  69448 Diario Oficial de la \nUni\u00f3n Europea -\nLegislaci\u00f3n , \nDecisiones, \nRecomendaciones y \notros  \nAlgunos \nboletines de \nComunidades \nAut\u00f3nomas  688 Algunos conjuntos de \ndiarios oficiales de \nComunidades \nAut\u00f3nomas  \nTabla 1. Categor\u00edas de documentos en el BOE \nlegislativo . \nPara esta fase , se ha decidido limitarse a  los \nsubconjunto s BOE -A y DOUE  porque son los \nmayor es conjunto s y, por tanto, son m\u00e1s \nrepresentativo s. Se han descartado los \nsubconjuntos de boletines oficiales de \ncomunidades aut\u00f3nomas porque pueden \ncontener lengua s oficiales que no est\u00e1n en el \nalcance del presente estudio. Los conjuntos del \nDOUE  emplean diferentes normas a la hora de \nreferenciar leyes o directivas , lo cual implica \nincluir distintos patrones para el \nreconocimiento  de estas entidades . A \ncontinuaci\u00f3n,  se muestran algunos ejemplos de \nreferencias a leyes en el BOE y en el DOUE \nteniendo en cuenta las variaciones en el BOE a \nlo largo de los a\u00f1os  \n \nEjemplo s de referencia a ley es en el BOE  \n \n- Ley 13/2016, de 28 de julio  \n- Ley EDU/1234/2020  \uf0e0 En leyes recient es \nse a\u00f1aden a veces tres letras para indicar el \ncampo tem\u00e1tic o/ministerios en cuesti\u00f3n como \nEducaci\u00f3n, Fomento.  \n \n-EL DECRETO MIL QUINIENTOS \nSESENTA/MIL NOVECIENTOS SETENTA Y \nCUATRO, DE TREINTA Y UNO DE MAYO, \uf0e0 \nEn textos  de los 60 y los 70, se mencionaban  \nlas fechas de forma alfab\u00e9tica.  \nEjemplos de referencia a una Directiva \nEuropea  \n- Directiva 2006/123/CE, de 12 de \ndiciembre, del Parlamento Europeo y \ndel Consejo  \n- Directiva 2006/112/CE del Consejo, \nde 28 de noviembre de 2006  \nDirectiva 2006/112/CE  \n 4.2 Extrac ci\u00f3n de los datos  \nPara cada fichero XML  del conjunto, se ha \nprocedido a la extracci\u00f3n autom\u00e1tica del t\u00edtulo \na partir de los metadatos adem\u00e1s del contenido \ny se ha transformado el contenido extra\u00eddo a \nficheros en formato txt para facilitar su \nposterior trat amiento.  \nGracias a la nomenclatura , se ha podido \norganizar el proceso de extracci\u00f3n y \ntransformaci\u00f3n en grupos divididos por  \nventanas temporales correspondientes a las \ndiferentes d\u00e9cadas. Por ejemplo, se han \nagrupado los ficheros de BOE -A de la d\u00e9cada \nde los 70, los 80, los 90, etc.  Esta agrupaci\u00f3n \ntambi\u00e9n nos permite analizar la evoluci\u00f3n de \nlas formas de referenciar las leyes y las fechas . \nPor ejemplo, en los textos legislativos de los \na\u00f1os 70, los n\u00fameros en leyes y  fechas se \noscilaba entre las referenci as num\u00e9ricas y las \nreferencias alfab\u00e9ticas . Al finalizar el proceso, \nel recuento final del texto extra\u00eddo  de BOE -A \nse asciende a : 370860 624 tokens  y DOUE \n201840806  tokens . \nAunque se ha pre -anotado todo el conjunto, \nlos experimentos de este trabajo se centr an en \nel BOE -A, puesto que manejar este  volumen \nde datos es imposible por l a inviabilidad de \nvalidar  esta cantidad  y las limitaciones de \ninfraestructura y capacidad de c\u00f3mputo. Por \nestos motivos  y de cara al entrenamiento  del \nmodelo , se han creado de forma  aleatoria tres \nconjuntos  de datos  del BOE -A:  \n\uf0b7 Un conjunto de  entrenamiento  (training ) \n\uf0e0 1272254 tokens  (21116 oraciones) . \n\uf0b7 Un conjunto de  desarrollo \n(develop/validation ) \uf0e0 151600 tokens . \n\uf0b7 Un conjunto de  evaluaci\u00f3n  (test) \n\uf0e0200438 tokens . \nAl crear estos conju ntos, se ha tenido en \ncuenta que sean textos relativamente modernos \npara garantizar la utilidad de l modelo en el \ncontexto actual, ya que  un enfoque diacr\u00f3nico \nqueda fuera del alcance del presente estudio .  \nSiguiendo este criterio,  los tres conjuntos se \nhan creado  a partir de los textos del BOE de las \ntres \u00faltimas d\u00e9cadas: los a\u00f1os 90, la d\u00e9cada de \n2000-2010 y de 2010 -2019.   \n5 Pre-anotaci\u00f3n  \nPara realizar la pre -anotaci\u00f3n, es \nimprescindible decidir: 1) \u00bfQu\u00e9 se va a anotar? \n2) \u00bfC\u00f3mo se va a realizar esta pre -anotaci\u00f3n?  \nLas respuestas a estas preguntas suelen \nrecogerse en las gu\u00edas de anotaci\u00f3n. Establecer \nDoaa Samy\n108  unos criterios claros y un\u00edvocos garantiza la \nconsistencia de la anotaci\u00f3n y, por \nconsiguiente, la calidad del proceso. Para el \npresente trabajo, se ha n desarr ollado unas \ngu\u00edas de anotaci\u00f3n internas b\u00e1sicas como \ndocumentos preparativos para una tarea de \nevaluaci\u00f3n  (IberLegal2020@Iberlef)  que al \nfinal, no se ha celebrado  (PlanTL, 2020) . Para \nestas gu\u00edas, s e ha  partido de las  gu\u00edas de \nreferencia  empleadas en  tareas de evaluaci\u00f3n  \nde entidades nombradas en Iberlef (Porta -\nZamorano  y Espinosa -Anke, 2020) .  \nEs importante se\u00f1alar  que la decisi\u00f3n \nacerca de la t ipolog\u00eda de entidades surge de las \ncaracter\u00edsticas propias del corpus y el dominio. \nLos componentes NERC gen\u00e9ricos suelen \nincluir tipos b\u00e1sicos como Personas, \nOrganizaciones, Lugares y Miscel\u00e1nea u Otros. \nSe han realizado dos pruebas iniciales con los \ncomp onentes NER de las librer\u00edas de spaCy y \nStanza, pero los resultados no fueron \nsatisfactorios, ya que no se adapten al dominio \nen cuesti\u00f3n. Por eso , hemos estimado necesario \ncrear un modelo nuevo con una tipolog\u00eda que \nrefleje la naturaleza de l dominio y que  sea de \nutilidad. \n Por otro lado, en los textos legales , son \ncomunes las entidades anidadas, es decir, \ncompuestas. Por ejemplo, son comunes  las \nmenciones a leyes que incluyen una fecha \ncomo \u201cLa ley 1234/2010 , de 12 de mayo de \n2010\u201d. Ante estos casos, se ha  optado por una \naproximaci\u00f3n simplificada considerando cada \ntipo de entidad de forma independiente. Eso \nresulta en anotar la menci\u00f3n \u201cLey 1234/2010\u201d \ncomo una entidad legal y el segmento \u201c12 de \nmayo de 2010\u201d como fecha.  \nEn cuanto a la metodolog\u00eda  de anotar,  se \nha rec urrido a las tres estrategias mencionadas \nen la se cci\u00f3n 3  y se han integrado diferentes \nrecursos dependiendo de cada tipo de entidad.  \n\uf0b7 Leyes, r eferencias a leyes, decretos, \nnormativas, \u00f3rdenes, art\u00edculos . Se ha \ndesarrollado una serie de pa trones de \nexpresiones regulares  para identificar \nreferencias como \u201cLey 27/2014\u201d . \nAdem\u00e1s, se ha recopilado una lista con \nlos nombres oficiales completos de \ntodas las leyes aprobadas desde el a\u00f1o \n1977 disponibles en la p\u00e1gina del \nSenado . (Ej. Ley 27/2014, de 27 de \nnoviembre, del Impuesto sobre \nSociedades).  \n\uf0b7 Fechas absolutas.  Para este tipo de \nentidades se han desarrollado pat rones de expresiones regulares que cubre n \nmenciones alfab\u00e9ticas y num\u00e9ricas.  \n\uf0b7 Organismos.  Se han obtenido \ndiferentes listas del Directorio Com\u00fan \nde Unidades Org\u00e1nicas y Oficinas con \nun to tal de 16 mil entradas. Sin \nembargo, fue imprescindible  un proceso \nde depuraci\u00f3n para evitar duplicados, \nnorma lizar formatos y corregir faltas de \nortograf\u00eda , etc.  Se ha a\u00f1adido una lista \nadicional que incluye todos los  nombres \nde ministerios en todas las legislaturas.  \n\uf0b7 Lugares.  Las listas del Directorio \nCom\u00fan incluyen pa\u00edses, comunidades \naut\u00f3nomas, provincias y loc alidades, \ntipos de v\u00eda, etc. Para utilizar estas \nlistas, fue necesario un proceso de \ndepuraci\u00f3n porque presentaban los \nmismos problemas se\u00f1alados \nanteriormente . Adem\u00e1s, se ha optado \npor excluir algunos nombres de \nlocalidades por su ambig\u00fcedad  y por el \nposible ruido que puede causar en \nforma de falsos positivos . Por ejemplo, \nse han eliminado de la lista localidades \ncomo \u201cMar\u00eda\u201d, \u201cJavier\u201d  o \u201cCaso \u201d. \n\uf0b7 Personas.  Para esta categor\u00eda, hemos \noptado por utilizar el componente  NER \nde spaC y, dado que los resultados son  \naceptables, aunque hay un margen de \nmejora. Asimismo, se ha enriquecido la \npre-anotaci\u00f3n de este tipo con una lista \nde cargos y puestos.  \nLa salida de la pre-anotaci\u00f3n es un  texto \nenriquecido con las entidades marcadas  en \nformato de  \u201coffsets\u201d, es decir, incluyendo las \nposiciones de inicio y fin de cada entidad junto \na su tipo. Se ha optado por este formato de \nsalida porque es uno de los que admite  spaCy \npara el entrenamiento . En el siguiente ejemplo, \nse anotan dos entidades: a) Referencia a la \nOrden INT/985 /2005 empezando  en posici\u00f3n \n77 y termina en  95. b) Fecha \u201c7 de abril \u201d que \nempieza en el car\u00e1cter 100 y termina en el \ncar\u00e1cter 110.  \n(\"Uno. Se introducen las siguientes \nmodificaciones en el apartado Cuarto \nde la Orden INT/985/2005 , de 7 de \nabril:\", {\"entitie s\": [(100, 110, \n'TIME'), (77, 95, 'LEGAL')]})  \nSe ha pre -anotado el total del conjunto , \npero para gestionar esta cantidad de texto, se \nha realizado la pre-anotaci\u00f3n en agrupaciones  \ndividid as por las ventanas temp orales \nindicadas en la secci\u00f3n 4.1. El resultado es un \ntotal de  10424 216 entidades  nombradas pre-\nReconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol\n109 anotadas  en el BOE1. La cifra indicada refleja \nla alta frecuencia de entidades en los textos \nlegislativos, lo cual confirma la relevancia d e \nla tarea NERC en este dominio . Conviene \nse\u00f1ala que esta cifr a es antes de l proceso de  \nvalidaci\u00f3n y puede contener  falsos positivos, \nas\u00ed como  entidades anotadas dos veces por dos \ncategor\u00edas. No obstante , estas cifras nos \npuede n ofrecer algunos  indicadores generales \nacerca de  la distribuci \u00f3n de  los distintos tipos \nde entidades seg\u00fan  la tabla siguiente:  \n \nTipo  Porcentaj e2 \n% \n\uf0b7 Leyes -nombres completos . \n\uf0b7 Referencias a leyes , etc. 1-2% \n25-40% \nFechas  11-15% \nOrganizaciones  35-40% \nPersonas  6-7% \nLugares  10-11% \nTabla 2. Rangos de d istribuci\u00f3n de los tipos de \nentidades nombrad as. \n6 Validaci\u00f3n  \nUna vez terminada la fase de pre -anotaci\u00f3n, se \nprocede a una validaci\u00f3n parcial  de los \nconjuntos creados para el entrenamiento . Esta \nvalidaci\u00f3n pretende revisar de forma manual  \nlas anotaciones ambiguas  para asegurar un \nconjunto de entrenami ento de calidad . Se han \nobservado dos casos comunes de ambig\u00fcedad:  \n\uf0b7 Persona vs. Lugar.  Algunos apellidos \ncoinciden con nombres de lugares. Por \nejemplo, \u201cSegovia \u201d que aparece como \napellido y como ciudad . \n\uf0b7 Organizaci\u00f3n vs. Lugar.  Las entidades \ncomo \u201cComunidad de Madrid\u201d puede \nreferirse  a un lugar o a una instituci\u00f3n. Esa \ndistinci\u00f3n depende del contexto y requiere \nde un proceso de desambiguaci\u00f3n que \nqueda fuera del alcance de este estudio.  \nPor otro lado, la anotaci\u00f3n de las menciones \na leyes  en su forma completa  constituye un \nreto por la longitud y la complejidad sint\u00e1ctica \nde la entidad. Asimismo, a veces aparece \ncompleta y a veces aparece de forma parcial. \nDe est e modo , a la misma ley se puede referir \n                                                 \n1 Ejemplo  del conjunto disponible en:  \nhttps://github.com/dosamy/NERC -Legal-ES-\nExample -  \n2 Se incluyen rangos porque la distribuci\u00f3n var\u00eda \nentre los subconjuntos de las distintas ventanas \ntemporales.  de tres maneras: el t\u00edtulo completo, el t\u00edtulo \nparcial o la referencia con el n\u00famero, el a\u00f1o y \nla fecha.  \nRespecto a la categor\u00eda de Persona , esta \ncategor\u00eda incluye tanto las menciones a \nnombres propios como a puestos o cargos3. La \npreanotaci\u00f3n de esta categor\u00eda depende, en \nparte, de la anotaci\u00f3n autom\u00e1tica de spaC y y, \nen otra parte de listas de cargos. Por eso , \nrequiere de una revisi\u00f3n manual para evitar \nintroducir al modelo ejemplos err\u00f3neos, sobre \ntodo, en lo que se refiere a los nombres propios \nanotados de forma autom\u00e1tica.  \n7 Entrenamiento del modelo  \nSe ha elegid o la arqui tectura de spaCy por su \nflexibilidad y su eficiencia . Adem\u00e1s, ofrece \nuna forma relativamente sencilla de mane jar el \nentrenamiento de modelos permitiendo aplicar \nnuevas t\u00e9cnicas de aprendizaje profundo a la \ntarea de NERC de una forma flexible y \nsencilla.  \nEl entrenamiento de modelos en spaCy se \nbasa en la arquitectura de aprendizaje \nautom\u00e1tico \u201c thinc\u201d que implementa redes \nneuronales convolucionales profundas (Deep \nCNN ) integrando los Bloom embedding s \n(Honnibal y Mo ntani, 2017).  \nCabe se\u00f1alar que, aunque  spaCy 3.0 ha \nincluido modelo s de lenguaje de Transformers  \npara el espa\u00f1ol, pero hasta la fecha, el \ncomponente NER no est\u00e1 disponible para el \nmodelo de Transformers  espa\u00f1ol.  As\u00ed que , se \nhan utilizado  los vectores del modelo grande \nde spaCy para e l espa\u00f1ol (es_core_news_lg).  \nHemos  entrenado tres modelos  de NERC \npara comparar los resultados y valorar  las \ndiferentes opciones de entrenamiento que \nofrece la arquitectura de spaCy 3. Se han \nutilizado los mismos conjuntos de da tos de \nentrenamiento, desarrollo y evaluaci\u00f3n en el \nentrenamiento de los tres modelos.  En cuanto a \nla arquitectura del modelo, se ha utilizado los \npar\u00e1metros de entrenamiento de spaCy por \ndefecto, ya que el objetivo del estudio se centra \nen el recurso y no en la arquitectura  en s\u00ed. \n\uf0b7 NERC -Legal -1. S e trata de una \nactualizaci\u00f3n sobre el modelo original de \nspaCy  (model_update ).  \n\uf0b7 NERC -Legal -2. Se entrena un modelo \nnuevo desde cero ( blank -model ) bas\u00e1ndose \nen la arquitectura del modelo NER de \n                                                 \n3 El presente estudio no aplica sub -clases.  \nDoaa Samy\n110  spaCy, pero desde cer o sin tener en cuenta \nel modelo de NER ofrecido por defecto .  \n\uf0b7 NERC -Legal -3. Es  b\u00e1sicamente el mismo \nque el modelo anterior, pero utilizando la \narquitectura optimizada que ofrece la \n\u00faltima versi\u00f3n de spaCy. Es un modelo \nentrenado desde cero. El proceso se h a \nrealizado mediante el fichero de \nconfiguraci\u00f3n  de spaCy .  \n \nModel  NERC -\nLegal -1 NERC -\nLegal -2 NERC -\nLegal -3 \nIteraciones  20 20 10 \nDrop -out 0.1 0.1 0.1 \nBatch_size  256 256 1000 \nTabla 3. Par\u00e1metros  de entrenamiento.  \n8 Evaluaci\u00f3n  \nPara la evaluaci\u00f3n de los  tres modelos se ha \nutilizado  el mismo conjunto de evaluaci\u00f3n.  Se \nha realizado la evaluaci\u00f3n mediante el Scorer  \nde spaCy que aplica una evaluaci\u00f3n estricta.   \nLos resultados obtenidos demuestran que \nentrenar un modelo de NERC en el dominio \nlegal alcanza resultado s comparables con el \nestado de la cuesti\u00f3n.  Otro aspecto a resaltar, \nes que los altos valores de precisi\u00f3n y \ncobertura se deben a  la alta frecuencia de \nentidades y al  uso formal y normalizado del \nlenguaje en los textos jur\u00eddico, lo cual ayuda al \nmodelo a aprender estas estructuras y \ngeneralizarlas.  \n \nModelo  Precisi\u00f3n  Recall  f-score  \nNERC -Legal -1 0.9403  0.9230  0.9316  \nNERC -Legal -2 0.8942  0.8984  0.8963  \nNERC -Legal -3 0.8636  0.8449  0.8541  \nTabla  4. Evaluaci\u00f3n de los tres modelos . \n \nA continuaci\u00f3n, se presentan  los resultados \nobtenidos por cada tipo de entidad nombrada. \nLas fechas y las entidades legales han obtenido \nlos mejores resultados, mientras que los tipos \nde Persona y Organizaciones han tenido  \nvalores inferiores al resto de las categor\u00edas . \nEntidades legale s \n Precisi\u00f3n  Recall  f-score  \nNERC -Legal -1 0.9862  0.9862  0.9862  \nNERC -Legal -2 0.8855  0.8883  0.8869  \nNERC -Legal -3 0.9415  0.9538  0.9476  \nFechas  \n Precisi\u00f3n  Recall  f-score  \nNERC -Legal -1 0.9782  0.9782  0.9782  NERC -Legal -2 0.9865  0.9782  0.9836  \nNERC -Legal -3 0.9870 0.9261  0.9556  \nOrganizaciones  \n Precisi\u00f3n  Recall  f-score  \nNERC -Legal -1 0.9327  0.9505  0.9415  \nNERC -Legal -2 0.8914  0.9290  0.9098  \nNERC -Legal -3 0.8382  0.8695  0.8536  \nLugares  \n Precisi\u00f3n  Recall  f-score  \nNERC -Legal -1 0.9496  0.8650  0.9053  \nNERC -Legal -2 0.8821 0.8639  0.8728  \nNERC -Legal -3 0.8762  0.6008  0.7128  \nPersonas  \n Precisi\u00f3n  Recall  f-score  \nNERC -Legal -1 0.8426  0.7436  0.7900  \nNERC -Legal -2 0.8475  0.7883  0.8169  \nNERC -Legal -3 0.7151  0.7116  0.7133  \nTabla 5. Resultados de los tres modelos por \ncada tipo de enti dad. \nEn cuanto a la categor\u00eda Persona , la raz\u00f3n \ndetr\u00e1s de los bajos valores en  precisi\u00f3n y \ncobertura es la escasez de ejemplos en el \nconjunto de datos de entrenamiento, dada su \npoca frecuencia en comparaci\u00f3n con otros \ntipos en los textos legislativos.  Adem \u00e1s, la pre -\nanotaci\u00f3n de este tipo de entidades se llev\u00f3 a \ncabo de forma autom\u00e1tica salvo la anotaci\u00f3n \nde puestos y cargos que se han anotado a partir \nde una lista. Todo esto influye en la calidad de \nlos ejemplos y por tanto afecta negativamente \nal aprendiz aje del modelo.  \nPor otro lado, en este tipo de texto, las \nentidades de tipo Organizaci\u00f3n son frecuentes , \npero las listas empleadas en la pre -anotaci\u00f3n \nsolo incluyen entidades p\u00fablicas espa\u00f1olas. No  \nincluye entidades internacionales ni privadas. \nAdem\u00e1s, la lista del Directorio Com\u00fan es poco \nconsistente, lo cual influye el proceso de  la \npre-anotaci\u00f3n y por tanto  los resultados del \nentrenamiento.  \nPor \u00faltimo, se ha llevado a cabo una \nevaluaci\u00f3n de la anotaci\u00f3n autom\u00e1tica por \nreglas y listas compar\u00e1ndola con los  resultados \ndel modelo NERC -Legal -1. La Tabla 6 \npresenta  la comparativa entre la anotaci\u00f3n por \nreglas y la anotaci\u00f3n por  el modelo  NERC -1.  \n \n Precisi\u00f3n  Recall  f-score  \nReglas y listas  0.9666  0.871 0 0.9185 \nModelo  0.9403  0.9230  0.9316  \nTabla 6. Anotaci\u00f3n -reglas vs. Modelo.  \nEl uso de modelo supone una peque\u00f1a \nmejora (teniendo en cuenta que hay poco \nmargen de mejora dados los altos valores de la \nReconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol\n111 pre-anotaci\u00f3n) . De ah\u00ed, surge una cuesti\u00f3n : \u00bfEs \nviable  optar por modelos cuyo coste de \nentrenamiento es alto, cuando  se puede obtener \nresultados parecidos con t\u00e9cnicas menos \ncostos as como las reglas y las listas?  Sin \nembargo, la respuesta es s\u00ed, es m\u00e1s viable a \nmedio y largo plazo porque una vez entrenado , \nel modelo puede generalizar y, por tanto, \npermite mayor cobertur a y flexibilidad en \ncomparaci\u00f3n con  un anotador basado en reglas  \ny listas que requieren un alto coste de \nmantenimiento  pese a su precisi\u00f3n .  \nLa ventaja m\u00e1s destacada es que, el modelo \nofrece  mayor  eficiencia en cuanto a tiempos de \nanotaci\u00f3n, lo cual permit e mejor integraci\u00f3n en \nsoluciones que requieren tiempos de ejecuci\u00f3n  \nreducidos y , por consiguiente , permite una \nmejor escalabilidad. A dem\u00e1s , al ser entrenado  \ncon spaCy, permite beneficiarse d el abanico de \nposibilidades que ofrece esta librer\u00eda como  \nintegra r el componente en pipelines adaptados \nal dominio legal espa\u00f1ol como este  ejemplo de  \npipeline del ingl\u00e9s jur\u00eddico4. Por otro lado, este \nmodelo puede adaptarse para subdominios \ncomo la jurisprudencia o los textos \nadministrativos.  \n9 Visualizaci\u00f3n  \nPor \u00faltimo, p ara la visualizaci\u00f3n de los \nresultados del modelo, se ha utilizado \n\u201cdisplacy\u201d que ofrece spaCy. En Figura 2 , \npresentamos el resultado del texto de \nevaluaci\u00f3n anotado con el Modelo -NERC -\nLegal -1. \n10 Conclusiones  y trabajo futuro  \nEl presente trabajo  ha abordado la importancia \nde la tarea NERC en el dominio legal  \ndestacando los retos  en cuanto a los recursos y \nherramientas de PLN para el espa\u00f1ol legal.  \nEn la parte pr\u00e1ctica, se ha  presentado  una \nmetodolog\u00eda para la anotaci\u00f3n de 5 tipos \nb\u00e1sicos de entidades mediant e diferentes \nt\u00e9cnicas. La anotaci\u00f3n no pretende resolver \ntodos los retos de los diferentes tipos de \nentidades, sino que se trata de una \naproximaci\u00f3n transversal b\u00e1sica y como un \nprimer paso en un camino que requiere m\u00e1s \nesfuerzo y trabajo.  Se han entrenado  tres \nmodelos y se han presentado l os resultados de \nlos modelos entrenados  con spaCy. Los \nresultados son muy satisfactorios, ya que son \n                                                 \n4 https://spacy.io/univ erse/project/blackstone   comparables con los resultados de modelos de l \nNERC espa\u00f1ol en dominio general (Agerri y \nRigau, 2020) . La alta frecuencia  y el uso \nnormalizado de las menciones a leyes y fechas, \netc. son factores que ayudan a obtener altos \nvalores de precisi\u00f3n y cobertura.  \nPartiendo de estos  resultados \nesperanzadores , se abre camino para un \nabanico de posibilidades para  l\u00edneas futuras \ncomo  las entidades anidadas o tipolog\u00eda s \njer\u00e1rquica s donde se contemple n subtipos  de \nentidades  como art\u00edculos dentro de una ley, \netc. Asimismo, se plantea abordar los textos \nlegales hispanoamericanos, otros sub -dominios \ncomo las sentencias, los contratos. Por \u00fal timo, \notra l\u00ednea es  el tratamiento m\u00e1s completo de las \nexpresiones temporales para incluir \nexpresiones de\u00edcticas  o abordar los acr\u00f3nimos \ny abreviaturas . \nPor otro lado, se plantea organizar una tarea \nde evaluaci\u00f3n con la finalidad de contrastar \naproximacion es y asentar criterios en el \ntratamiento de los textos legales en espa\u00f1ol.  \nEn cuanto a las conclusiones generales, \nreiteramos  que el dominio legal es un \u00e1mbito \nque ofrece numerosas  oportunidades para la \nInteligencia Artificial, en general, y el PLN, en \nparticular.  \nPor \u00faltimo,  destacar la administraci\u00f3n \np\u00fablica como otro sector relacionado con el \ndominio legal donde  el PLN  puede  desempe\u00f1ar \nun papel relevante en el tratamiento de \ndocumentos  y en el desarrollo de aplicaciones \nque asistan en optimizar  procesos  internos y  \nagilizar los servicios p\u00fablicos de cara a la \nciudadan\u00eda.  \n \nFigura 2: Visua lizaci\u00f3n de la anotaci\u00f3n \nModelo -NERC . \nAgradecimientos  \nEste estudio  se ha hecho realidad gracias al \napoyo continuo del Coordinador del PlanTL , \nDavid P\u00e9rez -Fern\u00e1ndez. Asim ismo, agradezco \na Prof. Amal Shawer y a \u00d3scar Redondo -\nCarrasco por su apoyo en todo momento.  \nDoaa Samy\n112  Bibliograf\u00eda  \nAgerri, R. y G.  Rigau.  2020. Projecting  \nHeterogeneous Annotation s for Named \nEntity Recognition. E n Proceedings of \nIberlef Workshop . Co-located with 36 th \nConference of the Spanish Society for \nNatural Language Processing (SEPLN \n2020). M\u00e1laga, Spain, September 2020.  \nDisponible en: http://ceur -ws.org/Vol -\n2664/capitel_paper2.pdf   \nAndrew, J. y X. Tannie r. 2018. Automatic \nExtraction of Entities and Relation from \nLegal Documents.  En Proceedings of the \nSeventh Named Entities Workshop , \nAssociation for Computational Linguistics.  \npages 1 \u20138. Melbourne, Australia, July 20, \n2018.  \nBadji, I. 2018 . Legal enti ty extraction with \nNER Systems . Tesis (Master),  E.T.S. de \nIngenieros Inform\u00e1ticos (UPM) . \nCardellino, C., M. Teruel , L. Alemany, y S.  \nVillata . 2017. A low -cost, high -coverage \nlegal named  entity recognizer, classifier and \nlinker.  En Proceedings of the 16th edition \nof the International Conference on \nArtificial Intelligence and Law . \nChalkidis, I. , I. Androutsopoulos, y A. Michos.  \n2017 Extracting contract elements.  In \nProceedings of the 16th Int. Conf. on \nArtificial Intelligence and Law , pages 19 \u2013\n28, London, UK, 2017.  \nChalkidis I . e I. Androutsopoulos . 2017. A \ndeep learning approach to contract element \nextraction. En Proceedings of the 30th \nInternational Conference on Legal \nKnowledge and Infor mation Systems , \nLuxembourg, pp 155 \u2013164. \nCormack, G., M. R. Grossma n, B. Hedin ., y D. \nOard.  2010. Overview of the TREC 2010 \nLegal Track.  TREC.  \nDozier, C., R. Kondadadi , M. Light, A. \nVachher, S. Veeramachaneni , y R. Wudali. \n2010. Named entity recognition a nd \nresolution in legal text. En  Francesconi, E., \nMontemagni, S., Peters, W., Tiscornia, D. \n(eds.) Semantic Processing of Legal Text s. \nLNCS (LNAI), vol. 6036, pp. 27 \u201343. \nSpringer, Heidelberg (2010). \nhttps://doi.org/10.1007/978 -3-642-12837 -0 \n2.  Francesconi, E., S. Montemagni, W. Peters, y \nD. Tiscornia. 2010.  Semantic Processing of \nLegal Texts: where the language of law \nmeets the law of language (Lecture notes in \ncomputer science: lecture notes in arti ficial \nintelligence, Vol 6036) . \nGlaser, I.,  B. Waltl,  y F. Matthes . 2018. Named \nentity recognition, extraction  and link ing in \nGerman legal contracts. E n: IRIS: \nInternationales Rechtsinformatik \nSymposium , pp. 325 \u2013334. \nHonnibal, M. y I. Montani . 2017. spaCy 2: \nNatural language understanding with \nBloom embeddings, convolutional neural \nnetworks and incremental parsing . \nLandth aler, J., B. Waltl , y F. Matthes. 2016.  \nUnveiling references in legal texts \u2013 \nimplicit versus explicit network structures. \nEn IRIS: Inter nationales Rechtsinformatik \nSymposium , pp. 71 \u201378 (2016) . \nLeitner, E., G. Rehm,  y J. Moreno -Schneide r. \n2019.  Fine-grained Named Entity \nRecognition in Legal Documents . En \nMaribel Acosta, et al., (eds.), Semantic \nSystems. The Power of AI and Knowledge \nGraphs. Proceedings of the 15th \nInternational Conference \n(SEMANTiCS2019) , number 11702 . \nLecture Notes in Computer Science, pages \n272\u2013287, Karlsruhe, Germany, 9. Springer. \n10/11 September 2019.  \nMart\u00ednez -Gonz\u00e1lez, M., P. de la Fuente , y D.J. \nVicente . 2005. Reference extraction a nd \nresolution for legal texts. E n International \nConference on Pattern Recognition and \nMachine Intelligence , pages 218 -221. \nSpringer.  Nadeau, D., y  S. Sek ine. 2007. A \nsurvey of named entity recognition and \nclassification.  Lingvisticae Investigationes, \n30, 3-26. \nNavas -Loro, M.  2017.  Mining, Representation \nand Reasoning with Temporal Expressions \nin the Legal Domain.  Proceedings of the \nDoctoral Consortium, Cha llenge, Industry \nTrack, Tutorials and Posters . \nNavas -Loro, M. y V. Rodr\u00edguez -Doncel. 2020. \nAnnotador: a Temporal Tagger for Spanish, \nJournal of Intelligent and Fuzzy Systems, \nVol. 39 (2020)  \nPlanTL -IberLegal. 2019. Recursos y \naplicaciones de tecnolog\u00edas del  lenguaj e \nReconocimiento y clasificaci\u00f3n de entidades nombradas en textos legalesen espa\u00f1ol\n113 para el dominio legal en lenguas de la \nPen\u00ednsula Ib\u00e9rica. Disponible en: \nhttps://plantl.mineco.gob.es/tecnologias -\nlenguaje /comunicacion -\nformacion/eventos/Paginas/iberlegal -\n2019.aspx   \nPlanTL -IberLegal. 2020. Tarea de evaluaci\u00f3n \nde Entidades Nombradas en textos legales \n(Cancelada). Disponible en: \nhttps://temu.bsc.es/iberlegal/    \nPorta-Zamorano, J . y L. Espinosa -Anke. 2020. \nOverview of CAPITEL Shared Tasks at \nIberLEF 2020: Named Entity Recognition \nand Universal Dependencies \nParsing.  IberLEF@SEPLN . Disponible en: \nhttps://arxiv.org/p df/2011.05932.pdf   \nQi, P., Y. Zhang, Y. Zhang, J. Bolton , y C. D.  \nManning . 2020.  Stanza: A Python Natural \nLanguage Processing Toolkit for Many \nHuman Languages.  En Association for \nComputational Linguistics ( ACL) System \nDemonstrations . 2020.  \nQuaresma, P.  y T. Gon\u00e7alves . 2010. Using \nLinguistic Information and Machine \nLearning Techniques to Identify Entities \nfrom Juridical Documents.  Semantic \nProcessing of Legal Texts . \nRehm, G., J. Moreno -Schneider , J. Gracia, A. \nRevenko, V. Mireles, M. Khvalchik,  I. \nKernerman , A. Lagzdins , M. Pinnis,  A.  \nVasilevskis , E. Leitner,  J. Milde , y P. \nWei\u00dfenho rn. 2019 . Developing and \nOrchestrating a Portfolio of Natural Legal \nLanguage Processing a nd Document \nCuration Services. E n: Aletr as, N., et al. \n(eds.) Proceedings of Workshop on Natural \nLegal Language Processing  (NLLP 2019), \nco-located with NAACL 2019, \nMinneapolis, USA, 7 June 2019, pp. 55 \u201366. \nRios,  S. 2015. Lead Generation for BigLaw? \nThe Business and Ethics of Providing Free \nLegal  Tools and Information Online, 2015. \nWorking paper. Disponible en: \nhttps://law.stanford.edu/public ations/lead -\ngeneration -for-biglaw -the-business -and-\nethics -of-providing -free-legal-tools-and-\ninformation -online/  \nRodr\u00edguez -Doncel, V., M. Navas -Loro, E. \nMontiel -Ponsoda , y P. Casanovas . 2018. \nSpanish Legislation as Linked \nData.  TERECOM@JURIX . Roy, A. 2021.  Recent Tre nds in Named Entity \nRecognition (NER).   ArXiv, \nabs/2101.11420 . \nSamy, D., J. Arenas -Garc\u00eda , y D. P\u00e9rez -\nFern\u00e1ndez . 2020 . Legal -ES: A Set of Large \nScale Resources for Spanish Legal Text \nProcessing.  En Samy, D. et al. (eds.) \nProceedings of Workshop on Language \nTechnologies in Government and Public \nAdministration  (LT4Gov  2020), co -located \nwith LREC  2020, Marseille, France . \nSekine, S. 2004. Named Entity: History and \nFuture.  Disponible en:  \nhttp://cs. nyu. edu/sekine/papers/  \nWaltl, B. y  R. Vogl.  2018.  Explainable \nArtificial Intelligence \u2013 the New Frontier in \nLegal Informatics . En  Jusletter IT \n22. February 2018 . \n \nDoaa Samy\n114Un enfoque sem\u0013 antico en la selecci\u0013 on de\ncaracter\u0013 \u0010sticas basadas en l\u0013 exico para la detecci\u0013 on de\nemociones\nA semantic approach in the lexicon-based feature selection for\nemotion detection\nHarold Gonz\u0013 alez-Guerra1, Alfredo Sim\u0013 on-Cuevas1, Jos\u0013 e M. Perea-Ortega2,\nJos\u0013 e A. Olivas3\n1Universidad Tecnol\u0013 ogica de La Habana Jos\u0013 e Antonio Echeverr\u0013 \u0010a, La Habana, Cuba\n2Universidad de Extremadura, Badajoz, Espa~ na\n3Universidad de Castilla-La Mancha, Ciudad Real, Espa~ na\nhgonzalez@ceis.cujae.edu.cu, asimon@ceis.cujae.edu.cu, jmperea@unex.es,\njoseangel.olivas@uclm.es\nResumen: La detecci\u0013 on de emociones es una tarea del an\u0013 alisis de sentimientos que\ntrata la extracci\u0013 on y el an\u0013 alisis de las emociones en textos. Reconocer emociones\nimpl\u0013 \u0010citas es uno de los principales desaf\u0013 \u0010os en enfoques basados en palabras claves o\nlexicones. Este trabajo presenta un enfoque h\u0013 \u0010brido de detecci\u0013 on de emociones, que\ncombina la selecci\u0013 on de caracter\u0013 \u0010sticas relevantes de emoci\u0013 on basada en un lexic\u0013 on,\ncon un enfoque cl\u0013 asico de aprendizaje para determinar la emoci\u0013 on. El proceso de\nselecci\u0013 on de caracter\u0013 \u0010sticas propuesto se centra en capturar el signi\fcado emocio-\nnal del texto mediante el c\u0013 alculo de la relaci\u0013 on sem\u0013 antica entre su contenido y el\nvocabulario del lexic\u0013 on, con el objetivo de incrementar el reconocimiento de emocio-\nnes impl\u0013 \u0010citas. La soluci\u0013 on propuesta fue evaluada en la clasi\fcaci\u0013 on de emociones\nen tweets en espa~ nol incluidos en el corpus AIT, con diferentes alternativas para\ncomputar la relaci\u0013 on sem\u0013 antica y varios algoritmos de clasi\fcaci\u0013 on, obteni\u0013 endose\nresultados muy prometedores.\nPalabras clave: Detecci\u0013 on de emociones, selecci\u0013 on de caracter\u0013 \u0010sticas, medidas\nsem\u0013 anticas.\nAbstract: Emotion detection is a task of sentiment analysis that deals with the\nextraction and analysis of emotions in texts. Recognizing implicit emotions is one of\nthe main challenges in keyword or lexicon-based approaches. This paper presents a\nhybrid emotion detection approach, which combines lexicon-based emotion-relevant\nfeature selection with a classical learning-based approach to determine the emotion.\nThe proposed feature selection process focuses on capturing the emotional meaning\nof the text by computing the semantic relationship between its content and the\nlexicon vocabulary, with the goal of increasing implicit emotion recognition. The\nproposed solution was evaluated on the classi\fcation of emotions in Spanish tweets\nincluded in the AIT corpus, with di\u000berent alternatives to compute the semantic\nrelation and several classi\fcation algorithms, obtaining very promising results.\nKeywords: Emotion detection, feature selection, semantic measures.\n1 Introducci\u0013 on\nLas emociones son rasgos b\u0013 asicos que nos ca-\nracterizan como humanos y que in\ruyen en\nlas acciones, los pensamientos y, por supues-\nto, en nuestra forma de comunicarnos. A pe-\nsar de no considerarse entidades propiamen-te ling\u007f u\u0013 \u0010sticas, las emociones se expresan a\ntrav\u0013 es del lenguaje por lo que, desde hace va-\nrios a~ nos, han sido estudiadas por investiga-\ndores de diferentes disciplinas como la psi-\ncolog\u0013 \u0010a, la sociolog\u0013 \u0010a, la medicina, o la in-\nform\u0013 atica (Ekman, 1992).\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 115-126\nrecibido 09-05-2021 revisado 08-06-2021 aceptado 10-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-10\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalEn los \u0013 ultimos a~ nos, la comunidad cient\u0013 \u0010\f-\nca relacionada con el PLN ha mostrado espe-\ncial inter\u0013 es en la detecci\u0013 on de emociones en\nconversaciones textuales, ya que su investiga-\nci\u0013 on puede encontrar varias aplicaciones en\nel mundo digital actual (Strapparava, 2016;\nMohammad et al., 2018; Chatterjee et al.,\n2019). Por ejemplo, en el \u0013 ambito de la aten-\nci\u0013 on al cliente, redes sociales como Twitter\nest\u0013 an ganando protagonismo y los clientes es-\nperan respuestas r\u0013 apidas. En caso de un gran\n\rujo de tweets el tiempo de respuesta aumen-\nta, por lo que si los tweets se pudieran priori-\nzar seg\u0013 un su contenido emocional, la satisfac-\nci\u0013 on del cliente seguramente aumentar\u0013 \u0010a. Por\notro lado, en esta era de la mensajer\u0013 \u0010a ins-\ntant\u0013 anea, dado que los usuarios est\u0013 an cons-\ntantemente enviando mensajes, podr\u0013 \u0010a ocu-\nrrir que enviaran mensajes de enfado inapro-\npiados a otros usuarios. En estos casos, si se\nutilizara una aplicaci\u0013 on de detecci\u0013 on de emo-\nciones, se podr\u0013 \u0010an tomar medidas como, por\nejemplo, mostrar una advertencia al usuario\nantes de enviar el mensaje.\nLa tarea de la detecci\u0013 on de emociones en\ntextos presenta un importante desaf\u0013 \u0010o al ca-\nrecer de la ayuda que, en cualquier comu-\nnicaci\u0013 on visual, proporcionan las expresiones\nfaciales y las modulaciones de voz. Adem\u0013 as,\nel reto de detectar las emociones en un tex-\nto se ve agravado por la di\fcultad de com-\nprender algunos aspectos relacionados con la\ncomunicaci\u0013 on, como pueden ser el contexto,\nel sarcasmo, la ambig\u007f uedad del propio len-\nguaje natural, o la creciente jerga que est\u0013 a\nprovocando el uso masivo de aplicaciones de\nmensajer\u0013 \u0010a instant\u0013 anea (Shivhare y Khetha-\nwat, 2012; Khan et al., 2016). En la literatura\nexisten varios enfoques para abordar esta ta-\nrea. Uno de los m\u0013 as utilizados es el basado en\nreglas (Strapparava y Mihalcea, 2008; Syko-\nra et al., 2013), que trata de explotar el uso\nde palabras clave y su coocurrencia con otras\npalabras que tienen asociado un determinado\nvalor emocional o afectivo. Ese valor asocia-\ndo a determinadas palabras del lenguaje sue-\nle establecerse a partir de diferentes recursos\nl\u0013 exicos existentes, algunos muy conocidos co-\nmo WordNet-A\u000bect o SentiWordNet. Por esa\nraz\u0013 on, a los m\u0013 etodos que siguen este enfoque\ntambi\u0013 en se les conoce como m\u0013 etodos basados\nen palabras clave o en lexic\u0013 on.\nM\u0013 as recientemente, se reportan acerca-\nmientos donde se combinan el uso de lexic\u0013 on\ncon modelos de aprendizaje autom\u0013 atico, loscuales son reconocidos dentro de los enfoques\nh\u0013 \u0010bridos (Alswaidan y Menai, 2020). Dispo-\nner de un lexic\u0013 on o palabras clave con de-\nterminado valor emocional o afectivo per-\nmite determinar con alta e\fcacia el esta-\ndo emocional de los textos, sin embargo, se\nreconocen ciertas limitaciones en los enfo-\nques basados en lexic\u0013 on que est\u0013 an relacio-\nnadas con la cobertura del contenido textual\n(Hemmatian y Sohrabi, 2019; Chakriswaran\net al., 2019; Acheampong, Wenyu, y Nunoo-\nMensah, 2020). El procesamiento del voca-\nbulario en un lexic\u0013 on desde una perspecti-\nva sem\u0013 antica permite incrementar su cober-\ntura para identi\fcar caracter\u0013 \u0010sticas emocio-\nnales de un texto, sin necesidad de aumen-\ntar el tama~ no de dicho vocabulario. Sin em-\nbargo, esta orientaci\u0013 on sem\u0013 antica del uso de\nlos lexicones, por ejemplo, mediante c\u0013 ompu-\nto de relaciones sem\u0013 anticas subyacente entre\nlas palabras de emoci\u0013 on y el contenido tex-\ntual a procesar, ha sido muy poco explotado.\nPrecisamente, estudios realizados sobre solu-\nciones reportadas en la literatura se~ nalan co-\nmo limitaciones el no uso de caracter\u0013 \u0010sticas\nsem\u0013 anticas en la detecci\u0013 on de emociones en\ntextos (Alswaidan y Menai, 2020).\nEn este trabajo se propone un m\u0013 etodo pa-\nra la detecci\u0013 on de emociones en textos cor-\ntos escritos en espa~ nol (tweets), que se ba-\nsa en la sem\u0013 antica de las palabras clave del\ntexto. Nuestra hip\u0013 otesis se centra en que si\nlogramos determinar un buen grado de a\f-\nnidad sem\u0013 antica del texto del tweet con ca-\nda emoci\u0013 on a detectar (relaci\u0013 on sem\u0013 antica\ntweet-emoci\u0013 on), conseguiremos una selecci\u0013 on\nde caracter\u0013 \u0010sticas m\u0013 as enfocada a las emocio-\nnes, lo que supondr\u0013 \u0010a un mejor proceso de\naprendizaje en el algoritmo encargado de la\nclasi\fcaci\u0013 on. Por tanto, la principal novedad\nque aporta este trabajo est\u0013 a relacionada con\nel enfoque propuesto para capturar el gra-\ndo de a\fnidad sem\u0013 antica entre el contenido\ndel tweet y cada uno de los vocabularios que\ncaracterizan a cada emoci\u0013 on, lo que permite\nuna selecci\u0013 on de caracter\u0013 \u0010sticas de la opini\u0013 on\ndonde el an\u0013 alisis de relevancia est\u0013 e m\u0013 as orien-\ntado al dominio del problema (emociones),\nen lugar de a aspectos estad\u0013 \u0010sticos dentro del\ncontenido, como com\u0013 unmente se suele adop-\ntar. Como principal contribuci\u0013 on destaca el\nenfoque sem\u0013 antico propuesto con el que se\ntrata el vocabulario del lexic\u0013 on y su relaci\u0013 on\ncon el contenido del texto. En general, las so-\nluciones basadas en conocimiento (lexicones)\nHarold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 A. Olivas\n116hacen uso de este tipo de recursos como si\nfueran una lista de palabras sin signi\fcado.\nNuestra contribuci\u0013 on aporta una soluci\u0013 on al\ndesaf\u0013 \u0010o que representa extender la cobertura\nde ese conocimiento en el procesamiento del\ncontenido textual a clasi\fcar y lograr identi-\n\fcar caracter\u0013 \u0010sticas del texto m\u0013 as relevantes\npara predecir las clases de emociones.\nEl resto del art\u0013 \u0010culo se estructura de la\nsiguiente manera: los principales trabajos re-\nlacionados se exponen en la Secci\u0013 on 2; la es-\ntructura general y los principios del m\u0013 etodo\npropuesto se describen en la Secci\u0013 on 3; los da-\ntos experimentales, la evaluaci\u0013 on, y el an\u0013 alisis\nde los resultados obtenidos se muestran en la\nSecci\u0013 on 4; en la Secci\u0013 on 5 se resume el trabajo\ny se propone una direcci\u0013 on futura.\n2 Trabajos relacionados\nEn la literatura se pueden encontrar diferen-\ntes enfoques para abordar la tarea de la de-\ntecci\u0013 on de emociones, siendo incluso clasi\fca-\ndos en varias categor\u0013 \u0010as por distintos investi-\ngadores (Cambria, 2016; Gupta et al., 2017;\nSailunaz et al., 2018; Acheampong, Wenyu,\ny Nunoo-Mensah, 2020). La clasi\fcaci\u0013 on m\u0013 as\ngen\u0013 erica es la propuesta por Cambria (2016),\nque establece dos categor\u0013 \u0010as: los basados en\nreglas y los basados en aprendizaje autom\u0013 ati-\nco. Al primero pertenecen aquellos m\u0013 etodos\nque hacen uso de recursos l\u0013 exicos tales como\nlexicones, bolsas de palabras e incluso onto-\nlog\u0013 \u0010as. El segundo engloba aquellos m\u0013 etodos\nque aplican algoritmos de aprendizaje basa-\ndos en caracter\u0013 \u0010sticas ling\u007f u\u0013 \u0010sticas.\nEn los enfoques basados en reglas al-\ngunos m\u0013 etodos explotan el uso de palabras\nclave en los textos y su coocurrencia con otras\npalabras clave con valor emocional/afectivo\nexpl\u0013 \u0010cito (Strapparava y Mihalcea, 2008). Pa-\nra ello, se utilizan varios recursos l\u0013 exicos co-\nmo WordNet-A\u000bect (Strapparava y Valitut-\nti, 2004) y SentiWordNet (Esuli y Sebastia-\nni, 2006) para el idioma ingl\u0013 es. En cuanto\na la disponibilidad de este tipo de recursos\nen otros idiomas, el n\u0013 umero es muy limitado.\nPara el espa~ nol, destacan el Spanish Emo-\ntion Lexicon (SEL) (Sidorov et al., 2012) y\nel Improved Spanish Opinion Lexicon (iSOL)\n(Molina-Gonz\u0013 alez et al., 2013). Otros m\u0013 eto-\ndos basados en este enfoque tambi\u0013 en tratan\nde explotar la sintaxis de las palabras cla-\nve mediante el uso de etiquetadores POS y,\naunque suelen obtener buena precisi\u0013 on, su-\nfren de una baja cobertura (recall ) porquemuchos textos no contienen palabras afecti-\nvas a pesar de transmitir emociones (Gupta\net al., 2017).\nLa mayor\u0013 \u0010a de los trabajos presentados en\nel conocido foro de evaluaci\u0013 on SemEval du-\nrante los \u0013 ultimos a~ nos (Mohammad et al.,\n2018; Chatterjee et al., 2019), utilizan l\u0013 exi-\ncos de afecto y concluyen que son una fuente\nde informaci\u0013 on muy valiosa porque propor-\ncionan informaci\u0013 on previa sobre el tipo de\nemoci\u0013 on asociada a cada palabra del texto.\nAdem\u0013 as, en WASSA, otro foro de evaluaci\u0013 on\nrelacionado con estas tareas, tambi\u0013 en se de-\nmostr\u0013 o que el uso de caracter\u0013 \u0010sticas proceden-\ntes de l\u0013 exicos de afecto es \u0013 util para tareas de\nminer\u0013 \u0010a de emociones (Mohammad y Bravo-\nMarquez, 2017). En este sentido, Bandhakavi\net al. (2017) estudian el problema de la selec-\nci\u0013 on de caracter\u0013 \u0010sticas de emoci\u0013 on utilizan-\ndo recursos l\u0013 exicos espec\u0013 \u0010\fcos del dominio y\nl\u0013 exicos de emoci\u0013 on de prop\u0013 osito general. A\u0013 un\nas\u0013 \u0010, existen varias revisiones recientes que po-\nnen de mani\festo las limitaciones asociadas\nal uso de la sem\u0013 antica que se encuentran en\nestos procesos para detectar o clasi\fcar emo-\nciones en los textos (Acheampong, Wenyu,\ny Nunoo-Mensah, 2020; Alswaidan y Menai,\n2020).\nEn los enfoques basados en aprendi-\nzaje autom\u0013 atico (Machine Learning, ML)\nla mayor\u0013 \u0010a de los m\u0013 etodos se basan en la ex-\ntracci\u0013 on de caracter\u0013 \u0010sticas, tales como la pre-\nsencia de n-gramas frecuentes, la negaci\u0013 on,\nla puntuaci\u0013 on, los emoticonos, los hashtags,\netc., para as\u0013 \u0010 formar una representaci\u0013 on de\ncaracter\u0013 \u0010sticas del texto que luego se utili-\nza como entrada por los clasi\fcadores para\npredecir la salida (Canales y Mart\u0013 \u0010nez-Barco,\n2014; Liew y Turtle, 2016). Estos m\u0013 etodos\nsuelen requerir un arduo proceso de selecci\u0013 on\nde caracter\u0013 \u0010sticas y no logran una alta cober-\ntura (recall ) debido a las diversas formas de\nrepresentar las emociones.\nDentro de los enfoques ML se han reporta-\ndo m\u0013 etodos basados en redes neuronales pro-\nfundas, que han tenido un \u0013 exito considera-\nble en diversas tareas aplicadas a texto, ha-\nbla e imagen. Variaciones de las redes neu-\nronales recurrentes (RNN), como la LSTM\ny la BiLSTM, han sido e\fcaces para mo-\ndelar informaci\u0013 on secuencial. Por su parte,\nla introducci\u0013 on de las redes neuronales con-\nvolucionales (CNN) en el dominio del texto\nha demostrado su capacidad para clasi\fcar\ncaracter\u0013 \u0010sticas de las emociones (Mundra et\nUn enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones\n117Figura 1: Flujo de trabajo de la soluci\u0013 on propuesta.\nal., 2017). A pesar de los buenos resultados,\nlos enfoques basados en aprendizaje profun-\ndo presentan algunas desventajas respecto a\nmodelos tradicionales de aprendizaje, lo que\nen algunos contextos pueden llevar a descar-\ntar su elecci\u0013 on. Algunas de ellas son: deman-\nda de mucha mayor cantidad de datos bien\netiquetados para entrenamiento, mayores exi-\ngencias en la capacidad de c\u0013 omputo, y por\n\u0013 ultimo, su naturaleza de funcionamiento ti-\npo \\caja negra\", que no permite una adecua-\nda comprensi\u0013 on del proceso de aprendizaje\n(Kowsari et al., 2019).\nEl gran \u0013 exito de los m\u0013 etodos tradiciona-\nles basados en aprendizaje autom\u0013 atico, jun-\nto con el hecho de que recursos l\u0013 exicos como\nSEL aportan informaci\u0013 on muy valiosa sobre\nel tipo de emoci\u0013 on asociada a cada palabra,\nnos ha motivado a probar una combinaci\u0013 on\nde ambos enfoques para detectar emociones.\nPartiendo del uso de SEL, proponemos calcu-\nlar la relaci\u0013 on sem\u0013 antica texto-emoci\u0013 on para\nas\u0013 \u0010 lograr una selecci\u0013 on de caracter\u0013 \u0010sticas m\u0013 as\nenfocada a las emociones, lo que se supone\nredundar\u0013 a en un mejor aprendizaje del algo-\nritmo de clasi\fcaci\u0013 on.\n3 Soluci\u0013 on propuesta\nEsta secci\u0013 on describe la soluci\u0013 on propuesta en\neste trabajo. Las principales etapas del pro-\nceso de desarrollo se describen en la Figura\n1. En primer lugar, los textos de opini\u0013 on son\npreprocesados y normalizados. A continua-\nci\u0013 on, se lleva a cabo el proceso de determinar\nla orientaci\u0013 on emocional de los textos, que\nconsiste en establecer un grado de a\fnidad\n(relaci\u0013 on sem\u0013 antica) de cada texto con cada\nvocabulario asociado a cada emoci\u0013 on del le-\nxic\u0013 on SEL. Finalmente, se realiza el proceso\nde selecci\u0013 on de caracter\u0013 \u0010sticas, tomando co-\nmo base el grado de a\fnidad obtenido en la\nfase anterior, y que permite generar los vec-\ntores caracter\u0013 \u0010sticos que ser\u0013 an utilizados por\nel algoritmo de aprendizaje supervisado.3.1 Preprocesamiento\nLa naturaleza no estructurada de los textos,\nm\u0013 as a\u0013 un en el caso de los tweets y textos\nde opini\u0013 on, requiri\u0013 o la ejecuci\u0013 on de tareas de\npreprocesamiento o normalizaci\u0013 on. Estas ta-\nreas consistieron en tokenizar los tweets usan-\ndo NLTK TweetTokenizer1, convertir todo el\ntexto a min\u0013 uscula y eliminar stop words, sig-\nnos de puntuaci\u0013 on y caracteres raros. No fue\nnecesario ning\u0013 un proceso de traducci\u0013 on pre-\nvia porque dicha herramienta soporta traba-\njar con textos en castellano.\n3.2 Extracci\u0013 on de caracter\u0013 \u0010sticas\norientadas a la emoci\u0013 on\n3.2.1 Determinar la orientaci\u0013 on\nemocional\nEn esta tarea se determina la a\fnidad del\ncontenido de los tweets a cada una de las\nemociones incluidas en el lexic\u0013 on de emocio-\nnes, desde el punto de vista sem\u0013 antico. Es-\npec\u0013 \u0010\fcamente, el lexic\u0013 on SEL (caso de estu-\ndio) contiene un total de 2.036 palabras en\nespa~ nol que se organizan en 6 emociones di-\nferentes: enojo ( anger ), miedo (fear ), triste-\nza (sadness ), alegr\u0013 \u0010a ( joy), sorpresa (surpri-\nse), y repulsi\u0013 on (disgust ). Por tanto, cada\nemoci\u0013 on est\u0013 a representada a trav\u0013 es de un vo-\ncabulario de t\u0013 erminos, donde cada t\u0013 ermino\nest\u0013 a etiquetado con un valor PFA (Probabi-\nlity Factor of A\u000bective ) (Sidorov et al., 2012).\nEjemplos de palabras que se incluyen en cada\ncategor\u0013 \u0010a son: amistad, bienestar, carcajada,\ncelebrar... (alegr\u0013 \u0010a), enfadado, enfurecer, en-\nrabiar, ira... (enojo), espeluznante, fobia, te-\nmor, terror... (miedo), asqueroso, detestable,\ninmundo, repugnante... (repulsi\u0013 on), asombro-\nso, incre\u0013 \u0010ble, maravilloso, perplejo... (sorpre-\nsa), infeliz, luto, pena, p\u0013 erdida... (tristeza).\nLa Figura 2 muestra el n\u0013 umero de palabras\npor emoci\u0013 on que contiene SEL. Al determinar\n1http://www.nltk.org/api/nltk.tokenize.\nhtml\nHarold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 A. Olivas\n118Figura 2: Estad\u0013 \u0010sticas del lexic\u0013 on SEL (# pa-\nlabras por emoci\u0013 on).\nesta a\fnidad (orientaci\u0013 on) es posible realizar\nun proceso de selecci\u0013 on de caracter\u0013 \u0010sticas de\nlos tweets m\u0013 as enfocado en las emociones, lo\ncual no es posible en los modelos de relevan-\ncia basados en frecuencia, tales como TF o\nTF-IDF.\nLa determinaci\u0013 on de la a\fnidad emocional\nse basa en el an\u0013 alisis de la relaci\u0013 on sem\u0013 antica\nexistente entre el contenido de los textos de\nopini\u0013 on y cada emoci\u0013 on del lexic\u0013 on, tomando\ncomo referencia su vocabulario. En este tra-\nbajo se proponen evaluar dos variantes para\nel c\u0013 omputo de esa relaci\u0013 on sem\u0013 antica opini\u0013 on-\nemoci\u0013 on: (1) basada en el corpus de opinio-\nnes, y (2) basada en un recurso sem\u0013 antico\nexterno. Entre las medidas basadas en cor-\npus que han sido aplicadas en el \u0013 ambito del\nan\u0013 alisis de sentimientos se encuentran Point-\nwise Mutual Information (PMI) y chi-square,\nentre otras, siendo la primera la adoptada\nen esta propuesta. En el caso de la varian-\nte que se apoya en recursos sem\u0013 anticos, se\nutilizar\u0013 an m\u0013 etricas basadas en WordNet (Pe-\ndersen, Patwardhan, y Michelizzi, 2004).\nLa relaci\u0013 on sem\u0013 antica entre los tweets y las\nemociones del lexic\u0013 on se representa mediante\nuna matriz de a\fnidad, la cual es construi-\nda para cada tweet u opini\u0013 on que se vaya\na procesar. En esta matriz, las \flas identi-\n\fcan cada una de las emociones del lexic\u0013 on\nSEL, y las columnas las palabras de la opi-\nni\u0013 on que constituyen caracter\u0013 \u0010sticas candida-\ntas. La orientaci\u0013 on emocional O(wi;Ej) entre\ncada palabra wide la opini\u0013 on y la cantidad\nde t\u0013 erminos del vocabulario de la emoci\u0013 on\nSEL presentes en la opini\u0013 on Ej, es calcula-\nda seg\u0013 un la Ecuaci\u0013 on 1, y se almacena en la\nintersecci\u0013 on t\u0013 ermino-emoci\u0013 on de la matriz dea\fnidad. Esta matriz constituye la base para\nla siguiente fase (selecci\u0013 on de caracter\u0013 \u0010sticas\nbasada en la emoci\u0013 on) donde son selecciona-\ndas las caracter\u0013 \u0010sticas m\u0013 as relevantes (las que\nm\u0013 as relaci\u0013 on tengan con las emociones).\nO(wi;Ej) =P\nvi;j2Ejrelsem(wi;vi;j)\njEjj(1)\nSeg\u0013 un se muestra en la Figura 2, en SEL\nexiste un desbalance signi\fcativo en el ta-\nma~ no de los vocabularios de cada emoci\u0013 on,\nlo que se traduce tambi\u0013 en en una mayor dis-\npersi\u0013 on de valores PFA en las palabras que\nlos componen, interpret\u0013 andose este tambi\u0013 en\ncomo un valor de relevancia para la emoci\u0013 on.\nTodo ello indica que en un acercamiento ba-\nsado en la relaci\u0013 on sem\u0013 antica entre opini\u0013 on-\nemoci\u0013 on, como el que se propone, se podr\u0013 \u0010a\nestar determinando esa relaci\u0013 on sem\u0013 antica a\npartir de rangos de valores muy diferentes de\nPFA y, derivado de ello, originarse afectacio-\nnes en el c\u0013 alculo de O(wi;Ej). Las emocio-\nnes que m\u0013 as palabras tengan en su vocabula-\nrio tendr\u0013 an m\u0013 as in\ruencia en el c\u0013 alculo de la\norientaci\u0013 on de la opini\u0013 on, dado que hay m\u0013 as\nprobabilidad de que los valores resultantes de\neste c\u0013 alculo sean superiores. Una manera de\nreducir el efecto de este problema es mediante\nun mecanismo de poda o \fltro del vocabula-\nrio de las emociones a partir de un umbral de\nrelevancia m\u0013 \u0010nima PFAmin.\n3.2.2 Pointwise Mutual Information\n(PMI)\nLa medida PMI se deriva de la teor\u0013 \u0010a de la\ninformaci\u0013 on y proporciona una v\u0013 \u0010a formal de\nmodelar la informaci\u0013 on mutua entre las ca-\nracter\u0013 \u0010sticas (ej. palabras de la opini\u0013 on) y las\nclases (ej. emociones a clasi\fcar) (Aggarwal\ny Zhai, 2013). En el \u0013 ambito de la clasi\fca-\nci\u0013 on de textos, este tipo de medidas cons-\ntituye una alternativa para evaluar la rele-\nvancia de caracter\u0013 \u0010sticas potenciales del tex-\nto con respecto a clases espec\u0013 \u0010\fcas. Esta in-\nformaci\u0013 on mutua puntual entre la palabra\nwiy la clase de emoci\u0013 on Ejse de\fne so-\nbre la base de la coocurrencia entre las pa-\nlabrasvi;jdel vocabulario de la emoci\u0013 on Ej\ny la palabra wide la opini\u0013 on, y se calcu-\nlar\u0013 \u0010a seg\u0013 un la Ecuaci\u0013 on 2, siendo en este caso\nrelsem(wi;vi;j) =PMI (wi;vi;j). Este plan-\nteamiento se basa en la suposici\u0013 on de que\nlas palabras afectivas (incluidas en el lexic\u0013 on)\nque coocurren con frecuencia con las palabras\nde la opini\u0013 on tienden a estar sem\u0013 anticamente\nUn enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones\n119relacionadas (Agrawal y An, 2012).\nPMI (wi;vi;j) =coocurrencia(w i;vi;j)\nocurr (wi)\u0003ocurr (vi;j)\n(2)\n3.2.3 M\u0013 etricas basadas en WordNet\n(WN)\nLas medidas de relaci\u0013 on sem\u0013 antica basadas\nen WordNet constituyen otra de las alterna-\ntivas viables para demostrar la hip\u0013 otesis que\nha motivado este trabajo. A diferencia del\nPMI, donde el c\u0013 omputo se sustenta en el pro-\ncesamiento solo del contenido de los textos,\neste tipo de medidas explotan la estructura\ntopol\u0013 ogica que forman los synsets y sus rela-\nciones en WordNet, como recurso sem\u0013 antico\nexterno. La relaci\u0013 on sem\u0013 antica es un concep-\nto que abarca la relaci\u0013 on entre dos palabras,\ntanto por similitud entre signi\fcados, como\npor v\u0013 \u0010nculos contextuales (ej. funcional, aso-\nciaci\u0013 on, parte de, etc.) (Budanitsky y Hirst,\n2006). Diversas m\u0013 etricas han sido de\fnidas\npara medir ambos tipos de relaciones, cuyas\nimplementaciones se ofrecen en el paquete de\nsoftware libre WordNet::Similarity2(Peder-\nsen, Patwardhan, y Michelizzi, 2004). La dis-\nponibilidad de estas m\u0013 etricas constituye una\nventaja, dado que se podr\u0013 \u0010an evaluar diferen-\ntes alternativas individuales y tambi\u0013 en com-\nbinarlas. No obstante, se reconoce que el uso\nde estas m\u0013 etricas tiene como limitante que las\npalabras que se comparan est\u0013 en incluidas en\nalg\u0013 un synset en WordNet. En esta propuesta\nse ha adoptado para su evaluaci\u0013 on la m\u0013 etri-\nca JCN (Jiang y Conrath, 1997), siendo en-\ntoncesrelsem(wi;vi;j) =JCN (wi;vi;j). Re-\nsultados reportados en Budanitsky y Hirst\n(2006) muestran que, en un an\u0013 alisis experi-\nmental comparativo, la medida JCN es una\nde las que mejor se comporta.\n3.2.4 Poda del vocabulario de las\nemociones\nEl valor PFA que posee cada palabra en el le-\nxic\u0013 on SEL sugiere cierta imprecisi\u0013 on y vague-\ndad subyacente en la relaci\u0013 on palabra-clase\nde emoci\u0013 on, cuyo grado o pertenencia a la cla-\nse se expresa a trav\u0013 es de dicho valor de PFA.\nEsta imprecisi\u0013 on se puede propagar hacia el\nresultado \fnal afectando su calidad. En este\nsentido, se propone un mecanismo de poda o\n\fltrado del vocabulario del lexic\u0013 on que per-\nmite descartar el subconjunto de vocabulario\n2http://wn-similarity.sourceforge.netque mayor imprecisi\u0013 on posee, aspecto no con-\nsiderado en otras soluciones reportadas.\nEl mecanismo propuesto de poda o \fltra-\ndo del vocabulario de las emociones a partir\ndelPFAminpermite reducir la dispersi\u0013 on en\nlos valores de PFA de las palabras a conside-\nrar en el c\u0013 omputo de la a\fnidad emocional\nde las opiniones, y llevar a cabo una selec-\nci\u0013 on de caracter\u0013 \u0010sticas guiada por un vocabu-\nlario de mayor relevancia para las emociones.\nDe esta forma, tambi\u0013 en se reduce la carga\ncomputacional de este proceso, dado que se\ndisminuir\u0013 \u0010a la cantidad de interacciones en el\nc\u0013 omputo de las relaciones sem\u0013 anticas, aspec-\nto muy relevante cuando se utilizan medidas\nbasadas en WordNet. En este sentido, se de\f-\nnieron y evaluaron tres criterios para obtener\nel umbral para la poda del vocabulario de las\nemociones en el lexic\u0013 on:\n1. Tomar como PFAminel valor m\u0013 \u0010nimo\nde la media de PFA calculada para cada\nuna de las emociones del SEL (enfoque\noptimista).\n2. Tomar como PFAminel valor m\u0013 aximo\nde la media de PFA calculada para ca-\nda una de las emociones del SEL (m\u0013 as\nrestrictivo que el anterior, enfoque pe-\nsimista).\n3. Tomar como PFAminel valor resultan-\nte de aplicar un operador de agregaci\u0013 on\ncompensatorio (enfoque fuzzy de la se-\nlecci\u0013 on del umbral de poda), que permi-\nta obtener un \u0013 unico valor representati-\nvo de los valores medios de PFA obteni-\ndos de los vocabularios de cada emoci\u0013 on\n(ei), como es el caso del operador pro-\npuesto por Zimmermann y Zysno (1980)\n(Ecuaci\u0013 on 3), donde \res el grado de\ncompensaci\u0013 on proporcionado y se podr\u0013 \u0010a\ncalcular seg\u0013 un Ecuaci\u0013 on 4 (Yager y Ry-\nbalov, 1998), en la que T(e1;e2;:::;en) es\nuna funci\u0013 on t-norma y se podr\u0013 \u0010a calcular\nseg\u0013 un la Ecuaci\u0013 on 5.\nPFAmin(e1;e2;:::;en) =\n nY\ni=1ei!1\u0000\r\n\u0003 \n1\u0000nY\ni=1(1\u0000ei)!\r\n(3)\n\r=T(e1;e2;:::;en)\nT(e1;:::;en) +T(1\u0000e1;:::;1\u0000en)(4)\nHarold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 A. Olivas\n120T(e1;e2;:::;en) =nY\ni=1ei (5)\nLuego de obtenido el umbral de poda\n(PFAmin), se \fltra el vocabulario de cada\nemoci\u0013 on seleccionando solo aquellas palabras\nque tengan un PFAi\u0015PFAmin, y se lleva a\ncabo el an\u0013 alisis de la relaci\u0013 on sem\u0013 antica entre\ncada una de las opiniones y las emociones del\nlexic\u0013 on (podadas).\n3.3 Selecci\u0013 on de caracter\u0013 \u0010sticas\nEn esta fase se seleccionan las caracter\u0013 \u0010sticas\nde las opiniones, tomando como base la ma-\ntriz de a\fnidad construida en la fase anterior\ny con el objetivo de generar los vectores ca-\nracter\u0013 \u0010sticos que ser\u0013 an utilizados por los algo-\nritmos de clasi\fcaci\u0013 on supervisados. Inicial-\nmente, a partir la matriz de a\fnidad de cada\nopini\u0013 on ( o), se obtiene un grado de a\fnidad\nsem\u0013 antica (SAD, Semantic A\u000enity Degree )\ncon respecto a cada una de las emociones\n(Ej), seg\u0013 un la Ecuaci\u0013 on 6. Luego, se seleccio-\nna la clase de emoci\u0013 on con la cual la opini\u0013 on\ntiene mayor a\fnidad, siendo esta la que arro-\nje un valor m\u0013 as alto de SAD(o,E j). La emo-\nci\u0013 on sobre la que se exprese mayor a\fnidad\nser\u0013 a la que determine qu\u0013 e palabras afectivas\n(de las candidatas representadas) caracteri-\nzan la opini\u0013 on (tweet).\nSAD (o;Ej) =Pn\ni=1=w i2oO(wi;Ej)\njoj(6)\nA partir de identi\fcar la emoci\u0013 on Ejcon\nmayor SAD(o,E j), se \fltra la matriz de a\fni-\ndadde la opini\u0013 on que se est\u0013 a procesando, eli-\nminando las \flas correspondientes a las emo-\nciones restantes. El vector caracter\u0013 \u0010stico de\nla opini\u0013 on se construir\u0013 a con las palabras de\nla opini\u0013 on que posean O(wi;Ej)>0. En es-\nte enfoque se logra un proceso de selecci\u0013 on\nde caracter\u0013 \u0010sticas donde la evaluaci\u0013 on de su\nrelevancia tiene m\u0013 as en cuenta la sem\u0013 antica\nalrededor de las emociones, a diferencia de\notras propuestas donde el peso fundamental\nde la relevancia est\u0013 a en enfoques basados en\nla frecuencia (Plaza-del Arco et al., 2020).\nEsta propuesta de selecci\u0013 on propicia la re-\nducci\u0013 on de caracter\u0013 \u0010sticas redundantes en la\nconstrucci\u0013 on de los vectores de las opiniones,\ndado que las caracter\u0013 \u0010sticas se determinan\npor una emoci\u0013 on en particular. Tambi\u0013 en per-\nmite reducir las caracter\u0013 \u0010sticas no informati-vas (o poco informativas) y que no tengan un\nalto poder discriminatorio, debido a la irrele-\nvancia o redundancia con respecto a la clase\n(una misma caracter\u0013 \u0010stica es relevante en di-\nferentes grados para varias clases). Todo ello\npropiciar\u0013 \u0010a la mejora de los resultados del re-\nconocimiento de emociones basado en un le-\nxic\u0013 on.\nLuego de identi\fcadas las caracter\u0013 \u0010sticas\nde cada opini\u0013 on, se procede a la \u0013 ultima ta-\nrea para la construcci\u0013 on del vector de las opi-\nniones referente al pesado de las caracter\u0013 \u0010sti-\ncas. El valor de peso de cada una de las ca-\nracter\u0013 \u0010sticas debe expresar un grado de re-\nlevancia de la misma, y como parte de este\ntrabajo se estudiaron algunas alternativas de\npeso, tales como O(wi;Ej), como relevancia\ndirecta de la caracter\u0013 \u0010stica wien funci\u0013 on de\nla emoci\u0013 on que determin\u0013 o su selecci\u0013 on; y la\nfrecuencia de ocurrencia de esa palabra ca-\nracter\u0013 \u0010sticas dentro del corpus de opiniones,\ndado que es la alternativa m\u0013 as com\u0013 un en solu-\nciones de clasi\fcaci\u0013 on supervisada de textos.\nSin embargo, resultados parciales experimen-\ntales arrojaron que un modelo binario de re-\npresentaci\u0013 on del vector obtuvo mejores resul-\ntados que esas dos alternativas, con\frm\u0013 ando-\nse lo reportado en Agarwal y Mittal (2016),\ndonde se plantea que, en el \u0013 ambito del an\u0013 alisis\nde sentimientos, este tipo de modelos ofrece\nmejores resultados que el basado en la fre-\ncuencia. En este sentido, se adopt\u0013 o el mode-\nlo de representaci\u0013 on binario para construir el\nvector caracter\u0013 \u0010stico de las opiniones, tenien-\ndo en cuenta que el peso de una caracter\u0013 \u0010stica\nwitiene valor 1 si el O(wi;Ej)6= 0, y valor 0\nen caso contrario.\n4 Resultados experimentales\nLa soluci\u0013 on propuesta fue evaluada en la\nclasi\fcaci\u0013 on de cuatro emociones (anger,\nfear,joy, sadness ) en tweets escritos en es-\npa~ nol, utilizando el corpus AIT empleado en\nSemEval-2018 Task 1: A\u000bect in Tweets (sub-\ntarea EI-oc ). A partir de ese corpus, se toma-\nron aleatoriamente 800 tweets por cada una\nde las emociones (3.200 tweets en total) pa-\nra conformar el corpus de prueba. La Tabla\n1 muestra la estad\u0013 \u0010stica del corpus de tweets\nutilizado durante la experimentaci\u0013 on.\nLos experimentos fueron realizados utili-\nzando los algoritmos de clasi\fcaci\u0013 on Support\nVector Machine (SVM), Logistic Regression\n(LR), Multilayer Perceptron (MLP) y Nai-\nve Bayes (NB), implementados en la librer\u0013 \u0010a\nUn enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones\n121Emoci\u0013\non#tw\neets #palabrasmedia pal.\npor\ntweet\nenojo 800 11.183 13,87\nmiedo 800 11.156 13,94\nalegr\n\u0013 \u0010a 800 10.091 12,61\ntristeza 800 10.967 13,70\nTotal 3.200 43.317 -\nTabla 1: Estad\u0013 \u0010stica del corpus de tweets uti-\nlizado durante la experimentaci\u0013 on.\nscikit-learn3de Python. El 80 % (2.560) de\nlos tweets incluidos en el corpus de prueba se\nutilizaron para el entrenamiento de los algo-\nritmos de clasi\fcaci\u0013 on y el 20 % (640) para\ntest. Los experimentos realizados pretendie-\nron, por un lado, evaluar el comportamiento\nde las medidas de relaci\u0013 on sem\u0013 antica seleccio-\nnadas y, por otro, evaluar el comportamiento\nde la poda del lexic\u0013 on por relevancia del vo-\ncabulario (en sus varias alternativas).\nEn la evaluaci\u0013 on de los resultados se uti-\nlizaron las m\u0013 etricas de Precision (P), Recall\n(R), y F-score (F1), y se de\fnieron los resul-\ntados de la selecci\u0013 on de caracter\u0013 \u0010sticas basada\nen la frecuencia de t\u0013 erminos (TF) como ca-\nso base. En las Tablas 2-5, se muestran los\nresultados obtenidos con cada uno de los cla-\nsi\fcadores, SVM, LR, MLP, y NB, respecti-\nvamente.\nLos resultados obtenidos muestran que el\nclasi\fcador que mejores resultados obtiene es\nSVM, al igual que los experimentos reporta-\ndos en Plaza-del Arco et al. (2020). La Figu-\nra 3 muestra una comparativa de los mejores\nresultados F1 obtenidos por cada clasi\fca-\ndor, independientemente de la medida utili-\nzada y el criterio empleado para la obtenci\u0013 on\ndelPFAmin. En todas las pruebas realizadas,\nrespecto a la alternativa m\u0013 as com\u0013 unmente\nusada, se obtienen mejores resultados cuan-\ndo el proceso de selecci\u0013 on de caracter\u0013 \u0010sticas\nest\u0013 a guiado solo por el an\u0013 alisis de relevancia\na partir de la a\fnidad sem\u0013 antica de la opi-\nni\u0013 on respecto a cada una de las emociones,\nutilizando como referencia el vocabulario que\nlas representa en el lexic\u0013 on SEL, lo que po-\nne en evidencia la contribuci\u0013 on del enfoque\npropuesto.\nEntre las m\u0013 etricas empleadas para el\nc\u0013 omputo de la relaci\u0013 on sem\u0013 antica destaca\nPMI con respecto a la m\u0013 etrica JCN basada\nen WordNet, independientemente del clasi\f-\ncador y del tratamiento del lexic\u0013 on (Figura\n4). Una de las posibles causas del compor-\n3http://scikit-learn.org\nFigura 3: Comparativa de los mejores resul-\ntados F1 entre los clasi\fcadores utilizados.\nFigura 4: Comparativa de los mejores resul-\ntados F1 entre las medidas PMI y WN.\ntamiento de la m\u0013 etrica basada en WordNet\nes la ausencia de palabras del lexic\u0013 on o los\ntweets en ese recurso externo, lo cual es m\u0013 as\nprobable en su versi\u0013 on en espa~ nol. No obs-\ntante, considerando que los resultados en ge-\nneral son bastante cercanos, podr\u0013 \u0010a resultar\nuna alternativa prometedora combinar estas\ny otras m\u0013 etricas en la evaluaci\u0013 on de la a\fni-\ndad emocional de las opiniones.\nCabe destacar tambi\u0013 en en estos resultados\nla contribuci\u0013 on que representa trabajar con\nun vocabulario de emociones que tenga un\nmayor equilibrio en cuanto a grados de rele-\nvancia de los t\u0013 erminos que integre, as\u0013 \u0010 como\nsu cantidad de t\u0013 erminos. La exactitud (me-\ndida Acc) alcanzada por todos los algoritmos\nde clasi\fcaci\u0013 on fue superior cuando los voca-\nbularios del lexic\u0013 on fueron podados a partir\ndel umbral de relevancia m\u0013 \u0010nimo ( PFAmin),\nindependientemente de los criterios de selec-\nci\u0013 on de este umbral y de la medida utilizada\n(Figura 5). El aporte al mejoramiento de cada\nHarold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 A. Olivas\n122Lexic\u0013 on Medidasenojo miedo alegr\n\u0013 \u0010a tristezaAccP R\nF1 P R\nF1 P R\nF1 P R\nF1Po\ndadoOptimistaPMI .62 .76 .68 .74 .70\n.72 .80 .69 .74 .65 .62\n.63 .69\nWN .36 .66\n.42 .60 .\n40 .48 .63 .51\n.56 .51 .38\n.43 .52\nPesimistaPMI .60 .73\n.66 .67 .\n61 .64 .72 .70 .70 .65 .60\n.62 .66\nWN .40 .68\n.50 .61 .\n50 .60 .64 .47\n.54 .50 .39\n.44 .52\nFuzzyPMI .62 .77\n.69 .74 .58 .65 .65 .66\n.66 .69 .66\n.68 .67\nWN .38 .65\n.48 .52 .\n37 .43 .65 .46\n.53 .52 .45\n.48 .51\nSinp\nodarPMI .51 .62\n.55 .53 .\n50 .51 .57 .51\n.53 .48 .49\n.48 .52\nWN .45 .38\n.41 .48 .\n42 .44 .55 .44\n.49 .36 .61\n.45 .46\nTF(caso\nbase) .33 .35\n.33 .35 .\n30 .32 .35 .36\n.35 .34 .33\n.33 .35\nTabla 2: Resultados obtenidos con el clasi\fcador SVM.\nLexic\u0013 on Medidasenojo miedo alegr\n\u0013 \u0010a tristezaAccP R\nF1 P R\nF1 P R\nF1 P R\nF1Po\ndadoOptimistaPMI .53 .73 .61 .64 .65 .64 .83 .66 .73 .63 .51\n.57 .64\nWN .38 .69\n.49 .57 .\n37 .45 .54 .53\n.54 .45 .26\n.33 .46\nPesimistaPMI .50 .71\n.59 .65 .60 .62 .71 .68 .69 .71 .52\n.60 .63\nWN .37 .65\n.47 .60 .\n45 .51 .51 .45\n.48 .44 .28\n.34 .48\nFuzzyPMI .53 .75\n.62 .61 .\n57 .59 .69 .61\n.64 .69 .54\n.61 .62\nWN .38 .73\n.50 .54 .\n33 .40 .47 .43\n.45 .48 .28\n.35 .44\nSinp\nodarPMI .49 .65\n.55 .53 .\n52 .52 .54 .57\n.55 .68 .43\n.53 .55\nWN .34 .61\n.44 .51 .\n30 .37 .51 .46\n.48 .45 .33\n.38 .44\nTF(caso\nbase) .33 .32\n.32 .32 .\n31 .31 .37 .34\n.37 .34 .30\n.32 .34\nTabla 3: Resultados obtenidos con el clasi\fcador LR .\nLexic\u0013 on Medidasenojo miedo alegr\n\u0013 \u0010a tristezaAccP R\nF1 P R\nF1 P R\nF1 P R\nF1Po\ndadoOptimistaPMI .55 .51\n.53 .56.60 .58 .62 .53 .57 .45 .52 .48 .54\nWN .38 .62\n.47 .53 .39\n.45 .55 .\n48 .51 .44 .36\n.39 .46\nPesimistaPMI .55 .59 .57 .53 .54\n.53 .59 .63 .61 .58 .49 .53 .56\nWN .38 .64 .48 .55 .43\n.48 .57 .\n44 .48 .41 .34\n.37 .47\nFuzzyPMI .56 .56 .56 .50 .42\n.46 .57 .63 .60 .47 .50\n.49 .53\nWN .38 .62\n.47 .57 .40\n.47 .50 .\n47 .48 .51 .40\n.44 .49\nSinp\nodarPMI .49 .48\n.48 .47 .34\n.40 .51 .\n45 .47 .41 .42\n.41 .47\nWN .37 .59\n.46 .45 .37\n.40 .57 .\n41 .47 .47 .38\n.42 .45\nTF(caso\nbase) .26 .25\n.28 .28 .29\n.28 .35 .\n31 .33 .33 .32\n.32 .31\nTabla 4: Resultados obtenidos con el clasi\fcador MLP.\nLexic\u0013 on Medidasenojo miedo alegr\n\u0013 \u0010a tristezaAccP R\nF1 P R\nF1 P R\nF1 P R\nF1Po\ndadoOptimistaPMI .43 .63 .52 .51 .\n29 .37 .48 .34 .40 .37 .47 .41 .45\nWN .34 .81 .48 .40 .\n24 .32 .45 .23\n.34 .34 .30\n.32 .38\nPesimistaPMI .42 .67\n.51 .41 .\n25 .31 .40 .26\n.31 .38 .43\n.41 .40\nWN .35 .81 .49 .30 .\n31 .30 .55 .20 .\n26 .36 .25\n.29 .39\nFuzzyPMI .42 .65\n.51 .42 .\n23 .30 .46 .41 .43 .41 .41 .41 .43\nWN .32 .80\n.46 .60 .32\n.41 .41 .12\n.18 .43 .31 .36 .38\nSinp\nodarPMI .32 .44\n.32 .31 .\n21 .21 .23 .21\n.21 .32 .34\n.32 .31\nWN .32 .75\n.44 .49 .\n29 .36 .37 .30\n.33 .33 .25\n.28 .37\nTF(caso\nbase) .29 .28\n.28 .28 .\n29 .28 .34 .30\n.32 .32 .28\n.29 .31\nTabla 5: Resultados obtenidos con el clasi\fcador NB.\nuna de las m\u0013 etricas de evaluaci\u0013 on de los crite-\nrios para la obtenci\u0013 on del PFAminevaluados\nmuestra cierta dispersi\u0013 on, con in\ruencia del\nclasi\fcador y la m\u0013 etrica de relaci\u0013 on sem\u0013 antica\nque se emplee. En general, los mejores resul-\ntados se concentran en los criterios optimistayfuzzy, siendo el primero con el que se obtie-\nnen los mejores resultados combinando SVM\ny PMI.\nPor \u0013 ultimo, cabe rese~ nar que, aunque los\nexperimentos no se hayan realizado con el\n100 % del corpus (63,4 % del total de tweets\nUn enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones\n123Figura 5: Comparativa de los mejores resul-\ntados F1 entre aplicar poda y sin podar.\nde entrenamiento), los resultados obtenidos\nse consideran muy prometedores dado que,\nsi establecemos una comparaci\u0013 on con el me-\njor resultado de exactitud (Acc) obtenido en\nPlaza-del Arco et al. (2020) con el lexic\u0013 on\nSEL y el clasi\fcador SVM, que fue de 0,76,\nnuestra soluci\u0013 on alcanza 0,69 con ese mismo\nclasi\fcador pero con un 46 % menos de tweets\nde entrenamiento.\n5 Conclusiones y trabajo futuro\nEn este trabajo se aborda la tarea de la de-\ntecci\u0013 on de emociones en textos mediante un\nenfoque h\u0013 \u0010brido, que combina un proceso de\nselecci\u0013 on de caracter\u0013 \u0010sticas basado en lexic\u0013 on\ncon un enfoque cl\u0013 asico de aprendizaje au-\ntom\u0013 atico. La novedad de la propuesta radica\nen el enfoque sem\u0013 antico propuesto para cap-\nturar el grado de a\fnidad entre el contenido\nde opini\u0013 on y el vocabulario que caracteriza\ncada emoci\u0013 on, de manera que se consigue una\nselecci\u0013 on de caracter\u0013 \u0010sticas en las opiniones\nque es m\u0013 as adecuada para la tarea de clasi\f-\ncaci\u0013 on objetivo.\nPara evaluar la soluci\u0013 on propuesta se lle-\nvaron a cabo diversos experimentos utilizan-\ndo el corpus AIT de emociones en tweets en\nespa~ nol. En los experimentos se evaluaron di-\nferentes alternativas para calcular el grado de\na\fnidad sem\u0013 antica texto-emoci\u0013 on y se testea-\nron varios algoritmos de clasi\fcaci\u0013 on. Como\nprincipal conclusi\u0013 on, se considera que el en-\nfoque propuesto es bastante prometedor a la\nhora de realizar una selecci\u0013 on de caracter\u0013 \u0010sti-\ncas m\u0013 as adecuada, es decir, m\u0013 as enfocada en\nlas emociones. Los buenos resultados obteni-\ndos as\u0013 \u0010 lo avalan, habida cuenta que en la\nexperimentaci\u0013 on se utilizaron un 37 % menosde tweets del total disponible en el corpus.\nComo parte del trabajo futuro, adem\u0013 as de\naplicar el m\u0013 etodo propuesto a diferentes cor-\npus de tweets, evaluar otras medidas de rela-\nci\u0013 on sem\u0013 antica y otros operadores de agrega-\nci\u0013 on, pretendemos ampliar este enfoque para\nentrenar modelos de clasi\fcaci\u0013 on que tengan\nen cuentan otra informaci\u0013 on ling\u007f u\u0013 \u0010stica del\ncontexto de la opini\u0013 on como, por ejemplo, la\nnegaci\u0013 on.\nAgradecimientos\nEste trabajo ha sido parcialmente \fnancia-\ndo por el Fondo Europeo de Desarrollo Re-\ngional (FEDER), la Junta de Extremadura\n(GR18135), y el Ministerio de Ciencia, Inno-\nvaci\u0013 on y Universidades de Espa~ na, a trav\u0013 es\ndel proyecto SAFER (PID2019-104735RB-\nC42).\nBibliograf\u0013 \u0010a\nAcheampong, F. A., C. Wenyu, y H. Nunoo-\nMensah. 2020. Text-based emotion de-\ntection: Advances, challenges, and oppor-\ntunities. Engineering Reports , 2(7):1{24.\nAgarwal, B. y N. Mittal. 2016. Prominent\nFeature Extraction for Sentiment Analy-\nsis. Prominent Feature Extraction for\nSentiment Analysis, i:21{45.\nAggarwal, C. C. y C. X. Zhai. 2013. Mining\ntext data, volumen 9781461432234. Sprin-\nger.\nAgrawal, A. y A. An. 2012. Unsupervi-\nsed emotion detection from text using se-\nmantic and syntactic relations. En Pro-\nceedings - 2012 IEEE/WIC/ACM Inter-\nnational Conference on Web Intelligence,\nWI 2012, p\u0013 aginas 346{353.\nAlswaidan, N. y M. E. B. Menai. 2020. A\nsurvey of state-of-the-art approaches for\nemotion recognition in text. Knowled-\nge and Information Systems, 62(8):2937{\n2987.\nBandhakavi, A., N. Wiratunga, D. Padma-\nnabhan, y S. Massie. 2017. Lexicon based\nfeature extraction for emotion text clas-\nsi\fcation. Pattern Recognition Letters ,\n93:133{142.\nBudanitsky, A. y G. Hirst. 2006. Evaluating\nWordNet-based Measures of Lexical Se-\nmantic Relatedness. Computational Lin-\nguistics, 32(1):13{47.\nHarold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 A. Olivas\n124Cambria, E. 2016. A\u000bective Computing and\nSentiment Analysis. IEEE Intelligent Sys-\ntems, 31(2):102{107.\nCanales, L. y P. Mart\u0013 \u0010nez-Barco. 2014. Emo-\ntion detection from text: A survey. En\nProceedings of the Workshop on Natu-\nral Language Processing in the 5th Infor-\nmation Systems Research Working Days\n(JISIC), p\u0013 aginas 37{43. Association for\nComputational Linguistics.\nChakriswaran, P., D. R. Vincent, K. Srini-\nvasan, V. Sharma, C.-Y. Chang, y D. G.\nReina. 2019. Emotion ai-driven sentiment\nanalysis: A survey, future research direc-\ntions, and open issues. Applied Sciences,\n9(24).\nChatterjee, A., K. N. Narahari, M. Joshi, y\nP. Agrawal. 2019. SemEval-2019 Task\n3: EmoContext Contextual Emotion De-\ntection in Text. En Proceedings of the\n13th International Workshop on Semantic\nEvaluation (SemEval-2019), p\u0013 aginas 39{\n48. Association for Computational Lin-\nguistics.\nEkman, P. 1992. An Argument for Basic\nEmotions. Cognition and Emotion , 6(3-\n4):169{200.\nEsuli, A. y F. Sebastiani. 2006. SentiWord-\nNet: A Publicly Available Lexical Resour-\nce for Opinion Mining. Proceedings of\nLREC 2006, p\u0013 aginas 417{422.\nGupta, U., A. Chatterjee, R. Srikanth, y\nP. Agrawal. 2017. A sentiment-and-\nsemantics-based approach for emotion de-\ntection in textual conversations. arXiv.\nHemmatian, F. y M. K. Sohrabi. 2019.\nA survey on classi\fcation techniques for\nopinion mining and sentiment analysis.\nArti\fcial Intelligence Review, 52(3):1495{\n1545.\nJiang, J. J. y D. W. Conrath. 1997. Seman-\ntic similarity based on corpus statistics\nand lexical taxonomy. En Proceedings of\nthe 10th Research on Computational Lin-\nguistics International Conference, p\u0013 aginas\n19{33. The Association for Computational\nLinguistics and Chinese Language Proces-\nsing (ACLCLP), Agosto.\nKhan, M. T., M. Durrani, A. Ali, I. Inayat,\nS. Khalid, y K. H. Khan. 2016. Sentimentanalysis and the complex natural langua-\nge.Complex Adaptive Systems Modeling,\n4(1):2.\nKowsari, K., K. Jafari Meimandi, M. Hei-\ndarysafa, S. Mendu, L. Barnes, y\nD. Brown. 2019. Text classi\fcation\nalgorithms: A survey. Information, 10(4).\nLiew, J. S. Y. y H. R. Turtle. 2016. Exploring\n\fne-grained emotion detection in tweets.\nEnProceedings of the NAACL Student Re-\nsearch Workshop, p\u0013 aginas 73{80. Associa-\ntion for Computational Linguistics.\nMohammad, S. M. y F. Bravo-Marquez.\n2017. Wassa-2017 shared task on emotion\nintensity.\nMohammad, S. M., F. Bravo-Marquez,\nM. Salameh, y S. Kiritchenko. 2018.\nSemEval-2018 Task 1: A\u000bect in Tweets.\nEnProceedings of International Workshop\non Semantic Evaluation (SemEval-2018),\nNew Orleans, LA, USA.\nMolina-Gonz\u0013 alez, M. D., E. Mart\u0013 \u0010nez-C\u0013 ama-\nra, M. T. Mart\u0013 \u0010n-Valdivia, y J. M. Perea-\nOrtega. 2013. Semantic orientation\nfor polarity classi\fcation in Spanish re-\nviews. Expert Systems with Applications ,\n40(18):7250{7257.\nMundra, S., A. Sen, M. Sinha, S. Mannars-\nwamy, S. Dandapat, y S. Roy. 2017. Fine-\ngrained emotion detection in contact cen-\nter chat utterances. En Lecture Notes\nin Computer Science (including subseries\nLecture Notes in Arti\fcial Intelligence and\nLecture Notes in Bioinformatics) , volu-\nmen 10235 LNAI, p\u0013 aginas 337{349.\nPedersen, T., S. Patwardhan, y J. Michelizzi.\n2004. WordNet::Similarity - Measuring\nthe relatedness of concepts. En Procee-\ndings of the National Conference on Arti-\n\fcial Intelligence , p\u0013 aginas 1024{1025.\nPlaza-del Arco, F. M., M. T. Mart\u0013 \u0010n-\nValdivia, L. A. Ure~ na-L\u0013 opez, y R. Mit-\nkov. 2020. Improved emotion recognition\nin Spanish social media through incorpo-\nration of lexical knowledge. Future Gene-\nration Computer Systems, 110:1000{1008.\nSailunaz, K., M. Dhaliwal, J. Rokne, y\nR. Alhajj. 2018. Emotion detection from\ntext and speech: a survey. Social Network\nAnalysis and Mining, 8(1).\nUn enfoque sem\u00e1ntico en la seleccion de caracter\u00edsticas basadas en l\u00e9xico para la detecci\u00f3n de emociones\n125Shivhare, S. N. y S. Khethawat. 2012. Emo-\ntion detection from text.\nSidorov, G., S. Miranda-Jim\u0013 enez, F. Viveros-\nJim\u0013 enez, A. Gelbukh, N. Castro-S\u0013 anchez,\nF. Vel\u0013 asquez, I. D\u0013 \u0010az-Rangel, S. Su\u0013 arez-\nGuerra, A. Trevi~ no, y J. Gordon. 2012.\nEmpirical study of opinion mining in Spa-\nnish tweets. LNAI 7629 , p\u0013 aginas 1{14.\nStrapparava, C. y A. Valitutti. 2004.\nWordNet-A\u000bect: an a\u000bective extension of\nWordNet. Proceedings of the 4th Interna-\ntional Conference on Language Resources\nand Evaluation, p\u0013 aginas 1083{1086.\nStrapparava, C. 2016. Emotions and NLP:\nFuture Directions. En Proceedings of\nNAACL-HLT 2016, p\u0013 agina 180. Associa-\ntion for Computational Linguistics.\nStrapparava, C. y R. Mihalcea. 2008. Lear-\nning to identify emotions in text. Procee-\ndings of the 2008 ACM symposium on Ap-\nplied computing - SAC '08 , p\u0013 aginas 1556{\n1560.\nSykora, M. D., T. W. Jackson, A. O'Brien,\ny S. Elayan. 2013. Emotive ontology:\nExtracting \fne-grained emotions from ter-\nse, informal messages. En Proceedings of\nthe IADIS International Conference In-\ntelligent Systems and Agents 2013, ISA\n2013, Proceedings of the IADIS European\nConference on Data Mining 2013, ECDM\n2013.\nYager, R. R. y A. Rybalov. 1998. Full rein-\nforcement operators in aggregation tech-\nniques. IEEE Transactions on Systems,\nMan, and Cybernetics, Part B: Cyberne-\ntics, 28(6):757{769.\nZimmermann, H. J. y P. Zysno. 1980. La-\ntent connectives in human decision ma-\nking. Fuzzy Sets and Systems, 4(1):37{51.\nHarold Gonz\u00e1lez-Guerra, Alfredo Sim\u00f3n-Cuevas, Jos\u00e9 M. Perea-Ortega, Jos\u00e9 A. Olivas\n126Inducci\u0013 on autom\u0013 atica de una taxonom\u0013 \u0010a multiling\u007f ue\nde marcadores discursivos: primeros resultados en\ncastellano, ingl\u0013 es, franc\u0013 es, alem\u0013 an y catal\u0013 an\nAutomatic induction of a multilingual taxonomy of discourse\nmarkers: \frst results in Spanish, English, French, German\nand Catalan\nRogelio Nazar\nInstituto de Literatura y Ciencias del Lenguaje\nPonti\fcia Universidad Cat\u0013 olica de Valpara\u0013 \u0010so, Chile\nrogelio.nazar@pucv.cl\nResumen: Este art\u0013 \u0010culo presenta una propuesta metodol\u0013 ogica para la inducci\u0013 on\nautom\u0013 atica de una taxonom\u0013 \u0010a multiling\u007f ue de marcadores discursivos, que en el caso\ndel castellano corresponden a unidades tales como sin embargo, por lo tanto, por\nun lado, etc. Se propone primeramente un m\u0013 etodo para separar estas unidades del\nresto del vocabulario por medio del c\u0013 alculo de su cantidad de informaci\u0013 on, seguido\nde su agrupaci\u0013 on en categor\u0013 \u0010as funcionales mediante un corpus paralelo. Finalmente,\nesta categorizaci\u0013 on se utiliza como base para la obtenci\u0013 on y clasi\fcaci\u0013 on de nuevas\nunidades. Adem\u0013 as del m\u0013 etodo, se describen los primeros resultados, consistentes en\nuna base de datos que actualmente supera ya los 2.600 marcadores.\nPalabras clave: inducci\u0013 on de taxonom\u0013 \u0010as, marcadores discursivos, part\u0013 \u0010culas del\ndiscurso, lexicograf\u0013 \u0010a computacional.\nAbstract: This paper presents a methodological proposal por the automatic induc-\ntion of a multilingual taxonomy of discourse markers which, in the case of English,\ncorrespond to units such as however, therefore, by the way , etc. First, a method is\nproposed to separate such units from the rest of the vocabulary using a measure of\ninformation, followed by a method to group them using a parallel corpus. Finally,\nthis categorization is used as the basis for the extraction and classi\fcation of new\nunits. Apart from the method, the \frst results are described, which consist of a\ndatabase that currently surpasses 2600 units.\nKeywords: taxonomy induction, discourse markers, discurse particles, computatio-\nnal lexicography.\n1 Introducci\u0013 on\nAunque no es un tema nuevo en ling\u007f u\u0013 \u0010stica,\nlos marcadores del discurso (MD) han esta-\ndo en el foco de inter\u0013 es de la teor\u0013 \u0010a parti-\ncularmente en las \u0013 ultimas d\u0013 ecadas (Fraser,\n1999; Mart\u0013 \u0010n Zorraquino y Portol\u0013 es, 1999;\nPons Border\u0013 \u0010a, 2001, entre otros). Los MD\nson part\u0013 \u0010culas discursivas que cumplen una\namplia variedad de funciones, pero que no\nforman parte del contenido proposicional de\nlos segmentos a los que afectan. Los ejem-\nplos de estas part\u0013 \u0010culas pueden ser muy di-\nversos, como se explicar\u0013 a m\u0013 as adelante, pero\nentre los m\u0013 as frecuentes encontramos los co-\nnectores aditivos (adem\u0013 as, tambi\u0013 en, etc.), los\ncontraargumentativos ( sin embargo, no obs-tante, etc.), los causales (por este motivo, por\nlo tanto , etc.) los reformulativos (es decir, en\notras palabras, etc.), entre un variado n\u0013 umero\nde otras categor\u0013 \u0010as.\nLa gran mayor\u0013 \u0010a de las investigaciones que\nse han realizado sobre este tema han sido en\nel \u0013 ambito de la ling\u007f u\u0013 \u0010stica te\u0013 orica y con un en-\nfoque cualitativo (cf. Secci\u0013 on 2). Los m\u0013 etodos\ndominantes hasta ahora han sido la intros-\npecci\u0013 on y, en menor medida, el trabajo con\ncorpus. Sin embargo, en este \u0013 ultimo caso, el\ncorpus es utilizado como herramienta explo-\nratoria, mediante examen visual de l\u0013 \u0010neas de\nconcordancia de uno o algunos MD.\nComparativamente, son pocos los inten-\ntos de afrontar este tema con las herramien-\ntas del procesamiento del lenguaje natural\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 127-138\nrecibido 09-05-2021 revisado 07-06-2021 aceptado 09-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-11\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural(PLN), tanto en castellano como en otras len-\nguas. La ventaja m\u0013 as evidente del PLN sobre\nlos m\u0013 etodos cualitativos de investigaci\u0013 on tra-\ndicionales en ling\u007f u\u0013 \u0010stica en este caso particu-\nlar es la posibilidad de obtener un inventa-\nrio masivo de marcadores. Esto es porque, a\npesar de corresponder a la categor\u0013 \u0010a de uni-\ndades funcionales dentro del vocabulario, no\ncorresponden a una lista cerrada, como la de\nlas preposiciones, y no existe hasta la fecha\npor tanto un cat\u0013 alogo completo de los MD.\nTampoco se ha producido hasta ahora total\nacuerdo entre los especialistas acerca de c\u0013 omo\nse pueden clasi\fcar, ya que los enfoques y\nteor\u0013 \u0010as son muy diversos y a menudo incom-\npatibles.\nEl presente art\u0013 \u0010culo pretende hacer un\naporte precisamente en la l\u0013 \u0010nea del inventa-\nriado y la taxonomizaci\u0013 on de los MD existen-\ntes en distintas lenguas. Ofrece una descrip-\nci\u0013 on de los resultados preliminares de un pro-\nyecto de investigaci\u0013 on en curso en el campo\nde los MD mediante herramientas de PLN.\nSe trata de una propuesta metodol\u0013 ogica pa-\nra la inducci\u0013 on autom\u0013 atica de una taxonom\u0013 \u0010a\nmultiling\u007f ue de MD a partir de corpus parale-\nlos, utilizando algoritmos exclusivamente es-\ntad\u0013 \u0010sticos. En su estado actual, los resultados\ndel proyecto consisten en una base de datos\nde 2.636 MD clasi\fcados en 70 categor\u0013 \u0010as fun-\ncionales en castellano, ingl\u0013 es, franc\u0013 es, alem\u0013 an\ny catal\u0013 an. Estos datos se encuentran disponi-\nbles para descarga desde la web del proyecto1,\ny van aumentando en cantidad en la medida\nen que se contin\u0013 ua con el desarrollo.\nLa metodolog\u0013 \u0010a del proyecto incluye una\ncadena de procesamiento en la que en ning\u0013 un\nmomento existe intervenci\u0013 on humana. Los re-\nsultados que se ofrecen, sin embargo, han si-\ndo ya revisados por un grupo de ling\u007f uistas,\nhablantes nativos en cada caso, para corregir\nposibles errores. La tasa de error en los re-\nsultados de las diferentes lenguas no super\u0013 o\nel 5 % con excepci\u0013 on del alem\u0013 an, en donde la\ntasa de error lleg\u0013 o al 16 %.\nEl m\u0013 etodo propuesto tampoco utiliza re-\ncursos ling\u007f u\u0013 \u0010sticos externos tales como voca-\nbularios, diccionarios o etiquetadores morfo-\nsint\u0013 acticos. El \u0013 unico material con el que tra-\nbaja es un corpus paralelo de gran tama~ no, lo\nque facilita en gran medida la reproducci\u0013 on\nde los experimentos en otras lenguas. Como\nrecurso propiamente ling\u007f u\u0013 \u0010stico, se utiliza la\n1http://www.tecling.com/dismarkterminolog\u0013 \u0010a de Mart\u0013 \u0010n Zorraquino y Portol\u0013 es\n(1999) para los nombres de las categor\u0013 \u0010as de\nlos MD en castellano, pero esta funciona a\nmodo de metadato externo al propio m\u0013 etodo\ny es igual de v\u0013 alida para las distintas lenguas.\nLas unidades que el algoritmo inicialmen-\nte elige y segmenta como candidatos a MD\nconsisten en palabras o secuencias de pala-\nbras consideradas con bajo nivel de infor-\nmaci\u0013 on seg\u0013 un un c\u0013 alculo de entrop\u0013 \u0010a que se\ncorrelaciona con el signi\fcado l\u0013 exico. Seg\u0013 un\neste c\u0013 alculo, mientras mayor especi\fcidad\nsem\u0013 antica tiene una palabra, como es el caso\nde aquellas palabras con una denominaci\u0013 on\nprecisa (aerosol, marxismo, trastorno obse-\nsivo compulsivo, etc.), mayor es su cantidad\nde informaci\u0013 on. Las palabras funcionales o\ngramemas, tales como las preposiciones, pero\ntambi\u0013 en los MD, obtienen seg\u0013 un este coe\f-\nciente una cantidad de informaci\u0013 on m\u0013 as baja.\nUna vez obtenidos los listados de candida-\ntos a MD, estos son organizados en categor\u0013 \u0010as\nfuncionales a partir del corpus paralelo, ex-\nplotando su similitud en cuanto a equivalen-\ntes en la otra lengua. Esta organizaci\u0013 on en\ngrupos funcionales se convierte en una cla-\nsi\fcaci\u0013 on que se realiza, en un primer nivel,\ncon la ayuda de la taxonom\u0013 \u0010a ofrecida por\nMart\u0013 \u0010n Zorraquino y Portol\u0013 es (1999), de la\nque se obtienen los nombres para etiquetar\nlos grupos gracias a los ejemplos que se in-\ncluyen. Esta taxonom\u0013 \u0010a, sin embargo, es a su\nvez subdividida y enriquecida con subcate-\ngor\u0013 \u0010as que resultan emergentes del corpus, y\nque no pueden ser etiquetadas porque exce-\nden el nivel de granularidad de dicho recurso.\nAdem\u0013 as del inter\u0013 es que puede tener la pro-\npuesta en tanto metodolog\u0013 \u0010a, existe tambi\u0013 en\nel que ofrece el resultado mismo. Esto es por-\nque en la bibliograf\u0013 \u0010a sobre el tema es fre-\ncuente encontrar diferentes taxonom\u0013 \u0010as y lis-\ntados de ejemplos, pero en la mayor parte de\nlos casos estos alcanzan unos pocos centena-\nres, cuando las unidades utilizadas realmente\ncomo MD en la lengua se cuentan por mi-\nles. La base de datos que resulta puede te-\nner diversas aplicaciones. Por un lado, puede\ninformar los m\u0013 etodos y las conclusiones de\nestudios en ling\u007f u\u0013 \u0010stica te\u0013 orica sobre el tema.\nPor otro lado, puede ser utilizado tambi\u0013 en\ncomo herramienta en el PLN para el parsing\ndiscursivo en tareas de extracci\u0013 on de informa-\nci\u0013 on. Por \u0013 ultimo, en su estado actual puede\nser tambi\u0013 en de inter\u0013 es para usuarios \fnales,\nya sea traductores o quienes necesiten redac-\nRogelio Nazar\n128tar en su propia lengua o en una L2, y bus-\nquen equivalentes o deseen cuidar la riqueza\nde vocabulario de sus textos.\n2 Trabajo relacionado\n2.1 Antecedentes te\u0013 oricos\nEntre los pioneros del estudio de los MD se\nencuentran en particular muchos gram\u0013 aticos\nde la lengua castellana, tales como Antonio\nde Nebrija, Gregorio Garc\u0013 es, Andr\u0013 es Bello y\nm\u0013 as recientemente Gili Gaya (1943), pero la\nverdadera profusi\u0013 on de investigaciones en el\ntema es posterior. Comenz\u0013 o con el trabajo de\nvan Dijk (1973), quien describi\u0013 o las relaciones\nl\u0013 ogicas que se producen entre proposiciones a\ntrav\u0013 es del uso de distintos conectores, tales\ncomo los de disyunci\u0013 on, conjunci\u0013 on, causali-\ndad, condici\u0013 on, contraste, etc. Algunos a~ nos\nm\u0013 as tarde, esta l\u0013 \u0010nea de investigaci\u0013 on se vio\nextendida por el trabajo de Halliday y Hasan\n(1976), que presentaron ya una taxonom\u0013 \u0010a\nm\u0013 as completa para el caso del ingl\u0013 es, inclu-\nyendo otras categor\u0013 \u0010as adem\u0013 as de las mencio-\nnadas por van Dijk. En paralelo, en el \u0013 area de\nlos estudios de la argumentaci\u0013 on en franc\u0013 es,\nAnscombre y Ducrot (1976) profundizaron en\nlas funciones de part\u0013 \u0010culas y conectores que\nhoy englobar\u0013 \u0010amos en la categor\u0013 \u0010a de MD.\nTal como se~ nala Stubbs (1983), el an\u0013 alisis\nde este tipo de unidades evidenci\u0013 o las limita-\nciones de lo que hasta los a~ nos setenta hab\u0013 \u0010a\nsido una gram\u0013 atica oracional y justi\fc\u0013 o en\nbuena medida el lanzamiento de una gram\u0013 ati-\nca del texto, precedente de lo que luego ser\u0013 \u0010a\nel an\u0013 alisis del discurso. A partir de los a~ nos\nochenta se multiplicar\u0013 \u0010a la cantidad de inves-\ntigaciones en esta subdisciplina y, particular-\nmente, en el campo de los MD. Los sucesivos\ntrabajos de investigaci\u0013 on intentaron delinear\nlas propiedades de\fnitorias de estas unida-\ndes, es decir, aquellas que los de\fnen como\nsubconjunto del vocabulario, y tambi\u0013 en aque-\nllas propiedades que permiten organizarlas en\ncategor\u0013 \u0010as.\nParece existir consenso en que los MD re-\npresentan un fen\u0013 omeno com\u0013 un a todas las\nlenguas, pero no son f\u0013 acilmente de\fnibles co-\nmo conjunto de unidades. A menudo son de-\n\fnidos como part\u0013 \u0010culas discursivas que sir-\nven para facilitar las relaciones de coherencia\nen los textos (Fraser, 1999; Pons Border\u0013 \u0010a,\n2001), en el sentido de que ofrecen instruccio-\nnes para la interpretaci\u0013 on y van organizando\nla argumentaci\u0013 on. Su aparici\u0013 on, sin embargo,\nno es estrictamente necesaria ya que igual-mente en su ausencia es posible inferir rela-\nciones l\u0013 ogicas entre proposiciones como, por\nejemplo, la causalidad. A pesar de que a veces\nno hacen falta, son sin embargo un elemento\nclave para facilitar el trabajo interpretativo\ndel lector y reducen el riesgo de error o de\nambig\u007f uedad.\nEl rol de los MD tambi\u0013 en consiste en re-\ngular la interacci\u0013 on entre participantes. Esto\nsucede con mayor frecuencia en la comunica-\nci\u0013 on oral, aunque no exclusivamente. En este\nsentido, se puede decir que tienen tambi\u0013 en\nuna funci\u0013 on interpersonal adem\u0013 as de la tex-\ntual, o exof\u0013 orica en lugar de solo endof\u0013 orica.\nMosegaard Hansen (1998), por ejemplo, men-\nciona los indicadores de cambio de tema o de\ncambio de turno de los participantes en la in-\nteracci\u0013 on. Esto hace que consideremos en la\ncategor\u0013 \u0010a de MD a todas aquellas part\u0013 \u0010culas\npragm\u0013 aticas que tienen una funci\u0013 on interper-\nsonal, tales como part\u0013 \u0010culas modales e inter-\njecciones, lo que di\fculta el establecimiento\nde un l\u0013 \u0010mite preciso.\nDesde un punto de vista morfol\u0013 ogico, los\nMD pueden tener diversas categor\u0013 \u0010as grama-\nticales: conjunciones, adverbios o preposicio-\nnes, casi siempre como expresiones pluriver-\nbales. Es posible decir que se caracterizan por\nser (relativamente) invariables, ya que no pre-\nsentan la \rexi\u0013 on t\u0013 \u0010pica de otros tipos de uni-\ndades l\u0013 exicas. Como explican Mart\u0013 \u0010n Zorra-\nquino y Portol\u0013 es (1999), los MD no presentan\n\rexi\u0013 on de g\u0013 enero ( *por cierta ) ni de n\u0013 umero\n(*sin embargos ); casi nunca admiten modi\f-\ncadores (*muy sin embargo, pero s\u0013 \u0010 muy por\nel contrario ); no pueden ser negados ( *no a\nsaber ) ni coordinados (*a saber y sin embar-\ngo).\nDesde un punto de vista sint\u0013 actico, Schif-\nfrin (2001) ha se~ nalado que ocupan frecuente-\nmente una posici\u0013 on inicial en la oraci\u0013 on, pe-\nro tambi\u0013 en pueden ocupar otras posiciones.\nSuelen ser tambi\u0013 en parent\u0013 eticos, es decir que\nsuelen aparecer entre pausas o, en el caso de\nla lengua escrita, signos de puntuaci\u0013 on, co-\nmo comas o puntos. Esto parece indicar que\nno forman parte de la estructura sint\u0013 actica\nde la oraci\u0013 on. Sin embargo, nuevamente esta\ntampoco parece una regla \frme, ya que tam-\nbi\u0013 en es posible encontrarlos en una posici\u0013 on\nno parent\u0013 etica. En cualquier caso, no est\u0013 an\ncon\fnados a la oraci\u0013 on, y tienen la capacidad\nde afectar alternativamente a distintos nive-\nles, ya sea al intraoracional o bien al extra-\noracional o discursivo (Pons Border\u0013 \u0010a, 2001;\nInducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: \nprimeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n\n129Brinton, 2010).\nPosiblemente sea el punto de vista\nsem\u0013 antico el \u0013 unico que permita una distinci\u0013 on\nm\u0013 as clara del conjunto, ya que se caracterizan\npor una falta de contenido referencial o pro-\nposicional. Aqu\u0013 \u0010 tambi\u0013 en es preciso hacer la\nsalvedad, sin embargo, ya que es posible que\nalgunos conserven parte del signi\fcado l\u0013 exi-\nco que alguna vez tuvieron y que perdieron\ndurante la evoluci\u0013 on hist\u0013 orica de la lengua\na trav\u0013 es de un proceso de gramaticalizaci\u0013 on\n(Traugott y Dasher, 2002; Wichmann y Cha-\nnet, 2009).\nAdem\u0013 as de los intentos por de\fnir a los\nMD como conjunto, otro aspecto que ha preo-\ncupado a los te\u0013 oricos es el de diferenciar las\ndistintas clases que existen. En este aspec-\nto, sin duda el trabajo de Halliday y Hasan\n(1976) es pionero en el esfuerzo de establecer\ncategor\u0013 \u0010as. Sin embargo, nuevamente destaca\nla tradici\u0013 on espa~ nola como la que m\u0013 as se ha\ncentrado en la categorizaci\u0013 on, como se puede\napreciar en los trabajos de Casado Velarde\n(1993), Montol\u0013 \u0010o (2001), Calsamiglia y Tus\u0013 on\n(1999) y, en particular, Mart\u0013 \u0010n Zorraquino y\nPortol\u0013 es (1999).\nEl \u0013 ultimo trabajo es el que ha ofrecido la\ntaxonom\u0013 \u0010a m\u0013 as exitosa y que ha in\ruido in-\ncluso en la clasi\fcaci\u0013 on de MD en otras len-\nguas, como por ejemplo el alem\u0013 an (Bl\u007f uhdorn,\nFoolen, y Loureda, 2017). Consiste en una\nclasi\fcaci\u0013 on en dos niveles: primero ofrece\nuna serie de categor\u0013 \u0010as m\u0013 as generales que\nluego se subdividen en categor\u0013 \u0010as m\u0013 as es-\npec\u0013 \u0010\fcas. Las categor\u0013 \u0010as m\u0013 as generales coin-\nciden con las que ya han sido se~ naladas por\notros autores, tales como los estructuradores\nde la informaci\u0013 on, los conectores, reformula-\ndores, operadores argumentativos y marca-\ndores conversacionales. Pero luego cada una\nde estas grandes categor\u0013 \u0010as se subdivide y\nas\u0013 \u0010 tenemos entonces, por ejemplo en el caso\nde los conectores, los aditivos ( adem\u0013 as, en-\ncima, aparte, etc.); consecutivos ( por tanto,\npor consiguiente, por ende, etc.) y contraar-\ngumentativos ( sin embargo, no obstante, en\ncambio, etc.).\n2.2 Antecedentes de an\u0013 alisis de\nMD con herramientas de PLN\nEl tema de los MD ha recibido m\u0013 as atenci\u0013 on\npor parte de la ling\u007f u\u0013 \u0010stica te\u0013 orica que de la\nling\u007f u\u0013 \u0010stica computacional o del PLN. En par-\nticular, es llamativamente poco tratado en\nla bibliograf\u0013 \u0010a sobre an\u0013 alisis computacionaldel discurso, que es donde ser\u0013 \u0010a m\u0013 as natu-\nral encontrarlo. En comparaci\u0013 on con el enor-\nme volumen de t\u0013 \u0010tulos del \u0013 area, son pocos\nlos trabajos que tratan expl\u0013 \u0010citamente sobre\nMD, como Stubbs (1996) o Moore y Wiemer-\nHastings (2003).\nAdem\u0013 as, la gran mayor\u0013 \u0010a de las publica-\nciones del \u0013 ambito de la ling\u007f u\u0013 \u0010stica te\u0013 orica de-\ndicada al tema de los MD consiste en el an\u0013 ali-\nsis cualitativo de uno o unos pocos casos de\nMD, como por ejemplo el caso de Urgelles-\nColl (2010) en ingl\u0013 es o Cardona (2014) en\ncastellano, entre muchos otros. Comparati-\nvamente, son pocos los intentos por ofrecer\ncat\u0013 alogos exhaustivos de los MD que existen\nen distintas lenguas, que es justamente el \u0013 area\nen la que las herramientas de PLN podr\u0013 \u0010an\nprestar un mejor servicio. S\u0013 \u0010 existen algunos\nesfuerzos por recopilar inventarios amplios de\nMD, como pueden ser el trabajo de Knott\n(1996) en el caso del ingl\u0013 es, el de Stede (2002)\npara el caso del alem\u0013 an, el de Roze, Danlos,\ny Muller (2012) para el caso del franc\u0013 es, o los\nde Santos R\u0013 \u0010o (2003) y Briz, Pons, y Portol\u0013 es\n(2008) para el caso del castellano, entre otros.\nSin embargo, el esfuerzo humano que exige la\ncompilaci\u0013 on manual de estos listados implica\nuna gran di\fcultad para la obtenci\u0013 on de lis-\ntados verdaderamente exhaustivos. Tal como\nse~ nalan Lopes et al. (2015), las herramientas\nde PLN son ideales para esta tarea, y esto\npuede explicar la aparici\u0013 on de una nueva ten-\ndencia en ling\u007f u\u0013 \u0010stica computacional que des-\ncubre un renovado inter\u0013 es por la extracci\u0013 on y\ncatalogaci\u0013 on de MD. Y un rasgo com\u0013 un que\npresentan estos estudios m\u0013 as recientes parece\nser el an\u0013 alisis de pares de lenguas, frecuente-\nmente mediante corpus paralelos.\nEn el caso del citado trabajo de Lopes\net al. (2015), el par de lenguas viene dado\npor la aplicaci\u0013 on de un sistema de traducci\u0013 on\nautom\u0013 atica. Parten de un listado de MD en\ningl\u0013 es generado de manera manual y se limi-\ntan a realizar la traducci\u0013 on de este listado a\ndiferentes lenguas.\nEn un trabajo anterior (Robledo y Nazar,\n2018) se propuso un enfoque basado en clus-\ntering a partir en corpus paralelo aplicado al\ncaso de los MD en castellano. Aquel m\u0013 eto-\ndo consisti\u0013 o en obtener grupos de MD con\nequivalencia funcional, la cual viene dada por\ncompartir equivalentes en otra lengua. La li-\nmitaci\u0013 on de dicho m\u0013 etodo es que implica la\nutilizaci\u0013 on de variados recursos ling\u007f u\u0013 \u0010sticos\ncomo etiquetadores morfosint\u0013 aticos, gazetteer\nRogelio Nazar\n130y algoritmos de clustering aglomerativo que\nson computacionalmente costosos debido a su\ncomplejidad cuadr\u0013 atica.\nOtros autores han optado por el uso de\nalgoritmos de aprendizaje autom\u0013 atico, como\nSileo et al. (2019), en el que utilizan como\nmaterial de entrenamiento un grupo de MD\nen ingl\u0013 es generado de manera manual. Se con-\ncentran en la extracci\u0013 on de MD parent\u0013 eticos\nde alta frecuencia y en posici\u0013 on inicial de ora-\nci\u0013 on, y el insumo que utilizan son las pistas\ncontextuales, entendidas como enegramas de\npalabras. Tambi\u0013 en en este caso se trata de\nuna metodolog\u0013 \u0010a de alta complejidad, tanto\nconceptual como computacional, que necesi-\nta de variados recursos externos que di\fcul-\ntan la reproducci\u0013 on de experimentos en otras\nlenguas.\nEn relaci\u0013 on con estos esfuerzos recientes\npara el procesamiento de MD dentro de la\nling\u007f u\u0013 \u0010stica computacional, el presente art\u0013 \u0010cu-\nlo representa una contribuci\u0013 on m\u0013 as en la mis-\nma direcci\u0013 on, ya que se propone conseguir un\nlistado amplio de MD. De los trabajos men-\ncionados, el que m\u0013 as se le parece es el de Ro-\nbledo y Nazar (2018), en tanto explota el uso\nde corpus paralelos para encontrar la equi-\nvalencia entre MD de una misma lengua. En\ncontraste con todos los mencionados traba-\njos, sin embargo, la virtud principal del que se\npresenta ahora es que se trata de un m\u0013 etodo\nmucho m\u0013 as simple, ya que no requiere pr\u0013 acti-\ncamente de ning\u0013 un recurso externo. Esto re-\npresenta una gran ventaja en dos sentidos:\nen primer lugar, disminuye el coste compu-\ntacional, lo cual facilita el procesamiento de\ngrandes vol\u0013 umenes de datos, y en segundo\nlugar, posibilita la reproducci\u0013 on de los expe-\nrimentos en diferentes lenguas. Finalmente,\nen contraste con los estudios cualitativos, la\nventaja de un enfoque como el que se pre-\nsenta en este art\u0013 \u0010culo es la gran cantidad de\ndatos que genera, ya que se obtienen listados\nde miles de MD, en contraste con los pocos\ncentenares a los que llegan la mayor\u0013 \u0010a de los\nenfoques cualitativos e incluso varios de los\nque proponen m\u0013 etodos automatizados.\n3 Metodolog\u0013 \u0010a\nComo ya se mencion\u0013 o en la introducci\u0013 on,\ncon esta metodolog\u0013 \u0010a nos proponemos en pri-\nmer lugar identi\fcar los MD del corpus se-\npar\u0013 andolos del resto de las unidades del voca-\nbulario (Secci\u0013 on 3.1), para luego clasi\fcarlos\nde manera inductiva en categor\u0013 \u0010as funciona-les (Secci\u0013 on 3.2), que son luego etiquetadas\nde modo tambi\u0013 en autom\u0013 atico (Secci\u0013 on 3.3).\nUna vez que existe una taxonom\u0013 \u0010a nuclear o\nb\u0013 asica, comienza el proceso de poblamiento\nextensivo de esta estructura (Secci\u0013 on 3.4).\n3.1 Vaciado de MD a partir del\ncorpus\nLa primera fase de la metodolog\u0013 \u0010a consiste en\nresponder a la pregunta de c\u0013 omo separar las\nunidades consideradas MD del resto de las\npalabras del corpus. Para ello, la decisi\u0013 on fue\napostar por una caracter\u0013 \u0010stica propia, aun-\nque no exclusiva, de los MD, que es su bajo\ncontenido informativo.\nNaturalmente, no se puede decir que los\nMD no tengan informaci\u0013 on en el sentido de\nque no sean portadores de ning\u0013 un tipo de sig-\nni\fcado. Como se mencion\u0013 o en la Secci\u0013 on 2,\nlos MD poseen un signi\fcado funcional, ya\nque son el veh\u0013 \u0010culo de distintas relaciones de\nsentido. Pero este es un tipo de signi\fcado\ndistinto al valor designador o referencial que\ntienen t\u0013 \u0010picamente las unidades l\u0013 exicas. En el\nextremo de las palabras funcionales encontra-\nmos las preposiciones, clase cerrada y perfec-\ntamente catalogada en las lenguas conocidas,\ny en el extremo opuesto los t\u0013 erminos especia-\nlizados. Pero entre un extremo y otro de este\ncontinuum encontramos una gran diversidad\nde unidades que no poseen el signi\fcado l\u0013 exi-\nco espec\u0013 \u0010\fco de los nombres o, si lo tuvieron\nalguna vez, lo perdieron en un proceso de gra-\nmaticalizaci\u0013 on en la historia de la lengua (cf.\nSecci\u0013 on 2.1).\nEn este caso, de\fnimos cantidad de infor-\nmaci\u0013 on en un sentido formal como un valor\nque indica cu\u0013 anto ayuda a predecir una varia-\nble aleatoria el resultado de otras variables.\nClaramente, la distribuci\u0013 on de palabras en el\ncorpus no es aleatoria ya que, si lo fuera, la\naparici\u0013 on de una palabra no podr\u0013 \u0010a informar-\nnos acerca de la aparici\u0013 on de otras. Por ejem-\nplo, si en un texto aparece la palabra caballo ,\nexiste una probabilidad de que tambi\u0013 en apa-\nrezcan otras palabras de su campo sem\u0013 antico,\ny esta probabilidad se incrementa cuanto m\u0013 as\nespecializada sea esta unidad. De esta forma,\nsi encontramos una unidad como trastorno\nobsesivo compulsivo , existe una alta probabi-\nlidad de encontrar otras que tienen relaci\u0013 on\ncon este trastorno, t\u0013 erminos de la psiquiatr\u0013 \u0010a\ntales como los s\u0013 \u0010ntomas asociados o los f\u0013 arma-\ncos que se utilizan para tratarlo.\nNo todas las unidades del vocabulario po-\nInducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: \nprimeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n\n131seen esta propiedad, es decir, esta misma can-\ntidad de informaci\u0013 on, ya que encontramos\ntambi\u0013 en palabras en este sentido mucho me-\nnos informativas: su aparici\u0013 on en el texto no\nayuda a predecir la aparici\u0013 on de otras. Este es\nel caso de los MD, palabras funcionales cuya\naparici\u0013 on no tiene relaci\u0013 on con el contenido\nde los textos en los que aparecen.\nEs posible apreciar esta diferencia de ma-\nnera gr\u0013 a\fca. En el primer caso, la Figura 1\nmuestra la distribuci\u0013 on de frecuencias de las\npalabras que aparecen en los contextos de\naparici\u0013 on de democracia , en un corpus en cas-\ntellano, excluyendo gramemas (preposiciones\ny art\u0013 \u0010culos). Como puede apreciarse, el con-\njunto de las oraciones que contienen esta pa-\nlabra contienen tambi\u0013 en un grupo relativa-\nmente amplio de otras unidades que apare-\ncen con alta frecuencia, tales como humanos,\nrespeto, libertad, etc. Es en este sentido que\ndecimos que la aparici\u0013 on de la palabra demo-\ncracia nos permite predecir la aparici\u0013 on de\notras palabras.\nFigura 1: Distribuci\u0013 on de frecuencias de las\npalabras que coocurren con la expresi\u0013 on de-\nmocracia .\nSe trata, sin duda, de una propiedad uni-\nversal del lenguaje, en el sentido de que todas\nlas lenguas ofrecer\u0013 an un comportamiento si-\nmilar. No es, sin embargo, el caso de todas\nlas unidades del vocabulario, ya que no ser\u0013 a\nposible predecir qu\u0013 e palabras van a coocurrir\ncon aquellas que tienen un signi\fcado funcio-\nnal en lugar de l\u0013 exico. En este sentido es que\nse puede decir que estas palabras tendr\u0013 an un\ncomportamiento parecido al de una variable\naleatoria y, por tanto, su cantidad de infor-maci\u0013 on ser\u0013 a mucho m\u0013 as baja. Ser\u0013 \u0010a el caso de\nuna expresi\u0013 on como de todas maneras en el\nmismo corpus (Figura 2).\nFigura 2: Distribuci\u0013 on de frecuencias en el\ncaso de de todas maneras .\nComparativamente, las unidades de voca-\nbulario que aparece en las oraciones de unida-\ndes funcionales presentan muy baja frecuen-\ncia de coocurrencia y son, adem\u0013 as, ellas mis-\nmas formas poco informativas ( ser\u0013 \u0010a, siendo,\nhabr\u0013 \u0010a, etc.). No siempre funcionar\u0013 a esta dis-\ntinci\u0013 on, ya que hay MD como por un lado o\npor una parte que s\u0013 \u0010 permiten la predicci\u0013 on\nde otras unidades. Pero al menos es posible\nuna primera divisi\u0013 on del vocabulario en dos\nclases (palabras informativas vs. palabras no\ninformativas), y los MD genuinos que queden\nexcluidos aqu\u0013 \u0010 se podr\u0013 an recuperar m\u0013 as tar-\nde (apartado 3.4). La divisi\u0013 on se lleva a cabo\nutilizando el coe\fciente (1), que pone en con-\ntraste la suma de las frecuencia de los coocu-\nrrentes y la frecuencia de la unidad elegida\ncomo diana.\nI(x) =log2Pn\ni=1Rx;i\nlog2jm(x )j(1)\nConm(x) nos referimos a los contextos de\nuna unidad xyRx;ies la frecuencia de la\nunidad ien el ranking de los nvocablos m\u0013 as\nfrecuentes en esos contextos (en nuestros ex-\nperimentos, n= 20). Este coe\fciente asigna\na cada unidad un valor num\u0013 erico y, por lo\ntanto, continuo, en lugar de una separaci\u0013 on\ndiscreta entre dos clases. Ello obliga a elegir\nun valor de corte arbitrario kpara poder es-\ntablecer la clasi\fcaci\u0013 on binaria C(x) (2) entre\nla categor\u0013 \u0010a l\u0013 exica (L) y la funcional (F ).\nRogelio Nazar\n132C(x) =\u001a\nL I (x)> k\nFotherwise(2)\nPara llevar a cabo esta tarea de clasi\fca-\nci\u0013 on, todas las unidades l\u0013 exicas del corpus de-\nben ser analizadas. Esto requiere la de\fnici\u0013 on\nde un vocabulario V, en el que 8x2V,xde-\nbe ser una palabra o una secuencia de hasta\ncuatro palabras. En cuanto al material desde\nel cual obtener esta informaci\u0013 on, bastar\u0013 \u0010a con\nla utilizaci\u0013 on de un corpus monoling\u007f ue lo su-\n\fcientemente grande como para disponer de\nunos 5.000 contextos de cada unidad analiza-\nda. Sin embargo, como posteriormente vamos\na necesitar un corpus paralelo de todos mo-\ndos, utilizamos para todas las operaciones el\nmismo corpus, el Opus Corpus ofrecido por\nTiedemann (2012).\n3.2 Organizaci\u0013 on en grupos de los\nMD extra\u0013 \u0010dos\nEl paso anterior permite obtener, por cada\nlengua l(en, fr, es, de, ca), un conjunto MD l\nde candidatos. El paso siguiente consiste en-\ntonces en la agrupaci\u0013 on de estas unidades en\nconjuntos funcionales, para lo cual utilizamos\nel ya mencionado corpus paralelo.\nEs preciso observar aqu\u0013 \u0010 algunas de las\nparticularidades del Opus Corpus. Se trata de\nun conjunto de archivos TMX que se ofrece\nen pares de lenguas, t\u0013 \u0010picamente en 30 archi-\nvos por par, en el que cada uno representa un\ncorpus. Cada corpus re\u0013 une material de una\ndeterminada \u0013 area tem\u0013 atica o de especialidad,\naunque tambi\u0013 en se encuentra material que\ncorresponde al vocabulario general. Los ar-\nchivos se encuentran alineados generalmente\na nivel de oraci\u0013 on. La cantidad total de mate-\nrial disponible var\u0013 \u0010a, por supuesto, seg\u0013 un las\nlenguas elegidas, pero en el caso de las len-\nguas europeas, cada par est\u0013 a en torno a los\n3.500 millones de palabras.\nEn primer lugar, para poder agrupar los\nejemplares de MD obtenidos en el la Secci\u0013 on\n3.1, es necesario encontrar los equivalentes de\ncada uno en otra lengua. Esto es lo que lleva a\ntrabajar por pares de lenguas y, por ende, a la\nutilizaci\u0013 on de corpus paralelos. Por una cues-\nti\u0013 on pr\u0013 actica (la mayor disponibilidad de ma-\nterial) estos pares suelen involucrar al ingl\u0013 es\ncomo una de las lenguas, con excepci\u0013 on del\ncatal\u0013 an, donde tiene m\u0013 as sentido utilizar el\npar castellano - catal\u0013 an, que es mayor que el\npar ingl\u0013 es - catal\u0013 an. As\u0013 \u0010, para el caso de un\npar cualquiera, como por ejemplo castellano- catal\u0013 an, para la alineaci\u0013 on de los conjuntos\nMD esyMD caen un listado de equivalentes,\nutilizamos un coe\fciente de asociaci\u0013 on basa-\ndo en un criterio de coocurrencia (3) para en-\ncontrar la asociaci\u0013 on entre un candidato ien\ncastellano (como, por ejemplo, en todo caso )\ny uno jen catal\u0013 an (tal como en tot cas ).\nA(MD es;i; MD ca;j) =f(MD es;i; MD ca;j)p\nf(MD es;i):p\nf(MD ca;j)(3)\nEventualmente, se podr\u0013 \u0010a complementar\neste coe\fciente con otros como el de la simi-\nlitud ortogr\u0013 a\fca para el caso de los cognados\nque son frecuentes en lenguas emparentadas,\npor ejemplo, nuevamente, el caso del par cas-\ntellano - catal\u0013 an. Pero se ha preferido dejar\nese recurso de lado para simpli\fcar al m\u0013 axi-\nmo el m\u0013 etodo.\nEl prop\u0013 osito de alinear los MD en pares de\nlenguas es \u0013 unicamente poder agrupar despu\u0013 es\nlos MD de una misma lengua en funci\u0013 on de\nlos equivalentes que comparten en la otra. De\nesta manera, se descubrir\u0013 a la similitud entre\ndos MD en castellano tales como en todo ca-\nsoyen cualquier caso por su mutua relaci\u0013 on\nde equivalencia con un MD en catal\u0013 an como\nen tot cas . Un aspecto clave de este proceso\nes que un mismo MD puede ser alineado con\ndistintos equivalentes en otra lengua. Esto su-\ncede con mayor frecuencia en el caso de los\nMD que en el resto de las unidades l\u0013 exicas.\nPara el descubrimiento de estas relaciones\nde similitud es preferible evitar el uso de al-\ngoritmos de clustering aglomerativo. En lugar\nde esto, se opt\u0013 o por un m\u0013 etodo alternativo de\nmayor simplicidad.\nEste nuevo m\u0013 etodo de clustering est\u0013 a ins-\npirado en las din\u0013 amicas sociales que pueden\nobservarse, por ejemplo, en la forma en que\nse aglutina la gente en las pausas de caf\u0013 e de\nlos congresos. Imaginamos un espacio en el\nque entran personas de a pares, ya que es\nla situaci\u0013 on que tenemos con nuestros MD\nalineados. El primero puede ser un par cual-\nquiera, como por ejemplo en todo caso yen\ntot cas . Si un segundo par que entra no tiene\nrelaci\u0013 on con el anterior, entonces permane-\ncen como dos grupos independientes. Ser\u0013 \u0010a el\ncaso, por ejemplo, de un par como en otras\npalabras yen altres paraules. Ahora bien, si\nse presenta un tercer par constituido por en\ncualquier caso yen tot cas, en ese caso este\nnuevo par es asimilado el grupo 1, como si en\ncualquier caso fuese presentado a en todo ca-\nInducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: \nprimeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n\n133soporen tot cas. Esta din\u0013 amica continuar\u0013 \u0010a\nde la misma forma, creando distintos grupos,\nhasta agotar la cantidad de pares alineados.\nEl proceso resulta econ\u0013 omico porque no hay\nuna tabla de distancia en la que se comparen\ntodos los MD entre s\u0013 \u0010. En cambio, cada par\nse va comparando con cada uno de los grupos\ncreados hasta el momento. El orden en el que\nson examinados los pares es aleatorio.\n3.3 Etiquetado de los grupos con\ncategor\u0013 \u0010as funcionales\nEl paso anterior resulta en un n\u0013 umero inde-\nterminado de clusters de MD en cada lengua\ny que se presentan a su vez alineados entre s\u0013 \u0010.\nPor ejemplo, el cluster que re\u0013 une los conec-\ntores contraargumentativos en ingl\u0013 es aparece\nalineado con el cluster correspondiente en el\nresto de las lenguas. Estos grupos, sin em-\nbargo, no poseen un nombre, tal como suele\nsuceder con el resultado de cualquier proce-\nso de clustering. Esto es, el algoritmo re\u0013 une\nestos conectores por su similitud, pero no los\netiqueta con la categor\u0013 \u0010a correspondiente.\nAnte este resultado, interesa proporcio-\nnar una etiqueta a cada cluster por un cri-\nterio l\u0013 ogico de ordenamiento pero tambi\u0013 en\npara facilitar el descubrimiento de las rela-\nciones que es posible percibir a simple vista\nentre algunos clusters. Con este \fn, tal co-\nmo adelantamos ya en la introducci\u0013 on, uti-\nlizamos los nombres de categor\u0013 \u0010as aportados\npor Mart\u0013 \u0010n Zorraquino y Portol\u0013 es (1999). El\nprocedimiento es tambi\u0013 en aqu\u0013 \u0010 bastante sim-\nple. Gracias a que estos autores proporcionan\nvarios ejemplos por cada una de estas cate-\ngor\u0013 \u0010as, es posible encontrar coincidencias (4)\nentre los miembros de cada una de las cate-\ngor\u0013 \u0010as en esta taxonom\u0013 \u0010a ( MZP ) y los miem-\nbros de los clusters generados por el algorit-\nmo (CMD ).\nsim(MZP p; CMD q) =j~MZP p\\~CMD qj\nj~MZP pj(4)\nDe este modo, para cada cluster se se-\nleccionar\u0013 a la categor\u0013 \u0010a que ofrezca la coinci-\ndencia m\u0013 as alta. Naturalmente, como la ta-\nxonom\u0013 \u0010a de Mart\u0013 \u0010n Zorraquino y Portol\u0013 es\n(1999) est\u0013 a en castellano, el c\u0013 alculo de la in-\ntersecci\u0013 on solamente puede hacerse con los\nclusters que est\u0013 an en castellano. Pero esto,\npor supuesto, no representa un problema de-\nbido a que los clusters est\u0013 an alineados inter-\nling\u007f u\u0013 \u0010sticamente. De este modo se consiguetambi\u0013 en el efecto deseado de agrupar clus-\nters que pueden corresponderse a una misma\ncategor\u0013 \u0010a funcional.\n3.4 Poblamiento de la taxonom\u0013 \u0010a\ncon nuevos ejemplares\nEl resultado del paso anterior es una taxo-\nnom\u0013 \u0010a multiling\u007f ue nuclear o b\u0013 asica, que lla-\nmaremos TMD . A partir de este punto, dicha\ntaxonom\u0013 \u0010a puede ser enriquecida mediante la\nadici\u0013 on de nuevos MD extra\u0013 \u0010dos del corpus.\nPara cualquier nuevo candidato a MD ( c),\nla existencia de la TMD posibilita decidir si\nces efectivamente un MD y, si efectivamen-\nte lo es, asignarle una categor\u0013 \u0010a. Para ambas\ntareas recurrimos nuevamente al corpus pa-\nralelo inicial.\nSi un candidato ces un MD genuino, en-\ntonces su condici\u0013 on ser\u0013 a delatada por la pre-\nsencia de otros MD de la otra lengua en los\npares alineados, que ahora es posible descu-\nbrir sin di\fcultad gracias a la taxonom\u0013 \u0010a nu-\nclear. Por ejemplo, si c=de m^ eme fa\u0018 con\n^c =2TMD , encontraremos que, en el cor-\npus paralelo franc\u0013 es-ingl\u0013 es, caparece alinea-\ndo con elementos tales como in the same way,\nlikewise, similarly , etc., elementos que s\u0013 \u0010 apa-\nrecen en la TMD . Finalmente, para asignar\nuna categor\u0013 \u0010a a c, operamos de manera si-\nmilar a 3.3, eligiendo la categor\u0013 \u0010a que ofrece\nla coincidencia m\u0013 as alta. En el caso del ejem-\nplo, esta corresponder\u0013 \u0010a a la de los conectores\naditivos.\n4 Resultados\nEn el momento actual, los resultados del pro-\nyecto implican la creaci\u0013 on de una TMD mul-\ntiling\u007f ue de 2.636 elementos divididos en 70\ncategor\u0013 \u0010as funcionales. Todav\u0013 \u0010a no ha comen-\nzado el proceso de poblamiento masivo de es-\nta taxonom\u0013 \u0010a, pero s\u0013 \u0010 ha sido posible comple-\ntar una primera fase de evaluaci\u0013 on de la me-\ntodolog\u0013 \u0010a empleada en el proceso. Esta eva-\nluaci\u0013 on consiste en medir la capacidad del al-\ngoritmo para distinguir entre un MD genuino\ny una unidad l\u0013 exica de otra categor\u0013 \u0010a.\nLa tabla 1 muestra un ejemplo de cluster\nque corresponde a la categor\u0013 \u0010a de los conecto-\nres contraargumentativos seg\u0013 un la taxonom\u0013 \u0010a\nde Mart\u0013 \u0010n Zorraquino y Portol\u0013 es (1999).\nUn grupo de ling\u007f uistas hablantes nativos\nde cada una de las lenguas analizadas llev\u0013 o\na cabo una revisi\u0013 on manual de los resultados\npara evaluar si la selecci\u0013 on de marcadores era\ncorrecta. Es importante aclarar que lo que se\nRogelio Nazar\n134Ingl\u0013 es all the same; although; and yet; but; but still; despite all; despite the fact that; despite\nthese; despite this; even if; even so; even though; however; in spite of all; in spite of\nthe fact; nevertheless; nonetheless; that being said; that said; though; while; yet\nCastellano a cambio; ahora bien; al contrario; aparte de eso; a pesar de ello; a pesar de eso; a\npesar de esto; a pesar de todo; aun as\u0013 \u0010; aun cuando; aun en; aunque; bien que; con\ntodo; de cualquier forma; de cualquier modo; de todas formas; de todas maneras; de\ntodos modos; dicho esto; en cambio; en lugar de eso; en vez de eso; incluso aunque;\nno obstante; pero; pero aun as\u0013 \u0010; pese a ello; pese a todo; por el contrario; si bien ; sin\nembargo; todo lo contrario; y sin embargo\nFranc\u0013 es cependant; et pourtant; mais encore; mais toujours; malgr\u0013 e cela; malgr\u0013 e tout; m^ eme\nainsi; m^ eme si ; n\u0013 eanmoins; pourtant; toutefois\nAlem\u0013 an aber immer noch; aber nicht; aber trotzdem; allerdings; auch wenn; auftreten m\u007f ussen;\ndachte; dennoch; jedoch; obwohl; selbst wenn; sogar; trotzdem; trotz der tatsache; trotz\ndieser; trotz dieses\nCatal\u0013 an al contrari; ans al contrari; ben al contrari; de qualsevol manera; de tota manera; de\ntotes formes; de totes maneres; en comptes d'aix\u0012 o; en lloc d'aix\u0012 o; i no obstant aix\u0012 o;\nmalgrat aix\u0012 o; no obstant; pel contrari; per\u0012 o tot i aix\u0013 \u0010; tanmateix; tot el contrari; tot i\naix\u0013 \u0010; tot i aix\u0012 o\nTabla 1: Ejemplo de uno de los clusters que corresponde a la categor\u0013 \u0010a de conectores contraar-\ngumentativos.\nrevis\u0013 o fueron listados de MD fuera de con-\ntexto. Esto se debe a que analizar instancias\nde estas unidades en textos particulares equi-\nvaldr\u0013 \u0010a a una tarea diferente, ya que una mis-\nma unidad puede funcionar como MD en un\ncontexto y en otro no.\nLa revisi\u0013 on revel\u0013 o que los datos son de\nbuena calidad, con una pureza en torno el\n95 % de media en las distintas lenguas con\nexcepci\u0013 on del alem\u0013 an, donde la precisi\u0013 on al-\ncanz\u0013 o el 84 %. Las razones del desempe~ no in-\nferior en alem\u0013 an no est\u0013 an del todo claras, pe-\nro probablemente puedan estar relacionadas\ncon las caracter\u0013 \u0010sticas morfol\u0013 ogicas de esta\nlengua. Esto debe continuar estudi\u0013 andose en\ntrabajo futuro. Otra caracter\u0013 \u0010stica llamativa\nde los resultados es que en general parece ha-\nber una tendencia a tener una cantidad de\nMD en castellano ligeramente mayor que en\nlas otras, como si esta lengua permitiese ma-\nyor diversidad en el uso de estas part\u0013 \u0010culas.\nNuevamente, esto debe profundizarse en un\nestudio contrastivo entre las diferentes len-\nguas. La presente investigaci\u0013 on no ha preten-\ndido, en todo caso, dar respuesta a estos in-\nterrogantes sino ofrecer una propuesta meto-\ndol\u0013 ogica para la obtenci\u0013 on de los datos.\nEn relaci\u0013 on con el desempe~ no general del\nalgoritmo en comparaci\u0013 on con otros traba-\njos mencionados en la Secci\u0013 on 2, es posible\na\frmar que los resultados obtenidos con el\npresente m\u0013 etodo son m\u0013 as numerosos y pre-\nsentan menor tasa de error. Particularmente\nen el caso de Robledo y Nazar (2018), que\nes el m\u0013 as comparable en t\u0013 erminos de meto-dolog\u0013 \u0010a aunque solo trabajen en castellano,\nel m\u0013 etodo presentado aqu\u0013 \u0010 es m\u0013 as sensible a\nlos elementos de mediana y baja frecuencia,\ny la tasa de error a la hora de extraer MD es\ninferior. Hay que se~ nalar, de cualquier mane-\nra, que los objetivos de ambos estudios son\ndistintos. En el caso del estudio anterior se\ntrataba de encontrar categor\u0013 \u0010as de MD. En\nel presente estudio, en cambio, el foco est\u0013 a\npuesto en reunir un listado exhaustivo de MD\nparticulares.\nPara complementar la evaluaci\u0013 on manual\ngeneral y poner en perspectiva los resultados,\ninvitamos a un grupo de estudiantes avanza-\ndos en licenciatura en ling\u007f u\u0013 \u0010stica a participar\nde un experimento de evaluaci\u0013 on. En total\nparticiparon 6 j\u0013 ovenes, que fueron elegidos\nentre los que mejores cali\fcaciones obtuvie-\nron en la asignatura de Gram\u0013 atica del Tex-\nto, de la Ponti\fcia Universidad Cat\u0013 olica de\nValpara\u0013 \u0010so, que trata de manera extensiva el\ntema de los MD.\nCada estudiante recibi\u0013 o una planilla con\n720 unidades en castellano en los cuales se\nmezclaron MD aut\u0013 enticos con palabras o se-\ncuencias de palabras correspondientes a otras\ndiversas categor\u0013 \u0010as. La proporci\u0013 on fue de dos\ntercios de MD. La instrucci\u0013 on era marcar\ncon un 1 cada unidad que consideraran co-\nmo MD. No se les permiti\u0013 o consultar diccio-\nnarios ni ning\u0013 un otro recurso y la tarea era\nindividual, sin posibilidad de dialogar con los\ncompa~ neros. Tambi\u0013 en se les pidi\u0013 o que con\fa-\nran en su primera intuici\u0013 on como hablantes,\nsin dedicar mucho tiempo a cada decisi\u0013 on. La\nInducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: \nprimeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n\n135misma tarea fue realizada por el algoritmo, es\ndecir la de aceptar o rechazar los candidatos\ndel mismo listado. En la Tabla 2 se muestran\nlos resultados de cada uno.\nAnotador Pre Rec F1\nAlgoritmo 97 94 95\nEstudiante 1 96 50 65\nEstudiante 2 95 60 73\nEstudiante 3 95 41 57\nEstudiante 4 95 59 72\nEstudiante 5 94 65 76\nEstudiante 6 92 75 82\nTabla 2: Comparaci\u0013 on del desempe~ no entre\nalgoritmo y humanos en la tarea de separar\nMD de unidades l\u0013 exicas (precisi\u0013 on, cobertura\ny F1).\nEn general, todos los estudiantes tuvieron\nun buen desempe~ no en t\u0013 erminos de precisi\u0013 on,\nen el sentido de que, si seleccionaban una uni-\ndad como MD, casi siempre la decisi\u0013 on era\ncorrecta. El problema en general es que tu-\nvieron tendencia a ser poco exhaustivos. En\ncomparaci\u0013 on con los estudiantes, el algoritmo\npresent\u0013 o m\u0013 as o menos la misma tasa de pre-\ncisi\u0013 on, pero la tasa de cobertura fue mayor.\nEn una serie de entrevistas realizadas con\nposterioridad a la entrega del ejercicio, casi\ntodos los estudiantes coincidieron en explicar\nque adoptaron una actitud conservadora, de\nmodo que ante la duda pre\frieron no elegir\nunidades que, aunque puedan cumplir la fun-\nci\u0013 on de un MD, no presentan todav\u0013 \u0010a las mar-\ncas de los MD protot\u0013 \u0010picos o que todav\u0013 \u0010a no\nhan \fnalizado su proceso de gramaticaliza-\nci\u0013 on. Unidades como en estas circunstancias\noen t\u0013 erminos m\u0013 as generales, por ejemplo,\nfueron rechazadas en la mayor\u0013 \u0010a de los casos\na pesar de que en el listado original \fguraban\ncomo MD aut\u0013 enticos. En otros casos, los es-\ntudiantes consultados hicieron referencia a la\nalta polifuncionalidad (Pons Border\u0013 \u0010a y Fis-\ncher, 2021) de los candidatos inspeccionados,\nes decir, algunas unidades podr\u0013 \u0010an funcionar\ncomo MD solo en algunos casos muy espec\u0013 \u0010\f-\ncos, mientras que en general no tendr\u0013 \u0010an esa\nfunci\u0013 on.\nEste ejercicio puso de mani\festo el proble-\nma de la falta de acuerdo entre los hablantes\nacerca de lo que es un MD y tambi\u0013 en la di-\n\fcultad de tratar con MD fuera de contexto.\nM\u0013 as bien, lo propio ser\u0013 \u0010a decir que una de-\nterminada unidad funciona como MD en uncontexto determinado. Esto representa una\ninteresante v\u0013 \u0010a de trabajo futuro pero, nue-\nvamente, trasciende el objetivo de la presente\ninvestigaci\u0013 on.\n5 Conclusiones\nEste art\u0013 \u0010culo ha presentado una nueva pro-\npuesta metodol\u0013 ogica para la extracci\u0013 on au-\ntom\u0013 atica de una base de datos multiling\u007f ue\nde MD, incluyendo una evaluaci\u0013 on de sus pri-\nmeros resultados. Dicha propuesta es original\ny, en comparaci\u0013 on con trabajos aparecidos\nrecientemente sobre el mismo tema, resulta\nm\u0013 as simple en t\u0013 erminos conceptuales, de de-\npendencia de recursos y en materia de coste\ncomputacional. Esto resulta de gran impor-\ntancia para la reproducci\u0013 on de los experimen-\ntos en distintas lenguas.\nLa base de datos de MD desarrollada has-\nta el momento se encuentra disponible para\nsu descarga desde la p\u0013 agina web del proyecto\n(cf. nota 1) y, aun trat\u0013 andose de un traba-\njo en curso, puede ya servir para m\u0013 ultiples\nprop\u0013 ositos. Posibles usuarios \fnales pueden\nser traductores o redactores, y posiblemen-\nte tambi\u0013 en docentes de L1 o L2. Los datos\npueden ser \u0013 utiles tambi\u0013 en para la comunidad\ndel PLN, ya que pueden emplearse para di-\nversidad de tareas vinculadas con el an\u0013 alisis\ndiscursivo y la extracci\u0013 on de informaci\u0013 on.\nMuchas tareas han quedado pendientes,\ncomo continuar explorando distintas varia-\nciones en la metodolog\u0013 \u0010a. Esto puede incluir\nprobar con categor\u0013 \u0010as distintas para la cla-\nsi\fcaci\u0013 on, probar distintos tama~ nos para la\nventana de contexto y hacer un estudio m\u0013 as\nriguroso del desacuerdo entre anotadores en\nlas distintas lenguas. Otras posibilidades de\ntrabajo futuro ser\u0013 \u0010an reproducir experimen-\ntos en otras lenguas y, \fnalmente, una v\u0013 \u0010a\nque parece atractiva es la de utilizar la taxo-\nnom\u0013 \u0010a creada hasta el momento para el des-\ncubrimiento de MD polifuncionales.\nAgradecimientos\nEsta investigaci\u0013 on ha sido \fnanciada por el\nGobierno de Chile a trav\u0013 es del Proyecto Fon-\ndecyt Regular 1191481: Inducci\u0013 on autom\u0013 ati-\nca de taxonom\u0013 \u0010as de marcadores discursivos\na partir de corpus multiling\u007f ues (2019-2021).\nAgradezco a los revisores por sus comentarios\ny a Irene Renau, por ayudarme a mejorar el\nart\u0013 \u0010culo en diversos aspectos.\nRogelio Nazar\n136Bibliograf\u0013 \u0010a\nAnscombre, J.-C. y O. Ducrot. 1976.\nL'argumentation dans la langue. Langa-\nges, 42:5{27.\nBl\u007f uhdorn, H., A. Foolen, y \u0013O. Loureda.\n2017. Diskursmarker: Begri\u000bsgeschich-\nte { theorie { beschreibung. ein biblio-\ngraphischer \u007fUberblick. En H. Bl\u007f uhdorn\nA. Deppermann H. Helmer, y T. Spranz-\nFogasy, editores, Diskursmarker im Deu-\ntschen. Re\rexionen und Analysen. Verlag\nf\u007f ur Gespr\u007f achsforschung, G\u007f ottingen.\nBrinton, L. 2010. Discourse markers. En\nA. Jucker y I. Taavitsainen, editores, His-\ntorical Pragmatics . Gruyter Mouton, Ber-\nlin.\nBriz, A., S. Pons, y J. Portol\u0013 es. 2008. Diccio-\nnario de part\u0013 \u0010culas discursivas del espa~ nol.\nCalsamiglia, H. y A. Tus\u0013 on. 1999. Las cosas\ndel decir: manual de an\u0013 alisis del discurso .\nAriel, Madrid.\nCardona, A. L. 2014. Aproximaci\u0013 on funcio-\nnal a los marcadores discursivos. An\u0013 ali-\nsis y aplicaci\u0013 on lexicogr\u0013 a\fca . Peter Lang,\nFrankfurt am Main.\nCasado Velarde, M. 1993. Introducci\u0013 on a\nla gram\u0013 atica del texto del espa~ nol. Arco\nlibros, Madrid.\nFraser, B. 1999. What are discourse mar-\nkers? Journal of Pragmatics, (31):931{\n952.\nGili Gaya, S. 1943. Curso superior de sinta-\nxis espa~ nola. Minerva, Mexico.\nHalliday, M. y R. Hasan. 1976. Cohesion in\nEnglish. Longman, London.\nKnott, A. 1996. A data-driven methodology\nfor motivating a set of coherence relations .\nPh.D. tesis, University of Edinburgh, UK.\nBritish Library, EThOS.\nLopes, A., D. M. de Matos, V. Cabarr~ ao,\nR. Ribeiro, H. Moniz, I. Trancoso, y A. I.\nMata. 2015. Towards using machine\ntranslation techniques to induce multilin-\ngual lexica of discourse markers.\nMart\u0013 \u0010n Zorraquino, M. A. y J. Portol\u0013 es.\n1999. Los marcadores del discurso. En\nGram\u0013 atica Descriptiva de la Lengua Es-\npa~ nola. Espasa, Madrid, p\u0013 aginas 4051{\n4214.Montol\u0013 \u0010o, E. 2001. Conectores de la lengua\nescrita. Contraargumentativos, consecuti-\nvos, aditivos y organizadores de la infor-\nmaci\u0013 on. Ariel, Barcelona.\nMoore, J. D. y P. Wiemer-Hastings. 2003.\nDiscourse in computational linguistics and\narti\fcial intelligence. En A. C. Graesser\nM. A. Gernsbacher, y S. R. Goldman, edi-\ntores, Handbook of Discourse Processes .\nRoutledge.\nMosegaard Hansen, M.-B. 1998. The Fun-\nction of Discourse Particles : A study\nwith special reference to spoken stan-\ndard French. John Benjamins, Amster-\ndam/Philadelphia.\nPons Border\u0013 \u0010a, S. 2001. Connec-\ntives/Discourse markers. An Overview.\nQuaderns de Filologia. Estudis Literaris,\n(6):219{243.\nPons Border\u0013 \u0010a, S. y K. Fischer. 2021.\nUsing discourse segmentation to account\nfor the polyfunctionality of discourse mar-\nkers: The case of well. Journal of Pragma-\ntics, 173:101{118.\nRobledo, H. y R. Nazar. 2018. Clasi\fca-\nci\u0013 on automatizada de marcadores discur-\nsivos. Procesamiento del Lenguaje Natu-\nral, (61):109{116.\nRoze, C., L. Danlos, y P. Muller. 2012. Lex-\nconn: a french lexicon of discourse connec-\ntives. Discours - Revue de linguistique,\npsycholinguistique et informatique.\nSantos R\u0013 \u0010o, L. 2003. Diccionario de part\u0013 \u0010cu-\nlas. Luso-espa~ nola de ediciones, Salaman-\nca.\nSchi\u000brin, D. 2001. Discourse markers: Lan-\nguage, meaning, and context. En D. Schif-\nfrin D. Tannen, y H. Hamilton, edito-\nres,The Handbook of Discourse Analysis.\nBlackwell, Oxford, p\u0013 aginas 54{75.\nSileo, D., T. Van De Cruys, C. Pradel, y\nP. Muller. 2019. Mining discourse mar-\nkers for unsupervised sentence representa-\ntion learning. En Proceedings of the 2019\nConference of the North American Chap-\nter of the Association for Computational\nLinguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) ,\np\u0013 aginas 3477{3486, Minneapolis, Minne-\nsota, Junio. Association for Computatio-\nnal Linguistics.\nInducci\u00f3n autom\u00e1tica de una taxonom\u00eda multiling\u00fce de marcadores discursivos: \nprimeros resultados en castellano, ingl\u00e9s, franc\u00e9s, alem\u00e1n y catal\u00e1n\n137Stede, M. 2002. DiMLex: A lexical ap-\nproach to discourse markers. En A. Lenci\ny V. D. Tomaso, editores, Exploring the\nLexicon - Theory and Computation. Edi-\nzioni dell'Orso, Alessandria.\nStubbs, M. 1983. Discourse Analysis.\nThe Sociolinguistic Analysis of Natural\nLanguage. University of Chicago Press,\nChicago.\nStubbs, M. 1996. Text and Corpus Analysis .\nBlackwell, Oxford.\nTiedemann, J. 2012. Parallel data, tools\nand interfaces in OPUS. En Procee-\ndings of the Eighth International Confe-\nrence on Language Resources and Evalua-\ntion (LREC'12) , p\u0013 aginas 2214{2218, Is-\ntanbul, Turkey, Mayo. European Langua-\nge Resources Association (ELRA).\nTraugott, E. y R. Dasher. 2002. Regularity in\nsemantic change. Cambridge University\nPress, New York.\nUrgelles-Coll, M. 2010. The Syntax and\nSemantics of Discourse Markers. Conti-\nnuum, London.\nvan Dijk, T. 1973. Text Grammar and\nText Logic. En Studies in Text Grammar .\nReidel, Dordrecht, p\u0013 aginas 17{78.\nWichmann, A. y C. Chanet. 2009. Discour-\nse markers: A challenge for linguists and\nteachers. Nouveaux cahiers de linguistique\nfran\u0018 caise, 29(4):23{40.\nRogelio Nazar\n138Extraction of Terms Seman tically Related to Colp onyms: Evaluation  in \na Smal l Specialized Corpus  \nExtracci\u00f3n de T\u00e9rminos Relacionados Sem\u00e1nticamente con  Colp\u00f3nimos: Evaluaci\u00f3n \nen un Corpus Especializado de  Peque\u00f1o T ama\u00f1o  \nJuan Rojas -Garcia  \nUniversi ty of Granada, Granada, S pain \njuanrojas@ugr.es  \nAbstract: EcoLexicon  is a terminolog ical knowledg e base on environm ental science, whose  design \npermi ts the geographic contextualization of data.  For the geographi c contextualization of named \nentities  such as  colponyms (i.e.,  named bays such as Pensacola Bay ) in EcoLexicon , both \ncount -based and predict ion-based distributional semantic models (DSMs)  were appli ed to a \nsmall-sized, English specialized corpus to extract  terms re lated to each colponym  mentioned in i t and \ntheir semantic relation s. Since  the evaluation  of DSMs  in small, specialized  corpora has received \nlittle attentio n, this study  identif ied both parameter combinations in DS Ms and five \nsimilarity/distance  measures  suitable for the extr action of terms which  relate d to colponyms  through  \nthe semantic rel ations takes_place_in , located_ at, and attribute_ of. The models were  thus evaluated \nusing three gold standar d datasets . The results showed that : count -based models outperformed \nprediction-based ones ; the similarity/distance measures performed quite similar except for the \nEuclide an distance ; and the detection of a specific relati on depended  on the context window siz e. \nKeywords:  Colponym , Terminology , Knowledge Rep resentation, Semant ic Model. \nResumen:  EcoLexicon  es una base de  conocimiento termin ol\u00f3gica sobre el medioam biente, c uyo \ndise\u00f1o permite  la contextualizaci\u00f3n geogr \u00e1fica de colp\u00f3nimos, esto es,  bah\u00edas con nombre propio  \n(BNP) (v.gr., Bah\u00eda de Pensacola ). Se aplicaron model os sem\u00e1n ticos distribucionales ( MSD ), \nbasados en recuento s y predictivo s, a un corpus espec ializado de peque\u00f1o tama\u00f1o  en ing l\u00e9s para \nextraer  t\u00e9rminos relacionados con l as BNP y sus relaciones sem\u00e1nticas.  Puesto que la evaluaci\u00f3n  de \nMSD  en corpus especi alizado s de peque\u00f1o tama\u00f1o  ha sido menos explo rada, en este art\u00edculo se \nidentifica n tanto la combin aci\u00f3n de par\u00e1met ros como la s cinco medida s de similitud  adecuada s para \nextraer t\u00e9rminos q ue man tengan con las BNP las relaciones  tiene_lugar_ en, localizado_en y \natribu to_de . Los MSD se  eval\u00faan  con tres conju ntos de datos anotados m anualmen te. Los resultados \nindican que : los modelos  basados en  recuento s superan a los modelos predictivos ; las medidas d e \nsimilitud brindan resultados s emejantes , excepto l a distancia eucl\u00eddea ; y la detecci\u00f3n de una  relaci\u00f3n \nespec\u00edfica dep ende del tama\u00f1o de la vent ana contextual.  \nPalabras cla ve: Colp\u00f3nimo, Terminolog\u00eda, R epresentaci\u00f3n del Conocimiento , Modelo Sem\u00e1ntico . \n1 Introduction  \nAlthough  named landforms, among other na med \nentities,  are frequent ly foun d in s pecialized texts on \nthe environment, their representa tion and inclusion  in \nterminological knowledge bases ( TKBs) have \nreceived  little research attention, as evidenced by the \nlack of named  landforms in t erminological  resources \nfor the environme nt such as  DiCoEnviro1, GEMET2, \n1 https://cutt.ly/cbATjnQ  \n2 https://www.eionet.europa.eu/gemet/en/themes/  or the FAO Term Portal3. In contra st, AGROVOC4 \ncontains a list of named landforms with hyponymic \ninformatio n, whereas ENVO5 provides descriptions  \nwith onl y geographic details . \nThe semantic re presentation of named landforms, \nsuch as litonyms  (e.g., Sumiyoshi Beach ), potamonyms  \n(e.g., River Nile ), and colponyms  (e.g., San Francisco \nBay), is barely ta ckled in terminological resource s for \ntwo reasons, in our op inion: (1) They are considered \n3 http://ww w.fao.org/faoterm/en/  \n4 http://aims.fao.or g/en/agrovoc  \n5 http://www.envir onmento ntology.org/Browse -EnvO  \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 139-151\nrecibido 10-05-2021 revisado 08-06-2021 aceptado 10-06-2021\nISSN 1135-5948. DOI 10.26342/2021-67-12\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural \n mere ins tances (i.e., examples) of concepts such as \nBEACH , RIVER , or BAY, and their  relatio nal behavi or \nwith other concepts in a speci alized knowled ge domain \nis thus n eglected and not semantically described; (2) \ntheir semanti c representation depends on knowing \nwhich terms are related to each named landf orm, and \nhow these terms are related to ea ch other . This is \nevidently  a time -consuming task taki ng into  accoun t \nthat te rminologists do not often reso rt to natural language \nprocessing (NLP) systems beyond corpus  query  tools \nsuch as Sketch Engine (Kilgarriff A. et al., 2004) . \nAs a result , knowledge resources h ave limited \nthemselves to representing concepts such as BAY, \nRIVER  or BEACH, on the questionable  assumption that \nthe co ncepts linked to e ach of  them are also related  to \nall named bays, rivers and beaches in the real world . \nContrary to th is assumption , Rojas -Garcia J.  and Faber  \nP. (2019a  and 2019b)  have shown  that, in specialized \nknowledge domains,  each named landform  reveals a \nspecific  conceptua l structure . In othe r words, each \nnamed landform holds  different semantic relations  to \nspecialized  concepts eve n in the same  knowledge \ndomain. Therefore , TKBs should include the semantic \nrepresentation  of named landforms . \nIn this respect, EcoLexicon6 is a multilingual TKB  \non env ironmental  science tha t is the practical \napplication of Frame-based Terminology ( Faber  P., \n2012). The flexible design of EcoLexicon permits the  \nrepresentation and  contextual ization of data so that  \nthey a re more rele vant to sp ecific s ubdomains, \ncommunicativ e situations, and geographic areas. With \nthe ultimate goal  of repre senting in EcoLexicon the \nconcep tual structur es underlying  the usage of named \nlandforms mentioned in  a small -sized, English \nspecialized corpus on  Costal Engineerin g (7 million \ntokens), the terms related t o each named landf orm \nand their seman tic relations  have to be manually \nextracted from the corpus  by quer ying it in Sketch \nEngine . In this work, we focus ed on colponyms  \n(Room A., 199 6: 23), namely , named bays . \nAs suc h, terminologist s require to extract terms  \nwhich  relate to each colponym , at least,  by the semantic \nrelations takes_place_in , located_ at, and attribute_ of, \nthe most frequent  relations  held by named bays in the \ncorpu s. Since this is a time-consuming  task, th e overall \naim of thi s study was to  provide terminologist s with \nthree lists of term candidates  for a colponym , one li st \nper semantic relation , by applying  distribution al \nsemantic models  (DSMs ). \nAccordingly, this study  identif ied both paramet er \ncombinations in DSMs  and s imilarity/distance  \nmeasures  suitabl e for the extraction of those terms  \nfrom the small  specialized  corpus mentio ned above . \n \n6 http://ecolexicon.ugr.es  Hence, t he m odels were  evaluate d using gold \nstandard  evaluation dat a, which  contained pairs of \nsemantically rela ted terms , manually extracted  from \nthe same corpus . One of the terms was always a \ncolponym , and the  other one was either a proce ss \n(e.g., storm surge ), an entity (e.g., benthic geo logic \nhabitat), or a propert y (e.g., water quality ). The \nsemantic relatio ns that link ed the terms were: (a) \ntakes_place_in  (e.g., STORM SURGE  takes_place_in  \nESCAMBI A BAY ); (b) located_at  (e.g., BENTHIC  \nGEOLOGIC HABITAT  located _at GREEN WICH BAY ); \nand (c) attribu te_of (e.g., WATER QUALITY  \nattribut e_of NARRAGANSETT BAY ). Three gold \nstandard  datasets were  thus built, one for each of  the \nsemantic relations . \nAs shall be seen , the extra ction of terms that hold  \nthese specific  semantic relation s to name d bays \nlargely depend s on the  contex t windo w size \nparameter of the DSMs , namely, 4 wo rds for  \ntakes_place_in , 3 words f or attribu te_of, and 2 words \nfor located_at . A similar study  was also conducted for \nnamed riv ers by  Rojas -Garcia J.  and Faber  P. \n(2019c), but the relation s frequently  activated were \ntakes_pl ace_in , located_at , and affects (not \nattribute_of ). Interestingly enough , for name d rivers, \nthe w indow  size ha d to be 3 words to extra ct terms \nlinked to rivers with the affect relatio n, whereas  in the \ncase of named bays,  the same window size of 3 words  \nwas required to obtain terms  that held the attribute_of  \nrelation . These findin gs led to the conclusion th at it is \nnot possible to generali ze the results from named \nrivers to either  bays or other na med enti ties such as \nbeaches  and mounta ins. Hence , since each named \nlandform is characterized by  its own concept ual \nstructure , as previou sly stated, this study  on \ncolponyms cannot be considered either as a \"case of \nuse\" or as a \"toy problem \", but rather as a  research  \nobjective  itself. \nBesides the  analysis  of different D SMs and \nsimilarity measures  for a  small -sized, specialized \ncorpus , an important contribution  of this work  is the \ncreation of both the corpus on named  landforms in  the \nCoastal Engineering  domain, and  the three gold \nstandard dat asets for information retrieval system \nevaluation . \nThe rest of th is paper is or ganize d as f ollows. \nSection 2 provides background on DSM s, as wel l as a \nliterature review on their a pplication and evaluation . \nSection s 3 and 4  expla in the materials , methods, and \nDSMs evaluation  applied in this stu dy, and the  \nconstr uction of the g old standard d ataset s. Section 5 \nshows the results obtained. Finally, Sec tion 6 \ndiscusses the res ults, and presents the conclusions \nderived from this work along with  plans for future \nresearch.  \nJuan Rojas-Garcia\n140 \n 2 Background and Literature Revi ew \nDistributional seman tic models  (DSMs) repres ent th e \nmean ing of a term as a vector, based on its statist ical \nco-occurrenc e with other terms in the corpus. \nAccording to the distributional hypothesis, \nsemantic ally similar terms tend to have similar \ncontext ual di stributions (Miller G.A. and Charles  \nW.G. , 1991). The semantic relatedness of two terms \nis estimated  by calcu lating a  similarity/distance  \nmeasure of their vectors, such as Euclidean distance , \ncosine similarity , Jaccard coefficient , Pear son \ncorrelation coefficient, or averaged Kullback -Leibler \ndivergence , inter alia (see Huang A. (2008) for a \ndetailed des cription of the se five measures ). \nDepend ing on the la nguage model (B aroni M. et \nal., 2014),  DSMs are either count -based o r \nprediction-based. Count -based DSMs calculate the \nfrequency o f terms  within a term\u2019s conte xt (i.e., a \nsentence, paragraph, document, o r a sliding context \nwindow spanning a gi ven number of  terms on eithe r \nside of the target te rm). Corre lated Occurren ce \nAnalogue to Lexical Semantic (COALS) (Rohde D. \net al., 2006) is a n examp le of th is type of mod el. \nPredictio n-based models exploit  probabil istic \nlanguage models, which represent t erms by predi cting \nthe next term o n the  basis of previ ous terms. \nExamples of pred ictive  models based on neural \nnetworks include , among  others,  word2ve c (Mikolov \nT. et al., 2013), fastText  (Bojanowski  P. et al., 2017 ), \nand state-of-the-art transfer  learning  models  such as \nBERT ( Devlin J. et  al., 2019). Instead, GloVe  model \n(Pennington J. et al., 2014 ) makes pre dictions  drawn \non a re gression technique . \nCount-based DSMs have been amply studie d \n(Kiela D. and Clark  S., 2014; Lapesa G. et al., 2014; \nSahlgre n M. and Lenci A., 2016). Research  show s \nthat parameters, such as the context w indow size, \ninfluence  the seman tic relatio ns that are c aptured, \neither syntag matic  relations  or paradigmatic relati ons \n(i.e., synonymy , antonym y, hyponym y, and \nmeronymy ). The s yntagmat ic relations examined in  \nmuch research are either phrasal associates (e. g., help \n- wanted) (Lapesa G. et al., 2014) or syntagmat ic \npredicate  preferenc es (Erk K. et al., 2010) in general \nlanguage. The present  study  focused on the specific \nsyntagmatic relations takes_place_in , located_ at, and \nattribute_o f, which were the most frequent relations  \nactivated by colponyms  in the spec ialized language of \nCoastal Engineering  in our c orpus. \nCount -based mode ls and word2vec have also \nbeen rec ently compared . Baroni M. et al. (2014) \ncontrast ed them on seve ral datasets and found that the \nprediction -based  models provided better resu lts. In \ncontrast, Ferr et O. (2015) found that count -based models performed better.  In another study  that \ncompare d the abil ity of both DSMs to cap ture \nparadigmatic relation s (synony my, antonymy, and \nhyponymy) and syntactic derivatives , Bernier -\nColborne G. and Drouin P. (2016) not only observed \nthat the semantic rela tions detected by the DSMs \ndepend ed on the window si ze, but al so that the values  \nof this parameter  mostly coincided in both DSMs . \nLevy O. et al. (2015)  yielded  valuable  insights, \nshow ing the following: (1) When the paramete rs of \nthe model s were correctly tuned, count -based and \nprediction -based mod els obtained similar accu racy; \nand (2) the best m odel depend ed on the task  to be \ncarried out . Nevertheless, A sr F. et al. (2016) , \nSahlgren M. and Lenci A. (2016) , and Nematzade h \nA. et al. (2017) reporte d that count -based models \noutperform ed predic tion-based ones on small -sized \ncorpora of under 10 m illion tokens. \nWork in lexical sema ntics and DSMs includes, \ninter alia , the identification of semantic rel ations \n(Bertels A. and Speelman D., 2014), classification of \nverbs into semantic groups (Gries  S. and \nStefanowitsch A., 2010), and the use of word vec tors \nas features for automatic reco gnition of named entities \nin text corpora (El Bazi I. and Laachfoubi  N., 2016). \n3 Materials \n3.1 Corpus  Data  \nThe colponyms  and related terms were extracted from \na subcorpus of E nglish t exts on Coastal Engi neerin g, \non which t he DSMs were also built . This subcorpus,  \ncompris ing roughly 7 million tokens, is compos ed of \nspecialized texts (scient ific articles , technical repo rts, \nand PhD dissertations) , and semi -specialized texts \n(textb ooks and encyclop edias on Co astal Engineering) . \nIt is an integral part of the EcoLexicon E nglish Corpus \n(23.1 million tokens)  (Le\u00f3n-Ara\u00faz P. et al., 2018 ). \nIt is wo rth clarifying  that we  were interested in the  \nsemantic beh avior of colponyms  in the specialized \nlanguage of Coastal Engineering . Since this behavior of \ncolponyms , like that of all specialized terms, is different \nin the specialized language than it is in the general \nlanguage  (Pearson J., 1998 ; Sager J.C. et al., 1 980), \nfrom a n epistemological and  methodological  point of \nview, it makes no sense to expand our corpus neither \nwith a genera l language corpus such as Wikiped ia, nor \nwith other specialized corp ora dealing with topics other \nthan Coastal Engineering.  \nFurtherm ore, the domain of the training corpus \nhas an impact on the sema ntic relations represented \nby word  embeddings . Hence, it is recommended \nusing a doma in-specific corpus to train word \nembeddings for domain -specific text  mining tasks  \nExtraction of Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus\n141 \n (Chen Z.  et al., 2018). Consequently, it also makes no \nsense to  create  meta-embeddings joining specialized \nand general pre -traine d embeddings . \n3.2 GeoName s Geog raphic  Database  \nThe automatic detection o f the colponyms  in the \ncorpus was performed with a GeoNames  database \ndump.  GeoNames7 has over 10 million proper na mes \nfor 645 differ ent geogra phic entities, such as bays, \nbeaches, and rivers . For each entity, information about \ntheir normalized designations, altern ate designation s, \nlatitude, lo ngitude, an d location  name i s stored. \n3.3 Gold Stan dard Datasets  \nThe DSMs , built on our  domain -specific  corpus,  were \nevaluated on gold standard  data. We were unab le to \nfind gold standard res ources  suitable for evaluating  \nsystems that link  semant ically related  terms to a given  \ncolponym  in the domain of Coas tal Engineering.  \nConse quently , the gold standard  data were manually \nextracted from the same  corpus  and assesse d by \nTermino logy expert s on Coastal Engineering , a \ncommon  evaluation practice  both in Informa tion \nRetrie val (Manning C.D. et al., 2009: 164-166) and \nlinguistic annotation in corpora ( Ide N. and \nPustejovsky  J., 2017: 29 7-313). In doing so,  the \nresearch community  could also employ our corpus \nand the gold standard dat a as test collection for the \nevaluation of systems dealing with  semant ic relation \nextraction from  specialized  corpora. 8 \nThe gold standard dataset s contain ed pairs of \nsemant ically related terms , in wh ich the semantic \nrelations were takes_place_ in, located_ at, and \nattribute_of . Three gold standard datasets  were thus \nbuilt, one for each of th e semantic relations.  The \ndesigna tions and m eaning of these relations are those  \nused in EcoLexicon  (Faber  P. et al., 2009 ). \nThe three semantic  relations always lin ked the \nnorma lized designation of a colponym  (e.g., Josiah\u2019s \nBay and Josiah Bay  were normalized to Josias Bay ) to \neither a process, an entity, or a property  expressed by a \nnoun or noun phrase , whether  monolexical  (e.g., \nflooding ) or multiword  (e.g., high water m ark). More \nspecifically, the takes_place_in  relation holds between \na process  (e.g., storm surge) and the bay where th e \nprocess  occurs (see Table  1). The located_at  relation \nindicates the location of an  entity (e.g., inundation \narea) in a bay (see  Table  2). Finally, the attribute _of is \nused for terms that designate propertie s (e.g., wind \nspeed) of a bay (see Table  3). \n \n7 http://www .geonam es.org  \n8 The dat asets and the  corpus will be av ailable  on th e \nwebsite of  the LexiCon researc h group of the University o f \nGranada  (Granada, Spain ) (http://lexicon.ugr.es/ ). process takes_pl ace_in  named bay \nstorm sur ge takes_place_in  Escam bia Ba y \nflooding  takes_place_in  Pensacola Bay  \ngeological proces s takes_pl ace_in  Narrag ansett Bay \nExample from t he corpus:  \n(1) Within  the Pensacola Bay and Escambia  Bay, the sh allow \nestua rine water induces sig nificant storm surge ... \nTable 1:  Extract fro m the first gold standard da taset \nfor the takes_place_in  relation . \n \nentity located _at named bay \ninundation area  located _at Pensa cola Bay  \nPort Geelong  located_at  Port Phillip Bay  \nbenthic geolog ic hab itat located_at  Greenwich Bay  \nExample  from the c orpus : \n(1) The Port Geelong  located  on Port Phillip Bay  has a \nsignifica nt role in coastal governance arrangements . \nTable 2: Extract from t he second gold standard \ndataset f or the located_at  relation . \n \npropert y attribute_of  named bay  \nwater quality  attribute_of  Narragansett Bay  \nwind speed  attribute_of  Mobile Bay \nhigh water mark attribute _of Pensacola Bay  \nExample  from the corpus : \n(1) ... the simulated and observed high wate r marks  at six \nstations around  Pensa cola B ay and Escambia Bay agree ... \nTable  3: Extract from t he third gold st andard dataset \nfor the attribute_of relation . \n \nIn addition to what has been described , each of the \nthree datasets includ ed: (1) 100 triplet s for the \ncorresponding semantic  relation , which w ere all used \nfor the eva luatio n, therefore, the three dat asets added \nup to 300 tri plets; (2) the 50 most frequentl y \nmentioned bays in the corpus , the same 5 0 bays  in the \nthree dat asets, since 50 information needs  have \nusually been found to be a sufficient  minimum  for \ninformation retrieval system evaluation  (Manning  \nC.D. et al., 2009: 152); and (3) the two most frequen t \nterms related to the same bay, which amoun ted to 100 \ntriplets , therefore, the same bay w as related to a total \nof six terms , two terms  in each dataset .  \nThe semantic relation annotation  of the pair  of \nterms ext racted from  the corpus was c arried ou t by \nthree te rminologis ts from the LexiCon  research  group \nof the University of Granada (Granada, Spain ), with \nwide experience in environmental knowledge \nrepresentation . Cohen\u2019s kappa  coefficie nt was used as \nthe statistica l measure of inter -annotation agreement, \nand the scores for all  the annotator pairs stood over \n90% (p-value <0.05 for all the annotator pairs ). \nJuan Rojas-Garcia\n142 \n 4 Methodo logy \n4.1 Pre-processing  \nThe corpu s texts were tokenized, ta gged wi th parts of \nspeech, lemmatiz ed, and lowercased  with the Stanford \nCoreNL P package (Manning C.D. et al., 2014) for R \nprogramming language . The m ulti-word terms  stored \nin Eco Lexicon were then automatic ally m atched in the \nlemmatized c orpus an d joined  with underscores.  \nIn the DSM s, only terms larger than two \ncharacter s were conside red. Numbers , symbols,  and \npunctuation marks  were removed . Since close d-class \nwords a re often  consi dered too unin formative to be \nsuitable context words , stopwo rds were not us ed (i.e., \ndeterminer s, conjunction s, relative adverbs, and \nprepositi ons). Addition ally, the minimal occurrence \nfrequency was s et to 5 so that the co -occurrences \nwere statistic ally reliable (Evert  S., 2008). \n4.2 Named Bay Recognition  \nBoth norma lized a nd alterna te names o f the bays in \nGeoNames were searched in the lemmatiz ed cor pus. \nThe recog nized designations were normalized and \nautom atically joined with under scores . Most bays of \nthe corpus were in GeoNames  (90%), while others \nwere ide ntified  by manual ins pection  (10%). \nAnaphoric e lements referring to a  bay were replaced \nby the correspo nding colponym  in the l emmatized \ncorpus . For this task, the anaphora resol ution fu nction \nfrom CoreNLP  package was u sed, and other cases \nwere manua lly replaced. The 294 bays mention ed in \nthe corpus  are shown on the map in  Figure 1. \n \n \nFigure 1:  Heatmap with the location and color -coded \nfrequency of the 294 named ba ys. \n4.3 Constructio n of th e DSMs  \nOur experimen t involved  a comparative evaluation of  \nthree types of  DSM  for a small -sized, specialized \ncorpus, na mely, count -based , prediction-based , and \npre-trained models . The model t ypes produced the \nvector  repres entation of a term b ased on the contexts  \nin which it ap peared in our corpus. For this study , the contexts of a target t erm (i.e., a colponym ) were the \nterms that co -occurred with  it inside a s liding context \nwindo w, whic h spanned a certain numb er of terms on \neither side of th e target term.  \nThe count -based and prediction -based  DSMs \nhave various parameters  that must b e set to build the \nmodel s. The par ameters impinge on both the term \nrepresentations  and the accur acy of the similarity \nscores between term vectors when the models are \ncompared  (Baroni M. et al. , 2014). There fore, to \nassess  the influence  of the para meters of both DSMs \non their abil ity to capture the three semantic relations \ntargeted in this study , various  settings for ea ch \nparam eter were tried, and the combinations of th ese \nparameter setti ngs were ev aluated.  \n4.3.1 Parameter Settin g of  the Count -based \nModels \nThe first model type  evaluated w as a count-based  \nmodel, also called bag -of-words (BOW) model.  The \nBOW mode l was built with the R pa ckage quanteda  \n(Benoit K.  et al., 2018 ) for text mining.  \nTo bu ild a BOW model, a term-term matrix of \nco-occurren ce frequenci es was first computed , \naccording to a specific si ze for the sliding context \nwindow. Then, the  matrix  was subjected to a specific \nweighting scheme , namely, an association measu re \nthat increases t he importance of the context terms t hat \nare more i ndicative of  the m eaning of the target term . \nThe 1,000 most frequent terms wer e used, which \ninclude d all the colponyms  and term s stored in the \nthree evaluation datasets . \nRegardin g the cont ext window, we tested size \nvalues ranging from 1 to  10 words on either side  of \nthe target term, and the context window was all owed \nto span senten ce boundaries.  The context window \nshape was always  rectangular (i.e., the increment \nadded t o the co-occurrence frequency of a pair of \nterms was always 1, regardle ss of the distance \nbetwe en the t wo terms inside the context window ). \nThe frequen cies observed on the left and right  of a \ntarget te rm w ere added . \nWith respect to the weighting scheme s, thre e \nassoc iation mea sures, defined in Evert  S.\u2019s (200 8) \nwork on colloca tion, were tested : (1)  statistic al \nlog-likelihood ; (2) positive pointwise m utual \ninform ation ( PPMI);  and (3) t-score. Log-likelihood \nand P PMI are widely used in computational \nlinguistics,  whereas  t-score is popular in \ncomputational lexicography (Evert S. et al., 2017).  \nResearch in computatio nal linguistics reveals that \nlog-likelihood  is able to  capture  syntagmatic and \nparadigmatic relati ons (Lapesa G. et al., 2014), and \nperfor m better  for me dium- to low-frequenc y data \nthan other association measures (Alra bia M. et al., \nExtraction of Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus\n143 \n 2014). PPMI and t-score, on the other hand,  have \nbeen found to work adequately  for dif ferent \napplications in previous rese arch when comp ared to \nother association mea sures (Baroni M. et al. 2014 ; \nKiela D. and Clark  S., 2014). \nFinally, f ollowi ng Lapesa G. et al.  (2014), the \nassocia tion scores were transformed to reduce \nskewness in this way: l og-likelihood and PPM I scores \nwere b oth transforme d by adding 1 and calculating  \nthen the natural logarithm  (ln), whereas t-scores were \ntransformed by calculating the square root  (sqrt). \nThe sett ings tested for each of the two parameters \nwere: \n1. Size of the context window: 1 -10 words . \n2. Weighting scheme: ln(log-likelihood  + 1), \nln(PPMI + 1), s qrt(t-score).  \n4.3.2 Paramet er Setting of the Prediction -based \nModel s \nThree prediction -based mod els were evaluate d, \nname ly, the word2vec  (Mikolov  T. et al., 2013), the \nfastText (Bojanowski  P. et al., 2017 ), and the GloVe  \n(Pennington J.  et al., 2014) models . In word2vec \n(W2V ), the term vectors are learned by training a \nneural n etwork on a corpus accor ding to two different \narchitectures. The continuous  bag-of-words (CBOW) \narchitectur e predict s the target term base d on its \ncontext terms , while the skip-gram architect ure \npredicts the context terms of a target t erm. The W2V \nmodel wa s built with t he origina l word2vec  package . 9 \nFor W2V , five hyperparameters w ere examined , \nthe same a s those tested  by Bernier -Colborne  G. and \nDrouin P. (2016)  for paradigmatic rel ations and \nsyntactic derivati ves. The first one was the \narchitecture used to  learn the term vector s. The \nsecond one was the training algorithm, either us ing a \nhierar chical softm ax function, or by sampling \nnegative examples , in which case t he number of \nnegative sample s must be selected . The third \nhyperparameter was  the subsampling threshold for \nfrequent t erms, namely, some occurrences of those \nterms whose r elative freq uency in the  corpus is \ngreater than a threshold , are rand omly deleted befo re \nthe model is trained. Finally, the dimensio nality of the \nterm vector s, and the size of the  context window were \nthe other hyperparameters.  \nThe settings tested for each o f the five \nhyperparamete rs were:  \n1. Architecture: CBOW or ski p-gram.  \n2. Negative sampl es: 5, 10 or none  (in this c ase, \nhierarchic al softmax is used).  \n3. Subsampling threshold: low  (10\u20145), high ( 10\u20143) \nor none. \n \n9 https: //code.google. com/archive /p/word2vec/  4. Dimensionality of term embeddings: 100 or 30 0. \n5. Size o f context wi ndow: 1 -10 words . \nIn the fastText  model  (FTX), which is essent ially \nan extens ion of the W2V model , each word is trea ted \nas composed of subwords , namely , all the substrings \ncontain ed in a word between a minimum and a \nmaxim um size. Hence , the vector for a wo rd is made \nof the sum of th ese subword vector s. The FTX model \nwas bu ilt with the original fastTe xt package .10 For \nFTX, the same five hyperparameters as thos e for \nW2V were probed. All the subword s between 3 and 6 \ncharacters  were taken  (default values for the model ).  \nThe GloVe  model  optimi zes the likel ihood  of term \nprobabilities, based on context , to learn term \nrepresenta tion as in CBOW, but uses ratios of \nco-occurrence probabilities  as the basis  for learning.  \nThe model was built w ith the original GloVe  \npacka ge,11 and two hyperparameters  were explo red: \n1. Dimensionality of term embeddings: 100 or 3 00. \n2. Size o f context win dow: 1 -10 words . \nIn addition, f or both  GloVe , W2V , and FTX, the \nnumber of epochs was fixed to 1 0, and the learning \nrate to 0.05. \n4.3.3 Pre-trained Models  \nPre-trained  word vectors, estimated from \nexceptionally  large, general  corpora , typically  \nimprove the perform ance of NLP systems  (Baroni, \nM. and  Lenci  A., 2010). For that reason, we also \nassess ed the pre -trained word2vec  and fastText \nmodels (Mikolov  T. et  al., 2018 ),12 and the \npre-trained GloVe  mode l,13 all of t hem trained on  the \nCommon Cr awl cor pus (600-840 billio n tokens ) with \n300-dimension vectors . The pre-trained B ERT deep \nlearning model  was also considered . \nThe parameter  values  of the pre -trained mod els \nwere already set in the pre -training phase . For \ninstance,  the context window size of the  pre-trained \nword2vec  and fastText  models was fixed to  15 words , \nand that of the  pre-trained GloVe  mode l was fixed to \n10 words. Consequently , the window size  of these \nthree pre -trained models could not  be modified  for \nour evaluatio n. This was deemed to be a drawback  \nwith resp ect to the overall goal of this st udy, since it \naimed to provide  termi nologists  with three lists of \nterm candidates  for a colponym , one li st per semantic \nrelation . Instead, a  pre-trained model could only  \nextract  a single list of term candidates for a colpon ym. \nAnother downside to  the pre-trained word2vec , \nfastText , and GloVe  models w as found.  Despite  the \n \n10 https://github.c om/facebookresearch/fastT ext \n11 https:/ /nlp.stanford.edu/projects/glove/  \n12 https://fa sttext.cc/ \n13 https://nlp.stanfo rd.edu/data/ glove.840B.300d.zip  \nJuan Rojas-Garcia\n144 \n considerable  size of the tra ining corpus and vocabulary  \nin the three pre-trained model s, they had less \ntermino logy coverage t han the doma in-specific model s \nevalu ated in this work . This pitfall has been already \nreported by Nooralahzadeh F. et al.  (2018), and it is \nhardly  surprising given  that pre vious studies have \nobserved  that mu lti-word terms account for mo re than \n90% o f the te rms of a spe cialized knowledge domai n \n(Krieger MG. and Finatto  MJB. , 2004; Nakov  P., \n2013; Nguyen N .T.H. et al., 2017; Sager J.C. et al., \n1980). As a consequence , since the pre -trained models  \ndid not contai n most  of the multi-word terms  used in \nour special ized corpus and evaluat ion data  (96% of the \nterms in the gold standard data are multi -word units ), \nwe calculated  the missing multi-word term  vectors by  \napplying a comp ositional s emantic model  called Basic \nAdditive Model (BAM ) (Mitchell J. an d Lapat a M., \n2008). BAM com putes the vector of a mu lti-word term  \nby adding its component single -word vectors.  The \ncompositional pre-trained models are henceforth \nreferred t o as p t-W2V -BAM , pt-FTX-BAM, and \npt-GloVe -BAM . \nThe pre -trained B ERT model ( Devlin J.  et al., \n2019) was also eva luated. Context -free mod els such \nas W2V , FTX,  and GloVe produce  a single, fixed  \nembedding  representation for each word in a corpus . \nInstead , BERT is a contextual deep learning model  \nwhich generates  as many representations f or a target \nword as the number of tim es it appears in a c orpus, \nsince each representati on is based on the other words  \nthat accompany the target word in each sentence . \nWe employed  the uncased  version  of the \nBERT -Base model  in Python ,14 with 768-dimension \nvectors.  This mo del has 12 encoder layers, 768 \nhidden  units in t he feed-forward  networks , and 12  \nself-attention heads . The terms of our corpus were \nadded to  the vocabulary fil e of the model . Each of the \ncontextualized embeddings for a term  was obtained \nby adding up  the vectors from the  last four encoder \nlayers , a proce dure already  applied by Devlin J. et  al. \n(2019). Nevertheless, f or the model  evaluation, we \nused a si ngle, averaged embedding  for each term, \nwhich resulted from the average of all th e different  \ncontext ualized  embeddings fo r the same  term. As in \nthe case of  GloVe , W2V , and FTX, the number of \nepochs was fixed to 1 0, and the learning rate to 0. 05. In \naddition , the parameter for the  maximum sentence \nlength  was set to 64 because: ( 1) It is one of the values \nrecomme nded by  Devlin J. et  al. (2019); and ( 2) the \nmaximum sentence length  of our corpus was 57 words.  \nAlthough  there exists  the pre-trained SciBE RT \nmodel (Beltag y I. et al., 2019 ), based on BERT  but \ntrained  on a large corpus of scientific text s, SciBERT \n \n14 https://github.com/google -research/bert  was not  used because  the training corpus consisted  of \npaper s from  the computer science and biomedic ine \ndomains , which are far from bein g related  to the \nCoastal Engine ering domain  of our corp us. \nIn summary, we applied and evaluated eight \ndifferent  DSMs : BOW , W2V, FTX, G loVe, pre-trained \nBERT,  and the three compositional pre -trained models . \n4.4 Evalua tion of the DSMs  \nFirst, for each bay includ ed in the gold standard \ndatase ts, a sorted list  of neighbours was obtained by \ncomputi ng a similarity/distance measure  between the \nbay\u2019s vector and the vectors of a ll othe r context terms . \nThen, these context terms  were sorted in descending \norder of magn itude. As such , for each  bay, a list of \nranked retrieval  results was compiled . \nSubsequently, t he sorted lis ts of neighbo urs were \nevaluated on the whole gold standard dat aset \nconst ructed for each of the three semantic relations.  \nThe measure used to evaluate  the models was Mean \nAverage Precision (MAP)  (Manning  C.D. et al., \n2009: 158-162). Unlike the Precision, Reca ll, and \nF-score measures, which are computed  using \nunordered sets of items , MAP  is more appropriate  for \nthe evaluat ion of  ranked retriev al results, such as ours . \nMAP provide s a single -figure measure o f quality \nacross recall levels , and so it is roughly the  average \narea under the precision -recall curve for a set of queries . \nAdditionally, MAP has been shown to have especially \ngood discrimina tion and stability ( ibidem , p. 160).  This \nmeasu re tells  us the overall accuracy l evel of  the sorted \nlists of neighbours  obtained for all bay queries , based \non the rank of the related terms  according to the gold \nstandard. The n earer the related te rms are to the top of \nthe list for each  bay, the high er the MAP . \nThe evaluatio n proce ss delineated above  was \nrepeated  for each of the five similarity/distance \nmeasures  computed between a bay\u2019s vector and the \nvectors of a ll othe r context te rms. The five measures  \nevaluated  in this study  were Euclidean di stance , cosine \nsimilarity , Jaccard coefficient , Pear son correlation \ncoefficient, and averaged Kullback -Leibler divergence . \nFor space constraints, we refer reader s to Huang A. \n(2008) for a det ailed description of the properties and \nformulas of these measure s. \n5 Resu lts \nThe eight mode ls were compared by obser ving the \nMAP of each mo del on the three datasets. Regarding \nthe similarity/distance measures , it was found that, \nexcept for the Euclidean  distance , which  performed \nthe wor st, the other  four measures ha d comparab le \neffectiveness for  all the DSMs and semantic relation s, \naccording to the resu lts of the ANOVA t ests, run to \nExtraction of Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus\n145 \n determine the significance o f the performance -wise \ndifferences am ongst t he similarity/distance measures . \nMAP scores were used as the basis of comparison. \nThis behavio r is in line with previous  research  on \nsimilarity measure  comparison  by Huang A. ( 2008), \nand Strehl  A. et al. ( 2000). For th at reason,  Table  4 \nshows the m aximum M AP ac hieved by each mode l \nwhen applied cosi ne similarity, sin ce this measu re is \nwidely used in  NLP sy stems . \n \nDataset  BOW  model  \nMax imum  MAP  Weighting \nscheme  Window size  \ntakes_pl ace_in  0.552 (0.395 \u00b1 0.080) LL 4 \nlocated_at  0.410 (0.308  \u00b1 0.054 ) LL 2 \nattribu te_of 0.339 (0.197  \u00b1 0.052 ) LL 3 \nDataset GloV e model \nMax imum  MAP   Window  size \ntakes _place_in  0.522 (0.395 \u00b1 0.077 )  4 \nlocated_at  0.381 (0.278 \u00b1 0.050 )  2 \nattribute_of  0.302 (0.190  \u00b1 0.042 )  3 \nDataset FTX model  \nMax imum  MAP   Window size  \ntakes _place _in 0.482 (0.284  \u00b1 0.107 )  4 \nlocate d_at 0.339 (0.223  \u00b1 0.061 )  2 \nattribut e_of 0.274 (0.136  \u00b1 0.057)  3 \nDataset  W2V model  \nMax imum  MAP   Window size  \ntakes_place_in  0.349 (0.312  \u00b1 0.031 )  4 \nlocated _at 0.209 (0.183 \u00b1 0.014 )  2 \nattribute_of  0.170 (0.111  \u00b1 0.032 )  3 \nData set Uncased BERT -Base model  \nMax imum  MAP  \n(single value )   \ntakes_plac e_in 0.355   \nlocated _at 0.213   \nattribute_of  0.173   \nData set pt-GloVe-BAM  model  \nMax imum  MAP  \n(single valu e)  Fixed \nwindow size  \ntakes_place_in  0.264  10 \nlocated _at 0.151  10 \nattribute_of  0.109   10 \nDatase t pt-FTX -BAM  model \nMax imum  MAP  \n(single value )  Fixed \nwindow size  \ntakes_place_in  0.231  15 \nlocated _at 0.114  15 \nattribute_of  0.072  15 \nDataset  pt-W2V -BAM  model  \nMax imum  MAP  \n(single value )  Fixed \nwindow size  \ntakes_place_in  0.199  15 \nlocated _at 0.089  15 \nattribute_ of 0.046  15 \nTable 4: Maximum  MAP of the models on each \ndataset  when applied cosine similarity . Average  and \nstandard deviation are shown in b rackets, LL stands \nfor the log -likelihood we ighting scheme . \n The results indica ted that the BO W model obtained \nthe best performance in term s of MAP on the th ree \nsemant ic relations when  its parameters were correctly \ntuned . They also showed that  the takes_place_ in \nrelation was the most accur ately capt ured by all models  \nwhen th ey were tuned for thi s relation , followed b y the \nlocated_at  and attribute_ of relations . \nThe greater accuracy of takes_place_in  may be due \nto the large number of instances in specialized  texts in \nCoastal Engineering which  express t he pr ocesses that \noccur in named bays. As for the located_at  and \nattribute_ of relation s, these texts frequently m ention the \nentities in  named bays and the  propertie s of th ese \nlandforms . However, i t seems  that th e number of \ninstances of both seman tic relati ons in the whole \ncorpus is not large enou gh for the DSMs to represent  \nthem as accurate ly as takes_pl ace_in  instan ces. \nTable  4 also shows that  the maximum MAP  of \nthe BOW model  was achieved when:  \n1. The statistic al association measure  for the three  \nsemantic relations was log-likeliho od, transforme d by \naddin g 1 and calculating  the natural log arithm. \n2. The window size  for the takes_place_in  relation  \nwas 4 words . \n3. The window size for the attribute_of  relation \nwas 3 words.  \n4. The window size for the located_at  relation was \n2 words. \nStrikingly,  the BERT and the three compo sition al \npre-trained mode ls performed the worst of all DSMs . \nVarious factors are known to  be associated with  this \nbehavio r. First ly, in NLP systems  for speciali zed \ndomains,  the perfor mance of  domain -specific term \nvectors is higher  than that of pre-trained embedd ings, \neven when the  size of the speci alized corpus is  \nconsiderably smaller (Nooralahzadeh  F. et al., 2018). \nSecon dly, domain -specific terms are inefficiently \nrepresented in pre-trained  embeddings  since there are  \nfew statistical clues in  the underlyin g general-domain \ncorpora for these words (Bollegal a D. et al.,  2015; \nPilehvar M.T. and Collier  N., 2016) . Third ly, BAM \nmodels  tend to per form worse in comparison to  their \nnon-composit ional coun terparts  that learn multi -word \nterm vectors ( Nguye n N.T.H. et al., 2017). \nInterestingly,  in each data set, the max imum MAP \nof the BOW, GloVe, FTX, and W2V models was \nreached  when the window size was the same . For that \nreason , to assess the impact of the win dow si ze on the \naccuracy of the DSMs, the average M AP for each \nsetting of this parameter  (i.e., for each  window size \nbetwe en 1 and 10 word s) is illustr ated in Figure 2. The \naverage MAP was used, instead of the maximum, \nbecause it allowed us to de termine wh ich window -size \nsettings consis tently produced satisfactory resu lts, \nregardles s of the s ettings u sed for the oth er paramete rs. \nJuan Rojas-Garcia\n146 \n \n \nFigure 2:  Average MAP of B OW (always left), \nGloVe ( upper right ), FTX (middle  right), and W2V  \n(bottom right) w .r.t. window si ze. \n \nIn Figure 2 we can observe  that, in the four DSM s \n(BOW,  GloVe, FTX, and W2 V), the opti mal window \nsize was 4 words for the takes_place_in  relation, 3 \nwords for attribu te_of, and 2 wor ds for located_at . \nThe compo sitional pre -trained models were not \nshown owing t o their extrem ely suboptimal \nperformance  and the ir fixed window sizes . \nSince the  count-based model BOW notably \noutperf ormed  predic tive models  on the three datasets , \nfor the sake of simplicity, the setting  influence  of the \nother four hyperparamet ers of FTX and W2V are \nsuccinctly  reported becau se they did not le ad to \nsubstanti al accurac y improvements on either dataset . \nAs such, for bot h predictive  models, settings  can be \nsummarized as follows: (1) The neural network  \narchitec ture skip -gram worked, on average, better than \nCBOW ; (2) a negative samp ling o f 10 samples  reached \na larger MAP tha n the hierarchi cal softmax ; (3) the \nsubsampling threshold was not conducive  to significa nt \ngains; and (4) the optimal setting for the dimensiona lity \nof the term embedd ings was 300 dimensions.  \nThese optimal settings  for the  predictive models  \nFTX a nd W2V were thus in line with previous \nresearch  (Chiu B. et al., 2016 ). Moreover, FTX \nseemed to  perform  markedly  better  that W2V  for the \nthree semantic relations . This behavio r may be li nked \nto the fact that, as FTX exploit s character -level \nsimilarities between  terms, it is able to model \nlow-frequency  terms more effectively , thereby \nachieving better performanc e for small -sized corpo ra \n(Bojanowski P. et al., 2017 : 140-141). \nRegard ing GloV e, with 300 -dimension vectors, it \nwas the only predicti ve model whose  perform ance \nreached va lues similar to those of BOW . There  is \nsome evidenc e that the generalization ability of neu ral network-based  models, such as FT X, W2V,  and \nBERT , decreases w hen they learn on  a limited \namount of  data (Collobert R. et al., 2011).  \nAccord ingly, s ince GloVe is not imple mented with \nneural networks , the model  did not se em undul y \naffected by the reduced corpus s ize. \nIn order to verify our observation s on the be havio r \nof the BOW, GloVe, FTX, and W2V  models , \nstatistical tests were run to determine the signif icance \nof the performance -wise differences am ongst t he \nmodels . MAP scores were used as the basis of \ncomparison . As they  did not  deviate  from the normal \ndistribution according  to Shapiro -Wilk test  results  \n(p-value >0.05), parametric  statistical tests were thus \ncarried out. For each semantic re lation , we conducted  \nthe independent  measures one-way ANOVA  test, \nfollowed by  post-hoc multiple  pairwise  comparisons \nbetween the models . For the multiple testing \ncorrection , we employ ed false discovery rate using \nthe Ben jamini -Hochberg procedure  (Benjamini Y. \nand Ho chberg  Y., 1995). \nThe conclusions dr awn from the statistical test  \nresults  can be outlined as follows , and apply to the \nthree sem antic relations : (1) The performance of \nBOW was not significantly better than that of GloVe  \n(p-value> 0.05), whereas  both models  significantly  \noutperformed the remaining DSMs  (p-value s<0.05 ); \n(2) there was no significant difference between t he \nperformance  of the FTX and W2V  models  \n(p-value>0.05 ), despite the maximum MAP values \nfor FTX we re higher than those for W2V . \nThe evaluation indicated, in all  models, MAP \nscores that could  initial ly be re garded as quit e low . \nThese results are striking , given th at the models were \nspecially tuned to work in the sp ecified scen ario and \nwith three  semant ic relations . To truly appreciate the \nvalue  of thi s work and th e difficulty of the task  \ninvolved , we compare our  results  to those  of two \nother studie s that addressed similar scenarios,  and \nwhich also compar ed count -based and \nprediction -based DSMs . \nBernier -Colborne G. and  Drouin  P. (2016 ) \ncompar ed the abil ity of both types of DSM to capture \nrelations from the web-crawled PANACEA \nEnvironment Englis h monolingual corpus  \n(Prokopid is P. et  al., 2012),15 with a  size of over 50 \nmillion token s. The authors reported  maximum MAP \nfigures  ranging from  0.199 to 0.54 4. These values  are \nsurpris ingly simila r to th ose found  in our study  for the \nBOW, Glove, FTX, and W2V models  (from 0.170 to \n0.552, accor ding to Table  4), althou gh the size of our \ncorpus  is much small er (7 mill ion tokens ). \n \n15 http://catalog.elra.info/en -us/repository/brows e/ELRA -\nW0063/  \nExtraction of Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus\n147 \n On the othe r hand, Nguyen N .T.H. et al. (2017), \namong other objectives,  aimed to extract , with both \ntypes of DS M, scientific and ver nacular names  \nsynonymous to plant sp ecies from the English sub set \nof the Biodiversity He ritage Librar y (BHL) (Gwinn \nN. and Rinaldo C., 2009 ),16 an open -access repository \ncontaining millio ns of digiti zed pa ges of le gacy \nliterature  on biodiversity . The enormo us corpus size \nof the English subset o f BHL amounts to around 49 \ngigabytes of data.  None theless, t he authors reported \nmoderate maximum  MAP scores , ranging from  0.283  \nto 0.621.  In co ntrast, Table  4 shows that the \nmaximum  MAP valu es obtained  by the BOW model  \nvaried from 0.339 to 0.552. These are  extremely \npromising measures, especially  consider ing the tiny \nsize of our corpus compared to that of  BHL corpus.  \nOverall, the MAP values of our BOW model are \nstrikin g because  they are  quite high despite the small \nsize of the corpus . \nFinally, t he err or analysis  revealed  that the terms \nin the gold standard datasets with the  lower number of \nmentions in the corpus  systematically  occupied lower \npositions  in the lists of ra nked retr ieval results \ncompiled  for each DSM . Thus, this fact  negatively \naffected the MAP scores. \n6 Conclusions  \nThe representation in EcoLexicon of  the conceptu al \nstructures (Faber P., 20 12) that underlie the usa ge of \ncolponyms  in a small -sized, English Coastal \nEngin eering corpus  requires terminologists to  \nmanu ally extract  from the corpus  the terms which  \nrelate to each colponym  through  the semantic \nrelations takes_place_ in, located_ at, and attribute_ of, \nthe three most frequent  relation s held by named bays \nin the corpu s. Since this is a time-consuming  task, th e \noverall aim of this study was to  provide terminologist s \nwith three lists of term candidates  for a colponym , one \nlist per semantic relation , by applying DSMs . \nAccordingly, count-based and p rediction -based \nDSMs , pre-trained models , and fiv e similarity  \nmeasures  were applied  to the corpus . Since t he \nconstructi on of DSMs  is high ly parame terize d, and \ntheir evaluation  in small specia lized corpora has \nscarce ly received attention,  this study identified both \nparameter combinations in DS Ms and similarity \nmeasures  suitable for the extr action of terms which  \nrelate d to colponyms  through  the above mentione d \nsemantic rel ations . The mo dels were thus evalu ated \nusing three gold standard datasets.  \nCount-based  model s, with the log -likelihood \nassociation measure,  showed  the best performanc e for \n \n16 https://www.biodiversitylibrary.org/  the three semantic r elations. The se results  reinforce \nthe fi ndings  of previou s research  that states, on the \none hand , that count -based DSM s surpass  \nprediction -based ones on small -sized corpora of under \n10 million tokens (A sr F. et al., 2016; Sahlgre n M. \nand Lenci  A., 2016; Nematzadeh A. et al., 201 7), and \non the o ther h and, that log-likelihoo d achieve s greater \naccuracy  for med ium- to low-frequency data than \nother association measures (Alrabia M. et al., 2014). \nIn this respect , research on the applicati on of DS Ms in \nsmall  specia lized corpora , such as ours, is particularly  \nscarce, compar ed to the plethora  of work that \nanalyz es DSMs in  large gen eral corpora . Hence, more \nstudies  of this ty pe are needed  so that  further insights  \ncan be gained into the efficient representa tion of small \nspecialized  corpora in DS Ms. \nFor both count-based  and pred iction -based  DSMs , \nthe optimal  window size depended on the semantic \nrelation  that wa s to be captu red, and t he specific values \ncoincided in bo th types of DSM, namely, a window \nsize of 4 words  for the takes_place _in relation , 3 words \nfor attribute_o f, and 2 words for located_ at. The \ndependence of the  window size on the speci fic \nsemantic rel ation is in line with the findings  by \nBernier -Colbo rne G. and Drouin  P. (2016). \nIt was also fou nd that the takes_place_in  relation was \nthe most accurately represented by the DSMs, followed \nby located _at and attribute_o f. This was possibly  due to \nthe insufficient number of i nstances of both  semantic \nrelations in the corpus for the DSMs to re present them as \naccura tely as takes_plac e_in instances. \nThe pre -trained models GloVe , word2vec , \nfastText , and BERT  performed the worst of all DSMs . \nIn addition, they only provided a single  list of  term \ncandidate s for a colponym , which  became l ess \nmeaningful because it was not clear the relation of th e \nlisted terms to  the colponym . \nRegarding  the similarity  measures, it was found \nthat, except for the Euclidean  distance , which  \nperformed the w orst, the other  four measures had \ncomparab le effectiveness for  all the DSMs and \nsemantic relation s. This behavio r is in agreement  with \nprevious research  on similar ity measu re comparison  by \nHuang A. ( 2008), and Strehl  A. et al. (2000). \nFinally, a n extens ion of this work  will incl ude \ntesting the same DSMs and similarity/distance \nmeasures on gold sta ndard datasets f or named  beaches . \nAcknowledgements  \nThis research was c arried out as part of project \nPID2020 -118369 GB-I00, Transversal Integration of \nCulture in a  Terminological Knowledge Base on \nEnvironment  (TRANSCULTURE ), funded by the \nSpanish Ministry of Science and Innovation . \nJuan Rojas-Garcia\n148 \n Reference s \nAlrabia, M., N. Alhelewh , A. Al-Salman, and E. \nAtwell. 2014. An empirical study o n the Holy \nQuran base d on a l arge classical Arabic corp us. \nInternati onal Journal of Computational \nLinguis tics, 5(1): 1-13. \nAsr, F., J. Willits,  and M. Jones . 2016. Comparing  \npredictive and co -occurre nce-based mo dels of \nlexical semantics tr ained on child -directed speech. \nIn A. Papafrag ou, D. Grodner, D. Mirman , and J. \nTrues well (eds.), Proceedings of the 3 8th Annua l \nConference of the Cognitive Science Society \n(CogSci) , Philadelp hia (Pennsylvania) , \n1092-1097. \nBaroni, M., G. Dinu, and G. Krusz ewski . 2014. Don\u2019t  \ncount, predict! A sys temati c comparison of \ncontext -counting vs. co ntext-predict ing seman tic \nvectors . In Proceedings of the 52nd Annual \nMeeting of the Associ ation for Computa tional \nLinguistics , vol. 1 , 238-247. \nBaroni, M. , and A. Lenci . 2010. Distributiona l \nmemory: A general fr amewor k for corpus -based \nsemanti cs. Computati onal Linguist ics, 36(4):  \n673-721. \nBeltagy, I., K. Lo, and A. Cohan . 2019. SciBERT: A \npretrained language model for scientific text. In \nProce edings of the 20 19 Conference o n Empirical \nMethod s in Natural L anguage Proces sing, Hong \nKong, 3615-3620. \nBenjamini, Y., and Y. Ho chberg. 1995. Contro lling \nthe false discovery rate: a practical and powe rful \napproach to multiple testing. Journal of the Royal \nStatistical Soc iety, 57(1): 289-300. \nBenoit K ., K. Watanabe, H. Wang, P. Nulty, A. \nObeng, S. M\u00fcller , and A. Matsuo . 2018. \nquanted a: An R pa ckage for the quantitative \nanalysis of textual data.  Journal of Open Source \nSoftware , 3(30): 774. \nBernier-Colborne, G. , and P. Droui n. 2016. \nEvalua tion of distributi onal semantic models : a \nholistic appr oach. In Proceedi ngs of the 5t h \nInternational  Workshop on Computa tional \nTerminology (Computerm) , Osaka  (Japan), 52-61. \nBertels, A. , and D. Speelman. 2014. Clustering for \nseman tic purpose s: Exploration of seman tic \nsimila rity in a technical corpus. Terminolo gy, \n20(2): 279-303. \nBojanowski,  P., E. Grave,  A. Joulin, and T. Mikolov . \n2017. Enriching word vectors with subword  \ninformat ion. Transactions of the ACL, 5: 135-146. \nBollegala, D., T. Maehara, Y. Y oshida, and K. \nKawaraba yashi . 2015. Learning word representations from relati onal graphs. In \nProceedings of the 29th AAAI Conference on \nArtificial Intelligence , Palo Alto, 2146-2152.  \nChen, Z., Z. He , X. Liu, and J. Bian. 2018. Evaluating \nsemanti c relations in neural  word embedd ings \nwith biomedical and  general domain knowledge \nbases. BMC Medical Info rmatics a nd \nDecisionMaking , 18(Suppl 2 ): 65. \nChiu, B., G. Crichton , A. Korhonen, an d S. Pyysalo. \n2016. How t o train good word embeddings for \nbiomedica l NLP . In Proce edings of the 15th  \nWorkshop o n Bio medical NLP, Berlin, 166 -174. \nCollober t, R., J. Weston,  L. Bottou , M. Karlen , K. \nKavukcuoglu , and P. Kuksa . 2011. Natural \nlangu age processin g (a lmost) fr om scratch. \nJournal of Machine  Learning Research , 12(A ug): \n2493-2537.  \nDevlin, J., M.W. Chang, K. Lee, and K. Toutanova . \n2019. BERT: Pre -training of deep bidirectional \ntransformers for language understandi ng. In arXiv \npreprint  arXiv:1810.04805v 2. \nEl Bazi, I., and N. Laachfou bi. 2016. Arabic named \nentity  recogn ition using word re presentations.  \nInternatio nal Journal of Computer Science and \nInforma tion Security , 14(8): 956-965. \nErk, K.,  S. Pad\u00f3,  and U. Pad\u00f3. 2010. A flexible, \ncorpus -driven model of  regular an d inverse \nselectional p references. Computational \nLinguis tics, 36(4): 723-763. \nEvert, S. 2008. Corpor a and collocations . In A. \nL\u00fcdeling and M. K yt\u00f6 (eds.) , Corpu s Linguisti cs. \nAn Inte rnational Handbo ok. Berlin: Walter de \nGruyter , 1212-1248. \nEvert , S., P. Uhrig, S. Bartsc h, and T. Proisl . 2017. \nE-VIEW -alation \u2013 A large-scale evalua tion study \nof association  measures for collocation \nidentification. I n Proceedings of the eLex 20 17 \nConferen ce, Leiden , 531-549. \nFaber, P. (ed.). 2012. A Cogn itive Linguis tics View o f \nTerm inology and Sp ecialized Language . \nBerlin/Boston: D e Gruyter Mouton.  \nFaber, P., P. L e\u00f3n-Ara\u00faz, and J.A. Prieto. 2009. \nSemantic re lations, dynamicity, and \nterminologic al knowledg e bases. Current  Issues \nin Language Studies , 1: 1-23. \nFerret, O . 2015. R\u00e9or donnancer des  th\u00e9sau rus \ndistributionnels en combinant d iff\u00e9rents crit\u00e8res. \nTAL, 56(2): 21-49. \nGries , S., and A. Stefanowitsch . 2010. Cluster \nanalysis and the i dentification of colle xeme \nclasses.  In S. Ric e, and J. Newman (eds.) , \nEmpirical and E xperimental Methods in \nExtraction of Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus\n149 \n Cognitive /Functional Research . Stanford \n(California) : CSLI, 73 -90. \nGwinn N. , and C. Rinaldo. 2009. The Biodiversity \nHeritage Librar y: Sharing biodiv ersity with  the \nworld.  IFLA Journal , 35(1):25 -34. \nHuang, A. 2008. Similarity measures for text \ndocument clustering. In Proceedings of the New \nZealand Co mputer Science Research Student \nConference  2008, Christchurch , 49-56. \nIde, N. , and J. Pustejovsky (eds.). 2017. Handbook of \nLinguistic  Annotation . Dordrecht : Springer.  \nKiela,  D., and S. Clark . 2014. A systematic st udy of \nsemantic vector space model param eters. In \nProce edings of the 2 nd Worksho p on C ontinuous \nVecto r Space Models and th eir Com positionality \n(CVSC), Gothen burg, 21-30. \nKilgarrif f, A., P. Rychl\u00fd, P. Smrz, and D. Tugwell. \n2004. The Sketch Engine. In Procee ding of  the \n11th EURALEX International C ongress, Lorient , \n105-115. \nKrieger , M.G.,  and M.J.B. Finatto.  2004. Introdu\u00e7\u00e3o  \n\u00e0 Terminol ogia: teoria & pr\u00e1tica. S\u00e3o Paulo: \nContexto. \nLapes a, G., S. Evert, and S. Schulte im Wa lde. 2014. \nContrasting synt agmat ic and p aradigmatic \nrelations: Insights from distr ibutiona l semantic \nmodels. In Proceedings of the 3rd Joint \nConference on Lexical and  Computa tional \nSeman tics, Dubli n, 160-170. \nLe\u00f3n-Ara\u00faz, P., A. San Mart\u00edn, and A.  Reimerink . \n2018. The EcoLexicon English corpus as  an open \ncorpus in Sketch Engine. In Proce edings of the \n18th EURALEX Intern ational Congress , \nLjubljana,  893-901. \nLevy, O., Y. Goldberg,  and I. Dagan . 2015. \nImproving di stributional simila rity with lessons \nlearned from word embeddings. Transactions of  \nthe ACL, 3: 211-225. \nManning, C.D.,  P. Raghava n, and H. Sch\u00fctze . 2009. \nIntrodu ction to Informat ion Retriev al. Cambrid ge \n(Engl and): Camb ridge University Press. \nManni ng, C.D., M. Surdeanu , J. Bauer , J. Finkel, S. \nBethard, and D. McClosky . 2014. The St anford \nCoreN LP Na tural Language Processing  Toolkit. \nIn Proceedi ngs of the  52nd Annual  Meeting of the \nAssociation for Computa tional L inguistics : \nSystem Demonstration s, Baltimore, 55-60. \nMikolov, T., E. Grave , P. Bojanowski , C. Puhrsch , \nand A. Joulin . 2018. Advances in pre-training \ndistributed word representat ions. In Proceedings \nof the 11th International Co nference on  Language \nResources  and Evaluation , Miyazaki, 52-55. Mikol ov, T., K. Chen , G. Corrado,  and J. Dean . 2013. \nEfficien t estimati on of word representations in \nvector space . In Workshop  Proce edings of \nInternation al Conferenc e on Learn ing \nRepres entatio ns. Scottsd ale. \nMille r, G.A. , and W.G.  Charles. 1991. Contextual \ncorrelates of semantic similarity. Language and \nCognitive Pr ocesses, 6(1): 1-28. \nMitchell, J., and M.  Lapat a. 2008. V ector-based \nmodels of sema ntic compo sition . In Proceeding  of \nACL-08, Colum bus (Ohio), 236-244. \nNakov, P . 2013 . On the interpretation of noun \ncompound s: Syntax, semantics, and entailment. \nNatural Language Engineering , 19: 291\u2013330. \nNematzadeh , A., S.C. Meyl an, and T.L. Griffiths . \n2017. Evaluating vector-space models of word \nrepresen tation, o r, the unreasonable effectiveness \nof counting words near other words. In \nProceedings o f the 39th A nnual Meeting of the  \nCogni tive Science Society , London , 859-864. \nNguyen,  N.T.H, A.J. Soto, G. Konto natsios, R. \nBatista -Navarro, an d S. Ananiadou. 201 7. \nConstr ucting a biodiversity terminological \ninventory. PLoS ONE , 12(4): e0175277.  \nNooralahzadeh , F., L. \u00d8vrelid, and J.T. L\u00f8nn ing. \n2018. Evaluation of domain-specific word \nembed dings using knowledge resources. In \nProceedings of the 11th International Con ference \non Language Resources and Evaluation , \nMiya zaki, 1438-1445.  \nPearson, J. 1998. Terms in context . Amsterdam : John \nBenjamins . \nPennington, J. , R. Socher,  and C.D. Manni ng. 2014. \nGloVe: Global vectors for word repres entation. In \nProcee dings of the 2014 C onference  on Empirical \nMethods for Natural Language P rocessing \n(EMN LP), Doha (Qatar) , 1532 -1543. \nProkopidis,  P., V. Papavassiliou, A . Toral, M .P. \nRiera, F . Frontini , F. Rubino, and G . Thurmai r. \n2012. Final report on th e corpus acquisitio n & \nannotation subs ystem and  its components . \nTechnical Report WP -4.5, PANACEA Project.  \nPilehvar, M. T., and N. Collier . 2016. Improve d \nseman tic representation for domain -specific \nentities. In  Proceedings  of the 15th Workshop o n \nBiomedical  Natural Language  Processing, Berlin,  \n12-16. \nRohde, D., L. Gonnerman , and D. Plaut. 2006. An \nimpro ved mod el of semantic simi larity b ased on \nlexica l co-occur rence.  Communi cation s of the \nACM , 8: 627-633. \nJuan Rojas-Garcia\n150 \n Rojas -Garcia J. , and P. Faber. 2019a. Extr action of \nterms for the construction of semantic frames for \nnamed bays. Argentinian Journal of Applied \nLinguistics , 7(1): 27 -57. \nRojas -Garcia J. , and P. Faber. 2019 b. Extraction of \nterms related to named rivers. Languages , 4(3): 46.  \nRojas -Garcia J. , and P.  Faber. 2019 c. Evaluation of \ndistributional semantic models for the extraction \nof semantic relations for named rivers from a \nsmall specialized corpus . Procesamiento del \nLenguaje Natural , 63: 51-58. \nRoom, A. 1996. An Alphabetical  Guide to the \nLanguage of Na me Studies . Lanha m/London: The \nScarecrow Press.  \nSahlgren, M. , and A. Lenci . 2016. The effec ts of data  \nsize and frequency range on  distributional \nsemantic models. In Proceedings of the 201 6 \nConference on Empi rical Methods in Na tural \nLanguage Process ing, Aus tin (Texas) , 975-980. \nSager, J .C., D . Dungworth, and P .F. McDonald. \n1980. English Special Languages. Principles and \nPractice in  Science and Technol ogy. Wiesbaden: \nBrandstetter  Verlag.  \nStrehl, A., J. Ghosh, and R. M ooney. 2000. Impact of \nsimilarity measures  on web -page clus tering. In  \nAAAI -2000: Workshop on Artificial Intelligence \nfor Web Search , Austin, 58-64. \nExtraction of Terms Semantically Related to Colponyms: Evaluationin a Small Specialized Corpus\n151  \n \n \n \n \n \n \n \n \nIberLEF 2021: Res\u00famenes \nde las tareas   \n Overview of the EmoEvalEs task on emotiondetection for Spanish at IberLEF 2021\nResumen de la tarea de detecci\u0013 on de emociones en espa~ nol\nEmoEvalEs en IberLEF 2021\nFlor Miriam Plaza-del-Arco, Salud Mar\u0013 \u0010a Jim\u0013 enez-Zafra, Arturo Montejo-R\u0013 aez,\nM. Dolores Molina-Gonz\u0013 alez, L. Alfonso Ure~ na-L\u0013 opez, M. Teresa Mart\u0013 \u0010n-Valdivia\nComputer Science Department, SINAI, CEATIC\nUniversidad de Ja\u0013 en, Campus Las Lagunillas, 23071, Ja\u0013 en, Spain\nffmplaza, sjzafra, amontejo, mdmolina, laurena, maite g@ujaen.es\nAbstract: This paper presents the EmoEvalEs shared task, organized at IberLEF\n2021, as part of the 37th International Conference of the Spanish Society for Nat-\nural Language Processing (SEPLN 2021). The aim of this task is to promote the\nEmotion detection and Evaluation for Spanish . It consists of a \fne-grained emotion\nclassi\fcation of tweets from the EmoEvalEs corpus in one of these seven classes:\nanger, disgust ,fear,joy,sadness, surprise, or others. In this edition, 70 teams regis-\ntered, 15 submitted results and 11 presented papers describing their systems. Most\nteams experimented with neural networks, being Transformers the most widely used\nmodel. It should be noted that few of them also considered the features of o\u000ben-\nsiveness and event that were provided in the corpus apart from the tweet texts.\nKeywords: EmoEvalEs, emotion detection, natural language processing.\nResumen: Este art\u0013 \u0010culo presenta la tarea EmoEvalEs, organizada en IberLEF\n2021, en el marco del de la 37 edici\u0013 on de la Conferencia Internacional de la Sociedad\nEspa~ nola para el Procesamiento del Lenguaje Natural. El objetivo de esta tarea es\npromover la Detecci\u0013 on y Evaluaci\u0013 on de Emociones en Espa~ nol. Consiste en la clasi-\n\fcaci\u0013 on de grano \fno de los tweets del corpus EmoEvent en una de las siguientes\nsiete clases: ira,asco, miedo ,alegr\u0013 \u0010a ,tristeza, sorpresa uotros. En esta edici\u0013 on, se\nregistraron 70 equipos, 15 enviaron resultados y 11 presentaron art\u0013 \u0010culos describi-\nendo sus sistemas. La mayor\u0013 \u0010a de los equipos experimentaron con redes neuronales,\nsiendo Transformers el modelo m\u0013 as utilizado. Cabe destacar que pocos equipos con-\nsideraron tambi\u0013 en las caracter\u0013 \u0010sticas de ofensividad y evento que se proporcionaron\nen el corpus aparte de los textos de los tweets.\nPalabras clave: EmoEvalEs, detecci\u0013 on de emociones, procesamiento del lenguaje\nnatural.\n1 Introduction\nEmotion detection from text is a research\ntask in Natural Language Processing (NLP)\naimed at classifying words, phrases, or doc-\numents into prede\fned emotion categories.\nThis task can be considered an extension\nof the polarity classi\fcation task due to the\npresence of \fne-grained categories based on\nfundamental emotion theories, with the Ek-\nman (Ekman, 1992) and Plutchik (Plutchik,\n2001) models the most commonly cited ones.\nIn the last years, great e\u000borts have been\nmade to address one of the most well-known\nSentiment Analysis tasks, polarity classi\f-\ncation. In fact, a number of shared taskshave been accomplished (Rosenthal, Farra,\nand Nakov, 2017; D\u0013 \u0010az-Galiano et al., 2019;\nMart\u0013 \u0010nez-C\u0013 amara et al., 2018). However,\nemotion classi\fcation is still considered a\nchallenge for the NLP systems and its sig-\nni\fcance has increased in recent years.\nWith the aim of promoting research in the\nemotion analysis area in Spanish, the \\Emo-\ntion Detection\" task was introduced for the\n\frst time in the TASS workshop (Vega et al.,\n2020) at IberLEF 2020. This year we contin-\nued with the \\Emotion detection and Evalu-\nation for Spanish Shared Task\" (EmoEvalEs)\nat IberLEF 2021 (Montes et al., 2021). Al-\nthough only two teams participated in TASS\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 155-161\nrecibido 05-07-2021 revisado 12-07-2021 aceptado 16-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-13\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural2020, this edition has attracted 70 team reg-\nistrations and received 11 system description\npapers, which demonstrates the interest of\nthe research community in this topic.\nWith the \\Emotion detection and Evalua-\ntion for Spanish Shared Task\" (EmoEvalEs),\nparticipants had the challenge of classifying\nthe emotion expressed in tweets related to\ncertain events in seven categories: anger,\nfear,sadness, joy, disgust ,surprise and oth-\ners. Unlike the previous edition, this year,\ntwo new features in dataset have been pro-\nvided to participants: the event correspond-\ning to the domain associated with the tweet\nand whether the tweet expresses o\u000bensive-\nness. The competition was organized through\nCodaLab and is accessible at the follow-\ning link: https://competitions.codalab.\norg/competitions/28682.\nThe remainder of this paper is organized\nas follows: Section 2 describes the EmoE-\nvalEs shared task. Section 3 presents the\ndataset that the participants used in the com-\npetition. Sections 4 and 5 present the ap-\nproaches and results of the participants. Fi-\nnally, Section 6 concludes and suggests some\npossible directions for future work.\n2 Task description\nUnderstanding the emotions expressed by\nusers on social media is a hard task due to\nthe absence of voice modulations and facial\nexpressions. Our shared task, EmoEvalEs:\n\\Emotion detection and Evaluation for Span-\nish\", has been designed to encourage research\nin this area. The task consists in classify-\ning the emotion expressed in a tweet as one\namong the following emotion classes:\n\u2022anger (also includes annoyance and\nrage)\n\u2022disgust (also includes disinterest, dislike,\nand loathing)\n\u2022fear (also includes apprehension, anxi-\nety, concern, and terror)\n\u2022joy(also includes serenity and ecstasy)\n\u2022sadness (also includes pensiveness and\ngrief)\n\u2022surprise (also includes distraction and\namazement)\n\u2022others : the emotion expressed in a tweet\nis neutral or there is no emotionThe challenges faced in this task are:\n1. Lack of context: tweets are short (up to\n240 characters)\n2. Informal language: misspellings, emojis\nand onomatopoeias are common\n3. Multiclass classi\fcation: the dataset is\nlabeled with seven di\u000berent classes\n4. Imbalance dataset: the number of tweets\nper emotion category does not follow the\nsame distribution\nFor developing their approaches, partici-\npants received the development and training\npartitions of the dataset and, at a later stage,\nthe test partition was provided for evalua-\ntion. Finally, the participant's submissions\nwere evaluated against the gold standard an-\nnotations to test their methods and deter-\nmine the winner of the challenge.\nThe metrics used to evaluated the task\nwere accuracy and the macro-averaged ver-\nsions of Precision, Recall, and F1, being ac-\ncuracy the one selected for ranking the sys-\ntems.\n3 Dataset\nIn this section, we describe the dataset of the\nEmoEvalEs shared task. EmoEvent (Plaza-\ndel-Arco et al., 2020) is a multilingual emo-\ntion corpus of tweets based on events that\ntook place in April 2019. They are re-\nlated to di\u000berent domains such as entertain-\nment, catastrophe, political, global commem-\noration, and global strike. Each instance in\nthe dataset is labeled with the main emotion\nexpressed in the tweet by three annotators\naccording to the following categories: anger,\ndisgust ,fear,joy, sadness, surprise, \\ neutral\nor no emotion \". The \fnal emotion label of\nthe tweet is the majority emotion labeled by\nthe annotators, but in case the three anno-\ntators labeled the tweet with di\u000berent emo-\ntions, the \fnal label is \\neutral or no emo-\ntion\". In particular, for this task we used the\nSpanish version of EmoEvent.\nThis year, compared to the \frst edi-\ntion in TASS 2020 (Vega et al., 2020), two\nnew features from the EmoEvent dataset\nhave been released to the participants: of-\nfensive (OFF : the tweet contains o\u000bensive\nlanguage, NO: the tweet does not contain\no\u000bensiveness) and event corresponding to\nFlor Miriam Plaza-del-Arco, Salud Mar\u00eda Jim\u00e9nez-Zafra, Arturo Montejo-R\u00e1ez,  \nM. Dolores Molina-Gonz\u00e1lez, L. Alfonso Ure\u00f1a-L\u00f3pez, M. Teresa Mart\u00edn-Valdivia\n156Emotion Training Dev Test\nJoy 1,227 181 354\nSadness 693 104 199\nAnger 589 85 168\nSurprise 238 35 67\nDisgust 111 16 33\nFear 65 9 21\nOthers 2,800 414 814\nTotal 5,723 844 1,656\nTable 1: Distribution of emotions by subset\n(Training, Dev, Test) in EmoEvalEs 2021.\nthe domain associated to the tweet ( World-\nBookDay, GretaThunberg, Venezuela, Game-\nOfThrones, LaLiga, or SpainElection ).\nBefore providing the dataset to the partic-\nipants, we decided to replace the query hash-\ntags by the keyword HASHTAG in order to\nprevent the automatic classi\fer from relying\non hashtags to categorize the emotion asso-\nciated with a tweet. Moreover, we replaced\nthe user mentions by @USER to anonymize\nmentions to users.\nFinally, di\u000berent sets have been released\nfor the competition. During the pre-\nevaluation phase, training and development\n(Dev) sets were provided to the participants;\nfor the evaluation phase, the test set was re-\nleased. Table 1 shows the number and per-\ncentage of tweets by emotion corresponding\nto each subset. It can be noticed that classes\nare highly imbalanced in the dataset.\n4 Participant approaches\nIn this edition, 70 teams registered on the\ntask, 15 submitted results and 11 presented\nworking notes describing their systems. The\nfollowing is a brief summary of the \fnal pro-\nposals submitted.\n\u2022GSI-UPM team - 1st (Vera,\nAraque, and Iglesias, 2021). This\ngroup has studied the combination of\ndi\u000berent features (like TF-IDF, n-grams,\nsentiment scores and the provided event\nand o\u000bensiveness columns) with en-\ncodings of a \fne-tuned XLM-RoBERTa\nmodel. Although the best submission\nin the competition was their \fne-tuned\nversion of the multilingual RoBERTa\nmodel (XLM-RoBERTa), the reported\nscores on development set are not much\nhigher than those obtained with Logistic\nRegression over text representationsbased on provided event and o\u000bensive\ncategories along with sentiment analysis\nscores.\n\u2022BERT4EVER team - 2nd (Fu et al.,\n2021). The authors adopt the monolin-\ngual Spanish BERT to tackle the task\n(BETO). In addition, they leveraged two\naugmented strategies to deal with the\nimbalanced emotion categories in the\ndataset, namely continual pre-training\nand data augmentation. The best re-\nsult was obtained with the pre-training\nof BETO on the training set provided by\nthe EmoEvalEs organizers, by ignoring\nthe labels and performing back transla-\ntion on the three categories with lower\nproportion: disgust ,fear and surprise.\n\u2022Yeti team - 3rd (Luo, 2021). This\nauthor used back-translation data aug-\nmentation technology to solve the prob-\nlem of data scarcity and data imbalance.\nChinese and English were used as inter-\nmediate languages for back translation.\nHe mainly enhances the fear and disgust\ncategories. The best result was by enter-\ning the o\u000bensive labels plus tweets text\ninto the BETO-cased model.\n\u2022URJC team - 4th (S\u0013 anchez, Her-\nranz, and Unanue, 2021). The ap-\nproach to the emotion detection problem\nproposed by this team was a \fne-tuned\nversion of BETO. They tried both, cased\nand uncased models, and a third tun-\ning reducing, by a 30%, the number of\nsamples within the others category. The\nbest result was obtained with a voting\nsystem over the three trained models.\n\u2022haha team - 5th (Li, 2021) . The\ntweet, the event and o\u000bensive features\nare combined as a new text. Then,\nURLs, white-space characters and stop\nwords are removed. The author adopts\na masked language model technique for\ndata augmentation in order to increase\nthe training set and avoid over-\ftting.\nExperiments were conducted with three\ncross-language models: BERT, XLM,\nand XLM-RoBERTa. The best per-\nformance was obtained with the XLM-\nRoBERTa model. The author shows\nthat the technique used for data aug-\nmentation increased the generalization\nof the model.\nOverview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021\n157\u2022UMU team - 6th (Garc\u0013 \u0010a-D\u0013 \u0010az,\nColomo-Palacios, and Valencia-\nGarc\u0013 \u0010a, 2021). The authors explore\nthe combination of explainable linguis-\ntic features and state-of-the-art trans-\nformers. On the one hand, they used\nthe UMUTextStats tool (Garc\u0013 \u0010a-D\u0013 \u0010az,\nC\u0013 anovas-Garc\u0013 \u0010a, and Valencia-Garc\u0013 \u0010a,\n2020; Garc\u0013 \u0010a-D\u0013 \u0010az et al., 2021) to ex-\ntract the linguistic features. On the\nother hand, they used sentence embed-\ndings and word embeddings from fast-\nText, GloVe and word2vec (although no\ndetails on how to compute sentence em-\nbeddings were provided) and sentence\nembeddings from BETO (pre-trained\nmodel) and from a \fne-tuned BETO ver-\nsion on the EmoEvalEs dataset. Finally,\nthey combine the features using an en-\nsemble model based on the mode. In\ntheir analysis, they show the potential of\nthe linguistic features to provide model\nagnostic methods for explainability.\n\u2022RETUYT-InCo team - 9th\n(Chiruzzo and Ros\u0013 a, 2021). They\nincorporate a diverse set of features to\nclassical machine learning algorithms\nand traditional neural networks. The\n\fnal model is LSTM where authors\nincorporate features from word2vec and\nBERT embeddings along with a the k\nword feature selection by a variance\n(ANOVA F-value) method. They men-\ntioned that the most di\u000ecult emotion\ncategories to classify by the model were\ndisgust ,fear and surprise.\n\u2022WSSC team - 10th (Vitiugin and\nBarnab\u0012 o, 2021). The authors propose\na complex architecture which combines\nBiLSTM encodings with provided o\u000ben-\nsiveness and event features. Each of\nthese three sets of features are the input\nof one or more feed forward networks,\nalthough not clear details are provided.\nDespite this complexity, results are not\nbetter than attention based mechanisms\nreported by other participants.\n\u2022UPC team - 12th (de Arriba, Oriol,\nand Franch, 2021). The authors pro-\npose an approach based on a \fne-tuned\nBETO model on pre-processed tweets.\nThey pre-processed the tweets as fol-\nlows: i) they removed URLs, hashtags,and numbers; ii) they replaced emojis,\nemoticons, abbreviations and laughs; iii)\nthey removed punctuation marks, re-\npeated characters, stopwords and blank\nspaces; and iv) they lemmatized the\ntext. They concluded that the submit-\nted system is less accurate for detecting\nthe emotion categories with a small num-\nber of samples in the dataset: fear,dis-\ngust and surprise.\n\u2022Dong team - 14th (Qu, Yang, and\nQue, 2021). It presents a combina-\ntion of di\u000berent neural networks (XLM-\nRoBERTa, Transformer enconding layer,\nTextCNN and a \fnal linear one). In\n\frst place, they pre-processed the tweets\nby removing punctuation marks, emojis,\nempty characters and other special sym-\nbols. Then, they passedthe input data to\nXML-RoBERTa model to obtain word\nvectors with global features of sentences.\nThen they input the word vector into a\nTransformer Encoder for secondary fea-\nture extraction, and then pass the result\ninto a TextCNN network. Finally, they\npassed the model output to a fully con-\nnected layer for classi\fcation.\n\u2022Qu team - 15th (Qu, Jia, and\nZhang, ) . The authors use the XLM-\nRoBERTa model to extract the features\nfrom training samples and then input\nthe acquired word features into the Bi-\nGRU model to get the emotional fea-\ntures of comments. Finally, they classify\nthe sentiment tendency by the softmax\nactivation function.\n5 Results and discussion\nTable 2 shows the main results of the Emo-\nEvalEs Shared Task. We received submis-\nsions through CodaLab from 15 participants.\n11 teams provided their working notes ex-\nplaining the systems that were developed\nfor the competition. From all submissions,\nthe best scoring system was that by GSI-\nUPM team, followed by BERT4EVER and\nYeti. Between the \frst two participants, it\ncan be observed that the di\u000berence in terms\nof macro-F1 and accuracy is minimal. The\nbest team, GSI-UPM, achieved a macro-F1\nscore of 0.717028, exploring the combination\nof di\u000berent features with a \fne-tuned XLM-\nRoBERTa model. The team ranked in second\nFlor Miriam Plaza-del-Arco, Salud Mar\u00eda Jim\u00e9nez-Zafra, Arturo Montejo-R\u00e1ez,  \nM. Dolores Molina-Gonz\u00e1lez, L. Alfonso Ure\u00f1a-L\u00f3pez, M. Teresa Mart\u00edn-Valdivia\n158Team Accuracy Macro-P Macro-R Macro-F1\nGSI-UPM (1) 0.727657 (1) 0.709411 (1) 0.727657 (1) 0.717028\nBERT4EVER (2) 0.722222 (2) 0.704695 (2) 0.722222 (2) 0.711373\nYeti (3) 0.712560 (3) 0.704496 (3) 0.712560 (3) 0.705432\nURJC-TEAM (4) 0.702899 (4) 0.692397 (4) 0.702899 (4) 0.696675\nhaha (5) 0.692029 (6) 0.679620 (5) 0.692029 (8) 0.663740\nUMUTeam (6) 0.685990 (7) 0.672546 (6) 0.685990 (7) 0.668407\n\u000bm (7) 0.684179 (5) 0.682765 (7) 0.684179 (5) 0.682487\nfazlfrs (8) 0.682367 (8) 0.664868 (8) 0.682367 (6) 0.668757\nRETUYT-InCo (9) 0.678140 (9) 0.658314 (9) 0.678140 (10) 0.657367\nWSSC (10) 0.675725 (10) 0.657681 (10) 0.675725 (9) 0.661427\njob80 (11) 0.668478 (12) 0.652840 (11) 0.668478 (11) 0.646085\nUPCTeam (12) 0.652778 (14) 0.600479 (12) 0.652778 (12) 0.622223\nTimen (13) 0.617754 (15) 0.597877 (13) 0.617754 (13) 0.600217\nDong (14) 0.536836 (11) 0.653707 (14) 0.536836 (14) 0.557007\nqu (15) 0.449879 (13) 0.618833 (15) 0.449879 (15) 0.446947\nTable 2: EmoEvalEs o\u000ecial ranking by accuracy (ranking position per metric is shown in\nparenthesis).\nposition was BERT4EVER ,with a macro-\nF1 score of 0.711373. It used BETO along\nwith two augmented strategies to enhance\nthe classic \fne-tuned model, namely contin-\nual pre-training and data augmentation. The\nthird team, Yeti, achieved a macro-F1 score\nof 0.705432, using back translation data aug-\nmentation technology to solve the problem of\ndata scarcity and data imbalance, and tried\nto input the o\u000bensive labels and the text of\nthe tweet into the BETO-cased model.\nMost of the teams used neural network\nsolutions to address the task. In particu-\nlar, Transformers are the most widely used\nmodel by the participants in two ways: (1)\nas encoders to obtain contextualized sentence\nembeddings features from text, and (2) \fne-\ntuning the pre-trained model on the task of\nemotion classi\fcation. As tweets from the\ndataset were in Spanish, most of the teams\nadopted two available pre-trained language\nmodels on Spanish corpora, the multilingual\nXLM-RoBERTa model and the monolingual\nBETO model.\nOnly three teams considered o\u000bensive and\nevent information in their approaches (GSI-\nUPM, haha and WSSC). In most cases, this\nled to a slight improvement of the system,\nso both categories seem to retain certain\nsemantic information related to emotions.\nIn general, the enrichment of the learning\nprocess with additional data (by means of\ndata augmentation techniques) or with ad-\nditional features beyond neural network en-\ncodings, provide some insight on the rele-vance of hybrid methods for determining sub-\njective information from texts. Although\nend-to-end solutions like BERT based models\nare clearly bene\fcial, additional characteris-\ntics are worth exploring, promoting ensemble\nbased designs.\nFocusing on the classi\fcation by emotion\ncategories, some participants mention the\nchallenge of classifying those emotions whose\npresence in the dataset is lower. In particu-\nlar, these emotions are fear,disgust and sur-\nprise. Data augmentation by back transla-\ntion was applied by two of the teams to ad-\ndress class imbalance. Also, some teams indi-\ncated that the systems faced the challenge of\ndistinguishing complementary emotions, for\nexample, the pairs disgust and anger, fear\nand sadness were often confused, a fact that\nis re\rected by their close locations in the two-\ndimensional models of emotions.\n6 Conclusions\nEmotion classi\fcation is still considered a\nchallenge in NLP systems and its signi\fcance\nhas increased in recent years. With this task\n\\Emotion detection and Evaluation for Span-\nish Shared Task\" (EmoEvalEs) at IberLEF\n2021, we want to promote research in the\narea of emotion analysis in Spanish, using the\nSpanish version of EmoEvent dataset.\nAs organizers, we are very satis\fed with\nparticipation volume, as it was high. In this\nedition of EmoEvalEs, 70 participants reg-\nistered, 15 of them submitted valid predic-\ntions and 11 contributed with a description\nOverview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021\n159of their systems. As expected, deep learn-\ning approaches constitute the trend in this\ntext classi\fcation task. In addition, the com-\nbination of linguistic information con\frms\nthe bene\fts of opting for hybrid solutions.\nCertainly, some of the most interesting chal-\nlenges that participants faced were class im-\nbalance and how to combine additional fea-\ntures with deep neuronal networks encond-\nings.\nAlthough di\u000berent partitions from those\nof past editions have been provided this year,\nthere is a clear improvement in performance.\nBest result reported in macro-F1 in 2020 was\nof 0.447. Compared to the best macro-F1\nscore in this year edition (0.717). Clearly,\nteams have gained skills in applying deep\nneural networks and adapting them to spe-\nci\fc tasks. Besides, the participation has\nraised from 2 to 15 teams. We believe that\nmoving the competition to CodaLab had the\nadditional e\u000bect of more visibility, as can be\nnoticed by the fact that \fve teams are lo-\ncated in China (four of them from Yunnan\nUniversity), which represents one third of to-\ntal participants.\nAs future work, we plan to include the En-\nglish version of the EmoEvent dataset in the\ncompetition in order to promote multilingual\nemotion analysis research and explore how\nemotions are expressed for each event based\non the cultural di\u000berences between English\nand Spanish speakers.\nAcknowledgements\nThis work has been partially supported\nby a grant from Fondo Social Europeo,\nAdministration of the Junta de An-\ndaluc\u0013 \u0010a (DOC 01073 and P20 00956-PAIDI\n2020), Fondo Europeo de Desarrollo Re-\ngional (FEDER), LIVING-LANG project\n(RTI2018-094653-B-C21) and the Ministry\nof Science, Innovation and Universities\n(scholarship [FPI-PRE2019-089310]) from\nthe Spanish Government.\nReferences\nChiruzzo, L. and A. Ros\u0013 a. 2021. RETUYT-\nInCo at EmoEvalEs 2021: Multiclass\nEmotion Classi\fcation in Spanish. In\nProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021).\nCEUR Workshop Proceedings, CEUR-\nWS, M\u0013 alaga, Spain.\nde Arriba, A., M. Oriol, and X. Franch. 2021.Applying Sentiment Analysis on Spanish\nTweets Using BETO. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2021). CEUR Workshop Pro-\nceedings, CEUR-WS, M\u0013 alaga, Spain .\nD\u0013 \u0010az-Galiano, M. C., M. Vega, E. Casasola,\nL. Chiruzzo, M. \u0013A. G. Cumbreras,\nE. Mart\u0013 \u0010nez-C\u0013 amara, D. Moctezuma,\nA. M. R\u0013 aez, M. A. S. Cabezudo, E. S.\nTellez, M. Gra\u000b, and S. Miranda-Jim\u0013 enez.\n2019. Overview of tass 2019: One more\nfurther for the global spanish sentiment\nanalysis corpus. In IberLEF@SEPLN.\nEkman, P. 1992. An argument for basic emo-\ntions. Cognition & emotion , 6(3-4):169{\n200.\nFu, Y., Z. Yang, N. Lin, L. Wang, and\nF. Chen. 2021. Sentiment Analy-\nsis for Spanish Tweets based on Con-\ntinual Pre-training and Data Augmen-\ntation. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2021). CEUR Workshop Proceedings,\nCEUR-WS, M\u0013 alaga, Spain.\nGarc\u0013 \u0010a-D\u0013 \u0010az, J. A., M. C\u0013 anovas-Garc\u0013 \u0010a, and\nR. Valencia-Garc\u0013 \u0010a. 2020. Ontology-\ndriven aspect-based sentiment analysis\nclassi\fcation: An infodemiological case\nstudy regarding infectious diseases in latin\namerica. Future Generation Computer\nSystems, 112:614{657.\nGarc\u0013 \u0010a-D\u0013 \u0010az, J. A., R. Colomo-Palacios, and\nR. Valencia-Garc\u0013 \u0010a. 2021. UMUTeam at\nEmoEvalEs 2021: Emotion Analysis for\nSpanish based on Explainable Linguistic\nFeatures and Transformers. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2021). CEUR Workshop\nProceedings, CEUR-WS, M\u0013 alaga, Spain.\nGarc\u0013 \u0010a-D\u0013 \u0010az, J. A., M. C\u0013 anovas-Garc\u0013 \u0010a,\nR. Colomo-Palacios, and R. Valencia-\nGarc\u0013 \u0010a. 2021. Detecting misogyny\nin spanish tweets. an approach based\non linguistics features and word embed-\ndings. Future Generation Computer Sys-\ntems, 114:506 { 518.\nLi, K. 2021. HAHA at EmoEvalEs 2021:\nSentiment Analysis in Spanish Tweets\nwith Cross-lingual Model. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2021). CEUR Workshop\nProceedings, CEUR-WS, M\u0013 alaga, Spain.\nFlor Miriam Plaza-del-Arco, Salud Mar\u00eda Jim\u00e9nez-Zafra, Arturo Montejo-R\u00e1ez,  \nM. Dolores Molina-Gonz\u00e1lez, L. Alfonso Ure\u00f1a-L\u00f3pez, M. Teresa Mart\u00edn-Valdivia\n160Luo, H. 2021. Emotion Detection for\nSpanish with Data Augmentation and\nTransformer-Based Models. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2021). CEUR Workshop\nProceedings, CEUR-WS, M\u0013 alaga, Spain.\nMart\u0013 \u0010nez-C\u0013 amara, E., Y. Almeida-Cruz,\nM. C. D\u0013 \u0010az-Galiano, S. Est\u0013 evez-Velarde,\nM.\u0013A. G. Cumbreras, M. Vega, Y. G.\nV\u0013 azquez, A. M. R\u0013 aez, A. Montoyo,\nR. Mu~ noz, A. Piad-Mor\u000es, and J. Villena-\nRom\u0013 an. 2018. Overview of tass 2018:\nOpinions, health and emotions. In\nTASS@SEPLN.\nMontes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on,\nR. Agerri, M. \u0013Alvarez Carmona,\nE. \u0013Alvarez Mellado, J. Carrillo-de\nAlbornoz, L. Chiruzzo, L. Freitas,\nH. G\u0013 omez Adorno, Y. Guti\u0013 errez, S. M.\nJim\u0013 enez-Zafra, S. Lima, F. M. Plaza-\ndel-Arco, and M. Taul\u0013 e, editors. 2021.\nProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021).\nPlaza-del-Arco, F., C. Strapparava, L. A.\nUre~ na-Lopez, and M. T. Martin-Valdivia.\n2020. EmoEvent: A Multilingual Emo-\ntion Corpus based on di\u000berent Events.\nInProceedings of the 12th Language Re-\nsources and Evaluation Conference , pages\n1492{1498, Marseille, France, May. Euro-\npean Language Resources Association.\nPlutchik, R. 2001. The nature of emotions:\nHuman emotions have deep evolutionary\nroots, a fact that may explain their com-\nplexity and provide tools for clinical prac-\ntice. American scientist , 89(4):344{350.\nQu, S., Y. Yang, and Q. Que. 2021. Emo-\ntion Classi\fcation for Spanish with XLM-\nRoBERTa and TextCNN. In Proceedings\nof the Iberian Languages Evaluation Fo-\nrum (IberLEF 2021). CEUR Workshop\nProceedings, CEUR-WS, M\u0013 alaga, Spain.\nQu, Y., S. Jia, and Y. Zhang. Sentiment\nAnalysis in Spanish Tweets: The Model\nbased on XLM-RoBERTa and Bi-GRU.\nRosenthal, S., N. Farra, and P. Nakov. 2017.\nSemeval-2017 task 4: Sentiment analysis\nin twitter. In Proceedings of the 11th in-\nternational workshop on semantic evalua-\ntion (SemEval-2017), pages 502{518.\nS\u0013 anchez, J. A. F., S. M. Herranz, and R. M.\nUnanue. 2021. URJC-Team at EmoE-valEs 2021: BERT for Emotion Classi-\n\fcation in Spanish Tweets. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2021). CEUR Workshop\nProceedings, CEUR-WS, M\u0013 alaga, Spain.\nVega, M., M. C. D\u0013 \u0010az-Galiano, M. \u0013A. G.\nCumbreras, F. M. Plaza-del-Arco, A. M.\nR\u0013 aez, S. Jim\u0013 enez-Zafra, E. Mart\u0013 \u0010nez-\nC\u0013 amara, C. Aguilar, M. A. S. Cabezudo,\nL. Chiruzzo, and D. Moctezuma. 2020.\nOverview of tass 2020: Introducing emo-\ntion detection. In IberLEF@SEPLN 2020.\nVera, D., O. Araque, and C. A. Igle-\nsias. 2021. GSI-UPM at Iber-\nLEF2021: Emotion Analysis of Span-\nish Tweets by Fine-tuning the XLM-\nRoBERTa Language Model. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2021). CEUR Workshop\nProceedings, CEUR-WS, M\u0013 alaga, Spain.\nVitiugin, F. and G. Barnab\u0012 o. 2021. Emo-\ntion Detection for Spanish by Combining\nLASER Embeddings, Topic Information,\nand O\u000bense Features. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2021). CEUR Workshop Pro-\nceedings, CEUR-WS, M\u0013 alaga, Spain .\nOverview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021\n161Overview of Rest-Mex at IberLEF 2021:Recommendation System for Text Mexican Tourism\nResumen de la tarea Rest-Mex en IberLEF 2021: Sistemas de\nrecomendaci\u0013 on para textos tur\u0013 \u0010sticos mexicanos\nMiguel \u0013A.\u0013Alvarez-Carmona1;2, Ram\u0013 on Aranda1;2, Samuel Arce-Cardenas3,\nDaniel Fajardo-Delgado3,Rafael Guerrero-Rodr\u0013 \u0010guez4,\nA. Pastor L\u0013 opez-Monroy5,Juan Mart\u0013 \u0010nez-Miranda1;2,\nHumberto P\u0013 erez-Espinosa1;2,Ansel Y. Rodr\u0013 \u0010guez-Gonz\u0013 alez1;2\n1Centro de Investigaci\u0013 on Cient\u0013 \u0010\fca y de Educaci\u0013 on Superior de Ensenada\n2Consejo Nacional de Ciencia y Tecnolog\u0013 \u0010a\n3Tecnol\u0013 ogico Nacional de M\u0013 exico Campus Ciudad Guzm\u0013 an\n4Universidad de Guanajuato\n5Centro de Investigaci\u0013 on en Matem\u0013 aticas\nfmalvarez, aranda, ansel, hperez, jmirandag@cicese.edu.mx\nr.guerrero-rodriguez@ugto.mx, pastor.lopez@cimat.mx\nfdaniel.fd, samuel11290806 g@cdguzman.tecnm.mx\nAbstract: This paper presents the framework and results from the Rest-Mex track\nat IberLEF 2021. This track considered two tasks: Recommendation System and\nSentiment Analysis, using texts from Mexican touristic places. The Recommenda-\ntion System task consists in predicting the degree of satisfaction that a tourist may\nhave when recommending a destination of Nayarit, Mexico, based on places visited\nby the tourists and their opinions. On the other hand, the Sentiment Analysis task\npredicts the polarity of an opinion issued by a tourist who traveled to the most repre-\nsentative places in Guanajuato, Mexico. For both tasks, we have built new corpora\nconsidering Spanish opinions from the TripAdvisor website. This paper compares\nand discusses the results of the participants for both tasks.\nKeywords: Rest-Mex 2021, Recommendation System, Sentiment Analysis, Mexi-\ncan Tourist Text.\nResumen: Este art\u0013 \u0010culo presenta los resultados de la tarea del Rest-Mex en Iber-\nLEF 2021. Este evento consider\u0013 o dos sub tareas, Sistema de Recomendaci\u0013 on y\nAn\u0013 alisis de Sentimientos, ambas utilizando textos tur\u0013 \u0010sticos de lugares con inter\u0013 es\ntur\u0013 \u0010stico en M\u0013 exico. La tarea del Sistema de Recomendaci\u0013 on consiste en predecir\nel grado de satisfacci\u0013 on que tendr\u0013 a un turista al recomendar un destino de Nayarit,\nM\u0013 exico, a partir del historial de los lugares visitados por el turista y las opiniones\nque se le dan a cada uno de ellos. Por otro lado, la tarea de An\u0013 alisis de Sentimiento\nconsiste en predecir la polaridad de una opini\u0013 on emitida por un turista que viaj\u0013 o\na los lugares m\u0013 as representativos de Guanajuato, M\u0013 exico. Para ambas tareas, se\nhan construido dos nuevas colecciones utilizando las opiniones en espa~ nol del sitio\nweb TripAdvisor. Este art\u0013 \u0010culo compara y analiza los resultados de los participantes\npara ambas tareas.\nPalabras clave: Rest-Mex 2021, Sistemas de recomendaci\u0013 on, An\u0013 alisis de sentimien-\ntos, Textos Tur\u0013 \u0010sticos Mexicanos.\n1 Introduction\nTourism is a social, cultural, and economic\nphenomenon related to people's movement\nto places outside their usual place of resi-\ndence for personal or business/professionalreasons (Di-Bella, 2019). This activity is vi-\ntal in various countries, including Mexico1,\n1Mexico is in the world top ten and the second\nIberoamerican country related to the arrival of inter-\nnatinal tourists.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 163-172\nrecibido 01-07-2021 revisado 08-07-2021 aceptado 12-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-14\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalwhere tourism represents 8.7% of the national\nGDP, generating around 4.5 million direct\njobs (Elorza, 2020).\nWith the pandemic generated by the\nSARS-COV-2 virus, which spread out in\nMexico in mid-March 2020, tourism was one\nof the most a\u000bected sectors (Rivas D\u0013 \u0010az,\nCallejas C\u0013 arcamo, and Nava Vel\u0013 azquez,\n2020). Tourism is trying to re-establish it-\nself through improvements in the quality\nand safety of touristic products and services\n(Elorza, 2020).\nNatural Language Processing (NLP) is an\narti\fcial intelligence area that can help re-\nstore tourism by generating mechanisms for\ndetecting problems derived from the identi\f-\ncation of polarities in opinions that tourists\nshare on virtual platforms. Systems can also\nbe developed considering the user and desti-\nnation information to recommend the places\nwhere the user may have better tourist expe-\nriences. In this way, the tourism sector and\nthe tourists themselves could be supported\nby the NLP (Anis, Saad, and Aref, 2020).\nFew recommendation systems for tourist\nsites are based on the a\u000enity of a user's\npro\fle compared to each place's description.\nThe data collections to train these types of\nsystems are mainly obtained from users and\nplaces in English-speaking countries. Consid-\nering the relevance of Ibero-American coun-\ntries for international tourism, it is of utmost\nimportance to generate resources that allow\nthe generation of systems that help develop\nintelligent systems in Spanish-speaking coun-\ntries as well.\nOn the other hand, sentiment analysis\ntasks in tourist texts has gained relevance in\nthe last decade (Alaei, Becken, and Stantic,\n2019). However, as with NLP, the most sig-\nni\fcant attention of scienti\fc communication\ne\u000borts have focused on the English language.\nAlthough some studies have focused on Span-\nish, only a few of them address Spanish out-\nside from the country of Spain. These ap-\nproaches are typically applied to collections\ntaken from social networks such as tweets,\nso tourist texts have not been directly ad-\ndressed.\nFor this Rest-Mex edition, we proposed\ntwo sub-tasks: Recommendation System and\nSentiment Analysis on Mexican tourist texts.\nFor this purpose, two data sets have been\nbuilt. We collected 2,263 instances from\n2,011 users who visited 18 touristic placesin Nayarit, Mexico, for the recommendation\nsystem task. As for the sentiment analy-\nsis task, 7,413 opinions were collected from\ntourists who visited Guanajuato, Mexico.\nTo the best of our knowledge, this is the\n\frst time that an evaluation forum is dedi-\ncated to solving Tourism issues in Mexican\ndestinations.\nThe remainder of this paper is organized\nas follows: Section 2 describes the collection\nbuilding process for this forum and the met-\nrics for the evaluation. Section 3 summarizes\nthe solutions submitted by the participants\nfor both tasks. Section 4 shows the results\nobtained by the participants' systems and the\nanalysis. Finally, Section 5 presents the con-\nclusions obtained by this evaluation forum.\n2 Evaluation framework\nThis section outlines the construction of\nthe two used corpus, highlighting particu-\nlar properties, challenges, and novelties. It\nalso presents the evaluation measures used\nfor both tasks.\n2.1 Recommendation System\ncorpus\nThe \frst subtask consists in a classi\fcation\ntask where the participating system can pre-\ndict the degree of satisfaction that a tourist\nmay have when recommending a destination.\nThe collection consists of 2,263 in-\nstances with 2,011 tourists and 18 touris-\ntic places from Nayarit, Mexico. This col-\nlection was obtained from the tourists who\nshared their satisfaction on TripAdvisor be-\ntween 2010 and 2020. Each class of satisfac-\ntion is an integer between [1, 5], where:\n1. Very bad\n2. Bad\n3. Neutral\n4. Good\n5. Very good\nEach instance consists of two parts:\n1.User information :\n\u2022Gender: The tourist's gender.\n\u2022Place: The tourist place that the\ntourist recommends a visit.\nMiguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Arce-Cardenas, Daniel Fajardo-Delgado, Rafael Guerrero-Rodr\u00edguez, \nA. Pastor L\u00f3pez-Monroy, Juan Mart\u00ednez-Miranda, Humberto P\u00e9rez-Espinosa, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez\n164Class Train instances Test instances\n1 45 20\n2 53 24\n3 167 72\n4 457 196\n5 860 369\n\u0006 1582 681\nTable 1: Instances distribution for the rec-\nommendation system task.\n\u2022Location: The place of origin of\nthe tourist (the central, northeast,\nnorthwest, west, and southeast re-\ngions refer to the regions of Mexico).\n\u2022Date: Date when the recommenda-\ntion was issued.\n\u2022Type: Type of trip that the tourist\nwould do. The type would be\nin [Family, Friends, Alone, Couple,\nBusiness]\n\u2022History: The history of the places\nthe tourist has visited and his/her\nopinions on each of these places.\n2.Place information: A brief text de-\nscription of the place and a series of rep-\nresentative characteristics of the place as\na type of tourism that can be done there\n(adventure, beach, relaxation, among\nothers.), If it is a family atmosphere, pri-\nvate or public, it is free or paid, among\nothers.\nWe use a 70/30 partition to divide into\ntrain and test. This means that we used 1,582\nlabeled instances for the training partition,\nwhile we used 681 unlabeled instances for the\ntest partition.\nTable 2.1 shows the distribution of the in-\nstances for the recommendation system task\nfor the train and test partitions.\nThe class imbalance is clear since class 5\nrepresents around 50 % of the total instances,\nwhich makes this a task a very di\u000ecult one.\nFormally the problem of this task is de-\n\fned as:\n\\Given a TripAdvisor tourist and a Mexi-\ncan tourist place, the goal is to automatically\nobtain the degree of satisfaction (between 1\nand 5) that the tourist may have when visit-\ning that place.\"\n2.2 Sentiment Analysis corpus\nThe second subtask is a classi\fcation task\nwhere the participating system can pre-dict the polarity of an opinion issued by a\ntourist who traveled to the most representa-\ntive places of Guanajuato, Mexico. This col-\nlection was obtained from the tourists who\nshared their opinion on TripAdvisor between\n2002 and 2020. Each opinion's class is an\ninteger between [1, 5], where:\n1. Very negative\n2. Negative\n3. Neutral\n4. Positive\n5. Very positive\nEach tourist has information about\nhis/her nationality and gender. For example:\n\u2022\"Un callej\u0013 on donde tienes que besar a tu\namante por a~ nos de felicidad, en el amor\nes parte de un mito en esta ciudad espe-\ncial. El callej\u0013 on estrecho con escalones\nno es muy especial en s\u0013 \u0010 mismo. Lo que\nlo hace especial es toda la historia a su\nalrededor\"\n{Polarity: 5 (Very positive)\n{Nationality: Mexico\n{Gender: Male\n\u2022\"Este museo de tres pisos se vende como\nsede de muchas obras de Diego Rivera,\nsin embargo, despu\u0013 es de recorrer todo el\nmuseo, y ante la frustraci\u0013 on de no encon-\ntrar m\u0013 as que dibujos y bocetos, decid\u0013 \u0010\npreguntarle a uno de los guardas, aqu\u0013 \u0010\nme aclar\u0013 o que las obras de dos pisos com-\npletos se encuentran en restauraci\u0013 on, y\nen otra exhibici\u0013 on en Jap\u0013 on. No dejando\nas\u0013 \u0010 al p\u0013 ublico ni una sola hora de pintura\npara apreciar.\"\n{Polarity: 1 (Very negative)\n{Nationality: Nicaragua\n{Gender; Female\nThe corpus consists of 7,413 opinions\nshared by tourists. Like the recommendation\ntask, we use a 70/30 partition to divide into\ntrain and test. This means that we used 5,197\nlabeled instances for the train partition, while\nwe used 2,216 unlabeled instances for the test\npartition.\nTable 2.2 shows the distribution of the in-\nstances for the sentiment analysis task for the\ntrain and test partitions.\nOverview of Rest-Mex at IberLEF 2021: Recommendation System for Text Mexican Tourism\n165Class Train instances Test instances\n1 80 35\n2 145 63\n3 686 295\n4 1596 685\n5 2690 1138\n\u0006 5197 2216\nTable 2: Instances distribution for the senti-\nment analysis task.\nAs with the other subtask, the class imbal-\nance is clear since, again, class 5 represents\naround 50 % of the total instances, which\nmakes this a task with a signi\fcant degree\nof di\u000eculty too.\nFormally the problem of this task is de-\n\fned as:\n\\Given an opinion about a Mexican\ntourist place, the goal is to determine the po-\nlarity, between 1 and 5, of the text.\"\n2.3 Performance measures\nSystems are evaluated using standard eval-\nuation metrics, including accuracy and F-\nmeasure, but MAE (mean absolute error)\nwill rank the submissions for both subtasks.\nMAE are de\fned as equation 1.\nMAE Sx=1\nnnX\ni=1jT(i)\u0000Sx(i)j (1)\nWhere Sxis a participating system x,T(i)\nis the result of the instance iaccording to the\nGround Truth, and Sx(i) is the output of the\nparticipant system xfor instance i. Finally,\nnis the number of instances in the collection.\n3 Overview of the Submitted\nApproaches\nThis section presents the results obtained by\nthe participants for the tasks of recommen-\ndation system and sentiment analysis.\n3.1 Recommendation system\noverview\nFor this study, two teams have submitted\ntheir solutions for the recommendation sys-\ntem task. From what they explained in their\nnotebook papers, this section summarizes\ntheir approaches regarding pre-processing\nsteps, features, and classi\fcation algorithms.\n\u2022A Recommendation System for Tourism\nBased on Semantic Representationsand Statistical Relational Learning\n(Morales-Gonz\u0013 alez et al., 2021)\n{ Team: Labsemco-UAEM\n{ Summary: The team presented a\nmethod of text representation dif-\nferent from the methods of lexical\nco-occurrence in text. This method\nextracts the linguistic features in\nthe text, speci\fcally the lexical\nand semantic signals of synonymy-\nantonymy. They proposed to use\nthe ComplEx model for the recom-\nmendation task. The model was\nmodi\fed to predict the target label,\nconsidering it as a relationship be-\ntween a User and a Place.\n\u2022An Embeddings Based Recommenda-\ntion System for Mexican Tourism. Sub-\nmission to the REST-MEX Shared Task\nat IberLEF 2021 (Arreola et al., 2021)\n{ Team: Alumni-MCE 2GEN\n{ Summary: The team proposes two\nmethods, the \frst one is based on\nDoc2vec. The Doc2Vec model was\napplied to the user and place in-\nformation of the dataset. The ob-\ntained embeddings were matched\nwith the reviews' centroid embed-\ndings through similarity metrics,\nand these embeddings were assigned\nto the design matrix. Finally, for\nthe other user variables, a hot en-\ncoding was applied to be incorpo-\nrated in the design matrix to be\nmodeled through a Neural Network\nwith one hidden layer and ordinal\nencoding to deal with the unbal-\nanced problem of the data. They\nproposed a system based on dis-\ntributed representations of texts for\nthe second method, using the BERT\napproach.\n3.2 Sentiment analysis overview\nFor this study, seven teams have submitted\ntheir solutions and descriptions for the sen-\ntiment analysis task. From what they ex-\nplained in their notebook papers, this section\nsummarizes their approaches regarding pre-\nprocessing steps, features, and classi\fcation\nalgorithms.\nMiguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Arce-Cardenas, Daniel Fajardo-Delgado, Rafael Guerrero-Rodr\u00edguez, \nA. Pastor L\u00f3pez-Monroy, Juan Mart\u00ednez-Miranda, Humberto P\u00e9rez-Espinosa, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez\n166\u2022Bert-based Approach for Sentiment\nAnalysis of Spanish Reviews from Tri-\npAdvisor (V\u0013 asquez, G\u0013 omez-Adorno, and\nBel-Enguix, 2021)\n{ Team: Miner\u0013 \u0010a UNAM\n{ Summary: They apply two Bert-\nbased approaches for classi\fcation.\nThe \frst approach consists of \fne-\ntuning BETO, a Bert-like model\npre-trained in Spanish. The second\napproach focuses on combining Bert\nembeddings with the feature vectors\nweighted with TF-IDF.\n\u2022Cascade of Biased Two-class Classi-\n\fers for Multi-class Sentiment Analysis\n(Abreu and Mirabal, 2021)\n{ Team: UCT-UA\n{ Summary: The team proposes\ntwo methods. The results in their\nprimary submission were obtained\nfrom the model BETO. The sec-\nondary method has a better result\nfor this team. This method con-\nsists of a cascade of binary classi\fers\nbased again on BETO.\n\u2022DCI-UG participation at REST-MEX\n2021: A Transfer Learning Approach\nfor Sentiment Analysis in Spanish (Ve-\nlazquez Medina and Hernandez Farias,\n2021)\n{ Team: DCI-UG\n{ Summary: The proposed method\nis based on a modi\fed Span-\nish BERT-base architecture model.\nThe BERT-Base architecture was\nmodi\fed by removing the last layer\nof the network. Then, the last two\nlayers of the modi\fed BERT ar-\nchitecture were concatenated to be\nused as the input to a dense layer\nwith a swish activation function.\nAs a \fnal layer, a dense layer was\nused with \fve outputs (one for each\nclass) using softmax as activation\nfunction. For their \frst run, the\nmodel was trained with 70 percent\nof the training data (with data aug-\nmentation for classes 1 and 2) and\nused the remaining 30 percent as a\nvalidation set. On the other hand,\nthe second model was trained usingthe whole training data (again in-\ncluding the additional data) and the\nInHouseTest as the validation set to\nprevent the model from over-\ftting.\n\u2022Naive Features for Sentiment Anal-\nysis on Mexican Touristic Opinions\nTexts (Carmona-S\u0013 anchez, Carmona, and\n\u0013Alvarez-Carmona, 2021)\n{ Team: Arandanito Team\n{ Summary: The team proposes a\nsimple method based on naive fea-\ntures, which consist of extracting\nsimple measures such as number\nof words, number of digits, empty\nwords, among others. They test\nvarious classi\fers and \fnally pro-\npose a weighting scheme to deter-\nmine the best classi\fcation algo-\nrithm; for its representation, it was\nKNN with k= 7.\n\u2022Semantic Representations of Words and\nAutomatic Keywords Extraction for\nSentiment Analysis of Tourism Reviews\n(Toledo-Acosta et al., 2021)\n{ Team: Labsemco-UAEM\n{ Summary: Firstly, the team pro-\nposes an unsupervised method for\nkeyword extraction in order to con-\nstruct a list of prototypical words\nconveying a sentiment weight. Sec-\nondly, They emphasize the match\nof the scores of prototypical words\nwith the labels of the texts where\nthey appear. An SVM does the clas-\nsi\fcation task applied to vector rep-\nresentations of text entities.\n\u2022Techkatl: A Sentiment Analysis Model\nto Identify the Polarity of Mexican's\nTourism Opinions (Roldan Reyes, 2021)\n{ Team: Techkatl\n{ Summary: For this system, the\nmodel development and experi-\nments were carried out on the\nRapidMiner platform. The author\nproposes \fltered stemming words as\npre-processing. Their representa-\ntion is based on TF-IDF. Also, the\nauthor applies several classi\fcation\nalgorithms. Bayesian Methods ob-\ntain the best result.\nOverview of Rest-Mex at IberLEF 2021: Recommendation System for Text Mexican Tourism\n167\u2022Sentiment Classi\fcation for Mexican\nTourist Reviews based on KNN and\nJaccard Distance (Romero-Cant\u0013 on and\nAranda, 2021)\n{ Team: The last\n{ Summary: The proposal of this\nteam consists of calculating the Jac-\ncard distance between each instance\nin the test participation with the\naverage of each of the 5 classes\nin the train. Jaccard's distance is\nweighted by the number of repeti-\ntions of each word in each class. Fi-\nnally, the KNN algorithm is used to\ndetermine the class of each instance\nin the test.\n4 Experimental evaluation and\nanalysis of results\nThis section summarizes the results obtained\nby the participants, comparing and analyzing\nin detail the performance of their submitted\nsolutions. For the \fnal phase of the chal-\nlenge, participants sent their predictions for\nthe test partition, the performance on this\ndata was used to rank them. The MAE was\nused as the primary evaluation measure. In\nthe following, we report the results obtained\nby participants. Due to the nature of the\ndata being unbalanced, a system that always\nresults in the majority class would have an\nacceptable result; however, it would not be\nhelpful. For this reason, as a baseline, is pro-\nposed the system that always results in class\n5 for both tasks.\n4.1 Recommendation system\nresults\nTable 4.1 shows a summary of the results ob-\ntained by each team for the recommendation\nsystem task. The MAE was used to rank\nparticipants. The approach of the Alumni-\nMCE 2GEN Run2 team obtained the best per-\nformance for all metrics. It is remarkable to\nobserve how this system improved the base-\nline with 0.42 in MAE. It can also be seen\nthat it surpassed the baseline at 23.37 for ac-\ncuracy. Finally, it was expected that the F-\nmeasure of the baseline would not have good\nresults; this is evident since all the experi-\nments surpassed the baseline in this metric,\nalthough again, the result obtained by the\nAlumni-MCE 2GEN Run2 team exceeded the\nbaseline by 0.37.Team MAE F-measure Accuracy\nAlumni-MCE 2GEN Run1 0.31 0.50 77.28\nAlumni-MCE 2GEN Run2 0.32 0.47 76.21\nBaseline 0.73 0.13 53.81\nLabsemco-UAEM 1.65 0.16 20.91\nTable 3: Performance for the Recommenda-\ntion System task.\nF-measure class Best result Team\n1 0.32 Alumni-MCE 2GEN Run1\n2 0.24 Alumni-MCE 2GEN Run1\n3 0.30 Alumni-MCE 2GEN Run1\n4 0.67 Alumni-MCE 2GEN Run1\n5 0.96 Alumni-MCE 2GEN Run1\nTable 4: Performance for the Recommenda-\ntion System task per class.\nTable 4.1 shows the best F-measure results\nby class in the recommendation task. In this\ntask, for all classes, the best result was ob-\ntained by the same team that obtained the\nbest MAE result, that is, the Alumni-MCE\n2GEN Run1 team. Unlike which can be intu-\nited, the worst performance class was Class\n2 with 0.24, followed by Class 3 with 0.32,\nwhen the minority class is the Class 1, which\nobtained a performance of 0.32. For Class\n4, a result of 0.67 was obtained, and \fnally,\nfor Class 5, which is the majority class, a re-\nsult of 0.96 was obtained. It should be noted\nthat the baseline of the majority class gets\nzero from F-measure for all classes, except\nfor Class 5, where it gets 0.69.\n4.1.1 Perfect assemble for the\nrecommendation system task\nTo analyze the complementarity of the pre-\ndictions by the participants' systems, we\nbuilt a theoretically perfect ensemble from\ntheir runs, as calculated in (Arag\u0013 on et al.,\n2019). That is, we considered that a test in-\nstance was correctly classi\fed if at least one\nof the participating teams classi\fed it cor-\nrectly.\nAdditionally, we considered a vote ap-\nproach; we chose the class with the greatest\nnumber of predictions among the runs.\nFinally, it is essential to mention that 108\ninstances were not classi\fed correctly by any\nsystem. Within these instances, none belong\nto class 5. On the other hand, 88 instances\nwere correctly classi\fed by all systems. All\nthese instances belong to class 5.\nTable 4.1.1 shows the perfect assemble re-\nsult compared with the best result obtained\nby the Alumni-MCE 2GEN Run1 team. Also,\nthis table shows the vote approach result.\nMiguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Arce-Cardenas, Daniel Fajardo-Delgado, Rafael Guerrero-Rodr\u00edguez, \nA. Pastor L\u00f3pez-Monroy, Juan Mart\u00ednez-Miranda, Humberto P\u00e9rez-Espinosa, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez\n168Team MAE F-measure Accuracy\nPerfect assemble 0.28 0.77 84.01\nAlumni-MCE 2GEN Run1 0.31 0.50 77.28\nVote 0.41 0.43 71.84\nBaseline 0.73 0.13 53.81\nTable 5: Performance for the Recommenda-\ntion System task.\nTeam MAE F-measure Accuracy\nMiner\u0013 \u0010a UNAM Run1 0.47 0.42 56.72\nUCT-UA Run2 0.54 0.45 53.24\nUCT-UA Run1 0.56 0.40 53.83\nDCI-UG Run1 0.56 0.28 53.33\nMiner\u0013 \u0010a UNAM Run2 0.58 0.24 54.78\nDCI-UG Run1 0.60 0.25 53.70\nLabsemco-UAEM Run1 0.64 0.30 49.05\nTechkatl Run1 0.66 0.27 50.18\nBaseline 0.72 0.13 51.35\nArandanito Team 0.76 0.16 45.71\nTextMin-UCLV* Run1 0.78 0.17 36.23\nTechkatl Run2 0.81 0.21 44.76\nLabsemco-UAEM Run2 0.91 0.24 36.50\nTextMin-UCLV* Run2 1.00 0.18 38.31\nThe last 1.26 0.21 36.95\nTable 6: Performance for the Sentiment\nAnalysis task.\nFrom these results, it is possible to ob-\nserve that the perfect ensemble performance\nis considerably better than the Alumni-MCE\n2GEN Run1 approach, suggesting that the\nparticipants' systems are complementary to\neach other. Nevertheless, the result from the\nvote approach indicates that the intersection\nof correctly classi\fed instances by the sys-\ntems is relatively small, and therefore, au-\ntomatically taking advantage of this comple-\nmentarity is a complex task.\n4.2 Sentiment analysis results\nTable 4.2 shows a summary of the results ob-\ntained by each team for the sentiment analy-\nsis task2. In total, eight teams with 14 di\u000ber-\nent systems participated. For this task, the\nMiner\u0013 \u0010a UNAM Run1 team obtained the best\nMAE result and the best accuracy; however,\nthe UCT-UA Run2 team obtained the best re-\nsult for F-measure. In this task, eight sys-\ntems improved the baseline with the MAE\nmeasure, 7 improved it in accuracy, and in\nthe same way, as in the recommendation task,\nall the systems improved the majority class\nin F-measure.\nTable 4.2 shows the best F-measure re-\nsults by class in the sentiment analysis task.\nUnlike the recommendation task, di\u000berent\n2For systems with *, the authors did not send the\nsystem's description.F-measure class Best result Team\n1 0.37 UCT-UA Run2\n2 0.39 UCT-UA Run2\n3 0.47 Miner\u0013 \u0010a UNAM Run1\n4 0.44 Miner\u0013 \u0010a UNAM Run1\n5 0.71 Miner\u0013 \u0010a UNAM Run2\nTable 7: Performance for the Sentiment\nAnalysis task per class.\nTeam MAE F-measure Accuracy\nPerfect Assenbly 0.06 0.94 96.84\n3 best results 0.47 0.47 57.67\nMiner\u0013 \u0010a UNAM Run1 0.47 0.42 56.72\n5 best results 0.49 0.39 57.89\n8 best results 0.50 0.33 57.53\nUCT-UA Run2 0.54 0.45 53.24\nBaseline 0.72 0.13 51.35\nTable 8: Perfect assemble and some combi-\nnations for the Sentiment Analysis task.\nteams obtained the best result for some of the\nclasses. For minority classes like 1 and 2, the\nbest result was obtained by the UCT-UA Run2\nteam with 0.37 and 0.39, respectively. The\nbest results for classes 3 and 4 were obtained\nby the Miner\u0013 \u0010a UNAM Run1 team with 0.47\nand 0.44, respectively. Finally, the best result\nfor class 5, which is the majority class, was\nobtained by the Miner\u0013 \u0010a UNAM Run2 team.\n4.2.1 Perfect assemble for the\nsentiment analysis task\nAs in the section 4.1.1, the complementarity\nof the systems was analyzed for the sentiment\nanalysis task. We calculated the perfect as-\nsemble and the vote approaches.\nSince there are more participating systems\nin this task, it is also possible to experiment\nwith vote approaches but with fewer systems.\nThe simple vote approach considers all sys-\ntems; however, there are systems with results\nbelow the baseline, which could be putting\nmore noise in the vote. For this reason, it\nis proposed to select the approaches to vote\nconcerning the ranking of the table 4.2. In\nthis way, it is proposed to use only the sys-\ntems above the baseline, that is, the 8 best\nresults. It is also proposed to use the top 5\nof systems and \fnally the top 3.\nTable 4.2.1 shows the perfect assemble\nresult compared with the best results ob-\ntained by the Miner\u0013 \u0010a UNAM Run1 and UCT-\nUARun2 teams. Also, this table shows the\nvote approaches results.\nAs in the recommendation task, it is\npossible to observe that the perfect ensem-\nble performance is considerably better than\nOverview of Rest-Mex at IberLEF 2021: Recommendation System for Text Mexican Tourism\n169the Miner\u0013 \u0010a UNAM Run1 approach, suggest-\ning that the participants' systems are com-\nplementary to each other again, with an er-\nror result very close to zero. Nevertheless, the\nvote approach indicates that the intersection\nof correctly classi\fed instances by the sys-\ntems is also relatively small, and therefore,\nautomatically taking advantage of this com-\nplementarity is a complex task.\nInterestingly, the fewer teams are taken\ninto account for the vote, the better the com-\nbination result. This may be because the best\nsystems are taken, and the lower the num-\nber of systems, the noise decreases. How-\never, the trend of results indicates that the\nvote will obtain the same result as the best\nof the systems in the best cases, making a\nvote meaningless. Although the accuracy and\nF-measure statistics were improved in the 3\nbest results, the MAE measure could not be\nimproved.\n4.2.2 Interesting opinions\nTwo types of interesting opinions can be ob-\nserved.\n1. Opinions that were classi\fed correctly\nby all systems.\n2. Opinions that were not classi\fed cor-\nrectly by any system.\nFor the \frst type, there were 17 opinions\nin which all systems correctly predicted their\nclass. The 17 opinions belong to class 5. This\nmeans that they are very positive, and the\ntext of the opinion clearly shows it. Examples\nof these opinions are:\n\u2022\\Su arquitectura, sus columnas, todo\nsu interior es hermoso su iluminaci\u0013 on\nadem\u0013 as de la gente de Guanajuato que\nlo hacen un lugar mas para visitar \".\n\u2022\\Esta bas\u0013 \u0010lica es una maravilla tanto en\nsu exterior como interior. Vale la pena\nconocerla y admirar todos los detalles\nque tiene. \"\n\u2022\\Llegar de noche a este majestuoso lu-\ngar, brinda la oportunidad de contemplar\nuna parte bella de la ciudad .\"\nFor those of the second type, 70 opinions\nwere found that were not correctly classi\fed\nby any system. It is important to note that\nnone of these opinions are from Class 5. Ex-\namples of these opinions are:\u2022\\En tu visita pasa por ah\u0013 \u0010 es muy espe-\ncial que lo visites y te enteres de lo que\npasa con los cuerpos en ese lugar, es im-\npresionante.\"\n{Class: 1\n{Average of the systems output: 4.71\n\u2022\\A todos, este monumento est\u0013 a pre-\ncioso pero de d\u0013 \u0010a hay que visitarle, de\nnoche abstenerse ya que no hay seguri-\ndad p\u0013 ublica en el lugar y te pueden\nasaltar.\"\n{Class: 1\n{Average of the systems output: 4.14\n\u2022\\Siendo uno de sus atractivos tur\u0013 \u0010sticos\nm\u0013 as importantes, es una l\u0013 astima la\ncondici\u0013 on en que se encuentra el museo,\nsucio, sin gu\u0013 \u0010as, poca informaci\u0013 on,\nencerrado, un decorado sin sentido. \"\n{Class: 1\n{Average of the systems output: 3.57\nIn the \frst example, it is clear that\nthe opinion is positive. However, the class\nawarded by the same tourist is 1 (the low-\nest). It is possible that the tourist confused\nthe order of the scale, which makes it very\ndi\u000ecult to classify this type of opinion cor-\nrectly. In the second case, the tourist gives\na positive opinion but ends with a negative\nconnotation talking about safety issues. Al-\nthough the word assault (asaltar ) gives a neg-\native connotation, the other part of the opin-\nion makes the opinion have a higher value;\nhowever, the tourist gave it the lowest class.\nFinally, in the third example, a negative opin-\nion can be observed, but the systems gave\na higher rating, possibly due to the bias of\nthe class imbalance towards the more posi-\ntive classes.\nFor more details of the re-\nsults of both tasks, it is possible\nto go to the following web page:\nhttps://sites.google.com/cicese.edu.mx/rest-\nmex-2021/results.\n5 Conclusions\nThis paper described the design and results\nof the Rest-Mex shared task collocated with\nIberLef 2021. Rest-Mex stands for Recom-\nmendation system and Sentiment analysis in\nSpanish tourists text for Mexican places . Two\nMiguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Arce-Cardenas, Daniel Fajardo-Delgado, Rafael Guerrero-Rodr\u00edguez, \nA. Pastor L\u00f3pez-Monroy, Juan Mart\u00ednez-Miranda, Humberto P\u00e9rez-Espinosa, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez\n170tasks were proposed, one targeting recom-\nmendation tourist places systems and the\nother focused on sentiment analysis. Mainly,\ngiven a set of opinions in Spanish, the par-\nticipants had to determine the degree of sat-\nisfaction that a tourist may have when vis-\niting a Mexican place as well as the polar-\nity of a tourist opinion. For these tasks, we\nbuilt the two data sets derived from TripAd-\nvisor. The shared task lasted more than three\nmonths and attracted 31 teams from coun-\ntries such as Mexico, Spain, Cuba, Brazil,\nChile, Colombia, and the USA. Out of these\nteams, 9 sent the results of their systems, and\n8 sent their report and description of their\nsystems.\nThe best MAE result for the recommen-\ndation task was obtained by (Arreola et al.,\n2021), while the best result in the senti-\nment analysis task was obtained by (V\u0013 asquez,\nG\u0013 omez-Adorno, and Bel-Enguix, 2021).\nFor the two tasks, the best results were\nobtained through representations based on\nBERT, which again gives evidence that the\nfuture of textual classi\fcation is directed to\nthe use and application of this type of archi-\ntecture.\nFinally, it is shown that there is signif-\nicant complementarity between the partici-\npating systems of both; however, it does not\nseem easy to be able to take advantage of the\ninformation that each one of them correctly\nclassi\fes to unite it and improve individual\nresults. This could be an interesting research\ndirection in the future of these tasks.\nAcknowledgements\nOur special thanks go to all of Rest-Mex's\nparticipants, the organizers, and their insti-\ntutions.\nReferences\nAbreu, J. and P. Mirabal. 2021. Cascade\nof biased two-class classi\fers for multi-\nclass sentiment analysis. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR WS Proceedings.\nAlaei, A. R., S. Becken, and B. Stantic. 2019.\nSentiment analysis in tourism: capitaliz-\ning on big data. Journal of Travel Re-\nsearch, 58(2):175{191.\nAnis, S., S. Saad, and M. Aref. 2020. A\nsurvey on sentiment analysis in tourism.International Journal of Intelligent Com-\nputing and Information Sciences , pages 1{\n20.\nArag\u0013 on, M. E., M. A. \u0013Alvarez-Carmona,\nM. Montes-y G\u0013 omez, H. J. Escalante,\nL. V. Pineda, and D. Moctezuma. 2019.\nOverview of mex-a3t at iberlef 2019: Au-\nthorship and aggressiveness analysis in\nmexican spanish tweets. In IberLEF@ SE-\nPLN, pages 478{494.\nArreola, J., L. Garcia, J. Ramos-Zavaleta,\nand A. Rodr\u0013 \u0010guez. 2021. An embeddings\nbased recommendation system for mexi-\ncan tourism. submission to the rest-mex\nshared task at iberlef 2021. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR WS Proceedings.\nCarmona-S\u0013 anchez, G., A. Carmona, and\nM. A. \u0013Alvarez-Carmona. 2021. Naive\nfeatures for sentiment analysis on mexi-\ncan touristic opinions texts. In Proceed-\nings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR WS Proceedings.\nDi-Bella, M. G. 2019. Introducci\u0013 on al tur-\nismo.\nElorza, S. R. 2020. Turismo y sars-cov-2\n1 en m\u0013 exico. perspectivas hacia la nueva\nnormalidad. Desarrollo, econom\u0013 \u0010a y so-\nciedad, 9(1):93{98.\nMorales-Gonz\u0013 alez, E., D. Torres-Moreno,\nA. Ehrlich-Lopez, M. Toledo-Acosta,\nB. Martnez-Zaldivar, and J. Hermosillo-\nValadez. 2021. A recommendation sys-\ntem for tourism based on semantic repre-\nsentations and statistical relational learn-\ning. In Proceedings of the Third Workshop\nfor Iberian Languages Evaluation Forum\n(IberLEF 2021), CEUR WS Proceedings.\nRivas D\u0013 \u0010az, J. P., R. Callejas C\u0013 arcamo, and\nD. Nava Vel\u0013 azquez. 2020. Perspectivas\ndel turismo en el marco de la pandemia\ncovid-19.\nRoldan Reyes, E. 2021. Techkatl: A senti-\nment analysis model to identify the polar-\nity of mexican's tourism opinions. In Pro-\nceedings of the Third Workshop for Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR WS Proceedings.\nOverview of Rest-Mex at IberLEF 2021: Recommendation System for Text Mexican Tourism\n171Romero-Cant\u0013 on, A. and R. Aranda. 2021.\nSentiment classi\fcation for mexican\ntourist reviews based on k-nn and jaccard\ndistance. In Proceedings of the Third\nWorkshop for Iberian Languages Evalua-\ntion Forum (IberLEF 2021), CEUR WS\nProceedings.\nToledo-Acosta, M., B. Mart\u0013 nez-Zaldivar,\nA. Ehrlich-L\u0013 opez, E. Morales-Gonz\u0013 alez,\nD. Torres-Moreno, and J. Hermosillo-\nValadez. 2021. Semantic representations\nof words and automatic keywords extrac-\ntion for sentiment analysis of tourism re-\nviews. In Proceedings of the Third Work-\nshop for Iberian Languages Evaluation Fo-\nrum (IberLEF 2021), CEUR WS Proceed-\nings.\nV\u0013 asquez, J., H. G\u0013 omez-Adorno, and G. Bel-\nEnguix. 2021. Bert-based approach for\nsentiment analysis of spanish reviews from\ntripadvisor. In Proceedings of the Third\nWorkshop for Iberian Languages Evalua-\ntion Forum (IberLEF 2021), CEUR WS\nProceedings.\nVelazquez Medina, G. and D. I. Hernan-\ndez Farias. 2021. Dci-ug participation\nat rest-mex 2021: A transfer learning\napproach for sentiment analysis in span-\nish. In Proceedings of the Third Workshop\nfor Iberian Languages Evaluation Forum\n(IberLEF 2021), CEUR WS Proceedings.\nMiguel \u00c1. \u00c1lvarez-Carmona, Ram\u00f3n Aranda, Samuel Arce-Cardenas, Daniel Fajardo-Delgado, Rafael Guerrero-Rodr\u00edguez, \nA. Pastor L\u00f3pez-Monroy, Juan Mart\u00ednez-Miranda, Humberto P\u00e9rez-Espinosa, Ansel Y. Rodr\u00edguez-Gonz\u00e1lez\n172VaxxStance@IberLEF 2021: Overview of the Task onGoing Beyond Text in Cross-Lingual Stance\nDetection\nVaxxStance@IberLEF 2021: Descripci\u0013 on de la tarea de\ndetecci\u0013 on de actitudes basada en el uso de informaci\u0013 on m\u0013 as\nall\u0013 a del texto\nRodrigo Agerri1, Roberto Centeno2, Mar\u0013 \u0010a Espinosa2,\nJoseba Fernandez de Landa1,\u0013Alvaro Rodrigo2\n1HiTZ Center - Ixa, University of the Basque Country UPV/EHU\n2NLP&IR group at Universidad Nacional de Educaci\u0013 on a Distancia (UNED)\nrodrigo.agerri@ehu.eus, rcenteno@lsi.uned.es, mespinosa@lsi.uned.es,\njoseba.fernandezdelanda@ehu.eus, alvarory@lsi.uned.es\nAbstract: This paper describes the VaxxStance task at IberLEF 2021. The task\nproposes to detect stance in Tweets referring to vaccines, a relevant and contro-\nversial topic in the current pandemia. The task is proposed in a multilingual set-\nting, providing data for Basque and Spanish languages. The objective is to explore\ncrosslingual approaches which also complement textual information with contextual\nfeatures obtained from the social network. The results demonstrate that contextual\ninformation is crucial to obtain competitive results, especially across languages.\nKeywords: Stance Detection, Multilingualism, Computational Social Science, In-\nformation Extraction.\nResumen: En este art\u0013 \u0010culo se describe la tarea VaxxStance celebrada en el marco\nde IberLEF 2021. La tarea propone detectar la actitud de un conjunto de tweets rel-\nativos a las vacunas, a un tema muy actual y pol\u0013 emico en estos tiempos de pandemia.\nLa tarea se ha propuesto en un marco multiling\u007f ue, euskera y espa~ nol. Adem\u0013 as del\ntexto de cada tweet, se ha proporcionado adem\u0013 as informaci\u0013 on relacionada con la red\nsocial de los usuarios autores de los tweets. Los resultados de los participantes han\ncorroborado que el uso de informaci\u0013 on de la red social permite mejorar el rendimiento\nen esta tarea, particularmente en un entorno crossling\u007f ue.\nPalabras clave: Detecci\u0013 on de Actitudes, Multiling\u007f uismo, Ciencias Sociales Com-\nputacionales, Extracci\u0013 on de Informaci\u0013 on.\n1 Introduction\nStance detection is one of the tasks within\nthe universe of Fake News detection and as\nsuch is related to other tasks such as Hyper-\npartisanism (Kiesel et al., 2019), Hate Speech\nDetection (Basile et al., 2019), Fact-checking\nand Claim Veri\fcation (Thorne et al., 2018),\namong others. The most popular formula-\ntions are perhaps those proposed in 2016 by\nthe SemEval-2016 Task 6: Detecting Stance\nin Tweets (Mohammad et al., 2016) and by\nthe Fake News Challenge (Stage 1)1. In the\n\frst, stance is de\fned as establishing whether\na given tweet expresses a FAVOR, AGAINST\nor NEUTRAL (NONE) attitude with respect\n1http://www.fakenewschallenge.org/to a given, pre-de\fned topic. In the sec-\nond formulation, provided by the Fake News\nChallenge, stance has to be inferred between\na claim and a text commenting on the claim.\nIn this case the stance category can be one of\nAgrees, Disagrees, Discusses and Unrelated.\nOther subsequent contributions have fo-\ncused mostly on the static variant of stance\ndetection (with respect to a pre-de\fned\ntopic) rather than on the dynamic one (clas-\nsifying stance with respect to previous mes-\nsage), although there are some exceptions,\nnotably the RumourEval tasks (Derczynski\net al., 2017; Gorrell et al., 2019).\nFurthermore, as it is usually the case\nin the Natural Language Processing (NLP)\n\feld, most works have experimented on En-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 173-181\nrecibido 30-06-2021 revisado 07-07-2021 aceptado 11-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-15\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalglish only, with some exceptions. An Arabic\ncorpus integrated the tasks of fact-checking\nand stance detection (Baly et al., 2018), a\ndataset from comments of news was devel-\noped for Czech language (Hercig et al., 2017),\nand there also works for French (Evrard et\nal., 2020) and Russian (Vychegzhanin, 2019).\nFinally, an interesting new dataset for Italian\nwas released in 2020 as part of the SardiS-\ntance@Evalita 2020 shared task (Cignarella\net al., 2020), which included not only the\ntexts of the tweets labeled with stance, but\nalso social network information relative to\nthe authors of the tweets. This social net-\nwork information includes retweets, user ac-\ncounts pro\fle, friends and followers, among\nothers.\nOther interesting works have tried to ad-\ndress stance detection from a multilingual\npoint of view. The IberEval 2017 and 2018\nshared tasks (Taul\u0013 e et al., 2018) provided\na dataset in Catalan and Spanish to clas-\nsify stance with respect to the Independence\nof Catalonia, while Lai et al. (2020) pro-\nvided datasets in French and Italian. How-\never, these multilingual e\u000borts are hindered\nby the extremely skewed class distribution in\nthe Catalan IberEval data, or by the fact that\nthe data for each language was not collected\non the same timeframe and addressed di\u000ber-\nent topics. This makes it very di\u000ecult to\ninvestigate multilingual and crosslingual ap-\nproaches to stance detection. While Zotova,\nAgerri, and Rigau (2021) propose a method\nto address these shortcomings by providing\na semi-automatically generated multilingual\nstance detection corpus, they do not include\nsocial network features in their dataset.\nIn this context, we propose the VaxxS-\ntance shared task at IberLEF 2021 (Montes\net al., 2021), with the aim of detecting stance\nin social media on vaccines in general. The\ntask provides data in two languages, Basque\nand Spanish, and its objective is to promote\ncrosslingual research on stance detection us-\ning both the text and the information pro-\nvided by the Twitter social network. Thus,\nand unlike previous approaches, we provide,\nfor a given topic, multilingual coetanous data\nof gold-standard quality in a corpus which al-\nlows to experiment using both social and tex-\ntual features in multilingual and crosslingual\nsettings.2 Multilingual Dataset\nFollowing the formulation of stance provided\nby Mohammad et al. (2016), the VaxxS-\ntance task consists of determining whether\na given tweet expresses an AGAINST, FA-\nVOR or NEUTRAL (NONE) stance towards\nvaccines. Additionally, and inspired by the\nSardiStance 2020 shared task (Cignarella et\nal., 2020), the dataset includes two di\u000ber-\nent types of data: Textual and Contextual\n(retweets, friends and user data), for two lan-\nguage, Basque and Spanish. The dataset is\npublicly available in the task website2.\n2.1 Collection and Annotation\nIn a \frst attempt we tried to do the data\ncollection and annotation for both languages\nin the same manner. However, as it will be\nexplained below, due to the idiosyncrasies of\nBasque it was necessary to devise an alterna-\ntive, more viable, method for that language,\nespecially to obtain the required textual data.\nIn any case, we did specify a number of\ncriteria that both languages needed to com-\nply with. First, the datasets a required\nto have a balanced distribution in the ratio\nusers/tweets to avoid that a large number of\ntweets belonged to a very few users. Second,\nthe tweets in the training set had to be writ-\nten by di\u000berent users from those contained\nin the test set. This is to avoid obtaining\narti\fcially high results due to the existence\nof user-based information in both the train-\ning and test sets. As such, the general idea\nis that both the textual and user-based (or\ncontextual) knowledge would help each other\nin order to better classify stance. Finally, we\nuse the annotation guidelines from the Se-\nmEval 2016 task (Mohammad et al., 2016).\n2.1.1 Basque\nBasque is spoken by roughly the 30% of the\npopulation in the Basque Country, and un-\nderstood by around 50%. Due to the fact\nthat Basque is a co-o\fcial language, it does\nhave presence in the regional public admin-\nistration, as well as in the education sys-\ntem and some news media, including a public\ntelevision broadcaster. Still, the presence of\nBasque in mass media is extremely low, es-\npecially when compared to Spanish, the 4th\nmost spoken language in the world.\nIn this context, the increasing popular-\nity of Twitter among Basque speakers is of\n2https://vaxxstance.github.io/\nRodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo\n174particular importance for a low resource lan-\nguage, as a relatively large amount of tex-\ntual content written in Basque is generated\nin that social network. This provides a valu-\nable resource to study new NLP tasks such as\nstance detection not only for large and popu-\nlar languages, but also for low resourced ones.\nStill, the collection process of enough tweets\nrelevant to the VaxxStance task was rather\nchallenging.\nAt \frst we experimented with a keyword\nextraction method using the following spe-\nci\fc keywords: \\txertoa\" (vaccine) and \\txer-\ntaketa\" (vaccination), \\negazionista\" (nega-\ntionist), #p\fzer, #moderna, #astrazeneca\nand their respective in\rections. However,\nit was surprising to \fnd that the tra\u000ec of\nBasque tweets relative to those topics were\nrelatively low.\nWe therefore decided to try an alternative,\nmore brute-force, method. First, we collected\nall the available timelines of users that are\nidenti\fed to write mostly in Basque (around\n10k users). The content of these timelines\namount to around 8M tweets. Second, rel-\nevant tweets were selected following a sim-\nple keyword search using the same keywords\nlisted for the previous attempt. Third, a \frst\nannotator manually labeled a set of around\n1;400 tweets. Finally, those same 1 ;400\ntweets, belonging to 210 users, were blindly\nannotated by a second annotator. The \fnal\ncomposition of the textual part of the dataset\ncan be seen in Table 1.\nTrain Test\nTweets 1,072 312\nFavor 327 85\nNeutral 524 135\nAgainst 219 92\nUsers 149 61\nTable 1: Textual data in the Basque dataset.\nWe would like to note that the most\ndi\u000ecult part in the process was \fnding\nenough users that explicitly expressed a\nstance AGAINST vaccines.\n2.1.2 Spanish\nAround 2; 700 tweets written in Spanish stat-\ning an opinion about \\vaccines\" were col-\nlected and annotated, as shown by Table 2.\nIn order to avoid a potential bias derived\nfrom the current COVID-19 pandemic situ-\nation, the tweets were collected from the be-ginnings of Twitter until current time. They\nwere also restricted to the peninsular vari-\nant of the Spanish language in order to avoid\nproblems derived from the use of di\u000berent\nterms in other variants such as Colombian,\nPeruvian, etc. To guide this process we used\nthe Google tool \\Google Trends\"3which al-\nlowed us to locate temporal spaces where\nevents related to vaccines had occurred, iden-\ntifying the type of event and the date on\nwhich it happened. Some examples are the\npeaks in tra\u000ec for and against the vacci-\nnation against measles, which was a conse-\nquence of some measles outbreaks that hap-\npened in Spain during 2019. By using key-\nwords related to the event and restricting\nthe dates obtained, we managed to intro-\nduce tweets related to events other than the\nCOVID-19 vaccination process.\nTrain Test\nTweets 2,003 694\nFavor 937 359\nNeutral 591 195\nAgainst 475 140\nUsers 1,261 414\nTable 2: Textual data in the Spanish dataset.\nIn addition to the tweets collected through\nthe events identi\fed in Google Trends, for\nthe rest of the tweets collected we followed\nthe following process. First, we used a set of\nkeywords such as \\vaccine\", \\vaccination\",\nas well as terms related to diseases whose\nvaccines have generated some controversy in\nsociety and in anti-vaccine movements, e.g.,\n\\chickenpox\", \\autism\", \\MMR\", etc. After\na \frst manual analysis, we observed that the\nvast majority of the tweets collected did not\nexpress a stance. In order to solve this prob-\nlem, we then extracted the hashtags most\ncommonly used in these tweets and manu-\nally analysed those that were used to ex-\npress a position in favour and/or against\nvaccines. Some examples of these hash-\ntags are #YoMeVacuno, #VaccinesWork,\n#COVID19 ,#vacuna, #yomevacuno, #Va-\ncunaCOVID19 ,#YoNoMeVacuno ,#gripe,\n#Plandemia, #yosimevacuno, etc.\nBy using these hashtags, we managed to\nincrease the number of tweets to start with\nthe manual labeling. The labelling was per-\nformed manually by two annotators, using\n3https://trends.google.es/trends/?geo=ES\nVaxxStance@IberLEF 2021: Overview of the Task on Going Beyond Text in Cross-Lingual Stance Detection\n175a third annotator to resolve disagreements.\nFor this we used the web platform created by\nCignarella et al. (2020), to whom we would\nlike to thank for their help using it.\nOnce the manual annotation was com-\npleted, the set of AGAINST tweets was much\nsmaller than those expressing a FAVOR or\nNEUTRAL stance. To address this issue,\nwe identi\fed several accounts of users that\nmay potentially be identi\fed as supporters\nof anti-vaccine movements and manually col-\nlected tweets from these users expressing an\nAGAINST stance. This step was performed\ntaking care in complying with the general cri-\nteria of not including more than 10 tweets\nper user in the \fnal corpus, as well as not\noverlapping users between the training and\nevaluation set. In this \fnal process we man-\naged to increase by about 200-250 tweets the\nAGAINST class.\n2.2 Social Media Information\nThe main objective of this task is studying\nthe usefulness of the context provided by so-\ncial media information to classify stance in\na crosslingual setting. With this objective\nin mind, we collected contextual information\nrelative to the friends of the authors of the\ntweets as well as their retweets . The con-\ntext provided by friends and retweets can be\nleveraged to generate relation graphs that in\nturn may be used to improve the classi\fers.\nTable 3 shows the social media data gath-\nered with respect to the tweets in the train\nand test partitions for each of the languages.\nIn addition to the retweets of the tweets in-\ncluded in the datasets, for Basque we also de-\ncided to collect the all the retweets made by\nthe users, namely, by extracting the retweets\nfrom the users' timelines (TL). This strategy\nwas applied in order to alleviate the small\nnumber of retweets obtained from the tweets\nin the train and test partitions.\nTrain Test\nBasque Friends 119,977 53,029\nRetweets 203 0\nRetweets (TL) 130,369 61,438\nSpanish Friends 1,708,396 438,586\nRetweets 6,832 2,148\nTable 3: Social Media Information by lan-\nguage.\nFinally, apart from social media informa-tion, the dataset also includes the meta in-\nformation of each annotated tweet as well as\nthe information related to each user.\n2.3 Final Dataset\nTable 4 shows the composition of the VaxxS-\ntance dataset, including both textual and\ncontextual information. Regarding the tex-\ntual information, it can bee seen that the\nSpanish set is roughly double in size with re-\nspect to the Basque one, although the distri-\nbution of classes across the train and test set,\nas shown by Tables 1 and 2, is quite similar.\nTrain Test\nBasque Tweets 1,072 312\nUsers 149 61\nFriends 119,977 53,029\nRetweets 203 0\nRetweets (TL) 130,369 61,438\nSpanish Tweets 2,003 694\nUsers 1,261 414\nFriends 1,708,396 438,586\nRetweets 6,832 2,148\nTable 4: Composition of the VaxxStance\n2021 dataset.\nWith respect to the contextual informa-\ntion we can see that for Basque there are\nvery few users, around 10% of the number\nof users for Spanish. This is a re\rection\nof the much smaller community of Twitter\nusers that write in Basque. In this sense,\nthefriends graph also re\rects the same ratio,\nas the number of friends relations is around\n10% of those obtained for Spanish. If we look\nat the retweets , however, we can see that for\nBasque we only managed to obtain very few\nof them. That is why we decided to also pro-\nvide the retweets for each user in the train\nand test sets (retweets TL).\nIn summary, the VaxxStance dataset pro-\nvides an interesting benchmark to investigate\ncrosslingual approaches to stance detection\nbased on both textual and contextual fea-\ntures. While the Basque set is slightly smaller\nthan some previous approaches (Taul\u0013 e et al.,\n2018; Cignarella et al., 2020; Zotova, Agerri,\nand Rigau, 2021) it is still larger than the\ndata provided for any of the topics in the\nSemEval 2016 dataset, which is perhaps the\nmost popular benchmark for stance detection\n(Mohammad et al., 2016).\nRodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo\n1763 Task De\fnition\nIn this task we aimed to promote research on\nmultilingual and crosslingual approaches to\nstance detection in Twitter. Ideally, this type\nof research requires annotated datasets on a\ncommon topic for more than one language\nand obtained on the same dates (coetaneous\ndata). However, while previous work men-\ntioned in the Introduction includes datasets\nin several languages, they do not provide an\nadequate evaluation setting for multilingual\nand crosslingual studies to stance detection.\n3.1 Tracks\nAs the task contains tweets in two di\u000berent\nlanguages, we proposed the following partici-\npation tracks for each language (Basque and\nSpanish):\n\u2022Close Track: Language-speci\fc evalua-\ntion. Only the provided data for each of\nthe languages is allowed. There are two\nevaluation settings:\n{Textual: Only the provided tweets\nin the target language can be used\nfor development. No data augmen-\ntation is allowed.\n{Contextual: Text plus given\nTwitter-related information will be\nused by the participants. Contex-\ntual information refers to features\nrelated with user-based Twitter\ninformation: friends, retweets, etc.\ndescribed in Section 2.2.\n\u2022Open Track: Participants can use any\nkind of data, including additional tweets\nobtained by the participants. The main\nobjective consists of exploring data aug-\nmentation and knowledge transfer tech-\nniques for cross-lingual stance detection.\n\u2022Zero-shot Track: Texts (tweets) of the\ntarget language cannot be used for train-\ning. The main objective is to explore\nhow to develop systems that do not have\naccess to text in the target language, es-\npecially using Twitter-related informa-\ntion.\nParticipants could submit their systems to\nany of the tracks, but it was compulsory to\nparticipate in both languages for the chosen\ntrack.3.2 Evaluation\nFollowing previous work, we evaluate the sys-\ntems with the metric provided by the Se-\nmEval 2016 task on Stance Detection (Mo-\nhammad et al., 2016) which reports F1\nmacro-average score of two classes, FAVOR\nand AGAINST, although the NONE class is\nalso represented in the test data:\nF1avg=F1favor +F1against\n2(1)\nThe o\u000ecial evaluation script is distributed\ntogether with the dataset in the task web-\nsite4.\n3.3 Baselines\nWe provide two baselines, one using only tex-\ntual information and a second one using just\nsocial or contextual features:\n\u2022Textual: The textual baseline is based\non a SVM classi\fer with RBF kernel\nfunction. The text of the tweets is vec-\ntorized using a TF-IDF vectorizer and\nthen feed to the classi\fer. Both Cand\nGamma hyperparameters are tuned by\nmeans of grid search and 5 fold CV on\nthe training data. The best con\fgura-\ntion is used to evaluate on the test.\n\u2022Social: This classi\fer uses the metadata\nrelated to each user and tweet to obtain\na number of features (friends count, sta-\ntus count, emojis in bio, etc.) which are\nthen used to train a XGBoost classi\fer.\nBefore feeding the classi\fer, each class\ndata is weighted in order to create a bal-\nanced sample.\nAgainst Favor Average\nBasque Textual 51.80 57.01 54.41\nSocial 5.23 48.53 26.88\nSpanish Textual 71.38 81.68 76.53\nSocial 73.14 73.73 73.43\nTable 5: Baseline results on Test set.\nThe results obtained by the baselines show\nthat both tracks are harder for Basque. With\nrespect to the Textual track, stance in Span-\nish seems to be expressed more explicitly. Re-\ngarding the social baseline, the low results\nwere probably caused by the low number of\n4https://vaxxstance.github.io/\nVaxxStance@IberLEF 2021: Overview of the Task on Going Beyond Text in Cross-Lingual Stance Detection\n177Basque users from which to obtain the fea-\ntures.\n4 Participants and Results\nTwenty groups registered for the task and\ndownloaded the datasets. However, only\nthree groups \fnally submitted runs. Table\n6 shows the information of the participant\ngroups and the reference to their reports.\nTeam Report\nMultiAztertest (Gonzalez-Dios and Bengoetxea, 2021)\nSQYQP (Calleja and M\u0013 endez, 2021)\nWordUp (Lai et al., 2021)\nTable 6: Participants.\nIn total, the participants submitted 28\nruns, 14 per language. While all the three\nteams participated in both Textual and Con-\ntextual settings of the Close Track, only one\nteam, WordUp, participated in the Zero-shot\nand Open Tracks. Thus, any comparisons\nbetween the participant systems will be per-\nformed on the Close Track.\n4.1 Close Track\nTable 7 shows the results for the Close Track,\nwhich received 20 submission runs. We re-\nport results for each language and evaluation\nsetting (Textual and Contextual). For all the\nfour rankings, the best results are always ob-\ntained by the WordUp team.\nAs it was the case with our baseline re-\nsults, the participant systems score system-\natically higher for Spanish. The best results\nfor Spanish are over an 80 F1 score. These\nresults seem to con\frm that the Spanish set\nis easier than the Basque one.\nFor each language, the results improve by\nusing contextual information, except for the\nMultiAztertest Basque submissions. Still, re-\nsults con\frm the e\u000bectiveness of employing\nboth textual and social information.\nRegarding the results of baselines, the tex-\ntual baseline obtains competitive results in\nboth languages, being only outperformed by\nthe WordUp team. In contrast, the contex-\ntual baseline's results are improved by all\nthe teams in Basque (except one run from\nWordUp which obtains a very low score) and\nat least one run per group, except SQYQP,\nin Spanish. These results suggest that, de-\nspite their simplicity and use of linear classi-\fers, the approaches followed by both base-\nlines represent an adequate starting point.\nRegarding the techniques followed by the\nparticipants, MultiAzterTest tested two dif-\nferent approaches for the Textual setting: a\nsystem which used pre-trained transformers-\nbased language models (run 1) and another\none based on training a classi\fer with a set of\nlinguistic and stylistic features such as word\nfrequencies, semantic overlap, etc. (run 2).\nThe \frst approach performed much better\nthan the second in the textual track. For the\ncontextual track, they employ only the infor-\nmation relative to the user. More speci\fcally,\nfor each user they select the most common\nstance label, assuming that users tweets cor-\nrespond coherently to one stance type. While\nthis idea worked well in Spanish, it was detri-\nmental in (run 1) in Basque.\nThe SQYQP team addressed the textual\nsetting by training a LSTM initialized with\nmultilingual BERT embeddings using the\nFlair toolkit (Akbik, Blythe, and Vollgraf,\n2018). For the contextual setting, they added\nnetwork information and measured the dis-\ntance among users following the approach\ndone by Espinosa et al. (2020). While their\nresults for Basque are below the textual base-\nline, they improve results by adding contex-\ntual information.\nThe WordUp! team employed a large\nnumber of common features that have been\nproved useful for stance detection. Features\nwere extracted mostly from stylistic and de-\npendency analyses. Additionally, the also\ncrawled tweets speci\fcally for this task and\ntopic for both languages. The tweets were\nthen used to train FastText word embeddings\nand used to obtain several features. More-\nover, they created a dictionary of lemmas re-\nferring to stance in English, and translated\nit to Spanish and Basque. In the contex-\ntual setting they tried several network-based\nmeasures to be added as features to the logis-\ntic regression classi\fer. They report a large\nnumber of experiments which resulted in the\nbest performing team across all evaluation\ntracks.\nIn general, the results obtained by the par-\nticipants show that, even when using very\nsimple features, contextual information sub-\nstantially improved the results obtainecd by\nusing text only.\nRodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo\n178Against Favor F1 Macro\nBasque Textual WordUp 01 57.69 56.99 57.34\nWordUp 02 55.03 54.27 54.65\n*BASELINE 51.80 57.01 54.41\nMultiAztertest 01 48.23 52.25 50.24\nSQYQP 01 38.81 46.31 42.56\nMultiAztertest 02 34.38 34.18 34.28\nSpanish Textual WordUp 02 78.36 83.47 80.92\nWordUp 01 75.54 82.58 79.06\n*BASELINE 71.38 81.68 76.53\nMultiAztertest 01 66.67 81.53 74.10\nSQYQP 01 57.14 77.61 67.38\nMultiAztertest 02 56.47 71.60 64.04\nBasque Contextual WordUp 02 82.95 72.46 77.71\nSQYQP 01 65.17 52.94 59.06\nMultiAztertest 02 25.40 48.03 36.72\nMultiAztertest 01 16.36 56.06 36.21\n*BASELINE 5.23 48.53 26.88\nWordUp 01 0.00 0.08 0.04\nSpanish Contextual WordUp 02 91.17 87.09 89.13\nWordUp 01 88.97 86.56 87.77\nMultiAztertest 01 78.77 79.84 79.31\n*BASELINE 73.14 73.73 73.43\nSQYQP 01 66.27 80.06 73.17\nMultiAztertest 02 63.93 77.17 70.55\nTable 7: Close Track o\u000ecial results.\n4.2 Open Track\nThe only participant in this track was the\nWordUp! team, which performed data aug-\nmentation. They generated FastText word\nembeddings (Bojanowski et al., 2017) from\na set of tweets speci\fcally obtained for this\nparticular task and languages. They also\naugmented the contextual information by ex-\ntracting the social network of each user. As\nit can be seen in the results reported in Ta-\nble 8, their results are quite similar to those\nobtained in the Close Track - Contextual set-\nting. This might be due to the fact that they\nalso used the ad-hoc generated FastText em-\nbeddings also in the Close Track.\nAgainst Favor F1 Macro\nBasque WordUp 02 82.29 72.12 77.21\nWordUp 01 64.47 68.12 66.30\nSpanish WordUp 02 90.87 88.07 89.47\nWordUp 01 90.39 88.01 89.20\nTable 8: Open Track o\u000ecial results.\n4.3 Zero-shot Track\nTable 9 shows the results obtained by the\nonly participant in this track, in which theparticipants could not use the text (tweets)\nof the target language for training. The most\nsurprising aspect of the results is perhaps the\nfact that, for Basque, the zero-shot results\noutperform the results of the Textual eval-\nuation setting. This seems to indicate that\ncontextual information is far more important\nthan the texts themselves in order to perform\nstance detection.\nAgainst Favor F1 Macro\nBasque WordUp 01 64.47 68.12 66.30\nWordUp 02 55.70 39.74 47.72\nSpanish WordUp 01 88.03 46.13 67.08\nWordUp 02 18.63 62.77 40.70\nTable 9: Zero-Shot Track o\u000ecial results.\n5 Concluding Remarks\nIn this paper we provide an overview of\nthe VaxxStance@IberLEF 2021 shared eval-\nuation task, in which the objetive is to detect\nstance towards vaccines across two di\u000berent\nlanguages: Basque and Spanish. As a nov-\nelty for stance detection in these languages,\nsystems can use textual and contextual infor-\nVaxxStance@IberLEF 2021: Overview of the Task on Going Beyond Text in Cross-Lingual Stance Detection\n179mation to train their systems in multilingual\nand crosslingual settings.\nThe techniques employed by the di\u000berent\nparticipants showed that contextual informa-\ntion has a great impact across languages,\neven for small community of users such as\nBasque. In this sense, textual results are in\ngeneral improved by adding social network\nfeatures.\nThe datasets for both languages were\nbuilt following the same criteria and ob-\njectives. However, further analysis is re-\nquired to understand why results are sys-\ntematically better for Spanish than those ob-\ntained for Basque. Finally, given that just\none team participated in the Open and Zero-\nshot Tracks, one of the main objectives of\nthe task, to promote research on crosslin-\ngual approaches to stance detection, has not\ncompletely been achieved. Therefore further\nwork is required on this particular line of re-\nsearch.\nAcknowledgments\nThis work has been partially supported by\nthe European Social Fund through the Youth\nEmployment Initiative (YEI 2019) and the\nSpanish Ministry of Science, Innovation and\nUniversities (DeepReading RTI2018-096846-\nB-C21, MCIU/AEI/FEDER, UE), and by\nthe DeepText project (KK-2020/00088),\nfunded by the Basque Government. Rodrigo\nAgerri is also funded by the RYC-2017-23647\nfellowship.\nFinally, we are grateful to Mirko Lai and\nAlessandra Cignarella for sharing with us\ntheir experience organizing the SardiStance\n2020 shared task.\nReferences\nAkbik, A., D. Blythe, and R. Vollgraf.\n2018. Contextual string embeddings for\nsequence. In Proceedings of the 27th In-\nternational Conference on Computational\nLinguistics, pages 1638{1649, Santa Fe,\nNew Mexico, USA.\nBaly, R., M. Mohtarami, J. Glass,\nL. M\u0012 arquez, A. Moschitti, and P. Nakov.\n2018. Integrating Stance Detection and\nFact Checking in a Uni\fed Corpus. In\nProceedings of the 2018 Conference of\nthe North American Chapter of the As-\nsociation for Computational Linguistics:\nHuman Language Technologies, Volume2 (Short Papers) , pages 21{27, New\nOrleans, Louisiana, June. Association for\nComputational Linguistics.\nBasile, V., C. Bosco, E. Fersini, D. Nozza,\nV. Patti, F. M. Rangel Pardo, P. Rosso,\nand M. Sanguinetti. 2019. SemEval-\n2019 task 5: Multilingual detection of hate\nspeech against immigrants and women in\nTwitter. In Proceedings of the 13th In-\nternational Workshop on Semantic Eval-\nuation, pages 54{63, Minneapolis, Min-\nnesota, USA, June. Association for Com-\nputational Linguistics.\nBojanowski, P., E. Grave, A. Joulin, and\nT. Mikolov. 2017. Enriching word vectors\nwith subword information. TACL , 5:135{\n146.\nCalleja, J. and A. M\u0013 endez. 2021.\nSqyqp@vaxxstance: Stance detection for\nthe antivaxxers movement. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2021), CEUR Workshop\nProceedings.\nCignarella, A. T., M. Lai, C. Bosco,\nV. Patti, and P. Rosso. 2020. SardiS-\ntance@EVALITA2020: Overview of the\nTask on Stance Detection in Italian\nTweets. In V. Basile, D. Croce,\nM. Di Maro, and L. C. Passaro, editors,\nProceedings of the 7th Evaluation Cam-\npaign of Natural Language Processing and\nSpeech Tools for Italian (EVALITA 2020) .\nCEUR-WS.org.\nDerczynski, L., K. Bontcheva, M. Liakata,\nR. Procter, G. Wong Sak Hoi, and A. Zu-\nbiaga. 2017. SemEval-2017 task 8: Ru-\nmourEval: Determining rumour veracity\nand support for rumours. In Proceedings\nof the 11th International Workshop on Se-\nmantic Evaluation (SemEval-2017), pages\n69{76, Vancouver, Canada, August. Asso-\nciation for Computational Linguistics.\nEspinosa, M. S., R. Agerri, \u0013A. Rodrigo,\nand R. Centeno. 2020. Deepreading @\nsardistance 2020: Combining textual, so-\ncial and emotional features. In V. Basile,\nD. Croce, M. D. Maro, and L. C. Passaro,\neditors, Proceedings of the Seventh Evalu-\nation Campaign of Natural Language Pro-\ncessing and Speech Tools for Italian. Fi-\nnal Workshop (EVALITA 2020), Online\nevent, December 17th, 2020, volume 2765\nRodrigo Agerri, Roberto Centeno, Mar\u00eda Espinosa, Joseba Fernandez de Landa, \u00c1lvaro Rodrigo\n180ofCEUR Workshop Proceedings . CEUR-\nWS.org.\nEvrard, M., R. Uro, N. Herv\u0013 e, and B. Ma-\nzoyer. 2020. French Tweet Corpus for Au-\ntomatic Stance Detection. In Proceedings\nof The 12th Language Resources and Eval-\nuation Conference, pages 6317{6322, Mar-\nseille, France, May. European Language\nResources Association.\nGonzalez-Dios, I. and K. Bengoetxea.\n2021. Multiaztertest@vaxxstance-iberlef\n2021: Identifying stances with language\nmodels and linguistic features. In Proceed-\nings of the Iberian Languages Evaluation\nForum (IberLEF 2021), CEUR Workshop\nProceedings.\nGorrell, G., E. Kochkina, M. Liakata,\nA. Aker, A. Zubiaga, K. Bontcheva, and\nL. Derczynski. 2019. SemEval-2019 task\n7: RumourEval, determining rumour ve-\nracity and support for rumours. In Pro-\nceedings of the 13th International Work-\nshop on Semantic Evaluation, pages 845{\n854, Minneapolis, Minnesota, USA, June.\nAssociation for Computational Linguis-\ntics.\nHercig, T., P. Krejzl, B. Hourov\u0013 a, J. Stein-\nberger, and L. Lenc. 2017. Detect-\ning stance in czech news commentaries.\nInProceedings of the 17th ITAT: Sloven-\nsko\u0014 cesk\u0013 y NLP workshop (SloNLP 2017),\nvolume 1885 of CEUR Workshop Pro-\nceedings, pages 176{180, Bratislava, Slo-\nvakia. Comenius University in Bratislava,\nFaculty of Mathematics, Physics and In-\nformatics, CreateSpace Independent Pub-\nlishing Platform.\nKiesel, J., M. Mestre, R. Shukla, E. Vin-\ncent, P. Adineh, D. Corney, B. Stein, and\nM. Potthast. 2019. SemEval-2019 task\n4: Hyperpartisan news detection. In Pro-\nceedings of the 13th International Work-\nshop on Semantic Evaluation, pages 829{\n839, Minneapolis, Minnesota, USA, June.\nAssociation for Computational Linguis-\ntics.\nLai, M., A. Cignarella, D. Hernandez Farias,\nC. Bosco, V. Patti, and P. Rosso. 2020.\nMultilingual Stance Detection in So-\ncial Media Political Debates. Computer\nSpeech & Language, 02.Lai, M., A. T. Cignarella, L. Finos, and\nA. Sciandra. 2021. Wordup! at vaxxs-\ntance 2021: Combining contextual in-\nformation with textual and dependency-\nbased syntactic features for stance detec-\ntion. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings.\nMohammad, S., S. Kiritchenko, P. Sobhani,\nX. Zhu, and C. Cherry. 2016. SemEval-\n2016 task 6: Detecting stance in tweets.\nInSemEval-2016), pages 31{41.\nMontes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on,\nR. Agerri, M. \u0013Angel \u0013Alvarez Carmona,\nE.\u0013Alvarez Mellado, J. C. de Albornoz,\nL. Chiruzzo, L. Freitas, H. G. Adorno,\nY. Guti\u0013 errez, S. M. J. Zafra, S. Lima,\nF. M. P. de Arco, and M. T. (eds.). 2021.\nProceedings of the iberian languages eval-\nuation forum (iberlef 2021). CEUR Work-\nshop Proceedings.\nTaul\u0013 e, M., F. M. R. Pardo, M. A. Mart\u0013 \u0010,\nand P. Rosso. 2018. Overview of the\nTask on Multimodal Stance Detection in\nTweets on Catalan# 1oct Referendum. In\nIberEval@ SEPLN, pages 149{166.\nThorne, J., A. Vlachos, C. Christodoulopou-\nlos, and A. Mittal. 2018. FEVER: a\nlarge-scale dataset for fact extraction and\nVERi\fcation. In Proceedings of the 2018\nConference of the North American Chap-\nter of the Association for Computational\nLinguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 809{\n819, New Orleans, Louisiana, June. Asso-\nciation for Computational Linguistics.\nVychegzhanin, S. V. Kotelnikov, E. V. 2019.\nStance Detection Based on Ensembles of\nClassi\fers. Programming and Computer\nSoftware, pages 228{240, January.\nZotova, E., R. Agerri, and G. Rigau. 2021.\nSemi-automatic generation of multilingual\ndatasets for stance detection in Twit-\nter. Expert Systems with Applications,\n170:114547.\nVaxxStance@IberLEF 2021: Overview of the Task on Going Beyond Text in Cross-Lingual Stance Detection\n181Overview of MeO\u000bendEs at IberLEF 2021: O\u000bensiveLanguage Detection in Spanish Variants\nResumen de la tarea MeO\u000bendEs en IberLEF 2021: Detecci\u0013 on\nde lenguaje ofensivo en las variantes del espa~ nol\nFlor Miriam Plaza-del-Arco1, Marco Casavantes2, Hugo Jair Escalante2,\nM. Teresa Mart\u0013 \u0010n-Valdivia1,Arturo Montejo-R\u0013 aez1,Manuel Montes-y-G\u0013 omez2,\nHoracio Jarqu\u0013 \u0010n-V\u0013 asquez2,Luis Villase~ nor-Pineda2;3\n1Universidad de Ja\u0013 en, Campus Las Lagunillas, 23071, Ja\u0013 en, Spain\n2Laboratorio de Tecnolog\u0013 \u0010as del Lenguaje (INAOE), Mexico\n3Centre de Recherche GRAMMATICA (EA 4521), Universit\u0013 e d'Artois, France\nffmplaza, maite, amontejo g@ujaen.es\nfhugojair, mmontesg, villasen g@inaoep.mx\nAbstract: This paper is an overview of MeO\u000bendES 2021, organized at IberLEF\n2021 and co-located with the 37th International Conference of the Spanish Society\nfor Natural Language Processing (SEPLN 2021). The main purpose of MeO\u000bendEs\nis to promote research on the detection of o\u000bensive language in Spanish variants.\nThe shared task involve four subtasks, the \frst two correspond to the identi\fcation\nof o\u000bensive language categories in generic Spanish texts from di\u000berent social media\nplatforms, while subtasks 3 and 4 are related to the identi\fcation of o\u000bensive langua-\nge targeting the Mexican variant of Spanish. Two annotated datasets on o\u000bensive\nlanguage have been released to the Natural Language Processing community. MeOf-\nfendes attracted a large number of participants: a total of 69 signed up to participate\nin the task, 12 submitted o\u000ecial runs on the test data, and 10 submitted system\ndescription papers. Corpora and results are available at the shared task website at\nhttps://competitions.codalab.org/competitions/28679.\nKeywords: MeO\u000bendEs, detecci\u0013 on de lenguaje ofensivo, procesamiento del lenguaje\nnatural, clasi\fcaci\u0013 on de textos.\nResumen: Este art\u0013 \u0010culo presenta la tarea MeO\u000bendES 2021, organizada en iber-\nLEF 2021 junto a la 37\u00aa Conferencia Internacional de la Sociedad Espa~ nola para el\nProcesamiento del Lenguaje Natural (SEPLN 2021). El objetivo principal de MeOf-\nfendEs es promover la detecci\u0013 on del lenguaje ofensivo en las variantes del espa~ nol. La\ntarea compartida implica cuatro subtareas, las dos primeras corresponden a la iden-\nti\fcaci\u0013 on de categor\u0013 \u0010as de lenguaje ofensivo en textos gen\u0013 ericos en espa~ nol extra\u0013 \u0010dos\nde diferentes redes sociales, mientras que las subtareas 3 y 4 est\u0013 an relacionadas con\nla identi\fcaci\u0013 on de lenguaje ofensivo dirigido a la variante mexicana del espa~ nol.\nPara la competencia se han puesto a disposici\u0013 on de la comunidad del Procesamiento\ndel Lenguaje Natural dos conjuntos de datos anotados con lenguaje ofensivo. MeOf-\nfendes ha atrav\u0013 \u0010do a un gran n\u0013 umero de participantes: un total de 69 se inscribieron\npara participar en la tarea, 12 presentaron resultados o\fciales sobre los datos de\nevaluaci\u0013 on y 10 presentaron art\u0013 \u0010culos describiendo su sistema. Los conjuntos de da-\ntos y los resultados o\fciales est\u0013 an disponibles en el sitio web de la tarea compartida:\nhttps://competitions.codalab.org/competitions/28679.\nPalabras clave: MeO\u000bendEs, o\u000bensive language detection, natural language pro-\ncessing, text classi\fcation.\n1 Introduction\nO\u000bensive language detection is part of the\nbroader domain of text classi\fcation, andclosely related to the plethora of subjective\nlanguage classi\fcation tasks (Wiebe et al.,\n2004), including sentiment analysis (Medhat,\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 183-194\nrecibido 05-07-2021 revisado 12-07-2021 aceptado 16-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-16\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalHassan, and Korashy, 2014), emotion detec-\ntion (Canales and Mart\u0013 \u0010nez-Barco, 2014) and\nhate speech detection (MacAvaney et al.,\n2019). O\u000bensive language is more individual-\noriented, as o\u000benses are intended to upset or\nto embarrass someone by means of insults or\nimpolite expressions. The growing participa-\ntion of people in social media has raised the\nproblem of a frequent use of these communi-\ncation channels as an uncontrollable means\nto publish rude messages against others. It\nis not di\u000ecult to realize the large number of\nworks regarding this topic and focusing on\ndi\u000berent languages or targeted communities.\nThis interest, which extends through more\nthan a decade so far, has motivated several\nevaluation campaigns, being the more recent\nthe MEX-A3T task at IberLEF 2020 (Arag\u0013 on\net al., 2020b), O\u000benseval at SemEval 2020\n(Zampieri et al., 2020), or OSACT4 shared\ntask at AOSCT 2020 (Mubarak et al., 2020).\nRegarding the approaches applied to tac-\nkle the challenge of detecting o\u000bensive texts,\nthey share many common methods and al-\ngorithms usually explored in text classi\fca-\ntion tasks, from early lexical based approa-\nches (Razavi et al., 2010) to current deep lear-\nning based ones (Plaza-del Arco et al., 2021).\nWith the aim of promoting research in the\ndetection of o\u000bensive language for Spanish\nand its Mexican variant, we introduced the\nMeO\u000bendEs task at IberLEF 2021 (Montes\net al., 2021) with four subtasks. The \frst two\nsubtasks involve a novel dataset for o\u000bensive\nlanguage research in general Spanish (O\u000ben-\ndEs). The dataset contains users comments\nin response to posts from well-known in\ruen-\ncers in di\u000berent social media platforms (Twit-\nter, YouTube and Instagram). Comments are\nannotated on four di\u000berent classes that invol-\nve non-o\u000bensive and o\u000bensive categories. Par-\nticipants had to develop solutions for identif-\nying those categories from comments. Addi-\ntional information is provided in the dataset,\nincluding the in\ruencer ID and the social me-\ndia source of the post. Systems have to face\nwith di\u000berent challenges in these subtasks: (i)\ndi\u000berent language registers from three di\u000be-\nrent social media platforms (ii)multi-class\nclassi\fcation on non-o\u000bensive and o\u000bensive\ncategories, and (iii) multi-output prediction\nbased on annotators agreement. A total of\n\fve teams submitted their prediction sys-\ntems for subtask 1, and two teams for sub-\ntask 2. Finally, we have received four des-cription papers from the participants in the-\nse subtasks. Overall, the systems used by\nthe participants explore state-of-the-art clas-\nsi\fcation models including traditional neural\nnetworks and Transformer architectures. The\nresults obtained by the participants motiva-\ntes to further research on the o\u000bensive lan-\nguage detection in di\u000berent social media plat-\nforms and on the identi\fcation of di\u000berent\no\u000bensive language categories.\nIn addition to the previous subtasks,\nMeO\u000bendEs involves two subtasks on o\u000bensi-\nve language detection targeting the Mexican\nvariant of Spanish (subtasks 3 and 4). These\nare a continuation of previous e\u000borts in trying\nto detect aggressiveness and o\u000bensiveness in\ntweets in the context of the IberLEF (Arag\u0013 on\net al., 2020a; Arag\u0013 on et al., 2019) and Ibe-\nrEval forums ( \u0013Alvarez-Carmona et al., 2018)\nfor the same variant of Spanish. As in pre-\nvious editions, participants had to develop\nsolutions to recognize o\u000bensiveness in tweets.\nThis time, however, we released additional in-\nformation accompanying tweets, with the ho-\npe that such information could be bene\fcial\nfor improving the recognition performance.\nAlso, these subtasks consider a new dataset\nbuild upon those used in past editions. The\nmain di\u000berence being the annotation process:\nfor the IberLEF2021 campaign tweets were\ncarefully analyzed and labeled by a commit-\ntee of annotators (see Section 3.2). Our goal\nwas to provide a curated dataset that could\nresult in more reliable conclusions.\nAs in previous editions, the aim of sub-\ntasks 3 and 4 were to motivate research on\nthe analysis of o\u000bensive language in Mexi-\ncan Spanish. A language whose characteris-\ntics and variations make it unique in its kind\nand di\u000berent to other languages, making it\nnecessary to have tailored techniques for this\nlanguage. Likewise, cultural aspects like the\nuse of language with sexual connotation for\nnon o\u000bensive communications make it parti-\ncularly challenging in terms of disambigua-\ntion. This phenomenon is not present in the\nlanguages considered in other evaluation fo-\nrums and therefore it has not been studied\nelsewhere.\nSubtasks 3 and 4 attracted a considera-\nble number of participants, most of them im-\nplementing solutions based on cutting edge\nlanguage modeling tools available. In gene-\nral terms, performance of solutions was lo-\nwer than that achieved in previous years (see,\nFlor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn-Valdivia,  \nArturo Montejo-R\u00e1ez, Manuel Montes-y-G\u00f3mez, Horacio Jarqu\u00edn-V\u00e1squez, Luis Villase\u00f1or-Pineda\n184e.g., (Arag\u0013 on et al., 2020a)), this could be due\nto the more careful annotation process. For\nsubtask 3, which does not consider any ad-\nditional metadata, most participants outper-\nformed the baseline, whereas for subtask 4,\nno team outperformed1it. Contrary to what\nwe were expecting, subtask 3 received more\nsubmissions even when additional data was\nprovided for subtask 4. Overall, results are\nencouraging and motivate further research.\nAs with the other subtasks, we will keep the\nleaderboard of the competition open, so that\nusers can keep making submissions despite\nthe competition is over.\nThe remainder of this paper is organized\nas follows. Section 2 introduces the four sub-\ntasks that are part of MeO\u000bendEs. Then,\nSection 3 describes the corpora considered\nfor the di\u000berent subtasks. Next, the solu-\ntions proposed to approach the posed pro-\nblems and their results are reviewed in Sec-\ntion 4. Finally, conclusions are presented in\nSection 5.\n2 Task description\nThis section describes in detail the four sub-\ntasks that are part of the MeO\u000bendEs com-\npetition at IberLEF 2021.\nThe whole shared task was implemented\nin the CodaLab platform2and every sub-\ntask comprised two phases: (1) a develop-\nment phase in which participants had access\nto labeled trial and training data, and whe-\nre they could make submissions to test the\nplatform; and (2) a \fnal phase in which unla-\nbeled test data was released and participants\nuploaded the predictions of their systems to\nthe platform. The evaluation and raking for\nthe o\u000ecial results used the performance sco-\nre measured in the test phase. The di\u000berent\nsubtasks on di\u000berent corpora (see Section 3),\nand their evaluation measures, are described\nbelow:\n\u2022Subtask 1: Non-contextual multi-\nclass classi\fcation for generic Spa-\nnish. Participants had to classify com-\nments into the four di\u000berent categories\nassociated with the O\u000bendEs corpus (see\nsubsection 3.1). No information about\nthe comment (source or in\ruencer ID)\n1Please note that baselines were very competitive\nas described in Section 3.2.1\n2https://competitions.codalab.org/\ncompetitions/28679is provided. Participants can optionally\nsubmit con\fdence values to predictions\n(as a probability for each class, so they\nall sum 1.0) for the four considered ca-\ntegories, in order to evaluate the agree-\nment of predictions with con\fdence of\nten human annotators. For evaluation\nwe considered the micro-averaged pre-\ncision, recall and f1measures. In ca-\nses where participants submit con\fdence\nvalues (between 0 and 1) to their out-\nputs, Mean Squared Error (MSE) is ap-\nplied (with error value equal to one for\nwrongly predicted classes).\n\u2022Subtask 2: Contextual multiclass\nclassi\fcation for generic Spanish.\nSame problem as subtask 1, but meta-\ndata (information about targeted users\nand the related social media) is provided\nto participants. Participants had access\nto information associated with posts: so-\ncial media source, in\ruencer genre and\nin\ruencer name. The same evaluation\nmeasures as subtask 1 were taking into\naccount for this subtask.\n\u2022Subtask 3: Non-contextual binary\nclassi\fcation for Mexican Spanish.\nParticipants must classify tweets as of-\nfensive or non-o\u000bensive in the O\u000bend-\nMEX corpus, this is, a binary text classi-\n\fcation problem. For evaluation we con-\nsidered the precision, recall and f1mea-\nsure with respect to the o\u000bensive class.\n\u2022Subtask 4: Contextual binary clas-\nsi\fcation for Mexican Spanish. Sa-\nme problem as subtask 3, but metada-\nta about each tweet was provided to\nparticipants. For this subtask, partici-\npants had access to information associa-\nted with the tweets and their authors li-\nke: date, retweet count, followers count,\netc. The aim of including this informa-\ntion was to determine to what extent\ncontextual information of tweets and\nusers is useful for improving the detec-\ntion performance. The same evaluation\nmeasures as subtask 3 were used for this\none.\n3 Datasets and baselines\n3.1 O\u000bendES\nFor subtasks 1 and 2 we have released a no-\nvel dataset for o\u000bensive language research\nOverview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants\n185in general Spanish (O\u000bendEs). Focusing on\nyoung in\ruencers from the well-known social\nplatforms of Twitter, Instagram, and YouTu-\nbe, we have collected a corpus composed of\n47,128 Spanish comments manually labeled\non o\u000bensive pre-de\fned categories. A subset\nof the corpus is labeled with three annotators\nwhile another subset is labeled with ten an-\nnotators. The latter attaches a degree of con-\n\fdence to each label computed as the ratio of\nannotators that agreed on the majority label\nover the total number of annotators, so both\nmulticlass classi\fcation and multioutput re-\ngression studies can be carried out. For the\ncompetition, we have selected 30,416 posts\nfrom the total. The posts are labeled with\nthe following categories:\n- O\u000bensive, target is a person (OFP).\nO\u000bensive text targeting a speci\fc individual.\n- O\u000bensive, target is a group of peo-\nple or collective (OFG). O\u000bensive text\ntargeting a group of people belonging to the\nsame ethnic group, gender or sexual orien-\ntation, political ideology, religious belief or\nother common characteristic.\n- O\u000bensive, target is di\u000berent from a\nperson or a group (OFO). O\u000bensive text\nwhere the target does not belong to any of the\nprevious categories, e.g., an organization, an\nevent, a place, an issue.\n- Non-o\u000bensive, but with expletive lan-\nguage (NOE). A text that contains ru-\nde words, blasphemes or swearwords but\nwithout the aim of o\u000bending, and usually\nwith a positive connotation.\n- Non-o\u000bensive (NO). Text that is\nneither o\u000bensive nor contains expletive lan-\nguage.\nWe consider a post as o\u000bensive when lan-\nguage is used to commit an explicit or im-\nplicitly directed o\u000bense that may include in-\nsults, threats, profanity or swearing.\nAdditional to the text of the comment,\ntwo features were also provided as \\contex-\ntual\" information: the name of the social\nplatform where that comment was posted to,\nand the gender addressee of the comment, i.e.\nthe targeted user.\nFinally, di\u000berent sets have been relea-\nsed for the competition. During the pre-\nevaluation phase training and development\n(Dev) sets were provided to the participants\nand for the evaluation phase the test set wasrelease, Table 1 shows the number and per-\ncentage of posts corresponding to each set by\nthe above categories.\nLabel Training Development Test\nNO 13,212 64 9651\nNOE 1,235 22 2340\nOFP 2,051 10 1404\nOFG 212 4 211\nTotal 16,710 100 13,606\nTabla 1: Distribution of the O\u000bendES catego-\nries by subset (Training, Dev, Test) in MeOf-\nfendES subtasks 1 and 2.\n3.1.1 Baseline\nTo evaluate the non-contextual multiclass\nclassi\fcation task on the O\u000bendEs dataset,\nwe implemented a straightforward baseline\nsystem based on a bag-of-words of unigrams,\nbigrams and trigrams and an linear SVM\nclassi\fer. For the multiouput regression task\nwe use a multiouput regressor along with the\nEpsilon-Support Vector Regression. No pre-\nprocessing has been applied to the text, nor\nhas a hyperparameter search been performed.\nWe refer to these baselines as baseline-svm.\n3.2 O\u000bendMEX\nFor subtasks 3 and 4 we have released a\nnovel dataset in Mexican Spanish collected\nfrom Twitter and manually labeled for o\u000ben-\nsiveness. The resource is formed by tweets\nlabeled with binary and multiclass catego-\nries with the following types of o\u000bensiveness\naccording to a recent categorization (D\u0013 \u0010az-\nTorres et al., 2020): o\u000bensive, aggressive and\nvulgar (but not o\u000bensive). Although the inhe-\nrent problem is a multi label classi\fcation\none (e.g., a tweet can be o\u000bensive but not\nvulgar), we are approaching the underlying\nbinary-classi\fcation task of interest: distin-\nguishing o\u000bensive from non-o\u000bensive tweets.\nNevertheless, we released all of the labels for\nthe training data as additional information\nthat participants can exploit when develo-\nping their solutions. Such information was\nnot provided in the test set partition.\nAdditionally, for subtask 4, we distributed\nmetadata information associated with tweets\nin the corpus, these include: date, retweet\ncount, favourite count, reply status, quote\nstatus; and metadata derived from users, in-\ncluding: veri\fcation status, followers count,\nlisted count, favourites count, status count,\nFlor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn-Valdivia,  \nArturo Montejo-R\u00e1ez, Manuel Montes-y-G\u00f3mez, Horacio Jarqu\u00edn-V\u00e1squez, Luis Villase\u00f1or-Pineda\n186date the account was created, among a few\nothers associated to the user pro\fle and ima-\nge. Detailed information on the considered\nmetadata can be found in the corresponding\nAPI documentation (Twitter, 2021).\nPartition Tweets O\u000b. No O\u000b.\nTrial 76 41 35\nTraining 5,060 1,381 3,679\nTest 2,183 600 1,583\nTotal 7,319 2,022 5,287\nTabla 2: Number of tweets in the O\u000bendMex\ncorpus.\nTable 2 summarizes the O\u000bendMex data\nset used for subtasks 3 and 4. The Trial par-\ntition was rather small, as the idea was to use\nsuch partition for testing the submission sys-\ntem. Training and test partitions are larger\nand present an approximate class imbalance\nratio of 2;6 favoring the non-o\u000bensive class.\n3.2.1 Baselines\nIn order to approach the Contextual and\nNon-contextual binary classi\fcation for Me-\nxican Spanish, we implemented two popular\napproaches that have shown to be hard to\nbeat in both subtasks: i) a Bidirectional Ga-\nted Recurrent Unit (Bi-GRU) neural network\nfor the Non-Contextual binary classi\fcation,\nandii) a XGBoost + BETO ensemble for the\nContextual binary classi\fcation.\nFor the Bi-GRU neural network baseline,\nall text was pre-processed by removing spe-\ncial characters and stopwords (with the ex-\nception of personal pronouns); in order to\nenrich the vocabulary, all hashtags were seg-\nmented by words (e.g. #EsDeLesbianas - es\nde lesbianas), and all emojis were conver-\nted into words (e.g. ,- `cara sonriente'). As\ninput features pre-trained Spanish FastText\n(Grave et al., 2018) embeddings were used,\nand a fully-connected softmax layer handle\nthe class probabilities. Alternatively, for the\nXGBoost + BETO ensemble baseline the da-\nta pre-processing steps consisted of conver-\nting the text to lowercase and stripping it\nof emojis. This ensemble involves two stages.\nIn the \frst stage the messages were classi\fed\nconsidering only their textual content using\nBETO (Ca~ nete et al., 2020). Subsequently,\nin the second stage BETO predictions were\nconcatenated to the three most discriminati-\nve metadata features ( Tweet favorite count,User listed count andDefault pro\fle ) to form\nnew vectors, handled at the end by a XG-\nBoost classi\fer (Chen and Guestrin, 2016).\nWe refer to these two baselines as baseline-dl\nbelow.\nIn addition to the previous baselines\nwe evaluated the performance of a rather\nstraightforward baseline based on a bag-\nof-unigrams-bigrams-trigrams and an linear\nSVM classi\fer, where a similar preprocessing\nas above was applied. The goal of this baseli-\nne was to evaluate the added value metadata\nwhen using a linear model, and to assess the\nmargin of bene\fts of approaches over a di-\nrect baseline method. We will refer to these\nbaselines as baseline-bow.\n4 Participant approaches and\nresults\n4.1 Subtask 1\nThis subtask, as introduced previously, pro-\nposes a pure multi-class text classi\fcation\nproblem or a multi-output one. Here, a brief\ndescription of participants' systems is provi-\nded.\nNLP-CIC team (Aroyehun and Gel-\nbukh, 2021) used the multilingual model\nXLM-RoBERTa pre-trained on Twitter texts\nand Sentiment Analysis data. They show that\nSentiment Analysis and the social domain\nadaption is bene\fcial for the problem of of-\nfensive language detection. The system ran-\nked the \frst position in the competition.\nUMUTeam (Garc\u0013 a-D\u0013 \u0010az, Jim\u0013 enez-\nZafra, and Valencia-Garc\u0013 \u0010a, 2021) explo-\nred a wide range of features and how to com-\nbine them in a \fnal multi-layer perceptron\n(MLP) with several tentative con\fgurations.\nThe features considered were lexical features,\nnegation features, word and sentence embed-\ndings from di\u000berent embedding algorithms\n(fastText , word2vec, gloVe and a Spanish\nversion of BERT). Word embeddings were\nevaluated isolated from the rest of features\nusing convolutional networks and two well-\nknown recurrent architectures like LSTM and\nBi-GRU, although MLP was the one showing\nthe best behavior. In general, these featu-\nres were further pre-processed, with MinMax\nscaler for linguistic ones and Robust scaler for\nnegation features. All these features as \flte-\nred using mutual information. Also, several\napproaches to combine the total number of\nfeatures generated were evaluated, including\nOverview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants\n187majority voting, weighted voting and logis-\ntic regression. Di\u000berent kinds of shape and\ndi\u000berent number of layers, number of neu-\nrons, dropout probabilities, batch sizes and\nactivation functions de\fned a varied num-\nber of experiments in order to identify the\nbest con\fguration for system hyperparame-\nters. From o\u000ecial results it can be drawn\nthat a combination of BERT-based encodings\n(pre-trained and \fne-tuned), with sentences\nembeddings and lexical and negation featu-\nres became the best solution. When linguistic\nfeatures were removed, the system obtained\nthe second position in the competition.\nThe GDUFS DM team applied se-\nquence classi\fcation system \fne-tuned on a\npre-trained BERT model and composing the\n\fnal encodings for the text from a max poo-\nling of the sentence encondings from all layers\nand token encondings from last layer. Two\nadditional techniques were integrated in the\n\fnal system: pseudo-labeling and focal loss.\nThe former technique consists of a two-stage\ntraining, were test labels are predicted and\nre-entered into the learning process to produ-\nce a larger training set. Focal loss was used as\na way to correct class imbalance. The system\nranked in the third position in the competi-\ntion.\nMarta Navarr\u0013 on and Isabel Segura\n(Garc\u0013 \u0010a and Bedmar, 2021) explored dif-\nferent deep learning models including Long-\nShort Term Memory (LSTM) and Bidirectio-\nnal Encoder Representations from Transfor-\nmers (BERT). The best results was archived\nby the BERT model. The system ranked in\nthe fourth position in the competition.\n4.2 Subtask 2\nUMUTeam was the only team in submit-\nting results to this subtask. They applied the\nsame system to add to the set of features\napplied one-hot encodings of contextual co-\nlumns (gender and media). Robust scaler was\nalso applied to these two features, as done\nwith negation ones. Compared to what was\nobtained in subtask 1, the integration of con-\ntextual information contributed to a small,\nbut consistent improvement in \fnal scores.\n4.3 Analysis of subtasks 1 and 2\nTable 3 and Table 4 provides a summary\nof the o\u000ecial results for subtasks 1 and 2\nin terms of micro-average and macro-average\nof Precision, Recall and F1scores, respecti-vely. Regarding the multiclass classi\fcation\nsetting, it can be notice that all the teams\noutperformed our baseline-svm which shows\nthe success of the neural network models em-\nployed by the participants compared to clas-\nsical machine learning algorithms. However,\nfor the multioutput regression setting, two of\nthe four teams outperformed the SVM regres-\nsor baseline, which shows the success of the\nclassical learning algorithm in this setting.\nFor the non-contextual multiclass classi\fca-\ntion, it can be seen that the scores of the\n\frst three teams are very close. This close-\nness in performance could be due to the fact\nthat most of these top ranked participants\nrelied on similar pretrained models in their\nsolutions (Spanish BERT model, except for\nNLP-CIC, who \fne-tuned a multilingual Ro-\nBERTa model). But greater di\u000berences can\nbe observed when looking at the MSE error.\nThe lower MSE value is, the closer is the sys-\ntem to the behaviour of human annotators.\nIn that case, XML-RoBERTa almost reduces\nto a half the error of the second system in\nthe ranking. Finally, both F1scores and MSE\nerrors are consistent in terms of ranking or-\nder.\nFor the second subtask, only one team eva-\nluate their system. We can observe that the\ncontextual information did not improve per-\nformance, in terms of F1score, to that obtai-\nned by their system in subtask1. But regar-\nding MSE, including those additional featu-\nres (social media platform and gender of the\ntargeted user) do led to a system closer to\nhuman annotator behaviour.\nSubtask 1: Non-contextual classi\fcation\nTeam P R F1 MSE\nNLP-CIC 0.8815 0.8815 0.8815 0.0231\nUMUTeam 0.8782 0.8782 0.8782 0.0411\nGDUFS DM 0.8732 0.8732 0.8732 0.0672\nMarta Isabel 0.8416 0.8417 0.8416 0.0697\nbaseline-svm 0.8285 0.8285 0.8285 0.0615\nSubtask 2: Contextual classi\fcation\nTeam P R F1 MSE\nUMUTeam 0.8782 0.8782 0.8782 0.0409\nTabla 3: Subtasks 1 and 2 o\u000ecial ranking. Re-\nsults are in terms of Micro-precision, Micro-\nRecall and Micro- F1scores.\nFlor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn-Valdivia,  \nArturo Montejo-R\u00e1ez, Manuel Montes-y-G\u00f3mez, Horacio Jarqu\u00edn-V\u00e1squez, Luis Villase\u00f1or-Pineda\n188Subtask 1: Non-contextual classi\fcation\nTeam P R F1 MSE\nNLP-CIC 0.7679 0.7093 0.7324 0.0231\nUMUTeam 0.7861 0.6919 0.7301 0.0411\nGDUFS DM 0.7565 0.7002 0.7239 0.0672\nMarta Isabel 0.5781 0.5451 0.5595 0.0697\nbaseline-svm 0.6278 0.4831 0.5236 0.0615\nSubtask 2: Contextual classi\fcation\nTeam P R F1 MSE\nUMUTeam 0.7879 0.6921 0.7308 0.0409\nTabla 4: Subtasks 1 and 2 o\u000ecial ranking. Re-\nsults are in terms of Macro-precision, Macro-\nRecall and Macro-F 1scores.\n4.4 Subtasks 3 and 4: O\u000bensive\nlanguage identi\fcation in\nMexican Spanish\nWe now analyze the results obtained by parti-\ncipants of subtasks 3 and 4. For the former, a\ntotal of 10 teams submitted runs for the \fnal\nphase and were considered for the o\u000ecial lea-\nderboard. In addition, two other teams sub-\nmitted runs but these did not qualify for the\no\u000ecial ranking. For subtask 4 we received the\nsubmissions from three di\u000berent teams. This\nwas somewhat disappointing as we were ex-\npecting participants to exploit the metadata\nprovided with the dataset.\nSubtask 3: Non-contextual classi\fcation\nTeam P R F1\nCIMAT-MTY-GTO 0.7600 0.6533 0.7026\nNLP-CIC 0.7550 0.6407 0.6932\nDCCD-INFOTEC 0.6733 0.6966 0.6847\nCIMAT-GTO 0.6633 0.6958 0.6792\nUMUTeam 0.6650 0.6763 0.6706\nTimen 0.6000 0.6081 0.6040\nCIC-IPN 0.5350 0.6874 0.6017\nBaseline-bow 0.6040 0.5517 0.5767\nBaseline-dl 0.7192 0.4100 0.5222\nxjywing 0.8883 0.3417 0.4935\naomar 0.8750 0.3239 0.4728\nCEN-Amrita 0.9183 0.3143 0.4683\nSubtask 4: Contextual classi\fcation\nTeam P R F1\nBaseline-dl 0.6629 0.6983 0.6801\nUMUTeam 0.6683 0.6705 0.6694\nCIC-IPN 0.5383 0.6843 0.6026\nBaseline-bow 0.6062 0.5517 0.5777\nTimen 0.4233 0.4456 0.4341\nTabla 5: Final results for the Contextual and\nNon-contextual binary classi\fcation for Me-\nxican Spanish\nTable 5 provide a summary of the o\u000ecial\nresults for subtasks 3 and 4. For the former,i.e., non-contextual binary classi\fcation, it\ncan be seen that there were only 3 teams that\ndid not beat the baselines associated with the\ntask. The best performance was obtained by\nthe CIMAT-MTY-GTO team with a relative\nimprovement over the bow and dlbaselines\nof 21 % and 34 %, respectively. Followed clo-\nsely by the next 4 teams in the ranking. This\ncloseness in performance could be due to the\nfact that most of these top ranked partici-\npants relied on similar pretrained models in\ntheir solutions, see Table 6.\nInterestingly, baseline-bow outperformed\nthe one based on the Bi-GRU. This could be\ndue to the fact that the latter model was trai-\nned only on the available data, which may be\nof limited size given the complexity of GRU\nmodels. Other participants outperformed the\nbaselines because they used external resour-\nces and pretrained models (see Table 6).\nRegarding task 4, from Table 5 it can be\nseen that no team outperformed baseline-dl.\nThis is due in part to the competitiveness of\nthe baseline model, but also, to the fact that\nparticipants did not do any special processing\nfor the provided metadata (see below). Still,\ntwo out of the three teams were close to the\nbaseline. On the other hand, only one team\ndid not outperformed the baseline-bow. The\nimprovement of baseline-dl over baseline-bow\ncould be due to the fact that the former used\na representation based on a transformer, op-\nposed to the Bi-GRU baseline considered for\nsubtask 3.\nFinally, when comparing the results of\nbaseline-bow in both tasks, it is observed that\nthe added value of metadata in subtask 4 only\nyield a negligible improvement. This con\frms\nthat the sole inclusion the features is not\nenough to improve performance.\n4.4.1 Systems descriptions\nTable 6 summarizes the contributions from\nteams that participated in subtasks 3 and 4,\nit shows the adopted models and highlights\nany novel aspect of the di\u000berent approaches.\nIn the following, we outline the main \fn-\ndings from the methodologies proposed to ap-\nproach subtasks 3 and 4.\n\u2022Transformer-based solutions were\ncommon. Most teams relied on pretrai-\nned transformers for Spanish in the mo-\ndeling process, we assume this was with\nthe purpose of alleviating the small sam-\nple problem. This is in line with trends in\nOverview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants\n189general NLP, and in general it was very\nhelpful: most of top ranked participants\nused transformers. Despite these results,\nwe think that more specialized mecha-\nnisms could help to boost performance\nwhen using transformers.\n\u2022Advanced linguistic features were\nnot considered in most approaches.\nOnly a couple of teams (UMUTeam and\nCIC-IPN teams) proposed solutions that\nincluded linguistic analyses, and another\nteam relied on a genetic programming\nformulation (DCCD-INFOTEC). It is\ninteresting that their performance was\ncompetitive, even when no transformer\nwas used. It may be expected that an\nadequate combination of linguistic and\ndata-driven features could result in bet-\nter performance.\n\u2022External data. External data for\nfurther \fne tuning transformer models\nwas adopted by top ranked participants.\nSuggesting this is a promising way for\nfurther improving the performance of\ntransformers.\n\u2022No special treatment for processing\nmetadata for subtask 4. It was so-\nmewhat disappointing that participants\nof subtask 4, did not took full advantage\nof metadata. These features were conca-\ntenated to the other input spaces and\nfeed to classi\fcation models. We are still\ncon\fdent that recognition performance\ncan be improved when these features are\nused e\u000bectively.\n4.4.2 Analysis\nIn order to further analyze the participants'\nresults, we performed an analysis on the com-\nplimentariness and diversity of the predic-\ntions from the di\u000berent teams. For this analy-\nsis we used the last run from every team in\nthe di\u000berent subtasks. We measured the di-\nversity of predictions by using the Coincident\nFailure Diversity (CFD) measure (Tang, Su-\nganthan, and Yao, 2006). This measure ex-\npresses with a number in [0;1]the extend to\nwhich the errors made by di\u000berent classi\fca-\ntion system overlap (the higher the value the\nless correlated the errors are). On the other\nhand, to measure the complimentariness of\npredictions, we calculated the Maximum Pos-\nsible Accuracy (MPA), this is the accuracy\nthat can be obtained if we consider a tweetcorrectly classi\fed when at least one of the\nconsidered systems classi\fed it correctly.\nTable 7 shows the CFD and MPA values\nobtained for the considered runs from sub-\ntasks 3 and 4, for completion we also report\nthe best accuracy (BA) obtained by any of\nthe considered systems. From these results, it\nis possible to observe that the MPA in both\nsubtasks is considerably greater than the BA\nin both subtasks, suggesting that the partici-\npants systems and approaches are somewhat\ncomplementary to each other: performance\ncould improve by \u224816 % and \u224811 % if the pre-\ndictions from the available systems were com-\nbined optimally for subtasks 3 and 4, respec-\ntively. The values of the CFD measure show\nthat there is a high diversity among errors in\nthe predictions of both subtasks. Most no-\ntably for subtask 3. Results are consistent\nwith the number of participating teams in\nboth subtasks.\nAs a result of applying the MPA metric, it\nwas possible to identify those common errors\nacross all systems3. In fact, there are only 34\ntweets that no system could classify correctly.\nNine of them are o\u000bensive tweets that were\nclassi\fed as non-o\u000bensive. Below we present\nsome of these tweets (a rough translation to\nEnglish is provided), where we can identify\no\u000benses with no vulgar or profane words (e.g.\n\\gata\"), the use of out of the training voca-\nbulary words (e.g. \\whore\"), the masking of\npotentially o\u000bensive words (e.g. \\pen....\"), as\nwell as some ironic comments.\n\u2022Nada peor que whore y gorda #sorrynotsorry.\n(ENG: Nothing worse than whore and fat\n#sorrynotsorry )\n\u2022Yo no te quise decir gata, pero bueno. Eres una\ngata (ENG: I did not mean to say gata. But well.\nYou are a gata. )\n\u2022Soy yo o @USUARIO est\u0013 a bien pen.... hace dra-\nmas se pone loca y pierde jajaja @ExatlonMx.\n(ENG: It is me or @USER is pen... she gets\ncrazy and loses hahah @ExatlonMx. )\n\u2022B\u0013 asicamente, el feminismo se trata de feas pe-\nleando por los derechos de las guapas. (ENG:\nBasically, feminism is about the ugly girls \fgh-\nting for the rights of the pretty ones. )\nA couple of mistakenly classi\fed non-\no\u000bensive tweets are the following:\n\u2022@USUARIO Vas en micro, camina, se suben\nunos HDP y les quitan todo a todos En un taxi,\nte pueden secuestrar... En el metro hay n car-\nteristas. (ENG: @USER you are going in bus,\nit moves, some HPD get in, they steal everyo-\nne. In a can you can be kipnaped.... in the subay\nthere are n pinpockets.)\n3NOTE: In this section we include examples of\nlanguage that may be o\u000bensive to some readers, these\ndo not represent the perspectives of the authors.\nFlor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn-Valdivia,  \nArturo Montejo-R\u00e1ez, Manuel Montes-y-G\u00f3mez, Horacio Jarqu\u00edn-V\u00e1squez, Luis Villase\u00f1or-Pineda\n190Systems considered in the o\u000ecial ranking.\nTeam Novel elements Transformer / Model Reference\nCIMAT-MTY-GTO External data was from hate-\nspeecha detection and sentiment\nanalysis was used to augment the\ntraining set.Ensemble of BERT\nmodels for Spanish\n(BETO)(G\u0013 omez-Espinosa,\nMu~ niz-Sanchez, and\nL\u0013 opez-Monroy, 2021)\nNLP-CIC The model was trained with both\ntweets and sentiment analysis data\nin Spanish.XLM-RoBERTa (Aroyehun and Gelbukh,\n2021)\nDCCD-INFOTEC A combination of di\u000berent models\ntrained for humor, aggressiveness\nand misogyny detection, in addition\nto models trained on the provided\ntraining set and a reverse version of\nit.EvoMSA (genetic\nprogramming based\nmodel)(Calder\u0013 on, Tellez, and\nGra\u000b, 2021)\nCIMAT-GTO The models were trained taking ad-\nvantage of the auxiliary sentence for\nthe transformer. Two methods for\nobtaining auxuliary sentences were\nproposed.Ensemble of BERT\nmodels for Spanish\n(BETO)(S\u0013 anchez-Vega and\nL\u0013 opez-Monroy, 2021)\nUMUTeam\u2217A variety of linguistic features, in-\ncluding negation were considered\nand combined with learned repre-\nsentations.Ensembles of models\nbased on linguistic\nand learned features\n(embeedings).(Garc\u0013 a-D\u0013 \u0010az, Jim\u0013 enez-\nZafra, and Valencia-\nGarc\u0013 \u0010a, 2021)\nCIC-IPN\u2217A diversity of con\fgurations we-\nre tested, a model pretrained on\ntweets and sentiment analysis data\nobtained the best performance.XLM-RoBERTa (Huerta-Velasco and\nCalvo, 2021)\nCEN-Amrita Better results were obtained withe\nthe Bi-LSTM modelBidirectional LSTM\nand BERT (bert-\nbase-multilingual-\ncased)(Sreelakshmi, Premjith,\nand Soman, 2021)\nAdditional models\nQuSwe1d0n The representation obtained from\nthe transformer was feed to a CNN\nbased model.XLM and CNN (Qu, Que, and Shuang-\njun, 2021)\nYNU qyc The output of the transformer was\nfeed to an LSTM model, a K-folding\nensemble scheme was adopted.XLM-RoBERTa and\nLSTM(Qu, Yang, and Wang,\n2021)\nTabla 6: Summary of system descriptions that participated in subtasks 3 and 4.\u2217Indicates this\nteam participated in both tasks. We separate systems that quali\fed for the o\u000ecial results and\nadditional systems.\nSubtask BA MPA CFD NoT\n3 0.8277 0.9844 0.6073 10\n4 0.8185 0.9271 0.2685 3\nTabla 7: Comparison of MPA and CFD\nresults between the Contextual and Non-\ncontextual binary classi\fcation. The Best Ac-\ncuracy (BA) obtained by the participating\nteams in each subtask, was used to compare\nthe complementarity obtained with the MPA\nmetric; NoT stands for Number of Teams.\n\u2022Sus pinches relaciones empiezan con un Invita\na tus amigas las m\u0013 as putas y piden \fdelidad,\nmalditos ilusos. (ENG: Your damn relationships\nstart with an invite to your friends, the most\npromiscuous and you ask \fdelity, fkng dreamer.)This analysis suggests the most di\u000ecult\ninstances are those that require further lin-\nguistic analysis. This evidences the inherent\nchallenges of this variation of language and\nthe detection of o\u000bensiveness in text. Moti-\nvating further research on this subject.\nAnother important aspect to mention is\nthat the corpus used this year took a sub-\nset of last year's data and was relabeled with\nthe guide proposed by (D\u0013 \u0010az-Torres et al.,\n2020). For this task, a group of labelers of\ndi\u000berent ages was selected (3 adults, 6 youth)\nand balancing the number of males and fema-\nles (4 females, 5 males). This new relabeling\nwas the main decrease in the reported results\n(baseline decreased by 0.19 points compared\nOverview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants\n191to last year). Diversity in the group of labe-\nlers (generational change as well as gender)\nincreased the variations present in both the\ntraining set and the test set. The creation\nof robust systems for this task must consider\nthese scenarios both during the training pha-\nse and to provide a con\fdence rating of the\nprediction made by the automatic method.\n5 Conclusion\nThe MeO\u000bendEs shared task at IberLEF at-\ntempts to continue to the research in o\u000bensive\nlanguage detection in Spanish. A new data-\nset on generic Spanish has been prepared for\nthis edition, as a companion collection to the\nexisting one on Mexican Spanish, enabling in-\ntensive experimentation over a large number\nof messages from di\u000berent social media plat-\nforms. This evaluation campaign allowed par-\nticipants to test their systems on this classi-\n\fcation task. Di\u000berent algorithms, features,\ntechniques and con\fgurations were explored,\nreporting the e\u000bectiveness of the approaches\nand contributing to the advance of o\u000bensive\nlanguage detection systems.\nA total of 69 participants registered to\nthe MeO\u000bendEs shared task. However, only\n12 teams participated in the \fnal phase of\nthe challenge. Interesting \fndings and con-\nclusions have been drawn and very competi-\ntive approaches are now available to approach\nthe 4 proposed subtasks. Given the interest\nfrom the community we are keeping the cha-\nllenge website open so that anyone interested\nin trying their own methods can do it at any\ntime.\nAcknowledgements\nWe would like to thank CONACyT for par-\ntially supporting this work under grants CB-\n2015-01-257383 and the Thematic Networks\nprogram (Language Technologies Thematic\nNetwork). Hugo Jair Escalante is suppor-\nted by CONACyT under project grant CO-\nNACYT CB-S-26314.\nThis work is also partially supported by\nthe grant P20 00956 (PAIDI 2020) from An-\ndalusian Regional Government, a grant from\nEuropean Regional Development Fund (FE-\nDER), the LIVING-LANG project [RTI2018-\n094653-B-C21], and the Ministry of Scien-\nce, Innovation and Universities (scholarship\n[FPI-PRE2019-089310]) from the Spanish\nGovernment.References\n\u0013Alvarez-Carmona, M. \u0013A., E. Guzm\u0013 an-Falc\u0013 on,\nM. Montes-y-G\u0013 omez, H. J. Escalante,\nL. Villase~ nor-Pineda, V. Reyes-Meza, and\nA. Rico-Sulayes. 2018. Overview of MEX-\nA3T at ibereval 2018: Authorship and\naggressiveness analysis in mexican spa-\nnish tweets. In P. Rosso, J. Gonzalo,\nR. Mart\u0013 \u0010nez, S. Montalvo, and J. C. de Al-\nbornoz, editors, Proceedings of the Third\nWorkshop on Evaluation of Human Lan-\nguage Technologies for Iberian Languages\n(IberEval 2018) co-located with 34th Con-\nference of the Spanish Society for Natural\nLanguage Processing (SEPLN 2018), Se-\nvilla, Spain, September 18th, 2018 , volu-\nme 2150 of CEUR Workshop Proceedings,\npages 74{96. CEUR-WS.org.\nArag\u0013 on, M. E., M. \u0013A.\u0013Alvarez-Carmona,\nM. Montes-y-G\u0013 omez, H. J. Escalante,\nL. Villase~ nor-Pineda, and D. Moctezuma.\n2019. Overview of MEX-A3T at iber-\nlef 2019: Authorship and aggressiveness\nanalysis in mexican spanish tweets. In\nM.\u0013A. G. Cumbreras, J. Gonzalo, E. M.\nC\u0013 amara, R. Mart\u0013 \u0010nez-Unanue, P. Ros-\nso, J. Carrillo-de-Albornoz, S. Montal-\nvo, L. Chiruzzo, S. Collovini, Y. Gu-\nti\u0013 errez, S. M. J. Zafra, M. Krallinger,\nM. Montes-y-G\u0013 omez, R. Ortega-Bueno,\nand A. Ros\u0013 a, editors, Proceedings of the\nIberian Languages Evaluation Forum co-\nlocated with 35th Conference of the Spa-\nnish Society for Natural Language Pro-\ncessing, IberLEF-SEPLN 2019, Bilbao,\nSpain, September 24th, 2019, volume 2421\nofCEUR Workshop Proceedings, pages\n478{494. CEUR-WS.org.\nArag\u0013 on, M. E., H. J. Jarqu\u0013 \u0010n-V\u0013 asquez,\nM. Montes-y-G\u0013 omez, H. J. Escalante,\nL. Villase~ nor-Pineda, H. G\u0013 omez-Adorno,\nJ. P. Posadas-Dur\u0013 an, and G. Bel-Enguix.\n2020a. Overview of MEX-A3T at iber-\nlef 2020: Fake news and aggressiveness\nanalysis in mexican spanish. In M. \u0013A. G.\nCumbreras, J. Gonzalo, E. M. C\u0013 amara,\nR. Mart\u0013 \u0010nez-Unanue, P. Rosso, S. M. J.\nZafra, J. A. O. Zambrano, A. Miranda,\nJ. P. Zamorano, Y. Guti\u0013 errez, A. Ros\u0013 a,\nM. Montes-y-G\u0013 omez, and M. G. Vega, edi-\ntors, Proceedings of the Iberian Langua-\nges Evaluation Forum (IberLEF 2020) co-\nlocated with 36th Conference of the Spa-\nnish Society for Natural Language Pro-\nFlor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn-Valdivia,  \nArturo Montejo-R\u00e1ez, Manuel Montes-y-G\u00f3mez, Horacio Jarqu\u00edn-V\u00e1squez, Luis Villase\u00f1or-Pineda\n192cessing (SEPLN 2020), M\u0013 alaga, Spain,\nSeptember 23th, 2020, volume 2664 of\nCEUR Workshop Proceedings , pages 222{\n235. CEUR-WS.org.\nArag\u0013 on, M., H. Jarqu\u0013 \u0010n, M. M.-y. G\u0013 omez,\nH. Escalante, L. Villase~ nor-Pineda,\nH. G\u0013 omez-Adorno, G. Bel-Enguix, and\nJ. Posadas-Dur\u0013 an. 2020b. Overview\nof mex-a3t at iberlef 2020: Fake news\nand aggressiveness analysis in mexican\nspanish. In Notebook Papers of 2nd\nSEPLN Workshop on Iberian Languages\nEvaluation Forum (IberLEF), Malaga,\nSpain.\nAroyehun, S. T. and A. Gelbukh. 2021. Eva-\nluation of intermediate pre-training for\nthe detection of o\u000bensive language. In\nProceedings of the Iberian Languages Eva-\nluation Forum (IberLEF 2021), CEUR\nWorkshop Proceedings. CEUR-WS.org.\nCalder\u0013 on, J. J., E. S. Tellez, and M. Gra\u000b.\n2021. Dccd-infotec at meo\u000ben-\ndes@iberlef21 subtask 3: A transfer\nlearning approach based on evomsa's\nstacked generalization. In Proceedings\nof the Iberian Languages Evaluation\nForum (IberLEF 2021), CEUR Workshop\nProceedings. CEUR-WS.org.\nCanales, L. and P. Mart\u0013 \u0010nez-Barco. 2014.\nEmotion detection from text: A survey.\nInProceedings of the workshop on natu-\nral language processing in the 5th infor-\nmation systems research working days (JI-\nSIC), pages 37{43.\nCa~ nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u0013 erez. 2020. Spanish\npre-trained bert model and evaluation da-\nta. In PML4DC at ICLR 2020 .\nChen, T. and C. Guestrin. 2016. XGBoost:\nA scalable tree boosting system. In Pro-\nceedings of the 22nd ACM SIGKDD In-\nternational Conference on Knowledge Dis-\ncovery and Data Mining, KDD '16, page\n785{794, New York, NY, USA. Associa-\ntion for Computing Machinery.\nD\u0013 \u0010az-Torres, M. J., P. A. Mor\u0013 an-M\u0013 endez,\nL. Villasenor-Pineda, M. Montes-y\nG\u0013 omez, J. Aguilera, and L. Meneses-\nLer\u0013 \u0010n. 2020. Automatic detection of\no\u000bensive language in social media: De\f-\nning linguistic criteria to build a Mexican\nSpanish dataset. In Proceedings of theSecond Workshop on Trolling, Aggression\nand Cyberbullying, pages 132{136, Mar-\nseille, France, May. European Language\nResources Association (ELRA).\nGarc\u0013 a-D\u0013 \u0010az, J. A., S. M. Jim\u0013 enez-Zafra, and\nR. Valencia-Garc\u0013 \u0010a. 2021. Umuteam at\nmeo\u000bendes 2021: Ensemble learning for of-\nfensive language identi\fcation using lin-\nguistic features, \fne-grained negation and\ntransformers. In Proceedings of the Ibe-\nrian Languages Evaluation Forum (Iber-\nLEF 2021) , CEUR Workshop Procee-\ndings. CEUR-WS.org.\nGarc\u0013 \u0010a, M. N. and I. S. Bedmar. 2021. Detec-\nting o\u000bensiveness in social network com-\nments. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings. CEUR-\nWS.org.\nG\u0013 omez-Espinosa, V., V. Mu~ niz-Sanchez, and\nA. P. L\u0013 opez-Monroy. 2021. Transformers\npipeline for o\u000bensiveness detection in me-\nxican spanish social media. In Procee-\ndings of the Iberian Languages Evaluation\nForum (IberLEF 2021), CEUR Workshop\nProceedings. CEUR-WS.org.\nGrave, E., P. Bojanowski, P. Gupta, A. Jou-\nlin, and T. Mikolov. 2018. Learning word\nvectors for 157 languages. In Proceedings\nof the International Conference on Lan-\nguage Resources and Evaluation (LREC\n2018), page .\nHuerta-Velasco, D. A. and H. Calvo. 2021.\nUsing lexical resources for detecting of-\nfensiveness in mexican spanish tweets. In\nProceedings of the Iberian Languages Eva-\nluation Forum (IberLEF 2021), CEUR\nWorkshop Proceedings. CEUR-WS.org.\nMacAvaney, S., H.-R. Yao, E. Yang, K. Rus-\nsell, N. Goharian, and O. Frieder. 2019.\nHate speech detection: Challenges and so-\nlutions. PloS one, 14(8):e0221152.\nMedhat, W., A. Hassan, and H. Korashy.\n2014. Sentiment analysis algorithms and\napplications: A survey. Ain Shams engi-\nneering journal , 5(4):1093{1113.\nMontes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on,\nR. Agerri, M. \u0013Alvarez Carmona,\nE. \u0013Alvarez Mellado, J. Carrillo-de\nAlbornoz, L. Chiruzzo, L. Freitas,\nH. G\u0013 omez Adorno, Y. Guti\u0013 errez, S. Lima,\nOverview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants\n193S. M. Jim\u0013 enez-Zafra, F. M. Plaza-del-\nArco, and M. Taul\u0013 e, editors. 2021.\nProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021).\nMubarak, H., K. Darwish, W. Magdy, T. El-\nsayed, and H. Al-Khalifa. 2020. Over-\nview of osact4 arabic o\u000bensive language\ndetection shared task. In Proceedings of\nthe 4th Workshop on Open-Source Arabic\nCorpora and Processing Tools, with a Sha-\nred Task on O\u000bensive Language Detection,\npages 48{52.\nPlaza-del Arco, F. M., M. D. Molina-\nGonz\u0013 alez, L. A. Ure~ na-L\u0013 opez, and M. T.\nMart\u0013 \u0010n-Valdivia. 2021. Comparing pre-\ntrained language models for spanish ha-\nte speech detection. Expert Systems with\nApplications, 166:114120.\nQu, S., Q. Que, and Shuangjun. 2021. Non-\ncontextual binary classi\fcation for mexi-\ncan spanish with xlm and cnn. In Procee-\ndings of the Iberian Languages Evaluation\nForum (IberLEF 2021), CEUR Workshop\nProceedings. CEUR-WS.org.\nQu, Y., Y. Yang, and G. Wang. 2021.\nYnu qyc at meo\u000bendes@iberlef 2021:the\nxlm-roberra and lstm for identifying of-\nfensive tweets. In Proceedings of the Ibe-\nrian Languages Evaluation Forum (Iber-\nLEF 2021) , CEUR Workshop Procee-\ndings. CEUR-WS.org.\nRazavi, A. H., D. Inkpen, S. Uritsky, and\nS. Matwin. 2010. O\u000bensive language\ndetection using multi-level classi\fcation.\nIn A. Farzindar and V. Ke\u0014 selj, editors,\nAdvances in Arti\fcial Intelligence, pages\n16{27, Berlin, Heidelberg. Springer Berlin\nHeidelberg.\nS\u0013 anchez-Vega, F. and A. P. L\u0013 opez-Monroy.\n2021. Cimat-gto at meo\u000bendes 2021:\nBert's auxiliary sentence focused on\nword's information for o\u000bensiveness detec-\ntion. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings. CEUR-\nWS.org.\nSreelakshmi, K., B. Premjith, and K. P. So-\nman. 2021. Transformer based o\u000bensi-\nve language identi\fcation in spanish. In\nProceedings of the Iberian Languages Eva-\nluation Forum (IberLEF 2021), CEUR\nWorkshop Proceedings. CEUR-WS.org.Tang, E. K., P. N. Suganthan, and X. Yao.\n2006. An analysis of diversity measures.\nMach. Learn., 65(1):247{271.\nTwitter. 2021. Tweets { twitter developers.\nhttps://developer.twitter.com/. Ac-\ncessed: 2021-06-30.\nWiebe, J., T. Wilson, R. Bruce, M. Bell,\nand M. Martin. 2004. Learning subjec-\ntive language. Computational linguistics ,\n30(3):277{308.\nZampieri, M., P. Nakov, S. Rosenthal,\nP. Atanasova, G. Karadzhov, H. Mubarak,\nL. Derczynski, Z. Pitenis, and C \u0018 . C \u0018 \u007f oltekin.\n2020. Semeval-2020 task 12: Multilingual\no\u000bensive language identi\fcation in social\nmedia (o\u000benseval 2020). arXiv preprint\narXiv:2006.07235.\nFlor Miriam Plaza-del-Arco, Marco Casavantes, Hugo Jair Escalante, M. Teresa Mart\u00edn-Valdivia,  \nArturo Montejo-R\u00e1ez, Manuel Montes-y-G\u00f3mez, Horacio Jarqu\u00edn-V\u00e1squez, Luis Villase\u00f1or-Pineda\n194Overview of EXIST 2021:sEXism Identi\fcation in Social neTworks\nOverview de EXIST 2021:\nIdenti\fcaci\u0013 on de Sexismo en Redes Sociales\nFrancisco Rodr\u0013 \u0010guez-S\u0013 anchez1, Jorge Carrillo-de-Albornoz1, Laura Plaza1,\nJulio Gonzalo1, Paolo Rosso2, Miriam Comet3, Trinidad Donoso3\n1UNED NLP & IR Group, Universidad Nacional de Educaci\u0013 on a Distancia\n2PRHLT Research Center, Universitat Polit\u0012 ecnica de Val\u0012 encia\n3Universitat de Barcelona\nfrodriguez.sanchez@invi.uned.es, fjcalbornoz, lplaza, juliog@lsi.uned.es,\nprosso@dsic.upv.es, fmiriamcomet, trinydonosog@ub.edu\nAbstract: The paper describes the organization, goals, and results of the sEXism\nIdenti\fcation in Social neTworks (EXIST) challenge, a shared task proposed for the\n\frst time at IberLEF 2021. EXIST 2021 proposes two challenges: sexism identi\f-\ncation and sexism categorization of tweets and gabs, both in Spanish and English.\nWe have received a total of 70 runs for the sexism identi\fcation task and 61 for the\nsexism categorization challenge, submitted by 31 di\u000berent teams from 11 countries.\nWe present the dataset, the evaluation methodology, an overview of the proposed\nsystems, and the results obtained. The \fnal dataset consists of more than 11,000\nannotated texts from two social networks (Twitter and Gab) and its development\nhas been supervised and monitored by experts in gender issues.\nKeywords: Sexism Detection, Twitter, Gab, Spanish, English.\nResumen: El presente art\u0013 \u0010culo describe la organizaci\u0013 on, objetivos y resultados\nde la competici\u0013 on sEXism Identi\fcation in Social neTworks (EXIST), una tarea\npropuesta por primera vez en IberLEF 2021. EXIST 2021 propone dos tareas: la\nidenti\fcaci\u0013 on y la categorizaci\u0013 on de sexismo en ingl\u0013 es y espa~ nol. Se han recibido un\ntotal de 70 runs para la tarea de identi\fcaci\u0013 on de sexismo y 61 para la categorizaci\u0013 on\nde sexismo, enviadas por 31 equipos de 11 pa\u0013 \u0010ses. En este trabajo, se presentan el\ndataset, la metodolog\u0013 \u0010a de evaluaci\u0013 on, un an\u0013 alisis de los sistemas propuestos por los\nparticipantes y los resultados obtenidos. El dataset \fnal est\u0013 a compuesto por m\u0013 as\nde 11,000 textos anotados procedentes de dos redes sociales (Twitter y Gab) y su\nelaboraci\u0013 on ha sido supervisada por expertas en temas de g\u0013 enero.\nPalabras clave: Detecci\u0013 on de Sexismo, Twitter, Gab, Espa~ nol, Ingl\u0013 es.\n1 Introduction\nThe phenomenal development of web tech-\nnologies has facilitated the interaction among\npeople from many di\u000berent backgrounds.\nWith more than 4 billion people around the\nworld now using social media each month1,\nsocial networks are undoubtedly one of the\nmost important ways of communicating. Al-\nthough the advantages and positive e\u000bects of\nthis global communication are obvious, the\ninvisibility, anonymity and accessibility have\nmade the expression of xenophobic, racist\n1https://datareportal.com/reports/digital-2020-\noctober-global-statshotand sexist discourses easy and unpunished.\nThe anonymity online makes users report\ngreater hostile sexism (Fox, Cruz, and Lee,\n2015) and emboldens them to engage in be-\nhaviours they are unlikely to perform face-\nto-face. Furthermore, the rapid spread of on-\nline information in social networks has made\nthese behaviours extremely dangerous. In\nthis context, inequality and discrimination\nagainst women that remain embedded in so-\nciety are increasingly being replicated and\nspread on online platforms.\nHowever, the detection of sexist content is\nstill a di\u000ecult task for social media platforms.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 195-207\nrecibido 02-07-2021 revisado 09-07-2021 aceptado 13-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-17\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalFor instance, Amnesty International pub-\nlished a report2where they describe Twit-\nter as a \\toxic place\" for women. Accord-\ning to this report, Twitter is promoting vio-\nlence and hate against people based on their\ngender. The report also suggests that Twit-\nter is failing to protect women against ha-\nrassment and it could harm their freedom\nof speech. Recently, members of the U.S.\nCongress asked Facebook to do more to pro-\ntect women in their platform3. According\nto some lawmakers, social networks have be-\ncome \\the number one place\" in which psy-\nchological violence is perpetrated against fe-\nmale parliamentarians. The seriousness of\nthe problem, combined with the rapid dis-\nsemination of information online, the pos-\nsibility of anonymity and lastingness, espe-\ncially on social networks, has made these ha-\nrassment behaviours extremely dangerous so\nthat solutions are required to perform a faster\nand even better user generated-content mod-\neration or to serve as a tool that helps hu-\nman moderators to reduce the volume of sex-\nist content still present in online platforms.\nThe Oxford English Dictionary de\fnes\nsexism as \\prejudice, stereotyping or discrim-\nination, typically against women, on the ba-\nsis of sex\". As stated in (Rodr\u0013 \u0010guez-S\u0013 anchez,\nCarrillo-de Albornoz, and Plaza, 2020), sex-\nism is frequently found in many forms in\nsocial networks, includes a wide range of\nbehaviours (such as stereotyping, ideologi-\ncal issues, sexual violence, etc.) (Donoso-\nV\u0013 azquez and Rebollo-Catal\u0013 an, 2018; Manne,\n2017) and may be expressed in di\u000berent forms\n(direct, indirect, descriptive, reported, etc.)\n(Mills, 2008; Chiril et al., 2020). Sexism may\nsound \\friendly\": the statement \\Women\nmust be loved and respected, always treat\nthem like a fragile glass\" may seem posi-\ntive, but is actually considering that women\nare weaker than men. Sexism may sound\n\\funny\", as it is the case of sexist jokes or\nhumour (\\You have to love women... just\nthat... You will never understand them.\").\nSexism may sound \\o\u000bensive\" and \\hateful\",\nas in \\Humiliate, expose and degrade your-\nself as the fucking bitch you are if you want\na real man to give you attention\".\nHowever, subtle forms of sexism can be as\npernicious as other forms of sexism and af-\n2https://bit.ly/2TMPAJD\n3https://www.reuters.com/article/us-facebook-\nwomen-politics-idUSKCN2522KKfect women in many facets of their lives. Ac-\ncording to (Swim et al., 2001), non-hateful\nsexism can a\u000bect women's psychological well-\nbeing by decreasing their comfort, increasing\ntheir feelings of anger and depression, and\ndecreasing their stated self-esteem. Similarly,\n(Berg, 2006) found a relationship between the\nexperience of non-violent sexism and post-\ntraumatic stress disorder.\nCurrent research on sexism in online plat-\nforms is focused on detecting misogyny or ha-\ntred towards women (Waseem, 2016; Waseem\nand Hovy, 2016; Frenda et al., 2019). Conse-\nquently, previous works have dealt with hos-\ntile and explicit sexism, overlooking subtle\nor implicit expressions of sexism. An excep-\ntion is the approach proposed by (Rodr\u0013 \u0010guez-\nS\u0013 anchez, Carrillo-de Albornoz, and Plaza,\n2020), where authors released the \frst Span-\nish corpus of sexist expressions in Twitter,\nthe MeTwo dataset. They also compared\nMachine Learning (ML) methods to detect\nsexism and discussed the generalization of\ntheir approach with respect to misogyny de-\ntection systems. In line with previous hate\nspeech research, the AMI shared task focused\non the automatic identi\fcation of misogyny\n(hate or prejudice against women) in Twitter\n(Fersini, Rosso, and Anzovino, 2018). Teams\nwere proposed to identify misogynist tweets\nboth in Spanish and English.\nGiven this important social problem, the\nsEXism Identi\fcation in Social neTworks\n(EXIST) shared task has been proposed at\nIberLEF 2021 (Montes et al., 2021). The\nEXIST challenge is the \frst shared task on\nsexism detection in social networks whose\naim is to identify and classify sexism in a\nbroad sense, from explicit misogyny to other\nsubtle expressions that involve implicit sex-\nist behaviours. To this aim, we proposed\na new categorization of sexism and built a\ndataset using posts from Twitter and the un-\ncensored social network Gab.com (Gab) in\nEnglish and Spanish. To collect these posts,\nwe de\fned as seed terms a set of a number\nof popular terms, both in English and Span-\nish, commonly used to underestimate the role\nof women in our society. All these terms, as\nwell as the sexism categorization proposed in\nthis work, have been supervised by two ex-\nperts in gender issues. The EXIST dataset\nincorporates any type of sexist expression or\nrelated phenomena, including descriptive or\nreported assertions where the sexist message\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso\n196is a report or a description of a sexist be-\nhaviour. To the best of our knowledge, the\nEXIST dataset is the \frst multilingual cor-\npus designed to identify sexism in a broad\nsense, from hostile to subtle and benevolent\nsexism.\n2 Tasks\n2.1 Task Description\nThe EXIST 2021 shared task is de\fned as\na multilingual classi\fcation task. In particu-\nlar, the EXIST challenge is organized accord-\ning to two main subtasks: (i) sexism iden-\nti\fcation (task 1), which aims to identify if\na message or post contains sexist content;\nand (ii) sexism categorization (task 2), which\naims to classify the type of sexism contained\nin a given sexist message or post. Partici-\npants were welcome to present systems that\nattempt both subtasks or one of them.\nTask 1 is de\fned as a binary classi\fcation\nproblem, where every system should deter-\nmine whether a text or message is sexist or\nnot. It includes any type of sexist expression\nor related phenomena, like descriptive or re-\nported assertions where the sexist message is\na report or a description of a sexist event. In\nparticular, we consider two labels:\n\u2022Sexist: the tweet or gab expresses sexist\nbehaviours or discourses.\n\u2022Non-Sexist: the tweet or gab does\nnot express any sexist behaviour or dis-\ncourse.\nOnce a message has been classi\fed as sex-\nist, task 2 aims to categorize the message\naccording to the type of sexism it encloses.\nThe categorization has been revised by two\nexperts in gender issues, Trinidad Donoso\nand Miriam Comet from the University of\nBarcelona, and takes into account the di\u000ber-\nent aspects of women that are undermined.\nThis task is de\fned as a multi-class classi\f-\ncation problem where each sexist tweet or gab\nmust be categorized in one of the 5 following\nclasses:\n\u2022Ideological and inequality: The text\ndiscredits the feminist movement, rejects\ninequality between men and women, or\npresents men as victims of gender-based\noppression.\n\u2022Stereotyping and dominance: The\ntext expresses false ideas about womenthat suggest they are more suitable to\nful\fll certain roles (mother, wife, family\ncaregiver, faithful, tender, loving, sub-\nmissive, etc.), or inappropriate for cer-\ntain tasks (driving, hardwork, etc), or\nclaims that men are somehow superior\nto women.\n\u2022Objecti\fcation: The text presents\nwomen as objects apart from their dig-\nnity and personal aspects, or assumes or\ndescribes certain physical qualities that\nwomen must have in order to ful\fll tra-\nditional gender roles (compliance with\nbeauty standards, hypersexualization of\nfemale attributes, women's bodies at the\ndisposal of men, etc.).\n\u2022Sexual violence: Sexual suggestions,\nrequests for sexual favors or harassment\nof a sexual nature (rape or sexual as-\nsault) are made.\n\u2022Misogyny and non-sexual violence:\nThe text expresses hatred and violence\ntowards women.\nExamples of each category for both tasks\nare reported in Table 1.\n2.2 Evaluation Measures and\nBaselines\nIn order to evaluate the performance of the\ndi\u000berent approaches proposed by the partici-\npants, we will use the Evaluation Framework\nEvALL4(Amig\u0013 o et al., 2017; Amig\u0013 o, Spina,\nand Carrillo-de Albornoz, 2018; Amig\u0013 o et al.,\n2020). Within this framework, we will evalu-\nate the system outputs as classi\fcation tasks\n(binary and multi-class respectively) using\nstandard evaluation metrics, including Accu-\nracy, Precision, Recall, and macro-averaged\nF1-score.\nIn task 1, Sexism Identi\fcation, the re-\nsults of participants will be ranked using Ac-\ncuracy, as the distribution between sexist and\nnon-sexist categories is balanced. Besides,\nother measures will be computed, such as\nPrecision, Recall, and F1. All metrics will\nbe also computed by language. In particular,\nAccuracy has been computed as follows:\nAccuracy =number of correctly predicted instances\nnumber of instances\n4www.evall.uned.es\nOverview of EXIST 2021: sEXism Identification in Social neTworks\n197Text Task 1 Task 2\nWhere are all the white w omen\nat? non-sexist non-sexist\nFeminism is a war on\nmen, but it's also a war on women. It's a war on female nature, a war on femininity. sexist ideological-inequality\nWoman driving, be careful! sexist stereotyping-dominance\nNo o\u000bense but I've nev er\nseen an attractive african american hooker. Not a single one sexist objecti\fcation\nI wanna touch y\nour tits..you can't imagine what I can do on your body. sexist sexual-violence\nI hate misogyny more than I\nhate women sexist misogyny-non-sexual-violence\nTable 1: Examples of messages for each task.\nIn task 2, Sexism Categorization, we will\nuse macro-averaged F1-score to rank the sys-\ntem outputs. Similarly, we will compute\nother measures such as Precision and Recall.\nThe F1-score was computed as follows:\nF1=F1(sexism categorization)\n6\nwhere F1(sexism categorization) is calcu-\nlated as the sum of all classes (including non-\nsexist):\nF1(sexism categorization) =\nF1(non-sexist) + F1(ideological-inequality) +\nF1(misogyny-non-sexual-violence) +\nF1(objecti\fcation) + F1(sexual-violence) +\nF1(stereotyping-dominance)\nWe propose two di\u000berent baselines so\nthat we can establish an expected perfor-\nmance of the submitted runs. First, we\nprovided a benchmark (Baseline svm t\fdf)\nbased on Support Vector Machine (linear ker-\nnel) trained on tf-idf features built from the\ntexts unigrams. Second, a model that labels\neach record based on the majority class (Ma-\njority Class).\n3 Dataset\nThe EXIST 2021 shared task uses data from\nTwitter and Gab in English and Spanish.\nTwitter data was used for both training and\ntesting while Gab was only included in the\nEXIST test set so that it can be analysed\nthe di\u000berences between social networks with\nand without \\content control\". In order to\nprovide training and testing data for both\ntasks, we have collected a number of popular\nexpressions and terms, both in English and\nSpanish, commonly used to underestimate\nthe role of women in our society. The terms\nhave been extracted from di\u000berent sources:\n(i) previous works in the area; (ii) Twitter ac-\ncounts (journalist, teenagers, etc.) and hash-\ntags used to collects phrases and expressions\nthat women (Twitter users) have received on\na day-to-day basis or experiences; (iii) expres-\nsions extracted from the Everyday SexismProject5. We have also included other com-\nmon hashtags and expressions that are not so\nfrequently used in sexist contexts in order to\nensure a correct distribution between sexist\nand non-sexist expressions. These terms were\nanalysed and \fltered by Trinidad Donoso and\nMiriam Comet, which examined examples of\ntweets extracted using these terms as seeds.\nThe \fnal set contains 116 seed terms for\nSpanish and 109 for English.\nWe used the Twitter API to search for\ntweets written in English or Spanish contain-\ning some of the selected keywords selected\nkeywords. The setup of our crawler implies\ncollecting 100 tweets for each term daily.\nCrawling was performed during the period\nfrom the 1st December 2020 until the 28th\nFebruary 2021, gathering 545,717 tweets for\nSpanish and 662,895 for English. To ensure\nan appropriate balance between seeds, we\nhave removed those with less than 60 tweets.\nThe \fnal set of seeds used contains 91 seeds\nfor Spanish and 93 seeds for English. To ex-\ntract posts from Gab (gabs), we downloaded\nthe most recent Gab dump from pushshift6\n(Baumgartner et al., 2020) and searched for\ngabs containing the selected keywords. We\ngathered 1853 gabs for Spanish between the\n12th September 2016 and the 12th August\n2019, and 1,356,266 between 12th August\n2016 and 12th August 2019 for English. In\nthis case, we did not remove any information\nsince we did not have many gabs for Span-\nish. We only could \fnd 38 seeds for Spanish\nand 81 for English, introducing a consider-\nable seed bias for this subset of the dataset.\nThe sampling process was di\u000berent for\neach data source. Regarding Twitter, ap-\nproximately 50 tweets (50 tweets for Spanish\nand 48 for English) were randomly selected\nfor each seed term within the period from\n1st to 31st of December 2020 for the training\nset, and 22 tweets per seed within the period\nfrom 1st to 28th February of 2021 for the test\nset. We randomly resampled these tweets for\n5https://everydaysexism.com/\n6https://\fles.pushshift.io/gab/\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso\n198each language to build the \fnal sampled set\ncomposed of 4500 tweets per language for the\ntraining set and 2000 tweets per language for\nthe test set. The Gab sampling process was\nmore complex since we did not have an uni-\nform distribution of gabs by seed. We in-\ncluded all available seeds and removed gabs\ncontaining those seeds that were more numer-\nous. Previously, we removed gabs from users\nwith more information to mitigate user bias.\nThe \fnal sampled set was composed of 500\ngabs for each language.\nThe whole sampling process was de\fned\ntaking into account di\u000berent sources of bias.\nIn particular, we considered three main\nsources of bias: seed, temporal and user bias.\nWe tried to mitigate seed bias by includ-\ning a wide range of terms which are used in\nboth sexist and non-sexist context (116 terms\nfor Spanish and 109 for English). To con-\ntrol temporal bias, we set a temporal gap\nof one month (January) between the train-\ning and test data and checked the temporal\ngap between tweets for each seed (around 0.5\ndays for training and 1 day for testing) to\nensure that data is spread over all the pe-\nriod. Finally, we checked messages generated\nby users to ensure an appropriate balance. In\nparticular, around 1 message was generated\nper user except for gabs in Spanish where\neach user posted 2 gabs. We also took into\naccount this principle to split the dataset into\ntraining and test sets and removed from the\ntest set users who were also present in the\ntraining set to avoid user bias.\nThe sampled data sets were labelled\nthrough a majority voting approach by exter-\nnal contributors on the Amazon Mechanical\nTurk7(MTurk) platform involving di\u000berent\nsteps. Initially, we developed along with the\nexperts in gender issues an annotation guide\nin English and Spanish in which we provided\na clear explanation of each label along with\na number of examples. In order to evaluate\nthe quality of the annotation guide, three ex-\nperts (proposed by the gender issue experts)\nlabeled 50 Spanish tweets obtaining a 0.58\nkappa for task 1 and 0.45 for task 2. These\nresults indicated a moderate agreement that\naligns with the fact that the sexism detection\ntask from a broad perspective is not simple.\nSexism is even more subjective than misog-\nyny or hate speech to women thus the label-\n7https://www.mturk.com/Task 1 Task 2\nKappa %Agreement Kappa %Agreement\nSpanish 0.74 0.87 0.57 0.71\nEnglish 0.62 0.83 0.49 0.72\nTable 2: EXIST 2021 agreement analysis.\ning process is harder. The results from this\nexperiment were used to modify the annota-\ntion guide.\nThen, we did an annotation experiment\nusing MTurk. To this aim, a gold standard\nwas created and labeled by two experts (one\nman and one woman with 2 years of expe-\nrience in sexism classi\fcation), whose cases\nof disagreement were solved by a third ex-\nperienced contributor. It was composed of\n100 Spanish tweets and 100 English tweets.\nEach tweet from the gold standard was an-\nnotated by 5 crowdsourcing annotators, fol-\nlowing the modi\fed guidelines. Some \fl-\nters were applied to select annotators: loca-\ntion in USA or UK for English, location in\nSpain, USA or Chile for Spanish, work ap-\nproval rate bigger than 98% and more than 50\ntasks approved. In order to determine inter-\nannotator agreement, we compared the ma-\njority vote from crowdsourcing annotators to\nthe label selected by the experts. Table 2\nshows results for this experiment. As we can\nsee, results indicate a substantial agreement\nthus crowdsourcing annotators performed the\ntask correctly.\nThe \fnal labels were selected according to\nthe majority vote between 5 crowdsourcing\nannotators in all cases (same \flters as be-\nfore were used to select annotators). Texts\nwith 3 votes in one class for task 1 (binary\nproblem) and with disagreement for task 2\n(2 categories with 2 votes) were manually\nreviewed by two experts (one man and one\nwoman) with more than two years of expe-\nrience analysing sexist content in social net-\nworks. Around 10% of all posts were changed\nby the experts for English and 14% for Span-\nish. We implemented further quality con-\ntrol mechanisms to avoid random judgements\nthroughout the labeling process (deviation\nfrom label distribution by annotator, time\nto complete the task, etc.). The \fnal EX-\nIST dataset consists of 6977 tweets for train-\ning and 3386 tweets for testing, where both\nsets are randomly selected from the 9000 and\n4000 sampled sets, training, and test respec-\ntively, to ensure class balancing according to\nTask 1. Gab information was labeled follow-\nOverview of EXIST 2021: sEXism Identification in Social neTworks\n199ing the same process, obtaining 492 gabs in\nEnglish and 490 in Spanish from the 500 la-\nbeled sets. We discarded posts in both data\nsources due to a number of reasons: posts\nwritten in another language, messages con-\ntaining only hashtags or URLs, etc. Emojis\nwere also removed since Mturk does not sup-\nport them.\nThe training data was provided as tab-\nseparated, according to the following \felds:\n\u2022testcase: contains the string \\EX-\nIST2021\" needed for the evaluation tool\nEvALL.\n\u2022id: denotes a unique identi\fer of the\ntext.\n\u2022source: denotes the data source; it takes\nvalues \\twitter\" or \\gab\".\n\u2022language: denotes the language of the\ntext; it takes values \\en\" or \\es\".\n\u2022text: contains the actual text.\n\u2022task1: de\fnes whether the text is sexist\nor not; it takes values \\sexist\" and \\non-\nsexist\".\n\u2022task2: de\fnes the type of sexism (if ap-\nplicable); it takes values as:\n{\\ideological-inequality\": denotes\nthe category \\Ideological and in-\nequality\";\n{\\misogyny-non-sexual-violence\":\ndenotes the category \\Misogyny\nand non-sexual violence\";\n{\\objecti\fcation\": denotes the cate-\ngory \\Objecti\fcation\";\n{\\sexual-violence\": denotes the cat-\negory \\Sexual violence\";\n{\\stereotyping-dominance\": denotes\nthe category \\Stereotyping and\ndominance\";\n{\\non-sexist\": denotes that the\ntweet or gab does not express any\nsexist behaviours or discourses.\nConcerning the test data, we removed\n\\task1\" and \\task2\" labels from the \fle that\nwas provided to the participants. Once the\nevaluation phase was over, we shared the\nlabels for the test set in case participants\nwanted to perform further tests.\nThe entire EXIST dataset contains 11,345\nlabeled texts, 5644 for English and 5701 forSpanish. Table 3 summarizes the description\nof the dataset, as well as the number of texts\nper class for both training and test sets, and\nthe distribution by language.\n4 Overview of the Submitted\nApproaches\n76 groups from 11 countries (Spain, China,\nGermany, India, Italy, Mexico, Austria,\nSwitzerland, England, Greece, and Pakistan)\nsigned up for EXIST 2021, 31 of them sub-\nmitted runs for task 1, and 27 for task 2. In\nthis challenge, each team had the chance to\nsubmit a maximum of 6 runs, 3 runs for each\ntask. We received a total of 70 runs for task\n1 and 61 runs for task 2.\nRegarding the classi\fcation approaches,\nthe majority of participants exploited\ntransformer-based systems for both tasks. In\nparticular, 23 teams used some sort of trans-\nformer architecture, of which 14 teams used\nBERT (Devlin et al., 2019) (or multilingual\nBERT - mBERT), 10 used a Spanish version\nof BERT called BETO (Canete et al., 2020),\n6 used RoBERTa (Liu et al., 2019) and 5\nused a multilingual version of RoBERTa\ncalled XLM-R (Conneau et al., 2019).\nTraditional machine learning methods like\nSupport Vector Machines (SVM), Random\nForest (RF), or Logistic Regression (LR)\nhave been adopted by a subset of partici-\npants. Similarly, a few teams experimented\nwith other deep learning methods (i.e. Long\nshort-term memory networks - LSTM) and\nwith the fastText library (Joulin et al.,\n2017). Following, we list the participants\nand brie\ry describe the approaches used by\neach group.\nAI-UPV participated in both tasks and\nsubmitted one run for each task. They\nused an ensemble of di\u000berent transformer\nmodels with BERT for English, BETO for\nSpanish and mBERT for multilingual mod-\nels. They also implemented individual mod-\nels with translation for both English and\nSpanish texts.\nAIT FHSTP participated in both tasks\nand submitted 3 runs for each task. Their\nbest approach to the task is based on a\n\fne-tuned XLM-R on the provided EXIST\ndataset, and additionally on the MeTwo\ndataset (Rodr\u0013 \u0010guez-S\u0013 anchez, Carrillo-de Al-\nbornoz, and Plaza, 2020) and HatEval 2019\ndataset (Basile et al., 2019).\nAlclatos submitted 3 runs for each task.\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso\n200Training Test\nSpanish English Spanish English\nTwitter Twitter Twitter Gab Twitter Gab Total\nSexist 1741 1636 858 265 858 300 5658\nNon-sexist 1800 1800 812 225 858 192 5687\nIdeological-inequalit y 480 386 215 73 233 100 1487\nMisogyn y-non-sexual-violence 401 284 199 58 152 63 1157\nObjecti\fcation 244 256 124 50 121 29 824\nSexual-violence 173 344 131 71 150 48 917\nStereot yping-dominance 443 366 189 13 202 60 1273\nTable 3: Dataset distribution.\nTheir best system was based on transformers,\nwhere BETO was used for Spanish messages\nand RoBERTa for English.\nAlmuoes3 submitted one run for each\ntask. They employed RF, LR and SVM\ntrained on tf-idf features built from the texts\nunigrams. For task 1, they used an ensemble\nof the 3 models whereas LR was used for task\n2.\nAndrea Lisa submitted one run for each\ntask. They proposed a multilingual classi-\n\fcation system based on mBERT for both\ntasks.\nBilaUnwanPk1 submitted 3 runs for each\ntask. They used the fastText library and\ntuned models for di\u000berent n-grams con\fgu-\nrations.\nCIC submitted 3 runs for each task. They\nused back translation techniques to augment\nthe dataset and applied some preprocessing\nsteps like URLs, mails, numbers, and punc-\ntuation removal. Their best-performing algo-\nrithms were BERT, SVM, and RF.\nCodec submitted one run for each task.\nTheir system was an ensemble of 3 models for\nSpanish (BETO) and English (BERT) with\ndi\u000berent hyperparameter con\fgurations for\ntask 1. For task 2, they \fne-tuned one model\nfor each language using only the sexist texts.\nFree submitted one run for each task.\nThey trained one model for each language,\nRoBERTa for English and mBERT for Span-\nish.\nGuillemGSubies submitted 3 runs for each\ntask. Their best system used back transla-\ntion from English to Spanish and vice versa.\nThey \fne-tuned BERT for English texts and\nBETO for Spanish.\nIREL hatespeech group submitted 3 runs\nfor each task. They trained a RoBERTa\nmodel using unlabeled data (Parikh et al.,\n2019) and experimented with feature engi-\nneering using Empath tool (Fast, Chen, andBernstein, 2016), Hurtlex lexicon (Bassig-\nnana, Basile, and Patti, 2018), and Perspec-\ntive API8to create tweet representations. A\nbiLSTM and Attention layer was applied to\neach representation followed by a linear layer\nto obtain the \fnal predictions.\nLaSTUS submitted one run for each task.\nFor both tasks, they used a multilingual\nBERT (mBERT) transformer model.\nLHZ submitted one run for each task.\nThey used a di\u000berent transformer model for\neach language: DEBERTA (He et al., 2020)\nfor English and XLM-R for Spanish. They\napplied an LSTM network to each represen-\ntation to obtain the \fnal predictions.\nMB-Courage submitted 3 runs for each\ntask. Their best model was based on a Graph\nConvolutional Network (GCN) model where\nnodes contain word features from BERT en-\ncoding as well as morpho-syntactic annota-\ntions. For task 1, edges indicate the word\nneighborhood whereas for task 2 they in-\ndicate a syntactic dependency link between\nwords.\nMessGroupELL only participated in task\n1 with three di\u000berent runs. Their best ap-\nproach consisted in an ensemble of three clas-\nsi\fers for each language: XGBoost, SVM,\nRoBERTa for English and mBERT for Span-\nish. They also used the MeTwo dataset with\nbalanced classes to augment Spanish data.\nMiniTrue participated in task 1 and sub-\nmitted one run. They developed a voting\nmechanism for the sexist label prediction tak-\ning as input the output of three di\u000berent\nmodels. The \frst two models used BERT for\nEnglish texts and BETO for Spanish, and the\nlast model used mBERT.\nMultiaztertest only participated in task 1\nwith three di\u000berent runs. Their best run used\na di\u000berent transformer model for each lan-\nguage: BERT for English texts and BETO\n8https://www.perspectiveapi.com/\nOverview of EXIST 2021: sEXism Identification in Social neTworks\n201for Spanish.\nNerin participated in both tasks and sub-\nmitted 3 runs for each one. They used tradi-\ntional machine learning methods like SVM,\nLR and AdaBoost and processed each lan-\nguage independently. Their best approach\nfor task 1 was a SVM model for each lan-\nguage whereas LR was their best solution for\ntask 2.\nnlpuned team submitted 3 runs for each\ntask. They developed a multilingual system\nbased on pre-trained transformers and com-\npared single-task to multi-task learning ap-\nproaches. Their best approach for task 1\nwas a multilingual single-task model based\non XLM-T (Barbieri, Anke, and Camacho-\nCollados, 2021) whereas a multi-task model\nbased on XLM-R had the best results for task\n2.\nORDS CLAN submitted 3 runs for each\ntask. They applied some preprocessing (such\nas the removal of URLs, mentions, or stop-\nwords) before training one classi\fer per lan-\nguage using the fastText library.\nQMUL-SDS submitted 3 runs for each\ntask. They applied simple preprocessing and\nadded lexical features using the Hurtlex lex-\nicon. They used XLM-R as base model and\nthe outputs from the last 4 hidden layers were\nfed into a BiLSTM layer.\nRecognai submitted 2 runs for each task.\nTheir best run used a di\u000berent transformer\nmodel for each language: RoB-Tw (Barbieri\net al., 2020) for English texts and BETO for\nSpanish.\nSINAI-TL submitted 3 runs for each task.\nThey followed a multi-task learning approach\nusing di\u000berent auxiliary tasks. BETO for\nSpanish and BERT for English were used as\nbase models. Their best run used polarity\nclassi\fcation as the auxiliary task by training\na shared model with the InterTASS dataset\n(Mart\u0013 \u0010nez-C\u0013 amara et al., 2017).\nSoumya submitted 3 runs for task 1 and 2\nruns for task 2. They employed two machine\nlearning techniques (RF, SVM) and one deep\nlearning model (LSTM). Previously, they em-\nployed some extra features such as the num-\nber of slang words used in the English tweets\nor hashtags count. For task 1, RF was their\nbest result and SVM for task 2.\nSexist submitted 3 runs for each\ntask. They experimented with transformers\n(RoBERTa) and traditional machine learn-\ning methods such as SVM. They also trieddata augmentation techniques translating all\nSpanish tweets to English. For both tasks,\nthe transformer-based model achieved the\nbest results.\nUjaonly participated in task 1 with one\nrun. They used a di\u000berent transformer model\nfor each language: BERT for English texts\nand BETO for Spanish.\nUMUTeam submitted 3 runs for each task.\nTheir system combined linguistic features\nand state-of-the-art transformers using en-\nsemble techniques. They developed their tool\nto create linguistic features and a di\u000berent\ntransformer model for each language: BERT\nfor English texts and BETO for Spanish.\nUNEDBiasTeam submitted 3 runs for\neach task. They transferred features com-\nmonly used in the task of bias and propa-\nganda detection and studied the applicability\nof these features with the detection of sexism.\nThey combined these features with machine\nlearning methods (LR and Bi-LSTM).\nZimtstern submitted 3 runs for each task.\nTheir system was based on mBERT and\nexperimented with di\u000berent hyperparameter\ncon\fgurations.\nZK submitted one run for each task.\nThey \fne-tuned 3 di\u000berent models (mBERT,\nRoBERTa and XLM-R) and conducted soft\nvoting on the predicted results of the three\nmodels.\nZZW submitted one run for each task.\nThey proposed a multilingual classi\fcation\nsystem based on XLM-R.\n5 System Results\nTasks 1 and 2 were evaluated independently.\nIn the following subsections, we will show re-\nsults for each task and language. Teams were\nranked by accuracy for task 1 and macro-\naveraged F1-score (F1) for task 2. However,\nwe also report standard evaluation metrics\nsuch as Precision and Recall.\n5.1 Task 1\n31 teams participated in task 1 for both, En-\nglish and Spanish, presenting 70 runs in to-\ntal. In Table 4, the best run for each team\nis shown, as well as the two baselines: Base-\nlinesvm t\fdf and Majority Class. All runs\nranking is available at the task website9.\nRegarding the best run ranking, 26\nteams achieved an Accuracy above the Base-\nlinesvm t\fdf, while only 5 teams are below\n9http://nlp.uned.es/exist2021/\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso\n202Ranking Team run Accuracy Precision Recall F1\n1 task1 AI-UPV 1 0.7804 0.7801 0.7806 0.7802\n2 task1 SINAI TL1 0.78 0.7796 0.78 0.7797\n3 task1 AIT FHSTP 2 0.7754 0.7751 0.7756 0.7752\n4 task1 multiaztertest 1 0.774 0.7741 0.7727 0.7731\n5 task1 nlpuned team 1 0.772 0.7737 0.7696 0.7702\n6 task1 free1 0.7708 0.7712 0.7717 0.7708\n7 task1 GuillemGSubies 2 0.7683 0.7693 0.7695 0.7683\n8 task1 LHZ 1 0.7665 0.766 0.7661 0.7661\n9 task1 zk1 0.7647 0.7645 0.765 0.7645\n10 task1 Alclatos 1 0.7637 0.7635 0.764 0.7636\n11 task1 QMUL-SDS 2 0.761 0.7613 0.7618 0.7609\n12 task1 sexist 1 0.7598 0.7615 0.7614 0.7598\n13 task1 MiniTrue 1 0.7553 0.7551 0.7555 0.7551\n14 task1 IREL hatespeech group 3 0.7532 0.7536 0.754 0.7532\n15 task1 zzw1 0.7527 0.7525 0.753 0.7526\n16 task1 UMUTEAM 3 0.7514 0.7537 0.7532 0.7514\n17 task1 Zimtstern 3 0.7356 0.7354 0.7359 0.7354\n18 task1 LaSTUS 1 0.7317 0.7321 0.7325 0.7316\n19 task1 CIC 1 0.7278 0.7273 0.7269 0.727\n20 task1 MessGroupELL 3 0.7237 0.7254 0.7253 0.7237\n21 task1 Andrea Lisa 1 0.7186 0.7181 0.7183 0.7182\n22 task1 MB-Courage 1 0.7145 0.7154 0.7156 0.7145\n23 task1 Soumya 2 0.7115 0.7147 0.7137 0.7114\n24 task1 Nerin 1 0.7072 0.7129 0.7103 0.7068\n25 task1 UNEDBiasTeam 2 0.7056 0.7068 0.7069 0.7056\n26 task1 recognai 1 0.7044 0.7093 0.7073 0.7041\n27 Baseline svm t\fdf 0.6845 0.6943 0.6888 0.6832\n28 task1 BilaUnwanPk1 3 0.6763 0.6808 0.679 0.6759\n29 Majority Class 0.5222 0.5222 0.5 0.3431\n30 task1 uja1 0.519 0.5134 0.5122 0.5035\n31 task1 ORDS CLAN 1 0.4924 0.5417 0.5114 0.3934\n32 task1 almuoes3.0 1 0.4876 0.5173 0.5058 0.3979\n33 task1 codec 1 0.4096 0.7725 0.3922 0.3892\nTable 4: Results task 1 (best run).\nthe baseline. For the Majority Class base-\nline, 27 teams achieved a higher Accuracy,\nwhereas only 4 teams are below the bench-\nmark model. The best performing team is\nAI-UPV, which achieved an overall of 0.7804.\nInAI-UPV the participants exploited an\nensemble of transformers models for di\u000ber-\nent con\fgurations: multilingual, language-\nspeci\fc, and language-speci\fc with data aug-\nmentation (via translation). The baseline\nbased on majority vote was one of the worst-\nperforming solutions (29 of 33).\nAlthough the o\u000ecial ranking considered\nboth languages, we also presented two rank-\nings by language (English and Spanish) for\neach task. Table 5 shows the top-10 runs for\nEnglish and Table 6 for Spanish. Regarding\nthe English results, SINAI-TL achieved the\nbest results with an accuracy of 0.7772. They\nfollowed a multi-task learning approach withtwo base models for each language. The win-\nning team AI-UPV ranked third with around\n1% di\u000berence in terms of accuracy. Regard-\ning the Spanish results, AI-UPV ranked \frst.\nAs expected, transformer-based models\nperformed better than the other techniques,\nsince the top-10 teams are all based on\nthese techniques. Traditional machine learn-\ning approaches did not perform well even\nusing extra features based on external re-\nsources. Similarly, the use of external lexi-\ncons has been explored by two teams with-\nout success. Data augmentation techniques\nhave been successfully employed by the top-\nperformed teams. This may suggest that\ntransformer-based models bene\ft from train-\ning with more data from related tasks, even\nif the EXIST dataset is one of the largest cor-\npus in this area.\nIt is interesting to highlight the perfor-\nOverview of EXIST 2021: sEXism Identification in Social neTworks\n203Ranking Team run Accuracy Precision Recall F1\n1 task1 SINAI TL3.tsv en 0.7772 0.7805 0.7739 0.7747\n2 task1 multiaztertest 1.tsv en 0.7717 0.7753 0.7683 0.7691\n3 task1 AI-UPV 1.tsv en 0.7668 0.7666 0.7654 0.7657\n4 task1 free1.tsv en 0.7668 0.7662 0.7661 0.7662\n5 task1 zk1.tsv en 0.7663 0.7664 0.7646 0.7651\n6 task1 LHZ 1.tsv en 0.7659 0.7687 0.7626 0.7633\n7 task1 nlpuned team 1.tsv en 0.7609 0.7666 0.7566 0.7571\n8 task1 GuillemGSubies 1en 0.7604 0.7598 0.7599 0.7599\n9 task1 AIT FHSTP 2.tsv en 0.7595 0.7594 0.7579 0.7583\n10 task1 IREL hatespeech group 3.tsv en 0.7577 0.7575 0.7563 0.7566\nTable 5: Top-10 results task 1 English.\nRanking Team run Accuracy Precision Recall F1\n1 task1 AI-UPV 1.tsv es 0.7944 0.796 0.7958 0.7944\n2 task1 AIT FHSTP 2.tsv es 0.7917 0.7938 0.7933 0.7916\n3 task1 SINAI TL1.tsv es 0.7907 0.7955 0.7931 0.7906\n4 task1 nlpuned team 1.tsv es 0.7833 0.7832 0.7826 0.7828\n5 task1 Alclatos 1.tsv es 0.7792 0.7808 0.7806 0.7792\n6 task1 GuillemGSubies 2es 0.7764 0.7821 0.779 0.7761\n7 task1 multiaztertest 1.tsv es 0.7764 0.7764 0.7769 0.7763\n8 task1 free1.tsv es 0.775 0.7785 0.7771 0.7749\n9 task1 sexist 1.tsv es 0.7704 0.777 0.7733 0.77\n10 task1 LHZ 1.tsv es 0.7671 0.7705 0.7692 0.767\nTable 6: Top-10 results task 1 Spanish.\nmance di\u000berence (around 2%) between Span-\nish and English tasks. We expected that\ntransformers models would perform better\nin English since they have been trained on\ncorpus mainly composed of English texts.\nHowever, since Spanish is well-represented in\nthese datasets, multilingual transformers per-\nform very well for this language.\n5.2 Task 2\n27 teams participated in task 2 for both, En-\nglish and Spanish, presenting 61 runs in to-\ntal. In Table 7, the best run for each team is\nshown, as well as the two baselines. Among\nall the runs, 24 teams achieved an F1 above\nthe Baseline svm t\fdf, while only 3 teams are\nbelow the benchmark model. For the Major-\nity Class baseline, 27 teams achieved a higher\nF1, whereas only 1 team is below the base-\nline.\nIt is interesting to highlight the strong dif-\nference between the best and the worst sys-\ntems, underlying an F1 ranging from 0.5787\nto 0.1069. The best performing team for\ntask 2 is again AI-UPV. The worst results\nhave been obtained by teams that used tra-\nditional machine learning techniques such as\nSVM and RF to solve the task.\nTables 8 and 9 show results for the top-10 teams in English and Spanish respec-\ntively. Again, the task winner AI-UPV per-\nformed better in Spanish than in English,\nthey ranked \frst and third respectively. In-\nterestingly, LHZ performed really well in En-\nglish by using DeBERTa, an enhanced ver-\nsion of BERT and RoBERTa models.\nIn this task, the di\u000berence in performance\nbetween English and Spanish increases. How-\never, it is important to notice that most\nparticipants achieved relatively low results,\nshowing the di\u000eculty of this task.\n6 Conclusions\nIn this paper, we have presented the re-\nsults of the \frst shared task on sexism de-\ntection in a broad sense, from explicit misog-\nyny to other subtle expressions that involve\nimplicit sexist behaviours. The task setup\nprovided an opportunity to test classi\fca-\ntion systems in multilingual scenarios (En-\nglish and Spanish) along with di\u000berent so-\ncial networks (Twitter and Gab). The runs\nsubmitted show that the problem of sex-\nism identi\fcation can be reasonably well ad-\ndressed by using transformer-based models,\nwhile the sexism categorization still remains\na challenging problem. We found out that\nmodern transformer-based models overcome\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso\n204Ranking Team run Accuracy Precision Recall F1\n1 task2 AI-UPV 1 0.6577 0.5815 0.5774 0.5787\n2 task2 LHZ 1 0.6509 0.5772 0.5649 0.5706\n3 task2 SINAI TL1 0.6527 0.5848 0.5527 0.5667\n4 task2 QMUL-SDS 1 0.6426 0.5626 0.5573 0.5594\n5 task2 AIT FHSTP 2 0.6445 0.5689 0.5531 0.5589\n6 task2 Alclatos 1 0.6369 0.5668 0.5535 0.5578\n7 task2 IREL hatespeech group 3 0.6403 0.5717 0.5429 0.5556\n8 task2 zk1 0.649 0.5821 0.532 0.5521\n9 task2 nlpuned team 3 0.6232 0.5543 0.5515 0.5509\n10 task2 recognai 1 0.6243 0.5782 0.5303 0.55\n11 task2 UMUTEAM 2 0.617 0.5449 0.5332 0.5362\n12 task2 codec 1 0.6239 0.5377 0.5366 0.5354\n13 task2 sexist 1 0.5682 0.5126 0.5857 0.5342\n14 task2 GuillemGSubies 2 0.6293 0.5412 0.5201 0.5295\n15 task2 LaSTUS 1 0.612 0.5375 0.5128 0.5227\n16 task2 Zimtstern 1 0.6108 0.5344 0.5122 0.5208\n17 task2 Andrea Lisa 1 0.6129 0.534 0.5114 0.5204\n18 task2 zzw1 0.6296 0.5494 0.5068 0.5192\n19 task2 CIC 2 0.5527 0.4837 0.5064 0.4908\n20 task2 Nerin 3 0.6046 0.5744 0.4388 0.4817\n21 task2 UNEDBiasTeam 3 0.5797 0.5154 0.4484 0.4704\n22 task2 MB-Courage 2 0.5946 0.5307 0.428 0.459\n23 task2 Soumya 1 0.5923 0.6023 0.3999 0.4504\n24 task2 free1 0.5847 0.4792 0.4232 0.4194\n25 Baseline svm t\fdf 0.5222 0.4315 0.3772 0.395\n26 task2 BilaUnwanPk1 1 0.5062 0.4097 0.3709 0.3788\n27 task2 ORDS CLAN 1 0.4833 0.5724 0.1747 0.1244\n28 Majority Class 0.4778 0.4778 0.1667 0.1078\n29 task2 almuoes3.0 1 0.1291 0.1043 0.1792 0.1069\nTable 7: Results task 2 (best run).\nRanking Team run Accuracy Precision Recall F1\n1 task2 LHZ 1.tsv en 0.6336 0.5512 0.5742 0.5604\n2 task2 IREL hatespeech group 3.tsv en 0.6277 0.5486 0.5588 0.5531\n3 task2 AI-UPV 1.tsv en 0.6291 0.5468 0.5647 0.5507\n4 task2 zk1.tsv en 0.6368 0.5662 0.5359 0.5432\n5 task2 AIT FHSTP 2.tsv en 0.6187 0.539 0.5497 0.5419\n6 task2 SINAI TL3.tsv en 0.6255 0.5428 0.5405 0.5375\n7 task2 nlpuned team 1.tsv en 0.6178 0.545 0.5374 0.5371\n8 task2 QMUL-SDS 1.tsv en 0.6187 0.5306 0.5505 0.5351\n9 task2 recognai 1.tsv en 0.6123 0.5666 0.5022 0.5252\n10 task2 Alclatos 1.tsv en 0.6033 0.5241 0.5303 0.5245\nTable 8: Top-10 results task 2 English.\nRanking Team run Accuracy Precision Recall F1\n1 task2 AI-UPV 1.tsv es 0.687 0.6286 0.5913 0.6073\n2 task2 SINAI TL2.tsv es 0.6815 0.6425 0.5783 0.6014\n3 task2 Alclatos 1.tsv es 0.6713 0.6211 0.5759 0.5922\n4 task2 QMUL-SDS 1.tsv es 0.6671 0.6142 0.5652 0.5843\n5 task2 LHZ 1.tsv es 0.6685 0.6202 0.5565 0.5805\n6 task2 AIT FHSTP 2.tsv es 0.6708 0.613 0.5578 0.5761\n7 task2 nlpuned team 3.tsv es 0.6491 0.6089 0.5687 0.576\n8 task2 codec 1.tsv es 0.6648 0.618 0.5596 0.5759\n9 task2 recognai 1.tsv es 0.6366 0.6073 0.56 0.575\n10 task2 GuillemGSubies 2es 0.6634 0.6039 0.541 0.5646\nTable 9: Top-10 results task 2 Spanish.\nOverview of EXIST 2021: sEXism Identification in Social neTworks\n205considerably traditional machine learning ap-\nproaches. Overall, the results con\frm that\nsexism detection in social networks is chal-\nlenging, with a large room for improvement.\nThe high number of participating teams at\nEXIST 2021 con\frms the growing interest of\nthe community around sexism detection in\nsocial networks. We think that the provided\ndataset will foster research on this topic.\nAcknowledgments\nThis work was supported by the Spanish\nMinistry of Science and Innovation under\nthe MISMIS research project on Misinforma-\ntion and Miscommunication in Social Media\n(PGC2018-096212-B-C32).\nReferences\nAmig\u0013 o, E., J. Carrillo-de Albornoz,\nM. Almagro-C\u0013 adiz, J. Gonzalo,\nJ. Rodr\u0013 \u0010guez-Vidal, and F. Verdejo.\n2017. Evall: Open access evaluation\nfor information access systems. In\nProceedings of the 40th International\nACM SIGIR Conference on Research and\nDevelopment in Information Retrieval ,\npages 1301{1304.\nAmig\u0013 o, E., J. Gonzalo, S. Mizzaro, and\nJ. Carrillo-de Albornoz. 2020. An e\u000bec-\ntiveness metric for ordinal classi\fcation:\nFormal properties and experimental re-\nsults. arXiv preprint arXiv:2006.01245.\nAmig\u0013 o, E., D. Spina, and J. Carrillo-de Al-\nbornoz. 2018. An axiomatic analysis of\ndiversity evaluation metrics: Introducing\nthe rank-biased utility metric. In The 41st\nInternational ACM SIGIR Conference on\nResearch & Development in Information\nRetrieval , pages 625{634.\nBarbieri, F., L. E. Anke, and J. Camacho-\nCollados. 2021. Xlm-t: A multilingual\nlanguage model toolkit for twitter. arXiv\npreprint arXiv:2104.12250.\nBarbieri, F., J. Camacho-Collados, L. Es-\npinosa Anke, and L. Neves. 2020. Tweet-\nEval: Uni\fed benchmark and compara-\ntive evaluation for tweet classi\fcation. In\nFindings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages\n1644{1650, Online, November. Associa-\ntion for Computational Linguistics.\nBasile, V., C. Bosco, E. Fersini, N. Deb-\nora, V. Patti, F. M. R. Pardo, P. Rosso,M. Sanguinetti, et al. 2019. Semeval-\n2019 task 5: Multilingual detection of hate\nspeech against immigrants and women in\ntwitter. In 13th International Workshop\non Semantic Evaluation , pages 54{63. As-\nsociation for Computational Linguistics.\nBassignana, E., V. Basile, and V. Patti.\n2018. Hurtlex: A multilingual lexicon\nof words to hurt. In 5th Italian Confer-\nence on Computational Linguistics, CLiC-\nit 2018, volume 2253, pages 1{6. CEUR-\nWS.\nBaumgartner, J., S. Zannettou, B. Keegan,\nM. Squire, and J. Blackburn. 2020. The\npushshift reddit dataset. In Proceedings\nof the international AAAI conference on\nweb and social media , volume 14, pages\n830{839.\nBerg, S. H. 2006. Everyday sexism and\nposttraumatic stress disorder in women:\nA correlational study. Violence Against\nWomen, 12(10):970{988.\nCanete, J., G. Chaperon, R. Fuentes, and\nJ. P\u0013 erez. 2020. Spanish pre-trained bert\nmodel and evaluation data. PML4DC at\nICLR, 2020.\nChiril, P., V. Moriceau, F. Benamara,\nA. Mari, G. Origgi, and M. Coulomb-\nGully. 2020. He said \\who's gonna take\ncare of your children when you are at\nACL?\": Reported sexist acts are not sex-\nist. In Proceedings of the 58th Annual\nMeeting of the Association for Computa-\ntional Linguistics, pages 4055{4066, On-\nline, July. Association for Computational\nLinguistics.\nConneau, A., K. Khandelwal, N. Goyal,\nV. Chaudhary, G. Wenzek, F. Guzm\u0013 an,\nE. Grave, M. Ott, L. Zettlemoyer, and\nV. Stoyanov. 2019. Unsupervised cross-\nlingual representation learning at scale.\narXiv preprint arXiv:1911.02116.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. In Proceedings of\nthe 2019 Conference of the North Amer-\nican Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers) , pages 4171{4186, Minneapolis,\nMinnesota, June. Association for Compu-\ntational Linguistics.\nFrancisco Rodr\u00edguez-S\u00e1nchez, Jorge Carrilo-de-Albornoz, Laura Plaza, Julio Gonzalo, Paolo Rosso, Miriam Comet, Trinidad Donoso\n206Donoso-V\u0013 azquez, T. and Rebollo-Catal\u0013 an.\n2018. Violencias de g\u0013 enero en entornos\nvirtuales. Ediciones Octaedro.\nFast, E., B. Chen, and M. S. Bernstein. 2016.\nEmpath: Understanding topic signals in\nlarge-scale text. In Proceedings of the\n2016 CHI conference on human factors in\ncomputing systems, pages 4647{4657.\nFersini, E., P. Rosso, and M. Anzovino.\n2018. Overview of the task on automatic\nmisogyny identi\fcation at ibereval 2018.\nIberEval@ SEPLN, 2150:214{228.\nFox, J., C. Cruz, and J. Y. Lee. 2015. Perpet-\nuating online sexism o\u000fine: Anonymity,\ninteractivity, and the e\u000bects of sexist hash-\ntags on social media. Computers in Hu-\nman Behavior , 52:436{442.\nFrenda, S., B. Ghanem, M. Montes-y G\u0013 omez,\nand P. Rosso. 2019. Online hate speech\nagainst women: Automatic identi\fca-\ntion of misogyny and sexism on twitter.\nJournal of Intelligent & Fuzzy Systems ,\n36(5):4743{4752.\nHe, P., X. Liu, J. Gao, and W. Chen. 2020.\nDeberta: Decoding-enhanced bert with\ndisentangled attention. arXiv preprint\narXiv:2006.03654.\nJoulin, A., E. Grave, P. Bojanowski, and\nT. Mikolov. 2017. Bag of tricks for ef-\n\fcient text classi\fcation. In Proceedings\nof the 15th Conference of the European\nChapter of the Association for Computa-\ntional Linguistics: Volume 2, Short Pa-\npers, pages 427{431. Association for Com-\nputational Linguistics, April.\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettle-\nmoyer, and V. Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nManne, K. 2017. Down girl: The logic of\nmisogyny. Oxford University Press.\nMart\u0013 \u0010nez-C\u0013 amara, E., M. D\u0013 \u0010az-Galiano,\nM. Garc\u0013 \u0010a-Cumbreras, M. Garc\u0013 \u0010a-Vega,\nand J. Villena-Rom\u0013 an. 2017. Overview of\ntass 2017. IberEval@ SEPLN , 1896:13{21.\nMills, S. 2008. Language and sexism . Cam-\nbridge University Press.\nMontes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on,\nR. Agerri, M. \u0013Angel \u0013Alvarez Carmona,E.\u0013Alvarez Mellado, J. C. de Albornoz,\nL. Chiruzzo, L. Freitas, H. G. Adorno,\nY. Guti\u0013 errez, S. Lima, A. Montejo-R\u0013 aez,\nF. M. P. de Arco, and M. Taul\u0013 e. 2021.\nProceedings of the iberian languages eval-\nuation forum (iberlef 2021). In CEUR\nWorkshop Proceedings.\nParikh, P., H. Abburi, P. Badjatiya, R. Kr-\nishnan, N. Chhaya, M. Gupta, and\nV. Varma. 2019. Multi-label cate-\ngorization of accounts of sexism using\na neural framework. arXiv preprint\narXiv:1910.04602.\nRodr\u0013 \u0010guez-S\u0013 anchez, F., J. Carrillo-de Al-\nbornoz, and L. Plaza. 2020. Automatic\nclassi\fcation of sexism in social networks:\nAn empirical study on twitter data. IEEE\nAccess, 8:219563{219576.\nSwim, J., L. Hyers, L. Cohen, and M. Fer-\nguson. 2001. Everyday sexism: Evidence\nfor its incidence, nature, and psychologi-\ncal impact from three daily diary studies.\nJournal of Social Issues , 57:31 { 53.\nWaseem, Z. 2016. Are you a racist or am\nI seeing things? annotator in\ruence on\nhate speech detection on Twitter. In Pro-\nceedings of the First Workshop on NLP\nand Computational Social Science , pages\n138{142, Austin, Texas, November. Asso-\nciation for Computational Linguistics.\nWaseem, Z. and D. Hovy. 2016. Hateful\nsymbols or hateful people? predictive fea-\ntures for hate speech detection on Twitter.\nInProceedings of the NAACL Student Re-\nsearch Workshop, pages 88{93, San Diego,\nCalifornia, June. Association for Compu-\ntational Linguistics.\nOverview of EXIST 2021: sEXism Identification in Social neTworks\n207Overview of DETOXIS at IberLEF 2021:DEtection of TOXicity in comments In Spanish\nVisi\u0013 on general de DETOXIS en IberLEF 2021:\nDEtecci\u0013 on de TOXicidad en comentarios En Espa~ nol\nMariona Taul\u0013 e1, Alejandro Ariza1, Montserrat Nofre1, Enrique Amig\u0013 o2, Paolo Rosso3\n1CLiC, Universitat de Barcelona, Spain\n2Research Group in NLP and IR, Universidad Nacional de Educaci\u0013 on a Distancia, Spain\n3PRHLT Research Center, Universitat Polit\u0012 ecnica de Val\u0012 encia, Spain\nfmtaule, alejandro.ariza14, montsenofreg@ub.edu, enrique@lsi.uned.es, prosso@dsic.upv.es\nAbstract: In this paper we present the DETOXIS task, DEtection of TOxicity in\ncomments In Spanish, which took place as part of the IberLEF 2021 Workshop on\nIberian Languages Evaluation Forum at the SEPLN 2021 Conference. We describe\nthe NewsCom-TOX dataset used for training and testing the systems, the metrics\napplied for their evaluation and the results obtained by the submitted approaches.\nWe also provide an error analysis of the results of these systems.\nKeywords: Toxicity detection, Rank Biased Precision, Closeness Evaluation Mea-\nsure, NewsCom-TOX corpus.\nResumen: En este art\u0013 \u0010culo se presenta la tarea DETOXIS, DEtecci\u0013 on de TOxici-\ndad en comentarios en espa~ nol, que tuvo lugar en el Iberian Languages Evaluation\nForum workshop (IberLEF 2021) en el congreso de la SEPLN 2021. Se describe el\ncorpus NewsCom-TOX utilizado para entrenar y evaluar los sistemas, las m\u0013 etricas\npara evaluarlos y los resultados obtenidos por las distintas aproximaciones utilizadas.\nSe proporciona tambi\u0013 en un an\u0013 alisis de los resultados obtenidos por estos sistemas.\nPalabras clave: Detecci\u0013 on de toxicidad, Rank Biased Precision, Closeness Evalu-\nation Measure, NewsCom-TOX corpus.\n1 Introduction\nThe aim of the DETOXIS task is the detec-\ntion of toxicity in comments posted in Span-\nish in response to di\u000berent online news arti-\ncles related to immigration. The DETOXIS\ntask is divided into two related classi\fcation\nsubtasks: Toxicity detection task and Toxic-\nity level detection task, which are described\nin Section 2. The presence of toxic messages\non social media and the need to identify and\nmitigate them leads to the development of\nsystems for their automatic detection. The\nautomatic detection of toxic language, espe-\ncially in tweets and comments, is a task that\nhas attracted growing interest from the Nat-\nural Language Processing (NLP) community\nin recent years. This interest is re\rected\nin the diversity of the shared tasks that\nhave been organized recently, among which\nwe highlight those held over the last two\nyears: HateEval-20191(Basile et al., 2019) on\n1https://competitions.codalab.org/competitions/19935hate speech against immigrants and women\nin English and Spanish tweets; TRAC-2\ntask on Aggression Identi\fcation2(Kumar\net al., 2020) for English, Bengali and Hindi\nin comments extracted from YouTube; the\nO\u000bensEval-20203on o\u000bensive language iden-\nti\fcation (Zampieri et al., 2020) in Arabic,\nDanish, English, Greek and Turkish tweets;\nGermEval-2019 shared task4on the Identi-\n\fcation of O\u000bensive Language for German\non Twitter (Stru\u0019 et al., 2019); and the Jig-\nsaw Multilingual Toxic Comment Classi\fca-\ntion Challenge5, in which the task is focused\non building multilingual models (English,\nFrench, German, Italian, Portuguese, Rus-\nsian and Spanish) with English-only training\ndata from Wikipedia comments.\nDETOXIS is the \frst task that focuses\n2https://sites.google.com/view/trac2/shared-task\n3https://sites.google.com/site/o\u000bensevalsharedtask/\n4https://projects.fzai.h-da.de/iggsa/the\n5https://www.kaggle.com/c/jigsaw-multilingual-\ntoxic-comment-classi\fcation\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 209-221\nrecibido 03-07-2021 revisado 10-07-2021 aceptado 14-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-18\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalon the detection of di\u000berent levels of toxic-\nity in comments posted in response to news\narticles written in Spanish. The main nov-\nelty of the present task is, on the one hand,\nthe methodology applied to the annotation\nof the dataset that will be used for training\nand testing the participant models and, on\nthe other hand, the evaluation metrics that\nwill be applied to evaluating the participant\nmodels in terms of their system use pro\fle\napplying four di\u000berent metrics (F-measure,\nRank Biased Precision (Mo\u000bat and Zobel,\n2008), Closeness Evaluation Measure (Amig\u0013 o\net al., 2020) and Pearson's correlation co-\ne\u000ecient). The methodology proposed aims\nto reduce the subjectivity of the annotation\nof toxicity by taking into account the con-\ntextual information, i.e. the conversational\nthread, and by annotating di\u000berent linguis-\ntic features, such as argumentation, construc-\ntiveness, stance, target, stereotype, sarcasm,\nmockery, insult, improper language, aggres-\nsiveness and intolerance, which allowed us to\ndiscriminate the di\u000berent levels of toxicity.\nThe rest of the overview is structured as\nfollows. In Section 2 we present the two sub-\ntasks of DETOXIS. In Section 3 the corpus\nNewsCom-TOX used as dataset is described\ntogether with the way it was gathered and\nannotated. In Section 4 the di\u000berent metrics\nused for the evaluation of the systems and\nthe results they obtained are presented, as\nwell as a description of the techniques and\nmodels used by systems. Finally, in Section\n5 the conclusions are drawn.\n2 Task Description\nThe aim of the DETOXIS task is the detec-\ntion of toxicity in comments posted in Span-\nish in response to di\u000berent online news arti-\ncles related to immigration. The DETOXIS\ntask is divided into two related classi\fcation\nsubtasks:\n\u2022Subtask 1: Toxicity detection task is\na binary classi\fcation task that con-\nsists of classifying the content of a com-\nment as toxic (toxic=yes) or not toxic\n(toxic=no).\n\u2022Subtask 2: Toxicity level detection task\nis a more \fne-grained classi\fcation task\nin which the aim is to identify the level\nof toxicity of a comment (0=not toxic;\n1=mildly toxic; 2=toxic and 3=very\ntoxic).Although we recommended to participate\nin both subtasks, participants were also al-\nlowed to participate just in one of them.\nTeams were also encouraged to submit mul-\ntiple runs (maximum 5).\n3 Dataset: The NewsCom-TOX\ncorpus\nWe used as dataset the NewsCom-TOX cor-\npus, which consists of 4,359 comments posted\nin response to di\u000berent 21 articles extracted\nfrom Spanish online newspapers (ABC, el-\nDiario.es, El Mundo, NIUS, etc.) and dis-\ncussion forums (such as Men\u0013 eame6and Foro-\nCoches7) from August 2017 to July 2020.\nThese articles were manually selected tak-\ning into account their controversial subject\nmatter, their potential toxicity, and the num-\nber of comments posted (minimum 50 com-\nments). We used a keyword-based approach\nto search for articles related mainly to immi-\ngration. The comments were selected in the\nsame order in which they appear in the time\nthread in the web. The author (anonymized),\nthe date and the time when the comment was\nposted were also retrieved. The number of\ncomments ranges from 65 to 359 comments\nper article. On average, 31.16% of the com-\nments are toxic.\n3.1 Annotation Scheme\nWe considered that a comment is toxic when\nit attacks, threatens, insults, o\u000bends, deni-\ngrates or disquali\fes a person or group of\npeople on the basis of characteristics such\nas race, ethnicity, nationality, political ideol-\nogy, religion, gender and sexual orientation,\namong others. This attack can be expressed\nin di\u000berent ways {explicitly (through insult,\nmockery and inappropriate humor) or implic-\nitly (for instance through sarcasm){ and at\ndi\u000berent levels of intensity, that is at dif-\nferent levels of toxicity (from impolite and\no\u000bensive comments to the most aggressive,\nthe latter being those comments that incite\nhate or even physical violence). We use tox-\nicity as an umbrella term under which we\ninclude di\u000berent de\fnitions used in the lit-\nerature to describe hate speech ((Nockleby,\n2000), (Waseem and Hovy, 2016), (Schmidt\nand Wiegand, 2017), (Davidson et al., 2017))\nand abusive (Nobata et al., 2016), aggres-\nsive (Kumar et al., 2018), toxic (Kolhatkar et\n6https://www.meneame.net/\n7https://www.forocoches.com/\nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso\n210al., 2020) or o\u000bensive ( ?) language. In fact,\nthese di\u000berent terms address di\u000berent aspects\nof toxic language (Poletto et al., 2020).\nWe annotated each comment in two\ncategories `toxic' and `not toxic', and\nthen we assigned di\u000berent levels of tox-\nicity: `toxicity level 0=not toxic' (1),\n`toxicity level 1=mildly toxic' (2), `toxi-\ncitylevel 2=toxic'(example 3) or `toxic-\nitylevel 3=very toxic' (4) to those that were\nannotated \frst as toxic8.\n(1a) Contra la desinformaci\u0013 on datos. En el\nINE ten\u0013 eis los datos de los inmigrantes.\nDelitos, cotizaciones, prestaciones, etc.\n<toxicity=not> <toxicity level=0>\n(1b) Against misleading data. In the\nINE you have data for immigrants.\nCrimes, contributions, subsidies, etc.\n<toxicity=not> <toxicity level=0>\n(2a) Esta gentuza se carga al pa\u0013 \u0010s en dos tele-\ndiarios\n<toxicity=yes > <toxicity level=1>\n(2b) This rabble could destroy the coun-\ntry with two TV news programs\n<toxicity=yes > <toxicity level=1>\n(3a) Lo que importa en realidad es sentirse\nmejor con uno mismo. Sumar karma. A\nlos putos negros les pueden joder bien.\n<toxicity=yes > <toxicity level=2>\n(3b) What really matters is feeling better\nabout yourself. Accumulating karma.\nFucking blacks can get fucked up pretty\ngood.\n<toxicity=yes > <toxicity level=2>\n(4a) A estos putos animales sarnosos\nque los encierren y tiren la llave.\n<toxicity=yes > <toxicity level=3>\n(4b) With these mangy fucking animals, lock\nthem up and throw away the key.\n<toxicity=yes > <toxicity level=3>\nIn addition to annotating whether or not\na comment was toxic and its level of toxic-\nity, we also annotated the following features:\nargumentation, constructiveness, stance, tar-\nget, stereotype, sarcasm, mockery, insult, im-\nproper language, aggressiveness and intoler-\nance. All these features have binary val-\nues except the toxicity level (the value `0'\n8The examples contain text that may be consid-\nered o\u000bensive.is assigned for indicating the absence of the\ncorresponding feature in the comment, and\nthe value `1' is assigned when the feature\nis present). We \frst annotated these fea-\ntures and we then used them to establish the\ntoxicity of comments and to determine their\nlevel of toxicity. It is worth noting that the\nlevel of toxicity is especially determined by\nthe type of features combined. All this in-\nformation was included only in the training\ndataset that was used for the task. In or-\nder to assign each of these features and be\nable to interpret the global meaning of com-\nments, it was crucial to take the context into\naccount, that is, the conversational thread.\nFor instance, the conversational threat was\nvery useful for interpreting and annotating\nsarcasm, mockery and stance. The identi\fer\nof each conversation thread (`thread id' fea-\nture) was also provided for the participants,\nas well as the identi\fer of the previous com-\nment in the thread (`reply to' feature) and\nthe `comment level' feature. The latter is a\ncategorical attribute with two possible val-\nues: `1' for indicating that the comment is\na direct comment to an article and `2' for\nindicating that the comment is a reply to\nanother comment. The `topic' feature was\nalso provided in the training dataset. This\nfeature has three possible values (CR=crime,\nMI=migration, SO=social) to distinguish the\ntopic of the news article.\n3.2 Annotation Process\nEach comment was annotated in parallel\nby three annotators and an inter-annotator\nagreement test was carried out once all the\ncomments on each article had been anno-\ntated. Then, disagreements were discussed\nby the annotators and a senior annotator\nuntil agreement was reached. The team of\nannotators involved in the task consisted of\ntwo expert linguists and two trained anno-\ntators, who were students of linguistics. Ta-\nble 1 shows the results obtained in the inter-\nannotator agreement test, the average ob-\nserved percentage of agreement between the\nthree annotators, 81.99% for toxicity and\n73.95% for toxicity level, and Krippenddorfs'\nalpha (0.59 and 0.62 respectively), which en-\nsures annotation reliability. The results ob-\ntained are quite acceptable considering the\ndi\u000eculty of both tasks.\nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish\n211Feature Average observed agreement Krippendor\u000b's \u000b\nToxicity 81.99% 0.59\nToxicity level 73.95% 0.62\nTable 1: Inter-Annotator Agreement Test.\n3.3 Training and Test Dataset\nWe provided participants with 80% of the\nNewsCom-TOX corpus for training their\nmodels (3,463 comments), and the remain-\ning 20% of the corpus (896 comments) was\nused for testing their models.9Both train-\ning and test \fles were provided in csvfor-\nmat. The training dataset contains all the\nfeatures included in the annotation of the\nNewsCom-TOX corpus (see Subsection 3.1),\nwhereas the test dataset only contains the\nfollowing features: `thread id', `comment id',\n`reply to', `comment level' and `comment'.\nTable 2 shows the distribution of toxic\ncomments by toxicity level. The dataset con-\nsists of 4,359 comments, of which 1,385 are\ntoxic (31.16%): 996 mildly toxic (21.90%),\n310 toxic (7.36%) and 79 very toxic (1.81% ).\n4 Systems and Results\nThis section contains a brief description of\nthe baselines provided as a benchmark, fol-\nlowed by an overview of both subtasks (tox-\nicity detection and toxicity level detection)\nmentioning the reasons behind the selection\nof the evaluation metrics and their implica-\ntion when models are compared. Finally,\nsome interesting systems insights and a brief\nerror analysis of the submitted approaches\nare presented.\n4.1 Baselines\nA benchmark was set with the introduction of\nthree di\u000berent baselines: RandomClassi\fer,\nBOWClassi\fer and ChainBOW. First, Ran-\ndomClassi\fer assigns a random of toxicity\nfrom four possible values f0, 1, 2, 3g to each\ncomment in the test set without any kind\nof weighting strategy. Second, the BOW-\nClassi\fer consists of a simple Support Vector\nClassi\fer (SVC) that receives the features ex-\ntracted by a TF-IDF Vectorizer (an advanced\nversion of the classical Bag-Of-Words tech-\nnique). In particular, the SVC model uses\n9In order to avoid any con\rict with the sources\nof comments regarding their Intellectual Property\nRights (IPR), the data was privately sent to each par-\nticipant that was interested in the task. The corpus\nwill be only available for research purposes.a linear kernel with a regularization param-\neter equal to 1.0 and a \\one-versus-rest\" de-\ncision function strategy for the toxicity level\ndetection subtask. Furthermore, the TF-IDF\nVectorizer, which is responsible for feature\nextraction, constructs a vocabulary of 5,000\nentries including unigrams and bigrams af-\nter performing a simple preprocessing stage\n(lowercasing and removal of accents). Fi-\nnally, due to the fact of having an unbalanced\ndataset, we decided to include a baseline that\nprocesses both subtasks sequentially. There-\nfore, ChainBOW contains two BOWClassi-\n\fers, one for each subtask (toxicity detection\nand toxicity level detection), connected in a\nhierarchical fashion with the same con\fgura-\ntion as mentioned before. It is worth men-\ntioning that, given their baseline nature, no\nhyperparameter optimization was performed\non any of the models. In addition, all base-\nlines were implemented using the Python\nscikit-learn library.\n4.2 Subtask 1: Toxicity Detection\nTask\nSubtask 1 consists of a binary classi\fcation\n(toxic vs. non-toxic). In this subtask, the\nmetrics precision, recall and their combina-\ntion by means of the F-measure were used.\nTable 5 ranks the best run of each of the par-\nticipating teams according to F-measure. In\ngeneral, the SINAI system outperforms the\nrest of systems. SINAI is outperformed by\nsome of the other approaches in terms of\nrecall, although at the cost of a signi\fcant\nprecision loss. In relation to the baselines,\nRandomClassi\fer achieves a mid-ranking po-\nsition in the ranking, with low precision but\nmedium high recall. In a similar position\nis ChainBOW, with low recall and medium\nhigh precision. In general, there is signi\fcant\nroom for improvement between participants\nruns and the Gold Standard.\nFigure 1 illustrates the precision and recall\nscores achieved by the systems. The precision\nof all systems is between 0.25 and 0.75.\nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso\n212Feature Comments Percentage\nmildly toxic (level 1) 996 21.90%\ntoxic (level 2) 310 7.36%\nvery toxic (level 3) 79 1.81%\nTotal 1,385 31.16%\nTable 2: Distribution of toxic comments.\nFigure 1: Precision and Recall for the best\nrun of each team in Subtask 1.\n4.3 Subtask 2: Toxicity Level\nDetection Task\nRegarding Subtask 2, the toxicity level detec-\ntion task is a more \fne grained classi\fcation\ntask in which the aim is to identify the level\nof toxicity of a comment (0= not toxic; 1=\nmildly toxic; 2= toxic and 3: very toxic). For\nthis task we considered four evaluation met-\nrics.\nClosseness Evaluation Metric (CEM):\nThe metric CEM (Amig\u0013 o et al., 2020)\nconsiders the proximity between pre-\ndicted and real categories. Unlike\nmeasures based on absolute error, CEM\nhas two particularities. First, it assumes\nno equidistance between categories.\nThat is, an error between category 0\nand 2 is not necessarily twice as serious\nas an error between categories 0 and\n1. In addition, it assigns more weight\nto infrequent categories. This avoids\nover-weighting systems that tend to\nclassify items in majority classes. CEM\nis appropriate for unbalanced data sets,\nand when the relative distance between\ncategories beyond the way they are\nordered is unknown.\nRank Biased Precision (RBP): RBP is\na ranking metric (Mo\u000bat and Zobel,2008). We rank the output of the sys-\ntem and the Gold Standard on the basis\nof toxicity levels (from highest to lowest)\nand compare the two rankings. Basi-\ncally, RBP is computed as the sum of the\nactual values (Gold Standard) along the\nranking generated by the system with a\nweight function that decreases as we de-\ncay in the ranking of the system output.\ndcorresponds to the categorized items in\nthe system output s, withpos(d) being\nthe ranking position of din the system\noutputsandg(d) being the real cate-\ngory value in the Gold Standard for this\nitem. RBP is computed as:\nRBP = (1\u0000p)X\ndf(d)g(d)\nwherefis a decay function that\ndecreases with position i, concretely\nf(d) =ppos(i)\u00001. The value pis a param-\neter which is typically \fxed at X. Origi-\nnally, RBP applies to rankings with one\nitem in each ranking position. However,\nin our scenario, the items are ordered\nin four levels in the ranking generated\nby the system (level 4= not toxic; level\n3= mildly toxic; level 2= toxic and level\n1: very toxic). Therefore, we modify\nRBP by considering the average iposi-\ntion that each item would occupy in the\nsystem output if we were to randomly\norder the ties:\nf(d) =1\nns(d)MaxPos(d)X\ni=MinPos(d)pi\u00001\nwherens(d)represents the number of\nitems at level s(d) in the system out-\nput.MinPos(d) and MaxPos (d) repre-\nsent the minimum and maximum posi-\ntion that item dcould occupy in the sys-\ntem output saccording to its level. This\nmetric is appropriate when sorting items\naccording to their toxicity. For instance,\nthe scenario in which the user needs to\nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish\n213\fnd the most toxic comments \fts in this\nmetric.\nAccuracy: Accuracy is the most popular\nclassi\fcation metric. This metric does\nnot consider the order between cate-\ngories. That is, an error is penalised\nregardless of the distance to the actual\ncategory. Furthermore, it does not com-\npensate for the e\u000bect of imbalance in the\ndata set. This metric is not appropriate\nfor most possible scenarios, although it\nhas the advantage of being very easy to\ninterpret in terms of the percentage of\nhits.\nMean Average Error (MAE): MAE av-\nerages the absolute di\u000berence between\npredicted and actual categories. This\nmetric assumes equidistance between\ncategories and does not compensate for\nthe e\u000bect of imbalance between cate-\ngories in the data set. MAE is appro-\npriate, for example, when predicting the\naverage toxicity value in a comment set.\nNote this requires assuming numerical\nintervals between categories.\nPearson Coe\u000ecient: Like MAE, using\nPearson coe\u000ecient requires assuming\nequidistance between categories. The\ndi\u000berence with respect to MAE is that\nit does not require the system to predict\nthe category of each item, but gener-\nates a scale that is linearly correlated\nwith the actual scale. In addition, it\ncompensates for the e\u000bect of imbalance\nbetween categories by giving more\nweight to those categories that are more\ninfrequent. Obtaining a high Pearson\nvalue is interesting, for example, when\npredicting the evolution of toxicity over\ntime in a comment stream or compare\nthe average toxicity of two streams.\nTable 6 shows the results obtained by the\nbest run of each team in terms of CEM. The\nbaselines are in the middle positions of the\nsystems ranking. In this case, the BOW-\nClassi\fer approach performs better than the\nother baselines developed by the task organ-\nisers.\nFigure 2 illustrates the CEM results vs.\nother evaluation metrics. In this subtask,\nthe systems SINAI and Team Sabari outper-\nform most of the other approaches for all\nmetrics. There is only one exception. Thesystem output DCG outperforms SINAI and\nSabari by a wide margin in terms of ranking\n(RBP metric). This means that, in a scenario\nwhere the objective is to prioritise particu-\nlarly toxic comments, DCG is more e\u000bective\nthan SINAI.\nIn Figure 2, we \fnd a high correspondence\nbetween CEM and the Pearson correlation\ncoe\u000ecient. CEM does not consider numeric\nintervals. The Pearson coe\u000ecient does not\ntake into account the proximity of the esti-\nmated categories to the actual ones but the\ncorrelation between the values (numeric cat-\negory labels) in the system output and the\ngold standard. This means that the e\u000bect\nof 'scale shifts' and the e\u000bect of assuming\nnumeric intervals between categories is not\nparticularly relevant in this evaluation bench-\nmark.\nHowever, we did not \fnd as much corre-\nspondence between the CEM metric and the\nMAE and Accuracy metrics. The main dif-\nference is that CEM, like Pearson, compen-\nsates for the imbalance between categories in\nthe data set, giving more weight to errors\nor hits in infrequent categories. As Figure 2\nshows, there is a set of runs that obtain simi-\nlar Accuracy and MAE values, but neverthe-\nless present important di\u000berences in terms of\nCEM. From a usage scenario perspective, this\nresult indicates that these runs are compara-\nble if we are interested in predicting or ap-\nproximating the actual category in as many\ncases as possible. For example, this would be\nthe case if we wanted to calculate the average\ntoxicity of a set of comments. However, if we\nare interested in detecting particular cases of\ntoxicity (low, medium or high) then we can\nassert that some of these runs will be more\ne\u000bective than others.\n4.4 Systems Insights\nA total of 31 teams (Subtask 1) and 24\nteams (Subtask 2) sent a maximum of\n\fve submissions per subtask to be evalu-\nated. Not surprisingly, and based on the\nrecent success of transfer learning with pre-\ntrained Transformer-like language models,\nthe top \fve teams in both subtasks achieved\ntheir best scores using BETO10(the Span-\nish version of the BERT model). The dif-\nference in performance between their re-\nspective submissions thus lies in the \fne-\ntuning techniques, data augmentation strat-\n10https://github.com/dccuchile/beto\nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso\n214Figure 2: Correspondence between CEM and other metrics in Subtask 2.\negy and/or preprocessing steps. Although\nthe performance of classical machine learn-\ning models such as TF-IDF with Random\nForests/Support Vector Machines/Logistic\nRegression Classi\fers were not as high as\nthose achieved by BETO, they were also used\nas part of ensemble architectures and ana-\nlyzed by multiple participants. In fact, sev-\neral teams provided valuable insights and\ncomparisons of models for a task that is in-\ncredibly challenging, even for trained expert\nannotators. The most interesting conclusions\nare summed up below.\nThis competition introduced several chal-\nlenges in addition to the toxicity detection\nproblem. First, the language of the dataset\n(comments in Spanish) di\u000bers from the usual\nEnglish language present in most of the gen-\neral benchmark NLP datasets. Therefore,\nthe models needed to account for the di\u000berent\ndirectionality of the text, as well as for the\nselection of other pre-processing steps. For\ninstance, the GTH-UPM team performed an\nanalysis of the multiple pre-processing steps\napplied prior to a pre-trained Transformer-\nlike model (BETO). Apparently, a basic\ntext normalization (the replacement of spe-\ncial Named Entities such as MAIL, DATE,\nURL... with their respective shared token)\nimproves BETO's performance as opposed\nto other steps e.g. the removal of stop-\nwords and punctuation or lemmatization.GTH-UPM also validated the extension of\npre-trained models' vocabulary with domain-\nspeci\fc terms as a useful technique to im-\nprove task-oriented results.\nAnother analysis that is worth paying\nattention to regarding the language of the\ntexts is a performance comparison among\npretrained multilingual models, English mod-\nels with a proper Spanish-to-English transla-\ntor and Spanish models. Alejandro Mosquera\nbuilt a stacked model composed of at least\none model for each variant in conjunction\nwith additional features and extracted the\nfeature importance of the individual models\nvia cross-validation on the training dataset.\nThe results show that pretrained multilin-\ngual models such as XLM do not provide as\nmuch predictive power in this toxicity context\nas a neural network with a capsule network\narchitecture using SBWC i25 GloVe Span-\nish embeddings. However, the di\u000berence is\nnot so pronounced when we compare it with\nthe translator followed by the model trained\non English embeddings. A similar compari-\nson was performed by the AI-UPV team task\nin which classical machine learning models\n(both generative and discriminative), multi-\nlingual BERT and BETO performed cross-\nvalidation with a hyperparameter setup. Not\nonly did they conclude that transformer-like\narchitectures outperform classical statistical\nmodels in this complex task, but they also\nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish\n215found that BETO achieved substantially bet-\nter results than multilingual BERT for all of\ntheir top \fve best con\fgurations and in both\nsubtasks.\nFurthermore, Alejandro Mosquera's was\nthe only team to introduce side informa-\ntion related to the topic of the articles each\ncomment refers to and the thread to which\nthey belong. As a preliminary analysis\nusing cross-validation, the use of informa-\ntion related to the comments present in the\nthread/conversation seems to enrich overall\nmodel performance. Further research on how\nto better exploit thread and topic informa-\ntion may be an interesting and promising di-\nrection.\nLast but not least, the winners of both\nDETOXIS subtasks (SINAI team proved the\nimportance of enriching the model by \fne-\ntuning it on similar tasks related to senti-\nment and emotion analysis, thus increasing\nthe available training data and making more\nprecise predictions thanks to this Multi-Task\nLearning strategy. In fact, SINAI was the\nonly team in the competition that took the\nprovided extra features (see Section 3.1) into\naccount, thereby giving their systems an im-\nportant boost in comparison to the other par-\nticipants. However, a study of the in\ruence\nof each extra feature on the model's predic-\ntive power is yet to be performed.\n4.5 Error Analysis\nThe tasks of toxicity and toxicity level de-\ntection are particularly di\u000ecult for machine\nlearning models. In fact, even the most re-\ncent transformer models struggle with lin-\nguistic devices users sometimes use such as\nsarcasm or irony. However, thanks to the\nfact that the dataset includes 13 additional\nfeatures that classify the text according to\nother helpful semantic dimensions from the\nraw text, we can easily identify the most chal-\nlenging comments and the possible reasons\nbehind the di\u000eculty of their detection. Ta-\nble 3 contains the average performance of the\ntop \fve best submissions per task (based on\nthe F1-score and CEM metrics respectively)\nconsidering the subset of samples where each\nfeature is present and a single (best) run per\nteam.\nRegarding the \frst task (toxicity detec-\ntion), the o\u000ecial metric focuses on the abil-\nity of the models to detect all toxic com-\nments and pays zero attention to the neg-ative class. Therefore, comments with ar-\ngumentative cues (generally linked to non-\ntoxic comments) that are marked as toxic\nare usually the most di\u000ecult ones to detect\n(see examples 5, 8, 10, 13 and 14 in Table\n4). A similar situation appears in comments\nwith a positive stance that are actually toxic.\nOther dimensions that clearly increase the\ndi\u000eculty of this task are sarcasm andstereo-\ntype. The existence of a greater number of\nimplicit stereotypes rather than explicit ones\nin hate speech posts which ultimately require\nmodels to learn related world knowledge be-\nforehand is well known. Furthermore, this\ndataset contains comments that are replies\nto other users' comment. Consequently, a\nbroader context than just the current text\nis sometimes required to make an informed\ndecision on whether the comment is toxic or\nnot.\nOn the other hand, the di\u000eculty of lin-\nguistic cues related to aggressiveness, mock-\nery, sarcasm and stereotype is even clearer in\nthe task of toxicity level detection when CEM\nscores are considered. Even though a com-\nment may not apparently be very toxic ac-\ncording to the explicit words in the text, im-\nplicit messages and emitter intention can be\nreally harmful for the receiver. Consequently,\nan e\u000bort to build automatic systems able to\ncapture toxicity levels in comments in which\nsuch subtle hidden messages are included is\nof utmost importance. For instance, example\n12 is usually marked as non-toxic by the top\nperforming submitted systems although the\nactual toxicity is at its maximum level due\nto the implicit aggressiveness within the mes-\nsage. This di\u000berence between the predicted\nand ground truth labels is highly penalized\nby the o\u000ecial CEM metric.\n5 Conclusions and Future Work\nThis paper has described the DETOXIS chal-\nlenge at IberLEF 2021 and summarized the\nparticipation of several teams in both sub-\ntasks which evidenced interesting approaches\nand conclusions. Although all of the top-\nperforming systems made use of the Spanish\nversion of the BERT model (a Transformer\nencoder), a variety of insights into the impor-\ntance of di\u000berent strategies become clearer\nfor researchers to build upon or explore fur-\nther in their respective studies on hate speech\nand toxicity detection. As opposed to pre-\nvious challenges on hate speech, we have\nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso\n216Feature Size (Toxic) Subtask 1 Subtask 2\n1. argumentation 459 (113) 0.6006 0.7591\n2. constructiveness 282 (0) - 0.8929\n3. positive stance 22 (7) 0.6545 0.8465\n4. negative stance 252 (125) 0.6919 0.6764\n5. target person 108 (90) 0.7243 0.5672\n6. target group 67 (62) 0.7945 0.5444\n7. stereotype 41 (38) 0.6938 0.5110\n8. sarcasm 52 (42) 0.6653 0.5497\n9. mockery 80 (77) 0.7443 0.4824\n10. insult 85 (83) 0.8541 0.5202\n11. improper language 74 (51) 0.7985 0.6542\n12. aggressiveness 15 (15) 0.7091 0.3892\n13. intolerance 15 (13) 0.8516 0.5655\nTable 3: Average score (F1-score and CEM respectively) of the \fve best performing teams in\neach task for the subset of comments where the corresponding feature is positive together with\nthe size of each subset and the number of toxic instances in them.\nprovided 13 additional features and a \fne-\ngrained toxicity degree target with four pos-\nsible labels that go beyond the classical bi-\nnary toxicity classi\fcation. Furthermore, the\ncollection of comments includes the thread to\nwhich they belong and the topic of the article\nthey are posted in, thus, allowing a broader\ncontext to be used by innovative solutions.\nUnsurprisingly, multilingual models are\noutperformed by the Spanish counterparts\nin this challenge most probably due to the\nspeci\fc embedding space fully optimized in\nthe given language and the use of a more\nlanguage-oriented token vocabulary. Tech-\nniques such as data augmentation with\nMasked Language Modeling and the addition\nof Multi-Task Learning with datasets belong-\ning to similar tasks in order to increase the\navailable data used at the \fnetuning step has\nturned out to be bene\fcial in a scenario in\nwhich the number of comments is small and\nclasses were unbalanced. Moreover, despite\nthe fact that further analysis and more elab-\norate ways to extract information from the\nconversation and the topic are needed, the\nonly team that took advantage of such infor-\nmation achieved positive results in the \fnal\noutcome by combining them with other com-\nmon strategies.\nFinally, it is important to mention that,\ndepending on the \fnal application and its\nspeci\fc requirements, the selection of the\nmodel may require a di\u000berent evaluation met-\nric. Thanks to our selection of metrics, we\nwere able to provide a simple visualization of\ndi\u000berent trade-o\u000bs when opting for a certainsystem and how models perfectly suitable for\na particular goal may not perform at the\nsame level when those requisites change. A\nclear example we have presented is the com-\nparison between the SINAI and DCG sys-\ntems, in which the former was better at de-\ntecting all toxicity levels according to an or-\ndinal classi\fcation metric such as CEM but\nwas outperformed by the latter in scenarios in\nwhich prioritizing the most toxic comments is\nthe priority (according to the RBP ranking\nmetric).\nRegarding future work, systems are as yet\nfar from the Gold Standard and this is mainly\ndue to the di\u000eculty these models encounter\nwhen detecting implicit features such as ag-\ngressiveness, mockery and sarcasm. More-\nover, the lack of examples corresponding to\neach individual feature makes this an even\nmore challenging task. Additional research\non the in\ruence of these implicit features on\nthe \fnal toxicity of the comment and the cre-\nation of systems able to detect and mitigate\nsuch subtle cues is highly necessary in a so-\nciety in which messages and ideas are spread\nfaster than ever. Some of the paths that were\nnot explored by participants and could be\nworth looking into are: larger language mod-\nels or possible distillations of them, improve-\nments to the semantic contextualization of\nlanguage models by the use of techniques that\nreduce their anisotropicity problem and the\nincorporation of sentence encoders or compo-\nsitional representations to the ensemble ar-\nchitectures.\nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish\n217ID Features FN Ratio Toxicity Comment\n1 8 1.0000 1 Como se echa de menos la opini\u0013 on de los guardianes de la moral\nen la noticia de la ejecuci\u0013 on en Portland. Bueno, en muchos casos\nest\u0013 an sus negativos pero no sus siempre dignos comentarios.\n2 10, 11 1.0000 1 Directamente :peineta:\n3 4 1.0000 1 Los andaluces, no los espa~ noles\n4 - 0.9677 1 Y 600000 votos m\u0013 as, para el pandemias.\n5 1, 4, 12 0.9677 1 No intentaba poner ni a favor ni en contra. Ni intentaba realizar\nuna cr\u0013 \u0010tica en el origen de los EEUU. Mi comentario solo trataba\nde dar perspectiva del problema en USA. En mi opini\u0013 on personal\ntodos los usanos con armas podr\u0013 \u0010an salir a la calle hasta matarse\nlos unos a los otros que el mundo en un plazo m\u0013 as corto que largo\nir\u0013 \u0010a a mejor. Pero es mi opini\u0013 on.\n6 3, 4, 5, 10 0.9677 1 Que f\u0013 acil es hablar bocachancla\n7 5 0.9677 1 Expulsion de echenique de espa~ na ya1\n8 1, 4, 6, 9, 10 0.9355 1 En Madrid no hay campo, no hablo de la p\u0013 urria de ciudad. Hablo\nde la gente que todo los a~ nos entra en Espa~ na para hacer estos\ntrabajos estacionales + los que est\u0013 an siempre aqu\u0013 \u0010 y que se han\nido\n9 4, 11 0.9355 1 Como conseguir 600.000 votos. Vomitivo\n10 1, 5 0.9355 1 El segundo es el que le tira al suelo, pero mira el primero tambi\u0013 en,\n\fgura. youtu.be/neUnhYO2Ehc\n11 6, 7 0.9355 1 Pues m\u0013 as raz\u0013 on para hacer como ellos contra ellos. La diferencia es\nque nosotros sabemos que esta mal, ellos creen que es lo correcto.\nNosotros podemos parar cuando lo estimemos, ellos no.\n12 12 0.9355 3 Tengo esperanzas en que legalicen la noche de 'La Purga'. La peli\ncuenta como las clases acomodadas impulsan esta celebraci\u0013 on para\nque el lumpen se autorregule. Es sencilla, pero a mi me entretuvo\nmucho : )\n13 1, 4, 5, 9 0.9355 1 Echenique miente mas que corre....aun regularizando estos irreg-\nulares, no serian de hecho \"ciudadanos espa~ noles\" y por lo tanto\nSIN derecho a voto. Pura propaganda bolchevique.\n14 1, 4, 5 0.9355 1 ss. 29.304 simplemente exige que tengas licencia de caza (que\nno se sabe publicamente si el implicado la tiene o no) [...] Ergo,\npodemos deducir que el acusado no estaba violando la 948.60(2)(a)\nal usar un arma larga con 17 a~ nos. Hay que hacer los deberes\nantes de soltar a\frmaciones tan categ\u0013 oricas. O sea que necesita\nuna licencia de caza y como no sabemos si la tiene o no vamos\na suponer que SI la tiene pero el que tiene que hacer los deberas\nantes de hacer a\frmaciones categ\u0013 oricas soy yo. Pos fueno, pos fale,\npos malegro, campe\u0013 on. al parecer al \fnal no va a haber acusaci\u0013 on\npor la tenencia de armas, >Fuente?\n15 5, 8 0.9355 2 Si le hacen tantas pregunta a \"NUMERO 3\", es posible que se\ncortocircuite?\n16 4, 13 0.8710 1 Deportaciones masivas ya!\nTable 4: List of the top 16 comments in the test set that are misclassi\fed as non-toxic by the\nmajority of the systems. The Features column shows the feature identi\fers from Table 3 marked\nas positive for the given comment, FN Rate is the ratio of systems that marked the comment\nas non-toxic and Toxicity refers to the four-levels annotation for that speci\fc comment.\nAcknowledgments\nThe work has been carried out in the\nframework of the following projects:\nMISMIS project (PGC2018-096212-B),\nfunded by Ministerio de Ciencia, Inno-\nvaci\u0013 on y Universidades (Spain), CLiC SGR\n(2027SGR341), funded by AGAUR (Gener-\nalitat de Catalunya) and STERHEOTYPES\nproject (Challenges for Europe), funded by\nFondazione Compangia di San Paolo.References\nAmig\u0013 o, E., J. Gonzalo, S. Mizzaro, and\nJ. Carrillo-de Albornoz. 2020. E\u000bec-\ntiveness metric for ordinal classi\fcation:\nFormal properties and experimental re-\nsults. In Proceedings of the 58th Annual\nMeeting of the Association for Computa-\ntional Linguistics. Association for Compu-\ntational Linguistics.\nBasile, V., C. Bosco, E. Fersini, N. Deb-\nora, V. Patti, F. M. R. Pardo, P. Rosso,\nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso\n218M. Sanguinetti, et al. 2019. Semeval-\n2019 task 5: Multilingual detection of hate\nspeech against immigrants and women in\ntwitter. In 13th International Workshop\non Semantic Evaluation , pages 54{63. As-\nsociation for Computational Linguistics.\nDavidson, T., D. Warmsley, M. Macy, and\nI. Weber. 2017. Automated hate speech\ndetection and the problem of o\u000bensive lan-\nguage. In Proceedings of the International\nAAAI Conference on Web and Social Me-\ndia, volume 11.\nKolhatkar, V., H. Wu, L. Cavasso, E. Fran-\ncis, K. Shukla, and M. Taboada. 2020.\nThe sfu opinion and comments corpus: A\ncorpus for the analysis of online news com-\nments. Corpus Pragmatics , 4(2):155{190.\nKumar, R., A. K. Ojha, S. Malmasi, and\nM. Zampieri. 2018. Benchmarking ag-\ngression identi\fcation in social media.\nInProceedings of the First Workshop\non Trolling, Aggression and Cyberbullying\n(TRAC-2018), pages 1{11.\nKumar, R., A. K. Ojha, S. Malmasi, and\nM. Zampieri. 2020. Evaluating aggression\nidenti\fcation in social media. In Proceed-\nings of the Second Workshop on Trolling,\nAggression and Cyberbullying, pages 1{5.\nMo\u000bat, A. and J. Zobel. 2008. Rank-biased\nprecision for measurement of retrieval ef-\nfectiveness. ACM Transactions on Infor-\nmation Systems (TOIS), 27(1):1{27.\nNobata, C., J. Tetreault, A. Thomas,\nY. Mehdad, and Y. Chang. 2016. Abu-\nsive language detection in online user con-\ntent. In Proceedings of the 25th interna-\ntional conference on world wide web, pages\n145{153.\nNockleby, J. T. 2000. Hate speech. En-\ncyclopedia of the American constitution ,\n3(2):1277{1279.\nPoletto, F., V. Basile, M. Sanguinetti,\nC. Bosco, and V. Patti. 2020. Resources\nand benchmark corpora for hate speech\ndetection: a systematic review. Language\nResources and Evaluation , pages 1{47.\nSchmidt, A. and M. Wiegand. 2017. A sur-\nvey on hate speech detection using nat-\nural language processing. In Proceedings\nof the \ffth international workshop on nat-\nural language processing for social media ,\npages 1{10.Stru\u0019, J. M., M. Siegel, J. Ruppenhofer,\nM. Wiegand, M. Klenner, et al. 2019.\nOverview of germeval task 2, 2019 shared\ntask on the identi\fcation of o\u000bensive lan-\nguage. In Preliminary proceedings of\nthe 15th Conference on Natural Language\nProcessing (KONVENS 2019).\nWaseem, Z. and D. Hovy. 2016. Hateful\nsymbols or hateful people? predictive fea-\ntures for hate speech detection on twitter.\nInProceedings of the NAACL student re-\nsearch workshop, pages 88{93.\nZampieri, M., P. Nakov, S. Rosenthal,\nP. Atanasova, G. Karadzhov, H. Mubarak,\nL. Derczynski, Z. Pitenis, and C \u0018 . C \u0018 \u007f oltekin.\n2020. SemEval-2020 task 12: Multilingual\no\u000bensive language identi\fcation in social\nmedia (O\u000bensEval 2020). In Proceedings\nof the Fourteenth Workshop on Seman-\ntic Evaluation, pages 1425{1447. Interna-\ntional Committee for Computational Lin-\nguistics.\nA Appendix: Results for both\nSubtasks\nThis appendix provides the results obtained\nby each team, selecting their best scoring out-\nput, for both DETOXIS subtasks. Table 5\nranks the best run of each of the participat-\ning teams according to F-measure. Table 6\nshows the results obtained by the best run of\neach team in terms of CEM (column 3). The\nrest of columns show the results in terms of\nMAE, RBP, Pearson and Accuracy metrics.\nThe baselines are in the middle positions of\nthe systems ranking.\nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish\n219Ranking Team F-measure Precision Recall\n1 SINAI 0.6461 0.6356 0.6569\n2 GuillemGSubies 0.6000 0.5234 0.7029\n3 AI-UPV 0.5996 0.5672 0.6360\n4 DCG 0.5734 0.6225 0.5314\n5 GTH-UPM 0.5726 0.5852 0.5607\n6 alejandro mosquera 0.5691 0.4688 0.7238\n7 Team Sabari 0.5646 0.5379 0.5941\n8 maia 0.5570 0.4904 0.6444\n9 address 0.4930 0.4697 0.5188\n10 Javier Garc\u0013 \u0010a Gilabert 0.4911 0.3644 0.7531\n11 Dembo 0.4632 0.4798 0.4477\n12 ToxicityAnalizers 0.4562 0.4444 0.4686\n13 DetoxisLuciaYNora 0.4529 0.4346 0.4728\n14 YaizaMi~ nana 0.4449 0.4077 0.4895\n15 VG UPV 0.4377 0.3348 0.6318\n16 Datacism 0.4235 0.4839 0.3766\n17 Calabacines c\u0013 osmicos 0.4065 0.4603 0.364\n18 Ivan iJaume 0.3991 0.4194 0.3808\n19 CarlesyJorge 0.3844 0.3973 0.3724\n20 NEON2 0.3798 0.4463 0.3305\nRandomClassi\fer 0.3761 0.2540 0.7238\nChainBOW 0.3747 0.5071 0.2971\n21 JOSAND10 0.3636 0.5323 0.2762\n22 GCP 0.3535 0.4459 0.2929\n23 LlunaPerez-J\u0013 uliaGregori 0.3017 0.2806 0.3264\n24 GalAgo 0.2982 0.2841 0.3138\n25 Just Do It 0.2060 0.5000 0.1297\nBOW Classi\fer 0.1837 0.5909 0.1088\n26 Benlloch 0.1828 0.2705 0.1381\n27 LNR SaraSoto ClaraSalelles 0.1637 0.5476 0.0962\n28 LNR I~ nigoPicasarri JoanCastillo 0.1637 0.5476 0.0962\n29 ElenaLopez MartaGarcia LNR 0.1605 0.4000 0.1004\nWord2VecSpacy 0.1523 0.3651 0.0962\n30 Iker&Miguel 0.0405 0.6250 0.0209\n31 JOREST 0.0246 0.6000 0.0126\nTable 5: Results of Subtask 1.\nMariona Taul\u00e9, Alejandro Ariza, Montserrat Nofre, Enrique amig\u00f3, Paolo Rosso\n220Ranking Team CEM MAE RBP Pearson Accuracy\nGold Standard 1 0 0.8213 1 1\n1 SINAI 0.7495 0.2626 0.2612 0.4957 0.7654\n2 Team Sabari 0.7428 0.2795 0.2670 0.5014 0.7464\n3 DCG 0.7300 0.3019 0.3925 0.4544 0.7329\n4 GTH-UPM 0.7256 0.3019 0.1545 0.4298 0.7318\n5 GuillemGSubies 0.7189 0.349 0.2449 0.4451 0.6835\n6 AI-UPV 0.7142 0.3143 0.2101 0.4734 0.7127\n7 address 0.6915 0.3165 0.1136 0.3215 0.7228\n8 Dembo 0.6703 0.3468 0.1037 0.2677 0.6936\nChainBOW 0.6535 0.3389 0.0787 0.2077 0.7127\n9 Javier Garc\u0013 \u0010a Gilabert 0.6514 0.3345 0.2773 0.2158 0.7250\n10 JOSAND10 0.6424 0.3367 0.1306 0.2046 0.7239\n11 ToxicityAnalizers 0.6332 0.4871 0.0709 0.1805 0.6139\n12 NEON2 0.6324 0.3434 0.1339 0.1632 0.7116\nBOWClassi\fer 0.6318 0.3266 0.1657 0.1688 0.7329\n13 JOREST 0.6250 0.3389 0.0592 0.0972 0.7273\n14 Iker&Miguel 0.6250 0.3389 0.0592 0.0972 0.7273\n15 Ivan iJaume 0.6201 0.4366 0.0844 0.1604 0.6251\n16 DetoxisLuciaYNora 0.6200 0.3816 0.0903 0.1248 0.6869\nWord2VecSpacy 0.6116 0.3928 0.0855 0.0566 0.7015\nGloVeSBWC 0.6111 0.3356 0.1085 0.0623 0.7318\n17 CarlesyJorge 0.6094 0.3367 0.0535 NaN 0.7318\n18 ElenaLopez MartaGarcia LNR 0.6064 0.3490 0.0294 -0.0153 0.7217\n19 VG UPV 0.6041 0.6498 0.1224 0.1896 0.5589\n20 Just Do It 0.5928 0.4579 0.0320 0.0195 0.6207\n21 Benlloch 0.5913 0.4545 0.0711 0.0237 0.633\n22 GalAgo 0.5876 0.4759 0.0936 0.0372 0.6285\n23 LlunaPerez-J\u0013 uliaGregori 0.5498 0.5724 0.0369 0.0004 0.5365\n24 JosepCarles CandidogGarcia LNR 0.5376 0.6061 0.0705 0.0072 0.4949\nRandomClassi\fer 0.4382 1.4287 0.0390 -0.0455 0.2278\nTable 6: Results of Subtask 2.\nOverview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish\n221Overview of FakeDeS at IberLEF 2021: Fake NewsDetection in Spanish Shared Task\nResumen de FakeDeS en IberLEF 2021: Tarea compartida\npara la detecci\u0013 on de noticias falsas en espa~ nol\nHelena G\u0013 omez-Adorno1, Juan Pablo Posadas-Dur\u0013 an2,\nGemma Bel Enguix3, Claudia Porto Capetillo1\n1Instituto de Investigaciones en Matem\u0013 aticas Aplicadas y en Sistemas,\nUniversidad Nacional Aut\u0013 onoma de M\u0013 exico, Mexico\n2Instituto de Ingenier\u0013 \u0010a, Universidad Nacional Aut\u0013 onoma de M\u0013 exico, Mexico\n3Escuela Superior de Ingenier\u0013 \u0010a Mec\u0013 anica y El\u0013 ectrica, Unidad Zacatenco,\nInstituto Polit\u0013 ecnico Nacional, Mexico\n4Posgrado en Ciencia e Ingenier\u0013 \u0010a de la Computaci\u0013 on,\nUniversidad Nacional Aut\u0013 onoma de M\u0013 exico, Mexico\nhelena.gomez@iimas.unam.mx, jposadasd@ipn.mx,\ngbele@iingen.unam.mx, clauporto@comunidad.unam.mx\nAbstract: This paper presents the overview of FakeDeS 2021, the second edition\nof this lab under the IberLEF conference. The FakeDeS shared task aims to explore\ndi\u000berent methodologies and strategies related to fake news detection in Spanish.\nThis year edition brings two main challenges: thematic and language variation. For\nthis purpose, we introduce a new testing corpus containing news related to COVID-\n19 and news from other Ibero-American countries.\nKeywords: Fake news detection, FakeDeS, Iberlef.\nResumen: Este art\u0013 \u0010culo hace un presentaci\u0013 on general de la tarea compartida Fake-\nDeS 2021, cuya segunda edici\u0013 on ha tenido lugar en 2021 bajo el congreso IberLEF,\naunque se trata de la primera vez con esta denominaci\u0013 on. La tarea FakeDeS tiene\npor objetivo explorar diferentes m\u0013 etodos y estategias relacinados con la detecci\u0013 on\nde noticias falsas en espa~ nol, principalmente en su variante de M\u0013 exico. La edici\u0013 on de\nesta a~ no propone dos desaf\u0013 \u0010os principales: variaci\u0013 on tem\u0013 atica y variaci\u0013 on ling\u007f u\u0013 \u0010stica.\nPara ello, se introduce un nuevo corpus de prueba que contiene noticias relacionadas\ncon COVID 19 y noticias de otros pa\u0013 \u0010ses de Iber-Am\u0013 erica.\nPalabras clave: Fake news detection, FakeDeS, Iberlef.\n1 Introduction\nIn recent years, internet and social networks\nhave increased the fast spreading of news and\nthe power of individuals to create and share\ntheir own content, that is usually partial and\nun-veri\fed. Traditional media have been for-\nced to adapt to this new online scenario that\nfavours \\spectacle over restraint and veriica-\ntion\" (Chen, Conroy, and Rubin, 2015).\nThis situation has an impact in the arising\nof Fake News, which either have the objec-\ntive to manipulate people or have not been\nconveniently fact-checked. Misinformation is\npresent in every area: politics, science, even\nin sports. It spreads in seconds among thou-\nsands of people. Therefore, it is necessary tobuild computational systems in order to iden-\ntify the false information in the web and so-\ncial media.\nAs every task in NLP, Fake News detec-\ntion has been developed mainly for English.\nHowever, and given the number of speakers\nof Spanish around the world, it seems urgent\nto design methods that can identify fake con-\ntent in this language.\nA fake news detection system aims to help\nusers detect and \flter out potentially decep-\ntive news. An approach for the prediction of\nintentionally misleading news is based on the\nanalysis of truthful and fraudulent previously\nreviewed news, i.e., annotated corpora.\nThe FakeDeS task is the second edition\nof the fake new detection task organized in\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 223-231\nrecibido 01-07-2021 revisado 08-07-2021 aceptado 12-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-19\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalthe IberLeF conference. The \frst edition was\nintegrated in the MEX-A3T (Arag\u0013 on et al.,\n2020), a forum for the analysis of social media\ncontent in Mexican Spanish, at IberLeF.\nThe 2021 FakeDeS task at Iberlef 2021\n(Montes et al., 2021) has taken into account\nthe global pandemic situation. A new corpus\ncontaining news associated with COVID-19\nwill be used as a testing corpus, while the\ncorpus used in the 2020 edition has been pro-\nvided as the training set. Our aim is to ex-\nplore the robustness of methods when trained\non generic news and then evaluated in news\nassociated with a very speci\fc theme.\nThe rest of this paper is organized as fo-\nllows. Section 2 presents the evaluation fra-\nmework used at FakeDeS 2021. Section 3\nshows an overview of the participating ap-\nproaches. Section 4 reports and analyzes the\nresults obtained by the participating teams.\nFinally, Section 5 presents our conclusions\nfrom this shared task.\n2 FakeDeS 2021 Corpus and\nevaluation framework\nFor the development of the solution propo-\nsals for the FakeDeS task, the complete cor-\npus, i.e. both training and testing partitions\nof the corpus, used in the previous edition of\nthe task (Fake News Detection track at MEX-\nA3T) was made available to the participants.\nThe corpus used for the training is a com-\npilation of news collected between January\nand July 2018, using the following resour-\nces on the web: websites of established news-\npapers and media companies, special websi-\ntes dedicated to the validation of fake news\nand websites designated by di\u000berent journa-\nlists as sites that publish fake news regu-\nlarly. The news were tagged as true and fake\nmanually following the procedure described\nin (Posadas-Dur\u0013 an et al., 2019). The training\ncorpus includes true-fake news pairs of di-\nverse events to have a corpus as balanced as\npossible. The data contains 971 news divided\ninto 491 real news and 480 fake news. The\nnews compiled is related to 9 di\u000berent topics\n(Science, Sports, Economy, Education, En-\ntertainment, Politics, Health, Safety and So-\nciety) established by the source of the news.\nTable 2 shows the topic distribution in the\ntrain corpus.\nParticipants can evaluate their approach\nto achieve satisfying performance by applying\nthe corpus split used in the previous editionsTopic True Fake\nScience 46 43\nSport 66 58\nEconomy 24 19\nEducation 10 12\nEntertainment 70 78\nPolitics 175 148\nHealth 23 23\nSecurity 17 25\nSociety 60 74\n\u0006 491 480\nTabla 1: Topic distribution in the train cor-\npus.\nof the task (Arag\u0013 on et al., 2020).\nIn order to evaluate of the submitted ap-\nproaches, a new set of news were collected\nto conform the evaluation corpus. The test\ncorpus contains pairs of fake and true publi-\ncations about di\u000berent events, that were co-\nllected from November 2020 to March 2021.\nDi\u000berent sources from the web were used to\ngather the information, but mainly of two\ntypes: 1) newspapers and media companies\nwebsites, and 2) fact-cheking websites.\nThe following fact-cheking websites were\nused in the process of the corpus compilation:\n1.Maldito Bulo\n(https://maldita.es/malditobulo)\n2.Colombian Check\n(https://colombiacheck.com/)\n3.AFP Factual\n(https://factual.afp.com/)\n4.EcuadorChequea\n(http://www.ecuadorchequea.com/)\n5.Chequeado\n(https://chequeado.com/)\n6.Veri\fcado\n(https://verificado.com.mx/)\n7.Animal Pol\u0013 \u0010tico\n(https://www.animalpolitico.com/)\nMost of the revised fact-checking sites\nused follow the recommendations of the In-\nternational Fact-Checking Network1(IFCN)\nat Poynter that seeks to promote good prac-\ntice in fact-checking. Only two of the sites do\nnot show the badge issued by IFCN, however\n1https://ifcncodeofprinciples.poynter.org/\nHelena G\u00f3mez-Adorno, Juan Pablo Posadas-Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo\n224on their website they describe the methodo-\nlogy they follow for the fact-checking.\nFor the evaluation, the assembled test cor-\npus has 572 instances. The instances were la-\nbeled using the same two classes considered\nin train set, i.e., true or fake. The test corpus\nis balanced with respect to these two clas-\nses. To compile the true-fake news pair of the\ntest corpus, the following guidelines were fo-\nllowed:\n- A fake news is added to the corpus if any\nof the selected fact-checking sites deter-\nmines it.\n- Given a fake news, its true news coun-\nterpart is added if there is evidence that\nit has been published in a reliable site\n(established newspaper site or media si-\nte).\nThe topics covered in the test corpus\nthat match the training corpus are Science,\nSport, Politics, and Society. Three new to-\npics, COVID-19, Environment, and Interna-\ntional, were added to the test corpus. Table 2\nshows the topic distribution in the test cor-\npus. The test corpus includes mostly news\narticles, however, on this occasion social me-\ndia posts were also included in the category\nof fake news. Exactly 90 posts were inclu-\nded as fake news (15.73 % of the total). This\nposts were recovered mainly from Facebook\nand WhatsApp.\nTopic True Fake Posts\nEnvironment 2 2 1\nScience 6 7 3\nCOVID-19 118 119 59\nSport 1 1 0\nInternational 7 7 0\nPolitics 53 54 17\nSociety 99 96 10\n\u0006 286 286 90\nTabla 2: Topic distribution in the test corpus.\nThe use of the various fact-checking si-\ntes involved consulting pages from di\u000berent\ncountries that o\u000ber content in Spanish in ad-\ndition to Mexico, so di\u000berent variants of Spa-\nnish are included in the test corpus. These\nsites included countries like Argentina, Boli-\nvia, Chile, Colombia, Costa Rica, Ecuador,\nSpain, United States, France, Peru, Uruguay,\nEngland and Venezuela.3 Overview of the Submitted\nApproaches\nAt this edition, twenty one teams submitted\none or more solutions to the task through the\ncodalab platform2. CodaLab Competitions is\na powerful open source framework for run-\nning competitions that involve result or code\nsubmission. The evaluation methodology of a\ncompetition in this platform consists of recei-\nving as input the predictive outputs of sys-\ntems and returns a performance evaluation\nbased on the metrics de\fned for each task.\nThis section presents a summary of their\napproaches regarding preprocessing steps,\nfeatures, and classi\fcation algorithms. In Ta-\nble 3 we indicate the general approach used\nfor each team. It can be appreciated that\nparticipants used three general approaches:\ntransformers, deep neural networks, and tra-\nditional representations like BoW and n-\ngrams feeding a SVM classi\fer. Following,\nwe brie\ry describe each of the participating\nmethods\n\u2022GDUF DM at FakeDeS 2021: Spanish\nFake News Detection with BERT and\nSample Memory (Huang, Xiong, and\nJiang, 2021)\n{ Team name: GDUFS DM\n{ Summary: The authors detect\nSpanish fake news using BERT and\nSample Memory with an attention\nmechanism. The method consists of\ntaking the \frst and last segments\nof the texts and feeding them into\na BERT system, obtaining two em-\nbeddings (head and tail). Additio-\nnally, a so-called \\sample memory\"\nmatrix is obtained by taking a ran-\ndom sample of the head and tail em-\nbeddings; this matrix is used in an\nattention mechanism with the rest\nof the input (the previously explai-\nned head and tail embeddings of\neach entry). The \fnal embedding is\nformed from the concatenation of\nthe head, tail, and memory embed-\ndings.\n\u2022HAHA at FakeDeS 2021 :A Fake News\nDetection Method Based on TF-IDF and\nEnsemble Machine Learning (Li, 2021)\n2https://competitions.codalab.org/competitions/29545\nOverview of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task\n225Approach\nGDUFS DM\nHaha\nLcad UFES\nCiTIUS-NLP\nzk15120170770\nForceNLP\nTSIA\nYeti\nBribones\nTransformers X XX XX\nTraditional Deep Neural Networks X\nBoW, n-grams, Stylometrics XXX X X\nTabla 3: General approach of each participating team.\n{ Team name: Haha\n{ Summary: The authors presen-\nted a classic bag-of-words featu-\nre extraction with TF-IDF weigh-\nting scheme and a stacking ensem-\nble learning method based on weak\nclassi\fers. Their model not only\nanalyzes the content of the news,\nbut also combines information such\nas publishers and topics using label\nencoding to improve the performan-\nce of the model.\n\u2022LCAD - UFES at FakeDeS 2021 :Fake\nNews Detection Using Named Entity Re-\ncognition and Part-of-Speech Sequences\n(Spalenza et al., 2021)\n{ Team name: LCAD UFES\n{ Summary: The authors implemen-\nted a method using named entity\nrecognition and Part-of-Speech se-\nquences with a machine learning\napproach. The authors analyze the\ne\u000bectiveness of di\u000berent classi\fers,\nsupport vector machines, random\nforest, gradient boosting and Wi-\nSARD; achieving the best perfor-\nmance with gradient boosting.\n\u2022CiTIUS at FakeDeS 2021: A Hybrid\nStrategy for Fake News Detection (Ga-\nmallo, 2021)\n{ Team name:CiTIUS-NLP\n{ Summary: The authors explored\nthree di\u000berent BERT-based super-\nvised classi\fcation strategies: 1) a\n\fne tuned model approach that\nadds a dense layer on top of the last\nlayer of the pre-trained BERT mo-\ndel and then trains the whole mo-\ndel on the task speci\fc dataset, 2)a sentence similarity approach using\nsentence embeddings obtained with\nBERT model, and 3) an hybrid ap-\nproach that combines sentence simi-\nlarity with some linguistic heuristics\n(size of the news, the presence of\nsentences written in capital letters,\nor speci\fc fake statements).\n\u2022zk15120170770 at FakeDeS 2021: Fa-\nke news detection based on Pre-training\nModel (Zhao, Zhou, and Li, 2021)\n{ Team name: zk15120170770\n{ Summary: The authors presented\na method based on \fne-tuning the\nXML-Roberta pretrained language\nmodel. They trained the classi\fer\nusing Adam optimizer with a lear-\nning rate 2e-5 and cross entropy\nloss. The authors used 10 epochs for\nthe training process and the maxi-\nmum length of the news is set to\n512.\n\u2022ForceNLP at FakeDeS 2021: Analysis of\nText Features Applied to Fake News De-\ntection in Spanish (Reyes-Maga~ na and\nArgota Vega, 2021)\n{ Team name: ForceNLP\n{ Summary: The authors presented\nan approach based on traditional\nsupervised learning using of di\u000be-\nrent types and of n-grams (of dif-\nferent size) to represent the news.\nThe authors use classical classi\fers:\nLogistic Regression, SVM and Mul-\ntinomial Naive-Bayes to distinguish\nbetween Fake and True news. Their\nobjective is to analyze the impact\nof text features in the task of fake\nnews detection.\nHelena G\u00f3mez-Adorno, Juan Pablo Posadas-Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo\n226\u2022TSIA team at FakeDeS 2021: Fake News\nDetection in Spanish Using Multi-Model\nEnsemble Learning (Guan, 2021)\n{ Team name: Tsia\n{ Summary: The author presented\nthree model architectures based on\nthe pre-trained model BETO and\nXLM-RoBERTa-Large. The author\n\frst \fne-tuned the Spanish pre-\ntrained model BETO and then used\nthe multi-language model XLM-\nRoBERTa-Large to replace BETO,\nthe third model includes the addi-\ntion of CNN for feature extraction.\n\u2022YETI at FakeDeS 2021: Fake News De-\ntection in Spanish with ALBERT (Luo,\n2021)\n{ Team name: Yeti\n{ Summary: The authors presented\na modi\fcation of ALBERT base\nmodel adding richer semantic con-\ntent. ALBERT, unlike BERT, im-\nplements three improvements: Em-\nbedding layer, parameter sharing,\nand SOP task. The authors mo-\ndi\fed ALBERT to directly receive\nthe news and concatenate the hid-\nden state of the \frst token sequence\nof the last three hidden layers into\nthe classi\fer. The results of the mo-\ndi\fcation on 5-fold cross-validation\nshow an improvement over the AL-\nBERT base model.\n\u2022Bribones tras la esmeralda perdi-\nda@FakeDeS 2021: Fake news detection\nbased on random forests, k-nearest\nneighbors, and n-grams for a Spanish\ncorpora (Lomas-Barrie et al., 2021)\n{ Team name: Bribones tras la\nesmeralda perdida\n{ Summary: The authors used\na simple BOW-based classi\fca-\ntion system to detect fake news.\nThe methods implemented are\nK-nearest neighbors and random\nforests. The paper starts from an\noverly naive hypothesis: true and\nfake news can be easily detected\nwith the vocabulary they use.This hypothesis is rejected in the\nconclusions of the paper.\n4 Experimental evaluation and\nanalysis of results\nThis section summarizes the results obtained\nby the the participants of FakeDeS at Iber-\nLEF 2021: Fake News Detection in Spanish\nShared Task. We compare and analyze in de-\ntail the performance of the submitted solu-\ntions. For the \fnal phase of the challenge,\nparticipants sent their predictions for the test\npartition, the performance on this data was\nused to rank them. We used the F1score over\nthe fake class as the main evaluation measu-\nre.\nFor computing the evaluation scores we re-\nlied on the Codalab Competitions platform.\nIn this section, we report the results obtai-\nned by participants as evaluated by Codalab\nCompetitions and an analysis of their results.\nAs baseline methods, we implemented th-\nree popular approaches: i) a classi\fcation mo-\ndel trained on the bag of words (BoW) repre-\nsentation, ii) a classi\fcation model trained on\nthe n-grams representation, and iii) a classi-\n\fcation model based on transformers using\nBERT.\nFor the three baseline, the corpus was pre-\nprocessed using techniques described in the\nNLP literature, such as elimination of stop\nwords, punctuation marks, and tokens rela-\nted to numbers. Other techniques used are lo-\nwercasing, tokenization, and lemmatization.\nThe \frst baseline is a classi\fcation mo-\ndel trained on the BoW representation, whe-\nre all vocabulary was used for the vectoriza-\ntion. Particular preprocessing techniques for\nthis baseline includes tokenization (keeping\nthe stopwords) and lowercasing. For classi\f-\ncation we used an SVM classi\fer with linear\nkernel.\nThe second baseline is a classi\fcation mo-\ndel trained on the n-grams representation.\nThe preprocessing techniques are the same\nas in the \frst baseline. The text is represen-\nted by character 3-grams and all vocabulary\nwas used. For classi\fcation we used an SVM\nclassi\fer with linear kernel.\nThe last baseline follows a deep learning\napproach, it is based on the Bidirectional\nEncoder Representation from Transformers\n(BERT) architecture(Devlin et al., 2019). We\nuse the pre-trained model BETO: Spanish\nOverview of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task\n227BERT3. We trained the classi\fer using an\nAdam optimizer with a learning rate of 2 e\u00005.\nThe epoch and maximum sentence length is\n10 and 512, respectively. For the process of \f-\nne tuning and sequence classi\fcation, we use\nBert For Sequence Classi\fcation as imple-\nmented in HuggingFace's Transformers (Wolf\net al., 2020) which has a linear layer added\nat the top for classi\fcation to be used as a\nsentence classi\fer.\nTable 4 shows a summary of the results\nobtained by each team in the FakeDeS sha-\nred task. We report the F1score in both fa-\nke and true classes, the macro F1score, and\nthe accuracy. We used the F1over the fake\nclass to rank participants. In this edition of\nthe FakeDeS shared task, the approach sub-\nmitted by the GDUFS DM team outperfor-\nmed all the other approaches and the baseli-\nnes. GDUFS DM used an approach based on\na transformer architecture; however the se-\ncond best approach (HAHA) used a classic\napproach with an ensemble classi\fer. These\nresults show that for this task, classic approa-\nches are still competitive with respect to deep\nlearning. it is important to mention that the\nthird and fourth best approaches did not send\ntheir system description papers.\nAll participated teams that send their\nsystem description papers used a machine-\nlearning-based approach relying on style-\nbased features. Neither team used fact-\nchecking approach to verify the authenticity\nof the news.\nFor the analysis of the complementariness\nand the diversity of the predictions of the dif-\nferent approaches we use the Maximum Pos-\nsible Accuracy (MPA) and Coincident Failure\nDiversity (CFD) metrics(Tang, Suganthan,\nand Yao, 2006). The MPA is de\fned as the\nratio between the correctly classi\fed instan-\nces over the total number of test instances.\nAn instance is considered as correctly classi-\n\fed if at least one of the participating teams\nclassi\fed it correctly. On the other hand, the\nCFD metric give us the error diversity among\nthe participants predictions. This measure gi-\nves a minimum value 0 when all teams si-\nmultaneously predict an instance correctly or\nwrongly, while a maximum value of 1, when\nall teams classify incorrectly di\u000berent instan-\nces, i.e. misclassi\fcations are all unique.\nTable 5 shows the results of these me-\n3https://huggingface.co/dccuchile/bert-base-\nspanish-wwm-uncasedtrics according to the approaches used by the\nparticipants. Transformers approach includes\nthe following teams: GDUFS DM, CiTIUS-\nNLP, zk15120170770, TSIA, and Yeti. Tra-\nditional Deep Neural Networks includes only\nthe TSIA team. BoW, n-grams, and stylo-\nmetrics approaches is used by the following\nteams: Haha, Lcas UFES, CiTIUS-NLP, For-\nceNLP, and Bribones. The CFD for the tra-\nditional deep neural networks methodologies\ncould not be calculated because only one par-\nticipant team used this approach. The MPA\nfor the row of all teams has the highest va-\nlue, which means that the team's approaches\ncomplement each other. In terms of the di\u000be-\nrent approaches, the Transformes approach\nobtained similar MPA over the BoW, n-\ngrams, Stylometrics approach. However, the\nMPA of all approaches with paper submis-\nsions shows a 3 % of increase over these\nindividual approaches, suggesting that the\ntransformers approaches and traditional ap-\nproaches are complementary to each other.\nThe values for the CFD score are compara-\nble among all approaches, which means that\ntheir predictions are complementary to me\nextend, this lead us to conclude that there is\nstill di\u000berent information learned by traditio-\nnal and transformer based approaches.\nThe Table 6 shows the results of the F1\nscore for the fake class in the di\u000berent topics\nof the test corpus. It can be appreciated that\nthe Sports category achieved the overall lo-\nwer results for all the evaluated approaches,\neven though there were six systems that co-\nrrectly classi\fed all the instances in this to-\npic. This is related to the fact that in this\ncategory there are only two news items, one\nfake and one true. The performance of the\nsystems does not seem related to the availa-\nbility of the topics in the training set. While\nthere were no news related to Covid-19 in the\ntraining set, the results achieved in this topic\nare comparable to those on topics available in\nthe training set such as Politics and Society.\nThe news belonging to the International topic\nwere the easiest to classify in average, it can\nbe observed that systems achieved F1scores\nabove 15 % and 20 % in this topic.\nWe identi\fed the common prediction\nerrors across all the systems and \fnd that\nthere were only 2 news, both in the true class\nthat none of the approaches classify correctly.\nAll fake news were identi\fed by at least one\nsystem. Table 7 shows the classi\fed instan-\nHelena G\u00f3mez-Adorno, Juan Pablo Posadas-Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo\n228Team Fake True Fmacro Accuracy\nGDUFS DM 0.7666 0.7649 0.7666 0.7657\nHaha 0.7548 0.7522 0.7548 0.7535\nChats 0.7514 0.7690 0.7514 0.7605\nSINAI 0.7385 0.7821 0.7385 0.7622\nbaseline-BERT 0.7321 0.7432 0.7321 0.7378\nbaseline-BOW-SVM 0.7217 0.7359 0.7217 0.729\nLcad UFES 0.7102 0.6837 0.7102 0.6976\nCiTIUS-NLP 0.7098 0.4940 0.7098 0.6311\nbaseline-CHAR-3-GRAMS-SVM 0.7063 0.6883 0.7063 0.6976\nzk15120170770 0.7053 0.6053 0.7053 0.6626\nForceNLP 0.6925 0.4739 0.6925 0.6119\nGRX 0.6915 0.5624 0.6915 0.6381\nTSIA 0.6860 0.5263 0.686 0.6224\nFREE 0.6855 0.6519 0.6855 0.6696\nLIMCA 0.6812 0.7027 0.6812 0.6923\nZZWEI 0.6737 0.6794 0.6737 0.6766\nPremjithb 0.6576 0.7177 0.6576 0.6906\nSdamian 0.6542 0.75 0.6542 0.7098\nYeti 0.6316 0.609 0.6316 0.6206\nGulu 0.6226 0.476 0.6226 0.5612\nNicksss 0.6119 0.7592 0.6119 0.7028\nBribones tras la esmeralda perdida 0.5835 0.5878 0.5835 0.5857\nWSSC 0.5118 0.6657 0.5118 0.6031\nSkblaz 0.4838 0.649 0.4838 0.5822\nTabla 4: Results for the fake news detection task on the test set.\nApproach Best Accuracy MPA CFD Number of Systems\nTransformers 0.7657 0.9528 0.3834 5\nTraditional Deep Neural Networks 0.6224 0.6224 - 1\nBoW, n-grams, Stylometrics 0.7535 0.9580 0.3941 5\nAll teams (with submission) 0.7657 0.9895 0.3732 9\nAll teams 0.7657 0.9965 0.3382 21\nTabla 5: Comparison of MPA and CFD results between the di\u000berent general approaches.\nces, it can be observed that one of the news\nbelong to the Politics category and the other\none to the Society category. It is interesting\nthat all news (fake and true) in the Covid-19\ntopic were classi\fed correctly by at least one\nteam.\n5 Conclusions\nThis paper described the design and results\nof the FakeDeS shared task collocated with\nIberLef 2021. FakeDeS stands for Fake news\nDetection in Spanish . This has been the se-\ncond edition of the task, the \frst with this\nname, since the last year was presented as\nthe fake news detection track in the wider\nshared task MEX-A3T (Arag\u0013 on et al., 2020).\nAlthough the best results in this shared\ntask were reached by a team that proposed toapproach the problem with a method based\nin transformers, traditional machine learning\nstrategies obtained very similar results. This\nseems to indicate the complexity of the task,\nthat needs to be tackled by systems that con-\nsider multiple variables.\nThe results illustrate that fake news de-\ntection is a hard problem in the area of Na-\ntural Language Processing. The development\nof techniques especially designed to generate\ndisinformation is a challenge for the area that\nhas as a goal to identify such disinformation.\nSumming up, the FakeDeS evaluation task\npromotes the work in Spanish in this crucial\narea of NLP, encourages the scienti\fc exchan-\nge between researchers, and provides an an-\nnotated corpus in Spanish that is openly avai-\nlable for the scienti\fc community. All this,\nOverview of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task\n229Team\nCovid-19\nPolitics\nSociety\nInternational\nScience\nSports\nEnvironmental\nGDUFS DM 0.78 0.74 0.76 0.93 0.71 1.00 0.67\nHaha 0.76 0.73 0.74 0.92 0.88 0.00 0.80\nChats 0.77 0.76 0.71 0.92 0.62 1.00 1.00\nSINAI 0.74 0.74 0.74 0.92 0.50 1.00 0.67\nbaseline-BERT 0.72 0.75 0.72 0.88 0.77 1.00 0.67\nbaseline-BOW-SVM 0.72 0.77 0.72 0.92 0.50 0.0 0.50\nLcad UFES 0.71 0.74 0.70 0.77 0.71 0.67 0.50\nCiTIUS-NLP 0.71 0.73 0.70 0.70 0.70 0.67 0.80\nbaseline-CHAR-3-GRAMS-SVM 0.68 0.73 0.71 0.92 0.75 0.0 0.80\nzk15120170770 0.71 0.72 0.69 0.88 0.63 0.00 0.67\nForceNLP 0.69 0.67 0.71 0.67 0.67 1.00 0.80\nGRX 0.70 0.72 0.66 0.71 0.78 0.00 0.50\nTSIA 0.68 0.67 0.69 0.86 0.80 1.00 0.50\nFREE 0.66 0.74 0.68 0.71 0.71 0.00 0.67\nLimca 0.66 0.73 0.67 0.92 0.71 0.00 0.67\nZZWEI 0.66 0.70 0.68 0.80 0.67 0.00 0.67\nPremjithb 0.66 0.63 0.64 0.83 0.80 0.67 0.67\nSdamian 0.64 0.68 0.68 0.83 0.62 0.00 0.50\nYeti 0.58 0.67 0.65 0.75 0.71 1.00 0.67\nGulu 0.61 0.59 0.64 0.74 0.67 0.67 0.80\nNiksss 0.62 0.62 0.58 0.92 0.40 0.00 0.67\nBribones tras la esmeralda perdida 0.58 0.62 0.56 0.67 0.67 0.67 0.40\nWSSC 0.35 0.49 0.66 0.92 0.55 0.00 0.00\nSkblaz 0.40 0.34 0.61 0.86 0.46 0.00 0.50\nAverage 0.66 0.68 0.68 0.83 0.67 0.43 0.63\nTabla 6: Results for the fake news detection task by topics.\nTopic Source Title\nPolitics El Mundo No, no ha habido un misterioso apag\u0013 on en el Vaticano...\nSociety El Comercio La falsa noticia de la empleada que defeca en la mesa de su jefe ...\nTabla 7: True Instances Missclassi\fed as Fake by all Systems.\nand the cutting-edge methods implemented\nby the participants, help to place Spanish\namong the languages with an increasing num-\nber of resources and experiments oriented to\nfake news detection.\nThe use of transformers again achieved the\nbest results this year showing the appropria-\nteness of this method in a cross-topic sce-\nnario. Although this year's results are lower\nthan last year's, they are really promising\nconsidering the added di\u000eculty. The compli-\nmentariness analysis showed that hybrid ap-\nproaches, using both transformers and tradi-\ntional approaches (including traditional Deep\nNeural Networks) can help to increase theperformance of automatic fake news detec-\ntion methods.\nAcknowledgments\nThis research was funded by CONACyT\nproject CB A1-S-27780, DGAPA-UNAM\nPAPIIT grants number TA400121 and\nTA100520. The authors also thank CO-\nNACYT for the computer resources provided\nthrough the INAOE Supercomputing Labo-\nratory's Deep Learning Platform for Langua-\nge Technologies.\nHelena G\u00f3mez-Adorno, Juan Pablo Posadas-Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo\n230References\nArag\u0013 on, M., H. Jarqu\u0013 \u0010n-V\u0013 asquez, M. M.\ny G\u0013 omez, H. J. Escalante, L. V. Pineda,\nH. G\u0013 omez-Adorno, J. Posadas-Dur\u0013 an, and\nG. Bel-Enguix. 2020. Overview of MEX-\nA3T at IberLEF 2020: Fake news and\naggressiveness analysis in mexican spa-\nnish. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2020)\nco-located with 36th Conference of the\nSpanish Society for Natural Language Pro-\ncessing (SEPLN 2020).\nChen, Y., N. Conroy, and V. Rubin. 2015.\nNews in an online world: The need for an\n\\automatic crap detector\". Proceedings\nof the Association for Information Science\nand Technology, 52(1):1{4.\nDevlin, J., M.-W. Chang, K. Lee, and K. Tou-\ntanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for lan-\nguage understanding.\nGamallo, P. 2021. CiTIUS at FakeDeS 2021:\nA hybrid strategy for fake news detection.\nInProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021).\nGuan, Z. 2021. TSIA team at FakeDeS\n2021: Fake news detection in spanish using\nmulti-model ensemble learning. In Procee-\ndings of the Iberian Languages Evaluation\nForum (IberLEF 2021).\nHuang, X., J. Xiong, and S. Jiang. 2021.\nGDUF DM at FakeDeS 2021: Spanish fa-\nke news detection with BERT and sam-\nple memory. In Proceedings of the Ibe-\nrian Languages Evaluation Forum (Iber-\nLEF 2021).\nLi, K. 2021. HAHA at FakeDeS 2021: A\nfake news detection method based on TF-\nIDF and ensemble machine learning. In\nProceedings of the Iberian Languages Eva-\nluation Forum (IberLEF 2021).\nLomas-Barrie, V., N. Perez, V. M. Lara, and\nA. Neme. 2021. Bribones tras la esmeral-\nda perdida@FakeDeS 2021: Fake news de-\ntection based on random forests, k-nearest\nneighbors, and n-grams for a spanish cor-\npora. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) .\nLuo, H. 2021. YETI at FakeDeS 2021: Fake\nnews detection in spanish with ALBERT.\nInProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021).Montes, M., P. Rosso, J. Gonzalo, E. Arag\u0013 on,\nR. Agerri, M. \u0013A.\u0013Alvarez-Carmona, E. \u0013A.\nMellado, J. C. de Albornoz, L. F. Luis Chi-\nruzzo, H. G\u0013 omez-Adorno, Y. Guti\u0013 errez,\nS. M. J. Zafra, S. Lima, F. M. P. de Arco,\nand M. Taul\u0013 e, editors. 2021. Proceedings\nof the Iberian Languages Evaluation Fo-\nrum, Iberlef 2021. CEUR Workshop Pro-\nceedings, september.\nPosadas-Dur\u0013 an, J.-P., H. G\u0013 omez-Adorno,\nG. Sidorov, and J. J. M. Escobar. 2019.\nDetection of fake news in a new corpus for\nthe spanish language. Journal of Intelli-\ngent & Fuzzy Systems, 36(5):4869{4876.\nReyes-Maga~ na, J. and L. E. Argota Vega.\n2021. ForceNLP at FakeDeS 2021: Analy-\nsis of text features applied to fake news\ndetection in spanish. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2021) .\nSpalenza, M. A., L. Lusquino-Filho, F. M. G.\nFran\u0018 ca, P. M. V. Lima, and E. de Oliveira.\n2021. LCAD - UFES at FakeDeS 2021:\nFake news detection using named entity\nrecognition and part-of-speech sequences.\nInProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021).\nTang, E. K., P. N. Suganthan, and X. Yao.\n2006. An analysis of diversity measures.\nMach. Learn. , 65(1):247{271.\nWolf, T., L. Debut, V. Sanh, J. Chaumond,\nC. Delangue, A. Moi, P. Cistac, T. Rault,\nR. Louf, M. Funtowicz, J. Davison, S. Sh-\nleifer, P. von Platen, C. Ma, Y. Jerni-\nte, J. Plu, C. Xu, T. L. Scao, S. Gug-\nger, M. Drame, Q. Lhoest, and A. M.\nRush. 2020. Transformers: State-of-\nthe-art natural language processing. In\nProceedings of the 2020 Conference on\nEmpirical Methods in Natural Language\nProcessing: System Demonstrations, pages\n38{45, Online, October. Association for\nComputational Linguistics.\nZhao, K., S. Zhou, and W. Li. 2021.\nzk15120170770 at FakeDeS 2021: Fake\nnews detection based on pre-training mo-\ndel. In Proceedings of the Iberian Langua-\nges Evaluation Forum (IberLEF 2021).\nOverview of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task\n231Overview of the eHealth Knowledge Discovery\nChallenge at IberLEF 2021\nResumen de la Tarea de Descubrimiento de Conocimiento en\nSalud en IberLEF 2021\nAlejandro Piad-Mor\u000es1;\u0003, Suilan Estevez-Velarde1, Yoan Gutierrez2;3\nYudivian Almeida-Cruz1,Andr\u0013 es Montoyo2;3,Rafael Mu~ noz2;3\n1School of Math and Computer Science, University of Havana, Cuba\nfapiad, sestevez, yudyg@matcom.uh.cu\n2University Institute for Computer Research, IIUI, University of Alicante, Spain\n3Department of Language and Computing Systems, University of Alicante, Spain\nfygutierrez, montoyo, rafael g@dlsi.ua.es\nAbstract: This paper summarises the eHealth Knowledge Discovery Challenge\nhosted at IberLEF 2021. We describe the task, resources, and participating systems,\nhighlighting and discussing the main results achieved in the challenge. We analyse\nthe best performing systems and present recommendations for future research.\nKeywords: Named Entity Recognition, Relation Extraction, Knowledge Discovery,\nChallenge.\nResumen: Este art\u0013 \u0010culo resume la Tarea de Descubrimiento de Conocimiento en\nSalud presentada en IberLEF 2021. Se describen la tarea, los recursos creados, y\nlos sistemas que participaron. Se discuten los resultados principales obtenidos por\nestos sistemas, y se presentan recomendaciones para continuar la investigaci\u0013 on en\nesta tem\u0013 atica.\nPalabras clave: Reconocimiento de Entidades Nombradas, Extracci\u0013 on de Rela-\nciones, Descubrimiento de Conocimiento, Tarea.\n1 Introduction\nThe accelerated growth of the Internet and\nthe increased production of textual resources\nin all areas of human endeavour has cre-\nated both new opportunities and new chal-\nlenges for the research community. On one\nhand, larger datasets can be collected and\nused to build increasingly powerful machine\nlearning models, such as GPT-3 (Floridi and\nChiriatti, 2020) and similar. On the other\nhand, is becoming increasingly di\u000ecult to or-\nganise, categorise, cross-reference, and fact-\ncheck the textual information available on-\nline. The ease of publication and consump-\ntion of textual information is arguably one of\nthe root causes of the increasingly worrying\nphenomenon of fake news (Lazer et al., 2018).\nStaying up-to-date on information about\ntopics of interest is crucial in technical or\nscienti\fc domains. For this purpose, spe-\ncialists rely on a combination of curated\nsources (e.g., domain-speci\fc repositories like\n\u0003Corresponding author.arXiv.org1,bioRxiv.org2) and technologies\nfor search and recommendation (e.g., Google\nScholar3,Semantic Scholar4). These re-\nsources signi\fcantly improve the experience\nof collecting and consuming large amounts\nof relevant information on a speci\fc topic.\nTools like Connected Papers5and Papers\nwith Code6are one step beyond the index-\ning of documents, providing summarised and\nstructured representations of the content in a\ncollection of documents. However, it remains\nan open problem to automatically combine,\nsummarise, and present the relevant informa-\ntion in a collection of documents in a seman-\ntic structure (e.g., a knowledge graph) that\nallows a specialist to quickly grasp the essen-\ntial concepts of a speci\fc knowledge \feld.\nA potential solution to this problem would\n1https://arxiv.org/\n2https://www.biorxiv.org/\n3https://scholar.google.com/\n4https://www.semanticscholar.org/\n5https://www.connectedpapers.com/\n6https://paperswithcode.com/\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 233-242\nrecibido 05-07-2021 revisado 12-07-2021 aceptado 16-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-20\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalrequire methods to automatically detect in\nnatural language the most relevant concepts\nand the factual statements in which they are\nrelated, possibly normalise them into well-\nestablished taxonomies and classi\fcations,\nand store them in computational data struc-\ntures where they can linked with related\nconcepts, e.g., knowledge graphs (Estevez-\nVelarde et al., 2018). The \frst step of\nthis process, i.e., the detection of relevant\nconcepts and semantic relations in text, is\nalready a challenging computational task,\ngiven the complexity and variability of natu-\nral language. To encourage research and de-\nvelopment in this area, several academic com-\npetitions have been organised through the\nyears by organisations such as CLEF, SE-\nMEVAL, and more recently, IBERLEF.\nIn this context, the eHealth Knowledge\nDiscovery Challenge is designed to foster the\ndevelopment of automatic knowledge discov-\nery systems for natural language sentences\nin a cross-domain and multi-lingual setting.\nConcretely, a token-level annotation model\nof 4 entities and 13 semantic relations is de-\n\fned, and a corpus of 1800 sentences from\ndi\u000berent factual sources (i.e., Spanish Med-\nlinePlus articles, Spanish Wikinews articles,\nand English biomedical preprints related to\nCOVID-19) is annotated. Two computa-\ntional tasks are de\fned: the detection of\nmulti-span, non-contiguous, and potentially\noverlapping named entities; and the detec-\ntion of semantic relations between them. A\nshared annotation campaign was organised,\nwhere a total of 9 participants presented 10\ndi\u000berent systems with varied levels of perfor-\nmance.\nThe eHealth-KD task focuses on cross-\ndomain, multi-lingual and low-resource so-\nlutions. This setting presents a signi\fcant\nchallenge to existing state-of-the-art meth-\nods, which often rely on large amounts of\ntraining data. A successful approach to the\neHealth-KD must leverage transfer learning\nacross di\u000berent domains and languages, since\nthe vast majority of the training examples\nare provided in Spanish language and from\nthe MedlinePlus domain, while only a small\ndevelopment set is available in the remaining\nsettings. With this added complexity, we ex-\npect to encourage solutions that can be de-\nployed in low-resource environments, where\nis unfeasible to train large language models\nover longs periods of time.The remaining of this paper is orga-\nnized as follows. Section 2 describes the\neHealth-KD tasks in greater detail, includ-\ning the annotation model and performance\nmetrics. Section 3 describes the corpora and\nother resources created for this challenge and\npresents some qualitative analysis of their\ncharacteristics. Section 4 describes the dif-\nferent systems presented in the challenge and\nsummarises the main approaches and most\ncommon characteristics they share. Section 5\npresents the main results of the challenge and\ndiscusses the most interesting insights of the\nchallenge and highlights both the most rele-\nvant lessons learned and potential improve-\nments for future similar endeavours.\n2 Tasks Descriptions\nThe eHealth-KD challenge7consists on the\nautomatic sentence-level annotation of multi-\ntoken entities and binary relations among\nthem. A custom annotation model has been\nde\fned, comprising 4 entity types and 13 re-\nlations, that attempts to capture a large part\nof factual semantics in technical documents,\nincluding encyclopedias, news, and scienti\fc\npapers. The annotation model is explained in\ndetail in Piad-Mor\u000es et al. (2019). Figure 1\nshows an illustrative example of the annota-\ntion model in three Spanish sentences. The\nchallenge has been divided into two di\u000berent\nsubtasks: entity recognition and relation ex-\ntraction.\n2.1 Subtask A: Entity recognition\nThe goal of this subtask is to identify all the\nentities per document and their types. These\nentities are all the relevant terms (single word\nor multiple words) that represent semanti-\ncally important elements in a sentence. En-\ntities always consist of one or more complete\nwords (i.e., not a pre\fx or a su\u000ex of a word),\nand never include any surrounding punctua-\ntion symbols, parenthesis, etc. There are four\ntypes for entities:\nConcept: identi\fes a relevant term, con-\ncept, idea, in the knowledge domain of\nthe sentence.\nAction: identi\fes a process or modi\fcation\nof other entities. It can be indicated by\na verb or verbal construction, and also\nby nouns.\n7https://ehealthkd.github.io/2021/\nAlejandro Piad-Morffis, Suilan Estevez-Velarde, Yoan Gutierrez, Yudivian Almeida-Cruz, Andr\u00e9s Montoyo, Rafael Mu\u00f1oz\n234Figure 1: Examples the annotation model de\fned for the eHealth-KD Challenge.\nPredicate: identi\fes a function or \flter of\nanother set of elements, which has a se-\nmantic label in the text, and is applied\nto an entity with some additional argu-\nments.\nReference: identi\fes a textual element that\nrefers to an entity of the same sentence\nor of di\u000berent one.\n2.2 Subtask B: Relation extraction\nSubtask B continues from the output of Sub-\ntask A, by linking the entities detected and\nlabelled in the input document. The pur-\npose of this subtask is to recognise all rele-\nvant semantic relationships between the en-\ntities recognised. The semantic relations are\ndivided in di\u000berent categories:\nGeneral relations (6) are general-\npurpose relations between two entities that\nhave a pre-de\fned semantic:\nis-a: indicates that one entity is a subtype,\ninstance, or member of the class identi-\n\fed by the other.\nsame-as: indicates that two entities are se-\nmantically the same.\nhas-property: indicates that one entity has\na given property or characteristic.\npart-of: indicates that an entity is a con-\nstituent part of another.\ncauses: indicates that one entity provokes\nthe existence or occurrence of another.\nentails: indicates that the existence of one\nentity implies the existence or occur-\nrence of another.Contextual relations (3) allow to re\fne\nan entity by attaching modi\fers:\nin-time: to indicate that something exists,\noccurs or is con\fned to a time-frame.\nin-place: to indicate that something exists,\noccurs or is con\fned to a place or loca-\ntion.\nin-context: to indicate a general context in\nwhich something happens, like a mode,\nmanner, or state.\nAction roles (2) indicate which role play\nthe entities related to an Action:\nsubject: indicates who performs the action.\ntarget: indicates who receives the e\u000bect of\nthe action.\nPredicate roles (2) indicate which role\nplay the entities related to a Predicate:\ndomain: indicates the main entity on which\nthe predicate applies.\narg: indicates an additional entity that spec-\ni\fes a value for the predicate to make\nsense.\n2.3 Evaluation\nThe evaluation is based according to the\nnumber of (C)orrect, (I)ncorrect, (P)artial,\n(M)issing, and (S)purious annotations,\nweighted according to an F1measure, in-\ndependently per subtask, A or B. Partial\nannotations (i.e., where predicted entities\nOverview of the eHealth Knowledge Discovery Challenge at IberLEF 2021\n235overlap with the gold standard) are scored\nas half the value of correct annotations.\nRec=CA+CB+1\n2PA\nCA+IA+CB+PA+MA+MB\nPrec =CA+CB+1\n2PA\nCA+IA+CB+PA+SA+SB\nF1= 2\u0001Prec\u0001Rec\nPrec +Rec\nFurthermore, the challenge is graded in\nthree di\u000berent scenarios: one scenario for\neach subtask independently, and a main sce-\nnario where both subtasks are performed in\nsequence, which ultimately decides the win-\nner of the challenge. In the subtask-speci\fc\nscenarios, the previous formulas are rede\fned\nbased only on the relevant subset of annota-\ntions for each subtask.\n3 Corpora and Resources\nThe Challenge is based on a corpus of doc-\numents composed of sentences taken from\nprevious challenges and other resources, and\nnewly annotated sentences. The corpus is\ndivided into three collections: training, de-\nvelopment, and testing. All collections con-\ntain sentences extracted from MedlinePlus,\nWikinews, and the CORD-19 corpus, related\nwith health topics, but showing a signi\fcant\nvariety in terms of format and structure. Fur-\nthermore, the majority of sentences are in\nSpanish, but a small set of English sentences\nis also included, to evaluate generalisation\nacross domains. The corpus with both anno-\ntated \fles in ANN format and ready-to-use\n\fles for training and evaluation is available\nonline8.\nIn the training collection, each sentence is\nlabelled with its corresponding domain and\nlanguage (Spanish or English), so that par-\nticipants can potentially \fne-tune di\u000berent\nmodels for each domain/language and learn\nto identify them in the text. In the develop-\nment collection, sentences are from all di\u000ber-\nent sources and no labelling is provided, so\nthat participants can evaluate their systems\nin a similar environment to the testing set. In\nthe testing set, a large number of unlabelled\nsentences is added along with a small batch\n8https://github.com/ehealthkd/corporaCollection Source Language Size\nTraining MedlinePlus Spanish 1200\nWikinews Span is\nh 300\nDevelop MedlinePlus Spanish 25\nWikinews Span is\nh 25\nCORD English 50\nTesting MedlinePlus Spanish 75\nWikinews Span\nish 75\nCORD English 150\nTotal 1800\nTable 1: Composition of the corpus, high-\nlighting resources from previous challenges\nand newly annotated sentences.\nof labelled sentences. This serves to discour-\nage participants from manually labelling the\ntest set.\nAs in previous edition, the corpus for\neHealth-KD 2021 is based mostly on text\nextracted from MedlinePlus9sources, plus\nadditional resources. First, the same cor-\npus used in the 2020 edition will be pro-\nvided for training and development, while\na new set of previously unlabelled sentences\nwill be manually annotated and used for the\ntest collection. Additionally, health-related\nnews sourced from Wikinews10will be also\nprovided for training and development. Fi-\nnally, a small set of sentences from scienti\fc\npapers in the CORD-19 corpus (in English\nlanguage) are selected, annotated, and dis-\ntributed in the development, and testing col-\nlections (Wang et al., 2020). The \fnal com-\nposition of the corpus is presented in Table 1.\n4 Systems Descriptions\nThe challenge caught the attention of 8 par-\nticipants from across the globe, who pre-\nsented a variety of approaches clustered\naround deep learning architectures. Table 2\nsummarises the main characteristics of each\napproach presented. A brief summary of each\nsystem follows:\nCodestrange proposes a standard NER\npipeline trained with the spacy library, mod-\ni\fed to \ft the eHealth-KD annotation model.\nThey apply some preprocessing steps to deal\nwith multi-token annotations, which are not\nhandled automatically by the tool.\n9https://medlineplus.gov/spanish/\n10https://es.wikinews.org/\nAlejandro Piad-Morffis, Suilan Estevez-Velarde, Yoan Gutierrez, Yudivian Almeida-Cruz, Andr\u00e9s Montoyo, Rafael Mu\u00f1oz\n236System Features Tasks Task A Task B Multi-span\nCodestrange Spacy Only A Sequence - Custom\nIXA ROBERTa Sequential Sequence Sequence BIO\nJAD BERT Joint Token Pairwise Relation\nPUCRJ-PUCPR-UFMG BERT Joint Sequence Pairwise BIO\nuhKD4 Word2vec\nPOS-tag\nCharacter\nPositionSequential Sequence Pairwise BILOUV\nUH-MMM BERT\nFastText\nPOS-tag\nDependency\nCharacterSequential Sequence Pairwise BILOUV\nVicomtech Joint BERT Joint Sequence Pairwise BIO\nVicomtech Seq2Seq T5 Joint Text2text Text2text Text\nYunnan-1 BERT Only A Sequence - BIO\nYunnan-Deep BETO Only A Sequence - BIO\nTable 2: Summary of the approaches presented at the eHealth-KD Challenge.\nIXA models both substasks are sequence\nlabelling problems encoded with a BIO sys-\ntem and using a pre-trained XLM-RoBERTa\nlanguage model. Subtask A is solved with a\nstandard NER architecture. For Subtask B,\nthey solve a sequence labelling problem for\neach pair of entities, where tags correspond\nto relation labels (Andr\u0013 es, 2021).\nJAD proposes a single model based on\nBERT pre-trained embeddings that jointly\noutputs entity labels (at token level) and\npairwise relation labels. To deal with\nmulti-span entities, they add a virtual re-\nlation that links tokens from the same en-\ntity (Navarro Comabella, Valle Diaz, and\nHelguera Fleitas, 2021).\nPUCRJ-PUCPR-UFMG proposes a\njoint model that outputs token labels (mod-\nelled as a sequence labelling problem)\nand pairwise relation labels, based on\nBERT pre-trained embeddings as the main\nfeature (Pavanelli et al., 2021).\nuhKD4 models both subtasks sequentially,\nusing a standard NER architecture with\nBILOUV encoding for Subtask A and a pair-\nwise classi\fcation model for Subtask B. As\nfeatures, they employ a variety of syntac-\ntic characteristics (POS-tags, character em-\nbeddings, positional embeddings) as well as\nword2vec embeddings (Alfaro-Gonz\u0013 alez et\nal., 2021).UH-MMM also models both subtasks se-\nquentially, using a standard NER architec-\nture with BILOUV encoding for Subtask\nA and a pairwise classi\fcation model for\nSubtask B. As a key characteristic, they\nencode the shortest dependency path be-\ntween two entities for the relation prediction.\nThey also compare several di\u000berent embed-\ndings, including health-speci\fc and general-\npurpose approaches (Monteagudo-Garc\u0013 \u0010a et\nal., 2021).\nVicomtech presents two di\u000berent models.\nThe \frst consists of a joint architecture for se-\nquence labelling (Subtask A) and pairwise re-\nlation prediction (Subtask B) based on BERT\npre-trained embeddings. The second model\nis a text-to-text architecture, based on a T5\nmodel, \fne-tuned on a problem-speci\fc en-\ncoding of the entities and relations that solves\nboth subtasks in a single pass (Garc\u0013 \u0010a-Pablos,\nP\u0013 erez, and Cuadros, 2021).\nYunnan-1 presents a traditional NER ar-\nchitecture based on BERT pre-trained em-\nbeddings and BIO tags for Subtask A, com-\nbined with Bi-LSTM and CRF layers (Yang,\n2021).\nYunnan-Deep presents a traditional NER\narchitecture for Subtask A, based on contex-\ntual embeddings from a BETO pre-trained\nlanguage model, and a combination of Con-\nvolutional, Bi-LSTM, and CRF layers for de-\ncoding BIO tags (Guan and Liu, 2021).\nOverview of the eHealth Knowledge Discovery Challenge at IberLEF 2021\n237Main Task A Task B\nTeam F1 P R F1 P R F1 P R\nVicom tech 0.531 0.540 0.534 0.684 0.699 0.747 0.371 0.541 0.283\nPUCRJ-PUCPR-UFMG 0.528 0.568 0.502 0.706 0.714 0.697 0.263 0.366 0.205\nIXA 0.498 0.464 0.538 0.653 0.613 0.698 0.430 0.453 0.409\nuhKD4 0.422 0.485 0.374 0.527 0.517 0.537 0.317 0.556 0.222\nUH-MMM 0.338 0.291 0.403 0.607 0.546 0.685 0.053 0.077 0.041\nCodestrange 0.232 0.337 0.176 0.080 0.415 0.044 0.032 0.437 0.017\nJAD 0.109 0.234 0.071 0.262 0.315 0.224 0.007 0.375 0.003\nYunnan-Deep - - - 0.334 0.520 0.245 - - -\nYunnan-1 - - - 0.173 0.271 0.127 - - -\nhuman 0.644 0.673 0.618 0.831 0.862 0.802 0.621 0.596 0.647\nbaseline 0.232 0.337 0.176 0.306 0.350 0.271 0.032 0.437 0.017\nTable 3: Summary of results in the three evaluation scenarios of the eHealth-KD Challenge.\nThe top result in every metric is highlighted. For comparative purpose, an estimated human\nperformance (based on inter-annotator agreement) and a simple computational baseline are also\nreported.\nAs it can be seen in Table 2, the predom-\ninant solution for feature extraction is using\npre-trained language models such as BERT\nand its variants, although some approaches\nalso include classic NLP features. In con-\ntrast with previous editions, the joint ap-\nproach (solving both subtasks simultaneously\nwith a shared architecture) is used by more\nthan half of the systems that tackle both sub-\ntasks. As in previous editions, subtask A is\ncommonly modelled as a sequence labelling\nproblem using some variant of the BIO en-\ncoding. Likewise, subtask B is commonly\nmodelled as a pairwise classi\fcation problem,\nby combining all pairs of entities detected in\neach sentence.\nTwo approaches stand out as distinctly\nnovel in this edition of the challenge, pre-\nsented by IXA and Vicomtech, respectively.\nThe former models subtask B also as a se-\nquence labelling problem, reusing the same\narchitecture as in subtask A. The latter\npresents a text-to-text architecture which\ntranslates a raw sentence into a semi-\nstructured representation that encodes all the\nentities and their relations.\n5 Results and Discussion\nTable 3 summarises the main results ob-\ntained by all participants in the eHealth-\nKD 2021 Challenge. In the main scenario\nthe best performing system was the joint\narchitecture presented by Vicomtech, fol-\nlowed closely by a very similar architecture\npresented by PUCRJ-PUCPR-UFMG. Both\napproaches are based on the previous edi-tion's winner, further con\frming the hypoth-\nesis that using end-to-end architectures for\nsolving both subtasks simultaneously out-\nperforms task-speci\fc architectures. This\nadvantage seems to be mostly due to the\nstrength of these solutions for solving sub-\ntask A, since in subtask B the best perform-\ning system is the sequence labelling architec-\nture presented by IXA.\nAs expected by the relative complexity\nof the tasks, results for subtask A are sig-\nni\fcantly better than for subtask B. How-\never, neither subtask can be said to be fun-\ndamentally solved up to human performance.\nThe human benchmark, estimated by inter-\nannotator agreement, is approximately 11 :3,\n12:5, and 19:1 percent points above the best\nperforming systems in each scenario, respec-\ntively. This indicates that there is still a sig-\nni\fcant margin for improvement, especially\non the relation extraction subtask.\nThe use of pre-trained language models\nseems to provide a signi\fcant advantage, tak-\ning into consideration the complexities of\ndealing with multiple domains and languages.\nBy leveraging multi-lingual models, most sys-\ntems are capable of dealing with English\nsentences even though there is little to no\ntraining data (i.e., only 50 English sentences\nwere provided in the development collection).\nHowever, there is a gap in performance across\ndomains and languages, as shown by Table 4.\nMost participants have a better performance\non the subset of the test collection that cor-\nresponds to the MedlinePlus and Wikinews\nsources, and a correspondingly worse perfor-\nAlejandro Piad-Morffis, Suilan Estevez-Velarde, Yoan Gutierrez, Yudivian Almeida-Cruz, Andr\u00e9s Montoyo, Rafael Mu\u00f1oz\n238MedlinePlus Wikinews CORD\nTask A T ask\nB Task A T ask\nB Task A T ask\nB\nTeam F1 Di\u000b F1 Di\u000b F1 Di\u000b F1 Di\u000b F1 Di\u000b F1 Di\u000b\nVicom tech 0.843 0.156 0.467 0.093 0.794 0.106 0.478 0.104 0.576 -0.111 0.256 -0.118\nPUCRJ-PUCPR-UFMG 0.779 0.071 0.374 0.109 0.781 0.073 0.362 0.098 0.641 -0.067 0.170 -0.095\nIXA 0.699 0.045 0.562 0.130 0.714 0.060 0.493 0.062 0.609 -0.045 0.338 -0.094\nuhKD4 0.772 0.242 0.538 0.219 0.714 0.184 0.431 0.112 0.348 -0.182 0.127 -0.192\nUH-MMM 0.692 0.081 0.158 0.104 0.695 0.083 0.061 0.006 0.538 -0.073 0.014 -0.041\nCodestrange 0.177 0.096 0.138 0.105 0.124 0.042 0.008 -0.025 0.017 -0.065 0.000 -0.033\nJAD 0.238 -0.025 0.036 0.028 0.243 -0.02 0.000 -0.007 0.280 0.017 0.000 -0.007\nYunnan\n-Deep 0.807 0.470 0.000 0.000 0.286 -0.05 0.000 0.000 0.000 -0.336 0.000 0.000\nYunnan\n-1 0.266 0.094 0.000 0.000 0.269 0.097 0.000 0.000 0.000 -0.172 0.000 0.000\nMean di\u000berence 0.136 0.087 0.063 0.038 -0.114 -0.064\nTable 4: Individual results per domain in subtasks A and B. The Di\u000b column corresponds to\nthe absolute di\u000berence between the presented F1and the aggregated F1on the corresponding\nscenario, as reported in Table 3. A positive Di\u000b indicates that team had a correspondingly\nbetter result on that speci\fc subset than in the overall corpus.\nmance on the CORD subset. On average,\nperformance on MedlinePlus (the most com-\nmon source) is 25:2 and 15:2 percent points\nhigher than on CORD (the least common\nsource) for subtask A and B, respectively.\nThis suggests that multilingual pre-trained\nmodels alone are insu\u000ecient as an e\u000bective\ntransfer learning method in low-resource sce-\nnarios such as the one presented in this chal-\nlenge.\nFigure 2 represents the performance for\neach team across di\u000berent subset collec-\ntions of the testing set. As it can be ob-\nserved, most teams present a signi\fcant drop\nin performance, in both subtasks, for the\nCORD subset with respect to the Medline-\nPlus and the Wikinews subsets. This high-\nlights the relative di\u000eculty of the transfer\nlearning component of the challenge. IXA\npresented the most robust system in this re-\nspect. As an illustrative example, the system\npresented by Yunnan-Deep obtained the sec-\nond best results on the MedlinePlus subset of\nthe testing collection, but their performance\ndrops sharply in the remaining subsets, hence\nachieving an overall score marginally superior\nto the baseline.\nFigure 3 compares annotation labels ac-\ncording to their relative di\u000eculty. We de\fne\nthe di\u000eculty of a speci\fc annotation (e.g.,\na speci\fc entity or relation instance in the\ntesting set) as the relative number of sys-\ntems that correctly identi\fed that annota-\ntion. Afterwards, we compute the inter-\nquartile range of the di\u000eculty score for each\nof the annotations in the testing set. The\n\fgure suggests that entities are, on average,\neasier to recognise than relations, which isconsistent with the superior results obtained\nin Subtask A compared to Subtask B by\nall participants. It also shows which rela-\ntions are relatively easier to recognise. In\ngeneral, relations that are more-closely re-\nlated with a grammatical function (e.g., tar-\nget,subject, in-time ) are easier to detect than\nthose which imply a deeper semantic connec-\ntion (e.g., causes and entails ). The extent\nto which these results depend on the relative\nnumber of annotated instances of each entity\nand relation is an interesting question to ex-\nplore.\nModelling both subtasks independently\nseems to be less e\u000bective than using some\nform of shared architecture. The three best\nperforming systems exploit in some manner\nthe interrelation between both subtasks, ei-\nther by using a single architecture with sep-\narate outputs (thus sharing a large part of\nthe feature extraction model), or by reusing\nthe same architecture in both subtasks. In\ncontrast, systems that attempt to solve sub-\ntask B based on structured information ob-\ntained from subtask A have a signi\fcant drop\nin performance in this task. This suggests\nthat subtask B also bene\fts from direct ac-\ncess to the raw sentence, which cannot be ef-\nfectively captured with position embeddings\nor dependency subtrees alone.\nOne surprising result is that the best\nperforming system in subtask B uses a se-\nquence labelling approach, as opposed to the\nmost commonly deployed pairwise classi\fca-\ntion model. IXA's approach for subtask B\nconsists in feeding a regular NER architec-\nture (the same one used for subtask A) with\na raw sentence, where two entities of interest\nOverview of the eHealth Knowledge Discovery Challenge at IberLEF 2021\n239Figure 2: Graphical representation of the performance by team in each subset collection of the\ntesting set. Each bar represents the scores obtained by each team in both subtasks in a speci\fc\nsubset collection.\nFigure 3: Graphical representation of the relative di\u000eculty of each type of annotation. We\ncompute the relative number of systems that correctly identi\fed each annotation of each type.\nBox plots show the minimum, maximum, and inter-quartile ranges.\nare marked. The model is trained to out-\nput a BIO sequence where tags are aligned\nwith the marked entities and correspond to\nthe label of the relation among them, if any.\nThis requires O(jEj2) queries to the model\nfor each sentence with jEjentities detected.\nWe believe this approach can be improved by\ntraining the model with all entities at once,\npredicting one-to-many relations in a single\npass, thus reducing the complexity to O(jEj)\nper sentence.\nA second promising approach for sub-\ntask B is presented by Vicomtech, althoughit didn't obtain competitive enough results.\nThis approach consists of a text-to-text archi-\ntecture, \fne-tuned from a T5 language model\non the task of translating from raw sentences\nto a semi-structured output that encodes the\nresult of both subtasks. The output structure\nconsists of a list of tuples with all the pairs of\nentities detected and their corresponding re-\nlations. However, being a text-to-text model,\nit su\u000bers from hallucination, which requires\nsome post-processing to \fx. Furthermore,\nthe output structure still contains unneces-\nsary redundancy (the same entities need to\nAlejandro Piad-Morffis, Suilan Estevez-Velarde, Yoan Gutierrez, Yudivian Almeida-Cruz, Andr\u00e9s Montoyo, Rafael Mu\u00f1oz\n240be predicted for every relation in which they\nappear). However, we believe that this is\na fruitful direction for future research that\nshould be explored in greater depth.\nMoving towards a full solution for the\neHealth-KD challenge, there are some funda-\nmental obstacles that still need to be tackled.\nThe most salient one is e\u000bectively capturing\nthe interaction between entities and the se-\nmantic relations in which they appear. As\nwe have learned from observing human an-\nnotators, they usually annotate entities and\nrelations simultaneously. Furthermore, they\ndo not follow the left-to-right natural order of\nthe sentence, but rather annotate the most\nsalient entities \frst (e.g., the main Action\nand related Concepts) and then add the con-\ntextual relations and supporting entities. Al-\nthough it is unclear whether this strategy is\nnecessary, it presents an interesting alterna-\ntive approach for designing an automatic an-\nnotation system in the form of an agent that\nperforms annotation actions on the sentence,\nperhaps trained with a reinforcement learn-\ning approach. For this purpose, it could be\ninteresting to analyse not only the \fnal an-\nnotated sentence but the individual annota-\ntion events that human annotators produce,\nincluding the undoing previous annotations.\nThe four editions of the eHealth-KD chal-\nlenge have shown that knowledge discovery\nin natural language text is still an open and\nchallenging problem. Detecting the most rel-\nevant entities and relations is but a \frst step\nin a hypothetical knowledge discovery system\nthat could be used to intelligently explore the\nvast amount of information available in nat-\nural language. Moving up in this pipeline,\nthe next natural step is to create semantic\nstructures (e.g., knowledge graphs) that in-\ntegrate the entities and relations detected in\na corpus of related sentences. This could en-\nable using natural language queries that can\nbe answered with precise factual information\nencoded in such a semantic structure. Future\neditions of the eHealth-KD challenge will fo-\ncus on those types of problems, in an attempt\nto continue fostering research in the full prob-\nlem of knowledge discovery in natural lan-\nguage.\n6 Conclusions\nThe eHealth Knowledge Discovery Chal-\nlenge, in its fourth edition, demonstrated\nthat there is a growing interest in the NLPcommunity, both Spanish-speaking and in-\nternational, related to research in knowledge\ndiscovery in natural language. While the top\nperforming solutions are all based on state-\nof-the-art neural architectures, there is still a\nlarge margin for improvement, especially re-\ngarding transfer learning to low-resource do-\nmains. New approaches not based on the\nstandard NER pipeline seem promising, but\nstill require further development before they\ncan be applied successfully in such a challeng-\ning scenario.\nSeveral resources resulting from this chal-\nlenge are made available to the academic\ncommunity. A manually annotated corpus\nof entities and semantic relations, compris-\ning 1800 sentences in three domains and two\nlanguages is published online with a permis-\nsive Creative Commons license. Additionally,\nwe also provide baseline implementations and\nutility scripts for loading and manipulating\nthe corpus data, training machine learning\nmodels, and evaluating their results. Fur-\nthermore, the o\u000ecial testing output of partic-\nipant systems in the challenge are also pub-\nlicly available, which can be used as com-\nparison and for drawing further conclusions\nin future research. These resources are re-\nleased with the hope of enabling future re-\nsearchers to build on these results, and con-\ntinue improving the state of the art in auto-\nmatic knowledge discovery from natural lan-\nguage text.\nAcknowledgements\nThis research has been supported by a Car-\nolina Foundation grant in agreement with\nUniversity of Alicante and University of Ha-\nvana. Moreover, the research has been par-\ntially funded by the University of Alicante\nand the University of Havana, the General-\nitat Valenciana ( Conselleria d'Educaci\u0013 o, In-\nvestigaci\u0013 o, Cultura i Esport ) and the Spanish\nGovernment through the projects LIVING-\nLANG ( RTI2018-094653-B-C22), INTE-\nGER (RTI2018-094649-B-I00) and SIIA\n(PROMETEO/2018/089, PROMETEU/2018/089).\nAdditionally, it has been backed by the work\nof both COST Actions: CA19134 {\\Dis-\ntributed Knowledge Graphs\" and CA19142 {\n\\Leading Platform for European Citizens, In-\ndustries, Academia and Policymakers in Me-\ndia Accessibility\".\nIn addition, the authors would like to ac-\nknowledge the invaluable contribution of the\nOverview of the eHealth Knowledge Discovery Challenge at IberLEF 2021\n241annotators who voluntarily participated in\nthe construction of the corpus.\nReferences\nAlfaro-Gonz\u0013 alez, D., D. P\u0013 erez-Perera,\nG. Gonz\u0013 alez-Rodr\u0013 \u0010guez, and A. J. Ota~ no-\nBarrera. 2021. uhKD4 at eHealth-KD\nChallenge 2021: Deep Learning Ap-\nproaches for Knowledge Discovery from\nSpanish Biomedical Documents. In\nProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021).\nAndr\u0013 es, E. 2021. IXA at eHealth-KD Chal-\nlenge 2021: Generic Sequence Labeling as\nRelation Extraction Approach. In Pro-\nceedings of the Iberian Languages Evalu-\nation Forum (IberLEF 2021) .\nEstevez-Velarde, S., Y. Gutierrez, A. Mon-\ntoyo, A. Piad-Mor\u000es, R. Munoz, and\nY. Almeida-Cruz. 2018. Gathering ob-\nject interactions as semantic knowledge.\nInProceedings on the International Con-\nference on Arti\fcial Intelligence (ICAI),\npages 363{369. The Steering Committee\nof The World Congress in Computer Sci-\nence, Computer . . . .\nFloridi, L. and M. Chiriatti. 2020. Gpt-3: Its\nnature, scope, limits, and consequences.\nMinds and Machines, 30(4):681{694.\nGarc\u0013 \u0010a-Pablos, A., N. P\u0013 erez, and\nM. Cuadros. 2021. Vicomtech at\neHealth-KD Challenge 2021: Deep Learn-\ning Approaches to Model Health-related\nText in Spanish. In Proceedings of the\nIberian Languages Evaluation Forum\n(IberLEF 2021) .\nGuan, Z. and R. Liu. 2021. Yunnan-Deep\nat eHealth-KD Challenge 2021: Deep\nLearning Model for Entity Recognition in\nSpanish Documents. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2021) .\nLazer, D. M., M. A. Baum, Y. Benkler, A. J.\nBerinsky, K. M. Greenhill, F. Menczer,\nM. J. Metzger, B. Nyhan, G. Pennycook,\nD. Rothschild, et al. 2018. The science of\nfake news. Science, 359(6380):1094{1096.\nMonteagudo-Garc\u0013 \u0010a, L., A. Marrero-Santos,\nM. S. Fern\u0013 andez-Arias, and H. Ca~ nizares-\nD\u0013 \u0010az. 2021. UH-MMM at eHealth-\nKD Challenge 2021. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2021) .Navarro Comabella, J. G., J. D. Valle Diaz,\nand A. Helguera Fleitas. 2021. JAD at\neHealth-KD Challenge 2021: Simple Neu-\nral Network with BERT for Joint Classi-\n\fcation of Key-Phrases and Relations. In\nProceedings of the Iberian Languages Eval-\nuation Forum (IberLEF 2021).\nPavanelli, L., E. T. Rubel Schneider,\nY. Bonescki Gumiel, T. Castro Ferreira,\nL. Ferro Antunes de Oliveira, J. V.\nAndrioli de Souza, G. P. Meneghel Paiva,\nC. M. Silva e Oliveira, Lucas Emanuel\nCabral Moro, E. Cabrera Paraiso,\nE. Labera, and A. Pagano. 2021.\nPUCRJ-PUCPR-UFMG at eHealth-KD\nChallenge 2021: A Multilingual BERT-\nbased system for Joint Entity Recognition\nand Relation Extraction. In Proceedings\nof the Iberian Languages Evaluation\nForum (IberLEF 2021).\nPiad-Mor\u000es, A., Y. Guit\u0013 errez, S. Estevez-\nVelarde, and R. Mu~ noz. 2019. A General-\nPurpose Annotation Model for Knowledge\nDiscovery: Case Study in Spanish Clini-\ncal Text. In Proceedings of the 2nd Clin-\nical Natural Language Processing Work-\nshop, pages 79{88.\nWang, L. L., K. Lo, Y. Chandrasekhar,\nR. Reas, J. Yang, D. Eide, K. Funk,\nR. Kinney, Z. Liu, W. Merrill, et al.\n2020. Cord-19: The covid-19 open re-\nsearch dataset. ArXiv.\nYang, M. 2021. Yunnan-1 at eHealth-KD\nChallenge 2021: Deep-Learning Methods\nfor Entity Recognition in Medical Text. In\nProceedings of the Iberian Languages Eval-\nuation Forum (IberLEF 2021).\nAlejandro Piad-Morffis, Suilan Estevez-Velarde, Yoan Gutierrez, Yudivian Almeida-Cruz, Andr\u00e9s Montoyo, Rafael Mu\u00f1oz\n242NLP applied to occupational health: MEDDOPROF\nshared task at IberLEF 2021 on automaticrecognition, classi\fcation and normalization of\nprofessions and occupations from medical texts\nPLN aplicado a salud laboral: tarea MEDDOPROF en\nIberLEF 2021 sobre detecci\u0013 on, clasi\fcaci\u0013 on y normalizaci\u0013 on\nautom\u0013 atica de profesiones y ocupaciones en textos m\u0013 edicos\nSalvador Lima-L\u0013 opez1, Eul\u0012 alia Farr\u0013 e-Maduell1, Antonio Miranda-Escalada1,\nVicent Briv\u0013 a-Iglesias2,Martin Krallinger1\n1Barcelona Supercomputing Center, Spain\n2Dublin City University\nfsalvador.limalopez, antonio.miranda, eulalia.farre, martin.krallingerg@bsc.es\nvicent.brivaiglesias2@mail.dcu.ie\nResumen: Entre las caracter\u0013 \u0010sticas sociodemogr\u0013 a\fcas de los pacientes, las ocu-\npaciones juegan un papel fundamental tanto desde el punto de vista de la salud\nlaboral, accidentes laborales y exposici\u0013 on a t\u0013 oxicos y pat\u0013 ogenos como desde el de\nla salud f\u0013 \u0010sica y mental. Este art\u0013 \u0010culo presenta la tarea Medical Documents Profes-\nsion Recognition (MEDDOPROF), celebrada dentro de IberLEF/SEPLN 2021. La\ntarea se centra en el reconocimiento y detecci\u0013 on de ocupaciones en textos m\u0013 edicos\nen castellano. MEDDOPROF propone tres retos: NER (reconocimiento de pro-\nfesiones, situaciones laborales y actividades), CLASS (clasi\fcar cada ocupaci\u0013 on en\nfunci\u0013 on de su referente, como puede ser el paciente o un familiar) y NORM (nor-\nmalizar menciones a las terminolog\u0013 \u0010as ESCO y SNOMED-CT). De un total de 40\nequipos registrados, 15 han presentado un total de 94 sistemas. Los sistemas de\nmejor rendimiento se basan en tecnolog\u0013 \u0010as de aprendizaje profundo como trans-\nformers, llegando a conseguir una F-score de 0.818 en detecci\u0013 on de ocupaciones\n(NER), 0.793 en clasi\fcaci\u0013 on de ocupaciones por su referente (CLASS) y 0.619 en\nnormalizaci\u0013 on (NORM). Futuras iniciativas deber\u0013 \u0010an tener tambi\u0013 en en cuenta aspec-\ntos multiling\u007f ues y la aplicaci\u0013 on en otros dominios como servicios sociales, recursos\nhumanos, an\u0013 alisis del mercado legal y laboral o la pol\u0013 \u0010tica.\nPalabras clave: tarea compartida, dominio cl\u0013 \u0010nico, ocupaciones, castellano.\nAbstract: Among the socio-demographic patient characteristics, occupations play\nan important role regarding not only occupational health, work-related accidents\nand exposure to toxic/pathogenic agents, but also their impact on general physical\nand mental health. This paper presents the Medical Documents Profession Recogni-\ntion (MEDDOPROF) shared task (held within IberLEF/SEPLN 2021), focused on\nthe recognition and normalization of occupations in medical documents in Spanish.\nMEDDOPROF proposes three challenges: NER (recognition of professions, employ-\nment statuses and activities in text), CLASS (classifying each occupation mention\nto its holder, i.e. patient or family member) and NORM (normalizing mentions to\ntheir identi\fer in ESCO or SNOMED CT). From the total of 40 registered teams,\n15 submitted a total of 94 runs for the various sub-tracks. Best-performing systems\nwere based on deep-learning technologies (incl. transformers) and achieved 0.818\nF-score in occupation detection (NER), 0.793 in classifying occupations to their ref-\nerent (CLASS) and 0.619 in normalization (NORM). Future initiatives should also\naddress multilingual aspects and application to other domains like social services,\nhuman resources, legal or job market data analytics and policy makers.\nKeywords: shared task, clinical domain, occupations, Spanish.\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 243-256\nrecibido 04-07-2021 revisado 11-07-2021 aceptado 15-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-21\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural1 Introduction\nHighly relevant demographic patient charac-\nteristics such as age, gender and ethnicity are\noften stored in structured form in electronic\nhealth records (EHRs), facilitating e\u000ecient\nselection and comparative statistical analysis\nof patient subsets. Despite the relevance of\nour livelihood and economic status for health\nand lifestyles, occupations are not systemat-\nically collected in medical documents.\nRelations between disorders and occupa-\ntions might respond to accidents, increased\nrisk of contact and exposure to hazardous\nsubstances, allergens or infectious pathogens\n(Gambhir et al., 2011), and physical and psy-\nchological strain (Stansfeld et al., 2011). Oc-\ncupational health, a medical specialty, aims\nto de\fne and prevent all health issues derived\nfrom our professional activity. The char-\nacterization of the occupations of patients\nis extremely relevant in medicine, specif-\nically regarding occupational transmissible\nand non-transmissible risks, preventive mea-\nsures, health education and safety. Addition-\nally, EHRs can contain work history informa-\ntion, employment statuses and activities and\nhobbies that while not usually structured can\nbe very impactful (Vanotti et al., 2017).\nRecently, machine learning technologies\nhave been applied to the characterization of\noccupational data. Occupational data min-\ning is a sub-\feld within data mining that\nexplores di\u000berent occupational variables to\n\\build up knowledge to subsidize actions that\nmay improve workers' quality of life\" (Fer-\nnandes y Dias, 2019). It has been used,\nfor instance, to examine the relation between\nworkers' characteristics and workplace acci-\ndents (G\u007f ul et al., 2016) or the causes of these\naccidents (Cheng, Yao, and Wu, 2013). Even\nif some studies show that predictive models\nreturn better results when trained on narra-\ntive texts rather than tabular data (Yedla,\nKakhki, and Jannesari, 2020), there are not\nmany openly available resources that allow a\nthorough analysis of this type of data.\nAutomatic extraction of mentions of occu-\npations and employment status from unstruc-\ntured text is a Named Entity Recognition\n(NER) task, where prede\fned types of con-\ncepts are detected directly from documents.\nPrior NER benchmark e\u000borts traditionally\nfocused on general entities like person names,\norganizations and locations, while more spe-\nci\fc entities were left for specialized systems.Although the Sixth Message Understanding\nConference (muc, 1995) already included the\ndetection of management posts (a very spe-\nci\fc type of occupation), recent NER strate-\ngies have only marginally addressed this en-\ntity type when considering more specialized\ndomains, text genres or non-English content.\nNevertheless, there are some exceptions, such\nas the German NoSta-D dataset (Benikova,\nBiemann, and Reznicek, 2014), which consid-\nered professions as a type of organizations.\nIn clinical text processing, the detec-\ntion of profession mentions was partially ad-\ndressed by de-identi\fcation shared task ef-\nforts. Several shared tasks regarded occu-\npations as a type of Personal Health Infor-\nmation item that had to be detected, hid-\nden from plain text or pseudo-anonymized.\nIn English, the 2014 i2b2/UTHealth (Stubbs\ny Uzuner, 2015) and the 2016 CEGS N-\nGRID (Stubbs, Filannino, and Uzuner, 2017)\nshared tasks both included occupations in\ntheir datasets. For Spanish, the MEDDO-\nCAN track of IberEval 2019 (Marimon et\nal., 2019) also included occupations. How-\never, occupations were among the most dif-\n\fcult types of Personal Health Information\nwhen examining the results obtained by au-\ntomatic systems (Stubbs y Uzuner, 2015).\nOutside de-identi\fcation, recently the Indus-\ntrial and Professional Occupations Dataset\n(IPOD) (Liu et al., 2020) was released, which\nincludes a corpus of 475,085 job titles in En-\nglish crawled from LinkedIn and a gazetteer.\nThe limitation of these resources is that\nthey only consider occupations when job ti-\ntles are directly mentioned. However, natural\nlanguages use many expressions to describe\noccupations. In narrative texts, including\nclinical documents, mentions of occupations\ninclude a description of its mission, the ma-\nterials used by the worker, a reference to the\nworkplace and other. In order to detect all\nthese occupational nuances, more exhaustive\nresources that re\rect how speakers actually\ntalk about occupations are needed.\nAn interesting exception to this problem is\nSkillNER (Fareri et al., 2021), a named entity\nrecognition system trained to detect workers'\nskills and competences. Terminological re-\nsources like ESCO (European Skills, Com-\npetences, Quali\fcations and Occupations)\n(European Commission, 2013) { a multilin-\ngual classi\fcation based upon the Interna-\ntional Standard Classi\fcation of Occupations\nSalvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent Briv\u00e1-Iglesias, Martin Krallinger\n244(ISCO) (International Labour Organization,\n2021) { also give some more insight into the\noccupation ecosystem. ESCO \\identi\fes and\ncategorises skills, competences, quali\fcations\nand occupations relevant for the EU labour\nmarket and education and training\", as well\nas their relations, by providing each with a\nunique identi\fer. Clinical terminologies like\nSNOMED-CT also include a myriad of occu-\npations, highlighting the importance of these\nvariables in medicine. To the best of our\nknowledge, there are no other available re-\nsources that describe occupation mentions in\ndepth.\nCharacteristics that profession and em-\nployment status recognition systems need to\naddress in clinical texts include: (1) hetero-\ngeneous types of profession-relevant descrip-\ntions, (2) classi\fcation of whether the de-\ntected expressions refer to the patient, a fam-\nily member, or a healthcare professional and\n(3) mapping the extracted mentions to con-\ntrolled vocabularies or normative terminolo-\ngies to enable semantic interoperability. The\ncomplexity of this task lies in the lack of pre-\ncursors, the wide range of expressions refer-\nring to occupations, and the descriptive and\nheterogeneous nature of medical documents.\nSystems able to address these issues are also\ncrucial for social workers, the industry, policy\nmakers and Human Resources departments.\nIn order to \fnd solutions and following the\nsuccess of previous Information Extraction\nshared tasks in Spanish like CANTEMIST\n(Miranda-Escalada, Farr\u0013 e, and Krallinger,\n2020) and MEDDOCAN (Marimon et al.,\n2019), we have organized the MEDDOPROF\nshared task (MEDical DOcuments PROFes-\nsion Recognition). MEDDOPROF aims to\nfoster the creation of resources and occupa-\ntions detection systems in the \feld of occu-\npational data mining. It is the second shared\ntask on occupational text mining in Span-\nish, after the social media-focused ProfNER\n(Miranda-Escalada et al., 2021).\n2 Task Description\n2.1 Shared Task Goal\nMEDDOPROF focused on the detection and\nnormalization of occupation mentions in clin-\nical case reports written in Spanish. To fa-\ncilitate extrapolation to EHRs, the MED-\nDOPROF clinical cases were compared using\ntext similarity strategies to di\u000berent clinical\nrecords including discharge summaries. Ad-ditionally, to maximize the practical impact\nof this task, participating teams were asked\n(1) to detect di\u000berent types of occupations,\nnamely professions, employment status and\nnon-paid activities and (2) to classify them\naccording to who they refer to in the text\n(patient, family member, health professional,\nsomeone else). We also asked them to nor-\nmalize the detected mentions to one of two\nhighly used multilingual controlled vocabu-\nlaries: the European Skills, Competences,\nQuali\fcations and Occupations classi\fcation\n(ESCO) and SNOMED-CT. Figure 1 gives a\ngeneral overview of the task.\n2.2 Sub-tracks\nThe shared task was divided into three sub-\ntracks, each of them associated to di\u000berent\npractically relevant use cases:\n\u2022MEDDOPROF-NER track . Partici-\npants were asked to \fnd exact mentions\nof occupations in the text and label them\naccording to the type of occupation: pro-\nfession, employment status or activity.\n\u2022MEDDOPROF-CLASS track. This sub-\ntrack required \fnding mentions of occu-\npations in the text as well as determining\nthe person(s) referred to (patient, fam-\nily member, health professional, some-\none else).\n\u2022MEDDOPROF-NORM track. Given a\nlist of valid codes that includes all of\nESCO and a selection of SNOMED-CT\nterms, participants were asked to auto-\nmatically normalize the detected entity\nmentions to their corresponding concept\nidenti\fer. This sub-track is highly rele-\nvant to enable semantic interoperability,\ndata integration and practical exploita-\ntion of NER text mining systems.\n2.3 Shared Task Setting\nThe MEDDOPROF shared task had two dis-\ntinct phases:\nTraining phase . The training set was re-\nleased in April 2021 and participants had al-\nmost two months to build their systems.\nEvaluation phase. In June 2021, the test\nset was released without annotations. Partic-\nipants had around two weeks to make their\npredictions and submit them. Each team was\nallowed up to \fve runs per sub-track.\nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic recognition, classification and normalization of professions and occupations from medical texts\n245Figure 1: MEDDOPROF shared task overview.\n2.4 Evaluation: Metrics and\nBaseline\nAll three sub-tracks are evaluated using\nmicro averaged precision, recall and F-1\nscore. A strict evaluation setting is followed,\ni.e. only exact character o\u000bset matches\nbetween predictions and the manually la-\nbeled Gold Standard are considered cor-\nrect. In sub-tracks MEDDOPROF-NER\nand MEDDOPROF-CLASS, this means that\nboth the text span and label had to be cor-\nrect. In sub-track MEDDOPROF-NORM,\nthe predicted codes had to correspond to the\nmanually assigned concept codes from the\nGold Standard corpus; thus, parent/children\ncodes are not considered correct predictions.\nAs a baseline system, we implemented a\nLevenshtein lexical lookup system with a slid-\ning window. The system uses the annotations\nfrom the training set and scans the input text\nto \fnd new matches. For sub-track 3, the\npredictions generated by the lexical lookup\nwere compared to the training data to pre-\ndict codes. The results are shown in Table 1\nand the code is available on GitHub1.\n1https://github.com/TeMU-BSC/meddoprof-\nbaselineSub-trac k Precision Recall F-Score\nNER 0.465 0.508 0.486\nCLASS 0.391\n0.377 0.384\nNORM 0.502 0.533 0.517\nTable 1: Baseline results.\n3 Corpus and Resources\n3.1 MEDDOPROF Gold Standard\nThe Gold Standard corpus for MEDDO-\nPROF is a collection of 1844 clinical cases,\nreports of individual patients published in\nmedical journals. Unlike Electronic Health\nRecords (EHRs), which might include a lot of\npersonal information of patients and cannot\nusually be shared due to privacy issues, clini-\ncal cases are already anonymized. The MED-\nDOPROF corpus documents were manually\nselected to account for a wide range of occu-\npations and clinical specialities, including oc-\ncupational medicine, primary care, COVID-\n19 and psychiatry. The MEDDOPROF cor-\npus was split into training (around 80%) and\ntest (around 20%) sets. The complete Gold\nStandard corpus is available at Zenodo2.\nThe corpus' entities are distributed along\ntwo di\u000berent annotation axes, one related to\n2https://doi.org/10.5281/zenodo.5070541\nSalvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent Briv\u00e1-Iglesias, Martin Krallinger\n246Figure 2: Overview of the categories in the MEDDOPROF Gold Standard corpus.\noccupations and another related to the ac-\ntual person it refers to. Three di\u000berent types\nof occupations were considered: a) PRO-\nFESION ( profession ; paid jobs), b) SITUA-\nCION LABORAL (employment status ; oc-\ncupational and socioeconomic statuses), and\nc) ACTIVIDAD (activities ; non-paid tasks\nsuch as hobbies). Each mention was subse-\nquently classi\fed according to its referent in\nthe text as: a) PACIENTE ( patient ), b) FA-\nMILIAR (family member ), c) SANITARIO\n(health worker ) and d) OTRO ( other ). This\nclassi\fcation is visually explained, along with\nsome examples, in Figure 2.\n3.1.1 Annotation and Normalization\nProcess\nThe MEDDOPROF Gold Standard corpus\nwas manually annotated by a linguist in col-\nlaboration with a clinician. Based on our ex-\nperience with previous tasks (e.g. ProfNER\n(Miranda-Escalada et al., 2021) and MED-\nDOCAN (Marimon et al., 2019)), we con-\nducted an iterative annotation quality con-\ntrol and re\fnement process. The \frst step\ninvolves the creation and de\fnition of spe-\nci\fc guidelines that aim to include and con-\nstrain how occupations are expressed in clini-\ncal documents. These guidelines were re\fned\nthrough various annotation rounds by two\ndi\u000berent annotators. The iterations contin-\nued until a high Inter-Annotator Agreement\n(pairwise) was reached. Finally, around 15%\nof the documents were sistematically cross-checked by an internal annotator for consis-\ntency, reaching an IAA of 0.9. The anno-\ntation tool brat was used (Stenetorp et al.,\n2012).\nRegarding MEDDOPROF corpus normal-\nisation, we \frst tried to \fnd an ESCO code\nthat re\rected closely the profession/activity\nmentioned in the clinical reports. When we\ndid not \fnd an ESCO code, as was the case\nfor most employment statuses, we turned\nto SNOMED-CT codes, which proved to\nbe quite comprehensive and useful regarding\nemployment status situations (retired, unem-\nployed, homeless, etc). A general overview of\nthe normalisation process and some common\ncodes are described in the guidelines.\nThe \fnal version of the MEDDOPROF\nguidelines is 33 pages long and includes over\n70 rules and exceptions. They are freely\navailable at Zenodo3.\n3.1.2 Corpus Format\nFor the \frst two sub-tracks (NER and\nCLASS), the MEDDOPROF corpus' clini-\ncal cases are provided as UTF-8 text \fles\nwith the annotations as separate \fles in brat\nstando\u000b format (.ann \fles). For each clinical\ncase, there is an associated .ann \fle with the\nsame name in each sub-track.\nFor the normalization sub-track (NORM),\nmappings are distributed in a tab-separated\n\fle that includes four columns: \flename,\nmention text, span and code. For reference,\n3https://doi.org/10.5281/zenodo.4720833\nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic recognition, classification and normalization of professions and occupations from medical texts\n247Documents Annotations Unique Codes Sentences Tokens\nTrain 1,500 3,658 297 49,114 1,075,655\nTest 344 1,085 167 9,513 215,531\nTotal 1,844 4,657 346 58,627 1,291,186\nTable 2: General distribution of the MEDDOPROF corpus.\na list containing all valid codes from the used\nvocabularies (ESCO and SNOMED-CT) was\nalso shared.\n3.1.3 Corpus Statistics\nThe MEDDOPROF corpus is made up of\n1,844 documents, with a total of 58,627 sen-\ntences and 1,291,186 tokens. A complete cor-\npus overview is provided in Table 2, while\nTable 3 breaks down the label distribution of\nthe corpus.\nIn total, there are 4,743 annotations, of\nwhich 2,058 are unique mention strings. On\naverage, each document had between two and\nthree annotations. Each mention was manu-\nally mapped to either ESCO or SNOMED-\nCT, resulting in a total of 3,191 annotations\nmapped to ESCO (297 unique codes) and\n1,552 mapped to SNOMED-CT (49 unique\ncodes). More details and the challenges of\nthe annotation and normalization process are\ndiscussed in Section 5.\n3.2 MEDDOPROF additional\nresources\nComplementary Entities Dataset. In or-\nder to connect the occupational and clinical\naspects of the text, a version of the 1500\ndocuments of the training set that includes\nautomatic annotations for clinical and lin-\nguistic variables was released. This dataset\nwas tagged using an adapted and retrained\nversion of the system previously used for\nthe PharmacoNER Tagger (Amengol-Estap\u0013 e\net al., 2019) and in-house manually labelled\ncorpora. It includes the following enti-\nties: symptoms, diseases, procedures, drugs,\nspecies, negation trigger and scope and un-\ncertainty trigger and scope. Participants\nwere free to implement this extra knowledge\nin their systems. Table 4 gives a general view\nof the entities in this additional dataset, while\nFigure 3 provides an example an annotated\nclinical case from this corpus.\nThe MEDDOPROF complementary enti-\nties dataset can be downloaded together with\nthe training set at Zenodo4.\n4https://doi.org/10.5281/zenodo.4775741Occupations Gazetteer. A gazetteer of\noccupations in Spanish was also released as\na tab-separated \fle. This resource includes\nover 25,000 terms and was constructed by ex-\ntracting information from terminological re-\nsources from multiple terminologies (DeCS,\nESCO, SnomedCT and WordNet) and occu-\npations detected by Stanford CoreNLP in a\nlarge collection of social media Spanish pro-\n\fles. The gazetteer can be found in Zenodo5.\n4 Results\nMEDDOPROF contains three sub-tracks:\nMEDDOPROF-NER, MEDDOPROF-\nCLASS and MEDDOPROF-NORM. Only\nthe \frst two tracks are independent, as\nNORM requires the output of at least one of\nthe other tracks. Participants could choose\nto submit results for one, two or all three\nsub-tracks. Up to \fve submissions were\nallowed for each sub-track. This section\nsummarizes the task's participation, results\nand methodologies used.\n4.1 Participation overview\nA total of 40 teams registered for the\ntask, of which 15 submitted their predic-\ntions. All 15 teams participated in the\nMEDDOPROF-NER sub-track, 11 in the\nMEDDOPROF-CLASS sub-track and 8 in\nthe MEDDOPROF-NORM sub-track. Due\nto the fact that up to 5 systems could be sub-\nmitted, the \fnal number of prediction runs is\nhigh, a total of 94: the NER sub-track re-\nceived 39 runs, CLASS received 29 runs and\nNORM received 26 runs.\nMEDDOPROF attracted participants\nfrom very diverse backgrounds. Ten teams\nwere from academia, three were from\nindustry, one is a collaboration between\nacademia an industry and one participant is\na freelance. Even though most teams were\nfrom Spain, there were also MEDDOPROF\nparticipants from France, Germany, India,\nMexico and the United Kingdom. Table 5\n5https://doi.org/10.5281/zenodo.4524659\nSalvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent Briv\u00e1-Iglesias, Martin Krallinger\n248Patient Family Health Prof. Other Total\nProfession 1,158 134 1,525 410 3,227\nEmpl. Status 1,047 119 0 203 1,369\nActivity 122 7 0 18 147\nTotal 2,327 260 1,525 631 4,743\nTable 3: Label distribution of the MEDDOPROF corpus.\nFigure 3: Example of a clinical case from the MEDDOPROF test set annotated with comple-\nmentary clinical and linguistic entities.\nEntity Frequency\nS\u0013 \u0010ntoma (Symptom ) 17,009\nEnfermedad (Disease ) 20,889\nProcedimiento (Procedures ) 15,033\nF\u0013 armaco (Drug ) 7,960\nOrg. Vivo (Living Being ) 23,150\nNEG (Neg. Trigger ) 18,043\nNSCO (Neg. Scope ) 15,968\nUNC (Unc. Trigger ) 2,912\nUSCO (Unc. Scope ) 2,765\nTable 4: Distribution of the Complementary\nEntities Dataset.\nshows an overview of the MEDDOPROF\nparticipant teams.\n4.2 System results\nTable 6 shows the best results obtained by\neach team. The top results for each sub-track\nwere:\n\u2022MEDDOPROF-NER . The best perform-\ning system for this sub-track was imple-\nmented by the team NLNDE, with an\nF1-score of 0.818. Their precision was\n0.855 and their recall was 0.783. They\nused a combination of pretrained trans-\nformers with Masked Language Mod-\nelling (MLM) and CRFs, as well as\nstrategic data splits. The second best\nsystem was submitted by the MUCIC\nteam, who obtained a 0.8 F1-score and\nemployed a model based on Flair with\nBERT embeddings.\n\u2022MEDDOPROF-CLASS . NLNDE ob-tained the best F1-score (0.793), along\nwith the best precision (0.83) and recall\n(0.759). MUCIC's submission was also\nthe second best system, with an F1-score\nof 0.764. Both teams employed the same\nsystem described for the NER sub-track.\n\u2022MEDDOPROF-NORM . The TALP\nteam obtained the best results in this\ntask using a pre-trained multilingual\nDistilBERT (Sanh et al., 2019) plus a\nBiLSTM. They achieved an F1-score of\n0.619, a precision of 0.675 and a recall\nof 0.572. Fadi obtained the second best\nresult with an F1-score of 0.603 with a\nBERT transformer.\nFor a complete overview of the task's re-\nsults, please refer to the Annex at the end.\n4.3 Methodologies\nOver the past few years, some of the biggest\nadvancements in NLP have been achieved\nby large neural language models and their\ntransformer architecture. This trend has\nbeen observed in MEDDOPROF, where the\nmost common architecture used by partici-\npants are transformers-based language mod-\nels, mainly BERT (Devlin et al., 2019) or its\nSpanish version BETO (Ca~ nete et al., 2020).\nIn all three sub-tracks, almost every team\nused them either directly { \fne-tuning the\ntask data, or in the form of embeddings.\nConditional Random Fields were also uti-\nlized by a few teams, sometimes as a com-\nplete system (gbali team) and sometimes as\nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic recognition, classification and normalization of professions and occupations from medical texts\n249Team Name A\u000el\niation Country A/I Tasks Ref.\nEdIE-Kno wLab Universit y\nof Edinburgh UK A NE,CL,NO (Su\u0013 arez-Paniagua y\nCasey, 2021)\nFadi Universitat Rovira i Virgili Spain A NE,CL,NO -\nGaliza Universitat Oberta de Catalunya Spain A NE,NO -\ngbali Independent France - NE,CL -\nHULAT-UC3M Universidad Carlos III de Madrid Spain A NE -\nICC Instituto de Ingenier\u0013 \u0010a del Conocimiento Spain A NE,CL,NO -\nIITKGP Karaghpur Indian Institute of Technology India A NE (Harkawat y Vaidhya,\n2021)\nKaushikAcharya Philips India Limited India I NE,NO (Acharya, 2021)\nMUCIC Center for Computing Research, National Poly-\ntechnic InstituteMexico A NE,CL (Balouchzahi, Sidorov,\nand Shashirekha,\n2021)\nNLNDE Bosch Center for Arti\fcial Intelligence Germany I NE,CL (Lange, Adel, and\nStr\u007f otgen, 2021)\nSINAI Universidad de Ja\u0013 en Spain A NE,CL,NO (Mesa-Murgado et al.,\n2021)\nSMR-NLP Siemens AG / Ludwig Maximilian University of\nMunichGermany I NE,CL -\nTALP Universitat Polit\u0012 ecnica de Catalunya Spain A NE,CL,NO (Medina Herrera y\nTurmo Borr\u0012 as, 2021)\nURJC-UNED Team Universidad Rey Juan Carlos / Universidad Na-\ncional de Educaci\u0013 on a DistanciaSpain A NE,CL -\nVicomtech NLP-Team Vicomtech Foundation Spain I NE,CL,NO (Zotova, Garc\u0013 \u0010a-\nPablos, and Cuadros,\n2021)\nTable 5: MEDDOPROF team overview. A/I stands for academic or industry institution. In the\nTasks column, NE stands for MEDDOPROF-NER, CL for MEDDOPROF-CLASS and NO for\nMEDDOPROF-NORM.\nNER CLASS NORM\nTeam Name P R F1\nP R F1 P R F1\nEdIE-Kno wLab 0.585 0.712 0.643\n0.604 0.604 0.604 0.165 0.193 0.178\nFadi 0.802 0.678 0.735 0.761 0.644 0.698 0.682 0.541 0.603\nGaliza 0.731 0.597 0.657 -\n- - 0.72 0.482 0.577\ngbali 0.786 0.586 0.671 0.726 0.538 0.618 - - -\nHULAT-UC3M 0.412 0.53 0.464 - - - - - -\nICC 0.741 0.435 0.549 0.662 0.377 0.48 0.567 0.388 0.461\nIITKGP 0.654 0.5 0.567 - - - - - -\nKaushikAcharya 0.807 0.524 0.635 - - - 0.72 0.467 0.566\nMUCIC 0.813 0.788 0.8 0.77 0.75 0.764 - - -\nNLNDE 0.855 0.783 0.818 0.83 0.759 0.793 - -\n-\nSINAI 0.821 0.74 0.778 0.775 0.69 0.73 0.593 0.541 0.566\nSMR-NLP 0.854 0.751 0.799 0.802 0.699 0.747 - - -\nT\nALP 0.761 0.465 0.698 0.694 0.588 0.637 0.675 0.572 0.619\nURJC-UNED Team 0.765 0.706 0.734 0.71 0.664 0.686 - - -\nVicomtech NLP-team 0.758 0.739 0.748 0.71 0.691 0.701 0.488 0. 474 0.481\nBaseline 0.465 0.508 0.486 0.391 0.377 0.384 0.502 0.533 0.517\nTable 6: Best result per team. The best result in each sub-track is presented in bold letters, the\nsecond best is underlined. A dash indicates that the team did not participate in that sub-track.\nFor reference, P indicates Precision, R indicates Recall and F1 indicates F1-score.\na \fnal classi\fcation layer of another archi-\ntecture (NLNDE team). One team (SMR-\nNLP) tried a Bidirectional Recurrent Neu-\nral Network with Long Short Team Memory\n(BiLSTM). Teams like NLNDE also exper-\nimented with variables like domain-speci\fc\nmodels and splitting the data into strategic\npartitions with great success.\nThere were also some entries that used al-ternative methods to neural systems. For in-\nstance, one of the submissions of the Galiza\nteam was a non-neural lookup system based\non rules. Another team, HULAT-UC3M,\nproposed a system that used MeaningCloud's\ntopic extraction API (mea, 2021) to build a\ndictionary-based search engine.\nIt is also noteworthy that most partici-\npants used well-known NLP libraries such as\nSalvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent Briv\u00e1-Iglesias, Martin Krallinger\n250spaCy (Honnibal et al., 2020), HuggingFace\n(Wolf et al., 2020) or Flair (Akbik et al.,\n2019), either to build the entire system or\nat some point in their pipeline.\nAs for the choice of systems for each\nsub-track, many teams re-used their mod-\nels for all three challenges, either by train-\ning the model to output multiple labels or\nvia transfer learning. Even then, results for\nthe NER sub-track are overall higher than for\nthe CLASS sub-track, although the data used\nfor both is the same. This suggests that the\nCLASS sub-track is somewhat harder, some-\nthing that is to be expected given that match-\ning an occupation to its holder requires a\ngood understanding of the sentence's seman-\ntics and syntactic structure. For the normal-\nization sub-track, many teams also employed\nthe same systems as for the other sub-tracks.\nSome teams, like the Vicomtech NLP-team,\nopted to build more specialized systems that\nuse semantic search.\n4.4 Error analysis\nThis section goes through some of the most\nfrequent errors observed when comparing the\nGold Standard annotations and the predic-\ntions of the participants' systems.\nAmbiguous words. In Spanish,\nmany occupation terms are polysemous or\nhomonyms and can be used as nouns and ad-\njectives. For instance, the words \\m\u0013 edico\"\nand \\cl\u0013 \u0010nico\" both mean doctor as a noun\nand clinical as an adjective. This distinc-\ntion seems to be somewhat complex for\ncurrent systems to understand and it is a\ncommon source of false positives. This\ntype of error happens a lot with many fre-\nquent words: \\profesional\" ( professional ),\n\\ejecutivo\" ( executive ), \\ganadero\" (stock\nbreeder ), \\inform\u0013 atico\" ( computer scientist ),\n\\matem\u0013 atico\" (mathematician ), ...\nSomething similar happens with the\nword \\compa~ neros\" ( partner, colleague ).\nIt can refer to \\compa~ neros de trabajo\"\n(coworker ), \\compa~ neros de colegio\" ( class-\nmates ), \\compa~ neros de piso\" (\ratmates )\nand so on. These mentions are highly\ncontext-dependent, and in the Gold Standard\nthey were annotated only when they related\nto occupations. However, some systems la-\nbeled all instances of the word regardless of\nthe context.\nAn interesting point is that some sys-\ntems that use morphological features pre-dicted unrelated words that sound like com-\nmon occupations. For instance, one sys-\ntem decided that \\platanero\" (banana tree,\nwhich sounds like \\camarero\", waiter ) or\n\\des\fbrador\" ( de\fbrator, which sounds like\n\\le~ nador\", woodcutter ) were professions.\nAmbiguous mentions. In the Gold\nStandard, there are some frequent mentions\nthat appear in more than one category. The\ntwo main ones are:\n\u2022\\Trabajador\" ( worker ). On its own,\nwithout any speci\fcation, this word is\nconsidered an employment status. How-\never, whenever it is accompanied by\nsome type of job description (work-\nplace, specialty), it is considered a pro-\nfession (as in \\trabajador del sector\nmetal\u0013 urgico\", metalworker ). Many sys-\ntems falsely labelled \\trabajador\" on its\nown as a profession, failing to consider\nthe context and, thus, not including the\njob description in the prediction. The\nopposite is also true: some predictions\nincluded an occupation together with an\nadjectival or prepositional phrase that is\nnot part of the occupation.\n\u2022\\Cuidador\" ( caregiver ). Most caregivers\nare either a paid profession or a role\ntaken up by a family member, in which\ncase it was annotated as employment\nstatus. This distinction requires a good\nunderstanding of the sentence's context\nand thus proved to be di\u000ecult.\nThis situation was also replicated in the\ncase of activities that can also be paid occu-\npations such as \\deportista\" (sports player )\nor \\m\u0013 usico\" (musician ).\nScope. One of the main challenges of\nNER systems is correct boundary detection\n(Li et al., 2020). Due to the variety in the cor-\npus' annotations, predictions being too short\nor too long are often a source of errors. On\nthe one hand, some common examples of a\nprediction being too short are:\nA\u000exes. Some pre\fxes like \\ex-\" were in-\ncluded as part of the annotated mentions as\nthey provide important information. How-\never, when they were separated by a space\nrather than joined together with the word\nthey accompany (e.g. \\ex trabajadora de\nf\u0013 abrica textil\" former textile factory worker ),\nmultiple systems did not include the pre\fx.\nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic recognition, classification and normalization of professions and occupations from medical texts\n251Occupational granularity . Some occupa-\ntional terms are understood on their own\nor include information about their specialty,\nworkplace, ... (e.g. \\ingeniero\" engineer vs.\n\\ingeniero de caminos\" civil engineer ). Some\npredictions included only the standalone oc-\ncupation and failed to include the speci\fcs.\nCoordinated phrases. Some occupations\nmay reference more than one specialty, as in\n\\profesional de la est\u0013 etica y la belleza\" ( aes-\nthetics and beauty professional ). There were\nsystems who stopped at the \frst specialty\nand did not include the coordinated part.\nOn the other hand, some predictions were\ntoo long for reasons such as:\nSyntactically similar structures. Given the\nvariety of occupational references in the cor-\npus, a good system needs to understand the\nsemantics of the sentence to distinguish rel-\nevant from irrelevant information in syntac-\ntically similar structures. This was not al-\nways the case, as some predictions include\nirrelevant information due to their syntactic\ndistribution. For instance, in the sentence\n\\[...] clasi\fca al trabajador en tres categor\u0013 \u0010as:\napto / no apto / apto en determinadas condi-\nciones\" ( [...] classify the worker in three cate-\ngories: apt / not apt / apt under certain con-\nditions ), one system predicted \\trabajador\nen tres categor\u0013 \u0010as\" as if \\tres categor\u0013 \u0010as\" was\na specialty. Another example is \\profesor en\nCosta de Mar\fl\" ( teacher in Ivory Coast ),\nwhere \\Costa de Mar\fl\" was predicted even\nthough it is a geographical location.\nSeparate mentions joined together . Some-\ntimes, a profession may appear together\nwith an employment status or activity, as in\n\\agricultor jubilado\" (retired farmer ). These\ncases were annotated separately, but were at\ntimes predicted as a single entity.\nIn for the normalization task the main\nsource of errors was code granularity . Mul-\ntiple systems predicted a parent/child con-\ncept node of the GS code, but only exact\ncodes were considered correct. This phe-\nnomenon may be in\ruenced by the similarity\nbetween some of ESCO's concepts.\n5 Discussion\nWe present the MEDDOPROF shared task\nresults and resources on the automatic detec-\ntion and normalization of occupations from\nmedical documents written in Spanish. To\nthe best of our knowledge, it is the \frst at-\ntempt at characterizing occupations in clini-cal documents. Most NLP research has con-\nsidered only job titles as occupations, ne-\nglecting the considerable variability of lan-\nguage expressions used to refer to occupa-\ntional information in written texts. Robust\ndetection of occupation information in text\nrequires a thorough characterization that\ngoes beyond what gazetteer or dictionary-\nbased resources can handle.\nAlthough MEDDOPROF deals with doc-\numents in Spanish, it is clear that the release\nof the MEDDOPROF annotation guidelines\nand the use of multilingual terminologies for\nnormalization can serve as base for similar ef-\nforts in other languages and other application\ndomains.\nThe following resources have been re-\nleased as part of MEDDOPROF: a normal-\nized Gold Standard corpus that character-\nizes occupational language in medical doc-\numents in Spanish6, 33 pages long annota-\ntion guidelines that describe how to anno-\ntate this phenomenon7, a version of the train-\ning set (named MEDDOPROF Complemen-\ntary Entities) that includes automatic predic-\ntions of clinical and linguistic variables8and a\ngazetteer of over 25,000 occupational terms9.\nHigh quality annotation guidelines are key to\nallow extending the corpus beyond the cur-\nrent size and to be able to interpret and un-\nderstand the manual and automatic annota-\ntions preventing a Black box corpus scenario\noften encountered in NLP research.\nA total of 15 participants submitted at\nleast one system, achieving 0.818 F-score in\noccupation detection (MEDDOPROF-NER),\n0.793 in classifying occupations to their\nholder (MEDDOPROF-CLASS) and 0.619\nin normalization (MEDDOPROF-NORM).\nGiven the somewhat small dataset size and\nthe task's complexity (including highly am-\nbiguous and context-dependent mentions),\nthe results are promising. In terms of each\nspeci\fc sub-track, the results for the NER\ntrack are the highest overall. The CLASS\nsub-track's results are somewhat lower de-\nspite using the same data, which indicates\nthat it is more complex and harder to learn.\nWe anticipate that the systems resulting\nfrom the MEDDOPROF task will highlight\nthe importance of socio-demographic entities\n6https://doi.org/10.5281/zenodo.5070540\n7https://doi.org/10.5281/zenodo.4694675\n8https://doi.org/10.5281/zenodo.4694768\n9https://doi.org/10.5281/zenodo.4524658\nSalvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent Briv\u00e1-Iglesias, Martin Krallinger\n252like occupations, especially in the clinical do-\nmain, and help in their processing. Beyond\nthe healthcare application domain, we expect\nthat systems similar to those used for the\nMEDDOPROF track may be adapted and\napplied to heterogeneous \felds such as social\ncare, human resources, legal NLP and even\ngender studies. European research projects\nlike the Exposome Project for Health and\nOccupational Research (EPHOR) could also\nbene\ft from this type of resources and sys-\ntems. Finally, other NLP tasks such as\nanonymization might also bene\ft from hav-\ning more exhaustively annotated data.\nDespite the use of two comprehensive con-\ntrolled vocabularies (ESCO and SNOMED\nCT), entity normalization was challenging,\nas it had to manage concept code granularity\nand concept ambiguity. Some of the entries in\nESCO were highly similar, hindering the se-\nlection of the most appropriate code. For ex-\nample, codes 01, 011 and 0110 are all named\n\\commissioned armed forces o\u000ecers\". The\ndi\u000berence between them is given as a short\ntext description, which is di\u000ecult to exploit\nfor automatic tools. In case of mentions that\ncould potentially \ft several candidate con-\ncepts (e.g. \\militar\" soldier ), we opted for\nmapping to the least granular code.\nA number of employment situations, ac-\ntivities and some illegal occupations were not\ncovered by ESCO. For such entity types, we\nused SNOMED-CT as a target vocabulary.\nNonetheless, it was cumbersome to normalize\nsome important activities of social and med-\nical signi\fcance, including volunteering in\ncharities, and many physical activities prac-\ntised regularly (which we had to label with\nthe general code SCTID: 228447005 \\Physi-\ncally active (\fnding)\"). Similarly, there was\nno appropriate code for people who regu-\nlarly played instruments or went to dance\nlessons, which were simply ascribed as SC-\nTID:300758009 (\\Does engage in a hobby\n(\fnding)\"). Notably, while we do not dis-\npute the therapeutic e\u000bects of these activi-\nties, the most approximate codes for regular\npractice of yoga (SCTID:229224000, \\Par-\nticipation in yoga (regime/therapy)\") and\npilates (SCTID:404928000, \\Pilates exercise\n(regime/therapy)\") necessarily attached a\nmedical signi\fcance to these mentions.\nCurrent trends of preventive medicine and\nhealthy aging include the practice of many\nactivities like going to the gym, singing inchoirs, going for group walks and volunteer-\ning as part of healthy lifestyles. Primary care\ncentres assess the neighbourhood assets and\ngeneral practitioners use social prescribing to\nimprove the health, well-being and quality of\nlife of patients. We believe that these activ-\nities should be properly annotated and nor-\nmalised in medical records to obtain a com-\nprehensive understanding of the health of the\npatient, to further personalised medicine and\nto investigate the most bene\fcial activities\nfor di\u000berent population groups.\nUsually, clinical named entity recognition\nrequires a very strict evaluation setting due\nto the delicate nature of the clinical enti-\nties. Looking at the participant systems'\nFalse Positives, there are many predictions\nthat are almost correct. Sometimes, the dif-\nference between the Gold Standard and the\npredictions is simply a comma. Some other\ntimes, the predictions are approximately cor-\nrect: they include the GS annotation but also\na noun modi\fer or prepositional phrase that\nwas not captured in the GS because it was\nnot relevant.\nThere are some of these approximate pre-\ndictions which might look perfectly okay to a\nhuman even if they do not exactly match the\nGS. Because of this, in future tasks we would\nlike to look into integrating di\u000berent evalua-\ntion methods such as partial matching. This\nhas been done in previous shared tasks, for\ninstance Task 9 (\\Extraction of Drug-Drug\nInteractions from BioMedical Texts\") in Se-\nmEval 2013 (Segura-Bedmar, Mart\u0013 \u0010nez, and\nHerrero-Zazo, 2013).\nAs for the normalization's evaluation, the\nresults show that there were systems that\nhad problems choosing the right granularity\nwithin a family of codes. We initially con-\nsidered using a ranked evaluation, but given\nthe uneven di\u000berence in granularity of some\nESCO codes and the use of more than one\nterminology, we ultimately decided against it\nand also used a strict setting. Following the\nexploratory semantic similarity evaluation\ncriteria already used for the MESINESP2\nshared task (Gasco et al., 2021), such met-\nrics could provide alternative approaches to\nexploit better the di\u000berent levels of granular-\nity and structure of controlled vocabularies\nduring the evaluation process.\nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic recognition, classification and normalization of professions and occupations from medical texts\n253Acknowledgements\nMEDDOPROF was promoted through the\ncollaboration between the Spanish Plan for\nthe Advancement of Language Technology\n(Plan TL) and the BSC. We also want to\nacknowledge the 2020 Proyectos de I+D+i\n- RTI Tipo A (DESCIFRANDO EL PA-\nPEL DE LAS PROFESIONES EN LA\nSALUD DE LOS PACIENTES A TRAVES\nDE LA MINERIA DE TEXTOS (PID2020-\n119266RA-I00)) for support. Finally, we\nwould like to thank: the MEDDOPROF sci-\nenti\fc committee, in special Michelle Turner\n(ISGlobal) and Francisco Javier Sanz Valero\n(Escuela Nacional de Medicina del Trabajo,\nInstituto de Salud Carlos III), as well as Mar-\nvin Ag\u007f uero-Torales, Luis Gasc\u0013 o S\u0013 anchez and\neveryone who participated in the task.\nBibliography\n1995. Appendix F: Information extraction\ntask: Scenario on management succes-\nsion(vl.l). In Sixth Message Understand-\ning Conference (MUC-6): Proceedings of\na Conference Held in Columbia, Mary-\nland, November 6-8, 1995.\n2021. MeaningCloud.\nAcharya, K. 2021. Occupation Recognition\nand Normalization in Clinical Notes. In\nProceedings of the Iberian Languages Eval-\nuation Forum (IberLEF 2021), CEUR\nWorkshop Proceedings.\nAkbik, A., T. Bergmann, D. Blythe, K. Ra-\nsul, S. Schweter, and R. Vollgraf. 2019.\nFlair: An easy-to-use framework for state-\nof-the-art nlp. In NAACL 2019, 2019\nAnnual Conference of the North Ameri-\ncan Chapter of the Association for Com-\nputational Linguistics (Demonstrations),\npages 54{59.\nAmengol-Estap\u0013 e, J., F. Soares, M. Marimon,\nand M. Krallinger. 2019. Pharmaconer\ntagger: a deep learning-based tool for au-\ntomatically \fnding chemicals and drugs in\nspanish medical texts. Genomics Infor-\nmatics, 17:e15, 06.\nBalouchzahi, F., G. Sidorov, and H. L.\nShashirekha. 2021. ADOP FERT-\nAutomatic Detection of Occupations and\nProfession in Medical Texts using Flair\nand BERT. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR Workshop Proceedings .Benikova, D., C. Biemann, and M. Reznicek.\n2014. NoSta-D named entity annota-\ntion for German: Guidelines and dataset.\nInProceedings of the Ninth International\nConference on Language Resources and\nEvaluation (LREC'14), pages 2524{2531,\nReykjavik, Iceland, Mayo. European Lan-\nguage Resources Association (ELRA).\nCa~ nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u0013 erez. 2020. Span-\nish pre-trained bert model and evaluation\ndata. In PML4DC at ICLR 2020 .\nCheng, C.-W., H.-Q. Yao, and T.-C. Wu.\n2013. Applying data mining techniques to\nanalyze the causes of major occupational\naccidents in the petrochemical industry.\nJournal of Loss Prevention in the Process\nIndustries, 26(6):1269{1278.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for lan-\nguage understanding. In Proceedings of\nthe 2019 Conference of the North Amer-\nican Chapter of the Association for Com-\nputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers) , pages 4171{4186, Minneapolis,\nMinnesota, Junio. Association for Compu-\ntational Linguistics.\nEuropean Commission. 2013. European\nskills/competences, quali\fcations and oc-\ncupations.\nFareri, S., N. Melluso, F. Chiarello, and\nG. Fantoni. 2021. Skillner: Mining and\nmapping soft skills from any text. CoRR,\nabs/2101.11431.\nFernandes, F. and A. Dias. 2019. Perspecti-\nvas do uso de minera\u0018 c~ ao de dados e apren-\ndizado de m\u0013 aquina em sa\u0013 ude e seguran\u0018 ca\nno trabalho. Revista Brasileira de Sa\u0013 ude\nOcupacional , 44, 01.\nGambhir, R., G. Singh, S. Sharma, R. Brar,\nand H. Kakar. 2011. Occupational health\nhazards in current dental profession-a re-\nview. The Open Occupational Health and\nSafety Journal, 3, 12.\nGasco, L., A. Nentidis, A. Krithara,\nD. Estrada-Zavala, , R.-T. Murasaki,\nE. Primo-Pe~ na, C. Bojo-Canales,\nG. Paliouras, and M. Krallinger. 2021.\nOverview of BioASQ 2021-MESINESP\ntrack. Evaluation of advance hierarchical\nSalvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent Briv\u00e1-Iglesias, Martin Krallinger\n254classi\fcation techniques for scienti\fc\nliterature, patents and clinical trials.\nG\u007f ul, M., A. Guneri, Y. Fatih, and O. C \u0018 elebi.\n2016. Analysis of the relation between\nthe characteristics of workers and occupa-\ntional accidents using data mining. The\nTurkish Journal of Occupational / En-\nvironmental Medicine and Safety, 1:102{\n118, 04.\nHarkawat, J. and T. Vaidhya. 2021. Analysis\nof the Spanish Pre-Train Language Model\nfor HealthCare Name Entity Recogni-\ntion. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR Workshop Proceedings .\nHonnibal, M., I. Montani, S. Van Lan-\ndeghem, and A. Boyd. 2020. spaCy:\nIndustrial-strength Natural Language\nProcessing in Python.\nInternational Labour Organization. 2021.\nInternational standard classi\fcation of oc-\ncupations (ISCO).\nLange, L., H. Adel, and J. Str\u007f otgen.\n2021. NLNDE at MEDDOPROF: Boost-\ning Transformers in a Low-Resource Set-\nting. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR Workshop Proceedings .\nLi, J., A. Sun, J. Han, and C. Li. 2020.\nA survey on deep learning for named en-\ntity recognition. IEEE Transactions on\nKnowledge and Data Engineering.\nLiu, J., Y. C. Ng, K. L. Wood, and K. H.\nLim. 2020. Ipod: A large-scale indus-\ntrial and professional occupation dataset.\nInProceedings of the 2020 ACM Confer-\nence on Computer Supported Cooperative\nWork and Social Computing Companion\n(CSCW'20), pages 323{328.\nMarimon, M., A. Gonzalez-Agirre, A. In-\ntxaurrondo, H. Rodriguez, J. L. Martin,\nM. Villegas, and M. Krallinger. 2019. Au-\ntomatic de-identi\fcation of medical texts\nin spanish: the meddocan track, corpus,\nguidelines, methods and evaluation of re-\nsults. In IberLEF@SEPLN, pages 618{\n638.\nMedina Herrera, S. and J. Turmo Borr\u0012 as.\n2021. Everything Transformers: Recogni-\ntion, Classi\fcation and Normalisation ofProfessions and Family Relations. In Pro-\nceedings of the Iberian Languages Eval-\nuation Forum (IberLEF 2021), CEUR\nWorkshop Proceedings.\nMesa-Murgado, J.-A., P. L\u0013 opez- \u0013Ubeda, M.-\nC. D\u0013 \u0010az-Galiano, M. T. Mart\u0013 \u0010n-Valdivia,\nand L. A. Ure~ na-L\u0013 opez. 2021. BERT\nRepresentations to Identify Professions\nand Employment Statuses in Health\ndata. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR Workshop Proceedings .\nMiranda-Escalada, A., E. Farr\u0013 e, and\nM. Krallinger. 2020. Named entity\nrecognition, concept normalization and\nclinical coding: Overview of the can-\ntemist track for cancer text mining in\nspanish, corpus, guidelines, methods and\nresults. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2020), CEUR Workshop Proceedings .\nMiranda-Escalada, A., E. Farr\u0013 e-Maduell,\nS. Lima-L\u0013 opez, L. Gasc\u0013 o, V. Briva-\nIglesias, M. Ag\u007f uero-Torales, and\nM. Krallinger. 2021. The profner\nshared task on automatic recognition\nof occupation mentions in social media:\nsystems, evaluation, guidelines, embed-\ndings and corpora. In Proceedings of the\nSixth Social Media Mining for Health\n(#SMM4H) Workshop and Shared Task ,\npages 13{20.\nSanh, V., L. Debut, J. Chaumond, and\nT. Wolf. 2019. Distilbert, a distilled ver-\nsion of bert: smaller, faster, cheaper and\nlighter. ArXiv, abs/1910.01108.\nSegura-Bedmar, I., P. Mart\u0013 \u0010nez, and\nM. Herrero-Zazo. 2013. SemEval-2013\ntask 9 : Extraction of drug-drug interac-\ntions from biomedical texts (DDIExtrac-\ntion 2013). In Second Joint Conference\non Lexical and Computational Seman-\ntics (*SEM), Volume 2: Proceedings of\nthe Seventh International Workshop on\nSemantic Evaluation (SemEval 2013),\npages 341{350, Atlanta, Georgia, USA,\nJunio. Association for Computational\nLinguistics.\nStansfeld, S., F. Rasul, J. Head, and N. Sin-\ngleton. 2011. Occupation and mental\nhealth in a national uk survey. Social\npsychiatry and psychiatric epidemiology,\n46:101{10, 02.\nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic recognition, classification and normalization of professions and occupations from medical texts\n255Stenetorp, P., S. Pyysalo, G. Topi\u0013 c, T. Ohta,\nS. Ananiadou, and J. Tsujii. 2012.\nBrat: a web-based tool for nlp-assisted\ntext annotation. In Proceedings of the\nDemonstrations at the 13th Conference of\nthe European Chapter of the Association\nfor Computational Linguistics, pages 102{\n107.\nStubbs, A., M. Filannino, and O. Uzuner.\n2017. De-identi\fcation of psychiatric in-\ntake records: Overview of 2016 cegs n-grid\nshared tasks track 1. Journal of Biomedi-\ncal Informatics , 75, 06.\nStubbs, A. and O. Uzuner. 2015. Anno-\ntating longitudinal clinical narratives for\nde-identi\fcation: The 2014 i2b2/uthealth\ncorpus. Journal of biomedical informatics ,\n58S, 08.\nSu\u0013 arez-Paniagua, V. and A. Casey. 2021.\nBERT and Approximate String Matching\nfor Automatic Recognition and Normal-\nization of Professions in Spanish Medical\nDocuments. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR Workshop Proceedings .\nVanotti, S., M. Eizaguirre, C. Yastremiz,\nA. Marinangeli, R. Alonso, B. Silva, A. Io-\nrio, F. C\u0013 aceres, and O. Garcea. 2017.\nEstudio del estatus laboral y el nivel so-\ncioecon\u0013 omico en personas con esclerosis\nm\u0013 ultiple en 2 centros de buenos aires.\nNeurolog\u0013 \u0010a Argentina, 10, 09.\nWolf, T., L. Debut, V. Sanh, J. Chau-\nmond, C. Delangue, A. Moi, P. Cis-\ntac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y. Jernite, J. Plu, C. Xu, T. L.\nScao, S. Gugger, M. Drame, Q. Lhoest,\nand A. M. Rush. 2020. Transformers:\nState-of-the-art natural language process-\ning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural\nLanguage Processing: System Demonstra-\ntions, pages 38{45, Online, Octubre. As-\nsociation for Computational Linguistics.\nYedla, A., F. D. Kakhki, and A. Jannesari.\n2020. Predictive modeling for occupa-\ntional safety outcomes and days away from\nwork analysis in mining operations. In-\nternational Journal of Environmental Re-\nsearch and Public Health , 17(19).\nZotova, E., A. Garc\u0013 \u0010a-Pablos, and\nM. Cuadros. 2021. Vicomtech atMEDDOPROF: Automatic Information\nExtraction and Disambiguation in Clin-\nical Text. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2021), CEUR Workshop Proceedings .\nA Annex 1: Supplementary\nMaterials\nMore information on the MEDDOPROF\nGold Standard's occupation typology and the\nresults of all submitted runs can be found\non the MEDDOPROF Supplementary Mate-\nrials, available on Zenodo10.\n10https://doi.org/10.5281/zenodo.5086075\nSalvador Lima-L\u00f3pez, Eul\u00e0lia Farr\u00e9-Maduell, Antonio Miranda-Escalada, Vicent Briv\u00e1-Iglesias, Martin Krallinger\n256Overview of HAHA at IberLEF 2021:Detecting, Rating and Analyzing Humor in Spanish\nOverview de HAHA en IberLEF 2021:\nDetecci\u0013 on, Valoraci\u0013 on y An\u0013 alisis de Humor en Espa~ nol\nLuis Chiruzzo1, Santiago Castro2, Santiago G\u0013 ongora1,\nAiala Ros\u0013 a1, J. A. Meaney3, Rada Mihalcea2,\n1Universidad de la Rep\u0013 ublica, Montevideo, Uruguay\nfluischir, sgongora, aialarg@\fng.edu.uy\n2University of Michigan, Anne Arbor, USA\nfsacastro, mihalceag@umich.edu\n3School of Informatics, The University of Edinburgh, Edinburgh, UK\njameaney@ed.ac.uk\nAbstract: We present the results of HAHA at IberLEF 2021: Humor Analysis ba-\nsed on Human Annotation. This year's edition of the competition includes the two\nclassic tasks of humor detection and rating, plus two novel tasks of humor logic me-\nchanism and target classi\fcation. We describe the corpus created for the challenge,\nthe competition phases, the submitted systems and the main results obtained.\nKeywords: Computational humor, Spanish, Humor mechanism, Humor target.\nResumen: Presentamos los resultados de HAHA en IberLEF 2021: Humor Analysis\nbased on Human Annotation. La edici\u0013 on de la competencia de este a~ no incluye las\ndos tareas cl\u0013 asicas de detecci\u0013 on y valoraci\u0013 on de humor, m\u0013 as dos tareas nuevas de\nclasi\fcaci\u0013 on de mecanismo y objeto de humor. Describimos la creaci\u0013 on del corpus, las\nfases de la competencia, los sistemas enviados y los principales resultados obtenidos.\nPalabras clave: Humor computacional, Espa~ nol, Mecanismo de humor, Objeto de\nhumor.\n1 Introduction\nAmerican author E. B. White once said:\n\\Explaining a joke is like dissecting a frog.\nYou understand it better but the frog dies\nin the process.\" It is generally agreed upon\nthat analyzing humor is a di\u000ecult endeavor\nthat removes all the amusement of the acti-\nvity. However, we believe focusing on humor\nanalysis is important and it is one way of lin-\nking current work on computational humor\nwith a more theoretical background.\nThe \feld of computational humor has had\na surge in recent years, as can be seen by the\ngrowing number of shared tasks related to the\nsubject that have been organized. Most of the\ntime these tasks focus on humor detection,\nand on occasions also humor rating, but a\ndeeper analysis of the way humor works and\nthe topics it deals with continues to be largely\nunexplored (with some exceptions). Our ob-\njective with the HAHA task is to go further\nin the direction of analyzing humor structure\nand content, while at the same time conti-\nnuing to explore the more established tasksof humor detection and rating.\n1.1 Background\nThe study of humor from a computational\nand machine learning perspective is relati-\nvely new. Some noticeable previous works\ninclude (Mihalcea and Strapparava, 2005;\nSj\u007f obergh and Araki, 2007; Castro et al.,\n2016), but a characterization of humor that\nallows its automatic recognition and gene-\nration is far from being speci\fed. Figurati-\nve language, and in particular humor, has\nbeen a productive area of research as re-\ngards shared tasks for several years. SemEval-\n2015 Task 11 (Ghosh et al., 2015) focused\non the challenging aspects posed by \fgura-\ntive language, such as metaphors and irony.\nSemEval-2017 Task 6 (Potash, Romanov, and\nRumshisky, 2017) presented humorous tweets\nsubmitted to a comedy program, and asked\ncompetitors to predict the ranking that the\ncomedy program's audience and producers\ngave the tweets. The previous two editions of\nthe HAHA: Humor Analysis based on Human\nAnnotation task, at IberEVAL 2018 (Castro,\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 257-268\nrecibido 04-07-2021 revisado 11-07-2021 aceptado 15-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-22\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalChiruzzo, and Ros\u0013 a, 2018; Castro et al., 2018)\nand IberLEF 2019 (Chiruzzo et al., 2019),\nconsisted of two subtasks: Humor Detection\nand Funniness Score Prediction. SemEval-\n2020 Task 7 (Hossain et al., 2020) proposed\na task of humor rating in which participants\nhad to predict how humorous an edited head-\nline was and to predict which of two edits\nto the same headline was funnier. More re-\ncently, SemEval-2021 Task 7, called Hahac-\nkaton (Meaney, 2020; Meaney et al., 2021),\ncombined humor detection with o\u000bense de-\ntection, proposing the same subtasks as in\nHAHA 2018 and 2019, and adding two ad-\nditional tasks: O\u000bense Score Prediction and\nControversial Humor Classi\fcation.\nBesides these competitions focusing on the\nmore classical tasks of humor detection and\nrating, SemEval 2017 (Miller, Hempelmann,\nand Gurevych, 2017) took another approach\ntrying to analyze one particular (and very\ncommon) class of jokes: puns. The \frst task\nin this competition was the more usual ap-\nproach of detecting if a text in English con-\ntains a pun, in the second task the partici-\npants had to detect exactly which word of the\ntext is the pun, and the third task implied de-\ntecting what are the di\u000berent senses the pun\nword can be interpreted as. We believe focu-\nsing on this type of analysis is a promising\nway of moving forward in the \feld of compu-\ntational humor, but in our new tasks, instead\nof trying to explore one type of humor mecha-\nnism in depth, we take a broader approach to\ndetecting a larger set of humor mechanisms,\nas well as exploring the most common targets\nassociated to jokes.\n1.2 New Tasks\nMirroring the growing interest in compu-\ntational humor generally, the HAHA task has\nattracted more participants with each ite-\nration. Three research groups participated\nin two tasks during the \frst edition. Inter-\nest rose sharply in the second edition of the\ntask, with 18 participants. However, the per-\nformance achieved by systems in these \frst\nand second editions was still far from human-\nlevel for humor detection. For this reason, in\nthis third edition we included the same two\ntasks of humor detection and rating from the\nprevious editions, with some minor changes.\nFirstly, in the dataset used for the previous\nedition, there were about 38.7 % of humo-\nrous tweets, but this time the new test setwas created with the aim of keeping it as\nbalanced as possible between the humorous\nand non-humorous classes. Secondly, we en-\ndeavoured to include annotators from more\ndiverse backgrounds (see Section 2.1).\nWe also aimed to advance the \feld of\ncomputational humor by adding two new\ntasks which are directly inspired by one of the\nmost well-known and comprehensive theories\nof humor, the General Theory of Verbal Hu-\nmor (GTVH) (Attardo and Raskin, 1991).\nThis theory claims there are six Knowledge\nResources (KRs) used in jokes, which charac-\nterize the type of humor contained therein. In\nparticular, we propose to focus on these two:\nLogic Mechanism (LM) contemplates\nhow the joke works, what are the means by\nwhich it conveys humor (e.g., analogy, exag-\ngeration, wordplay).\nTarget (TA) identi\fes if somebody is being\nlaughed at (the butt of the joke) and who\nthat entity is, which relates to the content of\nthe text.\nNote that this is not the only possible ca-\ntegorization. (Tsakona, 2009) presents some\npractical examples of this theory, (Attardo,\nHempelmann, and Di Maio, 2002) presents\na deeper categorization, while (Reyes et al.,\n2009) describes another possible way of or-\nganizing jokes in a taxonomy, (Berger, 2017)\ndescribes a comprehensive list of 45 mecha-\nnisms that are used to convey humor in jokes,\nand (Buijzen and Valkenburg, 2004) follows\nthe same path for analyzing audiovisual hu-\nmor used in television commercials. We used\nthese ideas as a starting point for our cate-\ngories' de\fnition and, as we will see in Sec-\ntion 2.2, we adapted them to the types of\nmechanisms we found in our dataset.\n2 Corpus\nFor the 2019 edition of this task, we\nbuilt a corpus of 30,000 crowd-annotated\ntweets (Chiruzzo et al., 2019; Chiruzzo, Cas-\ntro, and Ros\u0013 a, 2020). The tweets are labe-\nled to indicate whether they are humorous or\nnot, and each humorous tweet is also annota-\nted with a funniness score, a number between\n1 and 5. All tweets considered humorous ha-\nve at least \fve annotations, while all tweets\nconsidered non-humorous have at least three\nnegative annotations. This corpus was split\nbetween 24,000 tweets for training and 6,000\nfor test. This year's edition of the corpus has\nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea\n25836,000 tweets in total: the 2019 training and\ntest sets were used this year as training and\ndevelopment sets, and we created a new test\nset of 6,000 tweets. We also annotated a sub-\nset of 20 % of each partition with information\nabout humor mechanism and targets.\n2.1 New Test Set\nThe new test corpus was developed by crowd-\nsourcing in a similar way to the corpus used\nin previous editions, using the clasi\fcahu-\nmor1tool. In previous editions of the com-\npetition, we relied on volunteers for the an-\nnotation, but this year we aimed to impro-\nve this situation by sourcing annotators from\nthe Proli\fc2crowd-sourcing platform and pa-\nying them accordingly.\nAs in previous editions, we searched Twit-\nter for accounts that regularly posted jokes in\nSpanish. We downloaded 15,655 tweets from\n18 humorous accounts. We then performed\na check to remove near-duplicates as in (Chi-\nruzzo, Castro, and Ros\u0013 a, 2020): we calculated\nthe Jaccard coe\u000ecient for every pair of tweets\nand built clusters of tweets with a similarity\nscore of more than 0.5, then we manually ins-\npected every cluster and tagged them as near-\nduplicates or not. From the near-duplicate\nclusters, only one tweet was kept and the\nrest were discarded. We also repeated this\nanalysis on the 2019 corpus to avoid inclu-\nding tweets that were too similar to the pre-\nvious ones. After this pruning, 13,032 tweets\nwere left in the new collection. For the non-\nhumorous texts, we also downloaded a ran-\ndom collection of 11,353 tweets in Spanish.\nThe aim was to create a test set of 6,000\ntweets, expecting 5 annotations for each of\nthem, with as much balance as possible bet-\nween the humorous and non-humorous ca-\ntegories. The dataset was annotated in six\nrounds, each consisting of 1,000 tweets an-\nnotated by 26 Spanish-speaking annotators\nhired through Proli\fc. Afterward, there was\na \fnal smaller round to annotate some tweets\nthat obtained fewer annotations. Each an-\nnotator labeled 200 tweets, selecting if each\ntweet is humorous, and if so how funny it is\non a scale from 1 to 5 (see Fig. 1). The task\ntook on average 30 minutes and the annota-\ntors were paid USD 5,00 (10,00 USD/hour).\nEach batch contained 1,000 tweets drawn\nfrom the humorous accounts and the ran-\n1https://www.clasi\fcahumor.com/\n2https://www.proli\fc.co/\nFigure 1: Screenshot of the web tool used for\nthe annotation.\ndom tweets collection. However, the number\nof tweets selected from each collection varied\nbetween batches to keep the collection as ba-\nlanced as possible. After each round, we cal-\nculated how many tweets were labeled as hu-\nmorous, and adapted the proportion of tweets\nbetween collections in the next round accor-\ndingly.\nIn each batch, ten hand-picked tweets we-\nre taken from the 2019 corpus for spam chec-\nking (\fve humorous tweets and \fve non-\nhumorous tweets). These tweets were man-\ndatory for all annotators. They were inten-\nded to check if the users had understood the\ntask and to gauge their attention level. If a\nuser failed to label more than 60 % of these\ntweets correctly, their annotations were dis-\ncarded. Fortunately, only a handful of anno-\ntators failed this quality check and they were\npromptly replaced with other annotators.\nThe result of the annotation process is a\ncorpus of 6,000 tweets with exactly 50 % of\nthe tweets classi\fed as humorous. All of the\nhumorous tweets have \fve annotations each\n(at least three positive ones, with their co-\nrresponding humor rating), while all of the\nnon-humorous tweets have at least three ne-\ngative annotations.\nAround 170 annotators from di\u000berent\nSpanish-speaking countries took part in the\nprocess. The countries with the most annota-\ntors were Mexico (72), Chile (45), and Spain\n(36); but there were also annotators from Ar-\ngentina, Bolivia, Colombia, and Venezuela,\namong others (see Fig. 2). The agreement\nbetween raters for humorous/non-humorous\nclassi\fcation measured with Krippendor\u000b's\nOverview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish\n259Figure 2: Country of origin of the annotators\nfrom Proli\fc. Our only \flter for annotators\nwas that they spoke Spanish as a \frst lan-\nguage.\nalpha was 0.604, similar to the 0.605 score\nreported in previous editions (Chiruzzo, Cas-\ntro, and Ros\u0013 a, 2020).\nIn contrast, the agreement for humor ra-\nting measured as Krippendor\u000b's alpha was\nconsiderably lower than in previous tasks. We\nobtained a 0.085 score (calculated as inter-\nval type of measurement), while it was 0.224\nfor HAHA 2019 (Chiruzzo et al., 2019) and\n0.124 for Hahackaton (Meaney et al., 2021).\nWe believe one reason this could be happe-\nning is the geographic diversity of the an-\nnotators sourced from Proli\fc, which might\nhave had the e\u000bect of increasing the diver-\nsity of opinions on the highly subjective mat-\nter of humor rating. Due to the way we pro-\nmoted the web annotation tool in previous\neditions, the distribution of annotators could\nhave been biased with a large number of an-\nnotators from Uruguay and fewer annotators\nfrom the rest of the Spanish-speaking coun-\ntries. This over-representation of annotators\nfrom one country, sharing a common back-\nground, might have produced more homoge-\nneous opinions on some jokes. To a lesser ex-\ntent, something similar might have happened\nin Hahackaton: although the annotators were\ncarefully selected to cover many age groups,\nthey were all from the United States.\n2.2 Corpus for Logic Mechanism\nand Target\nThe annotation of the corpus for the new\ntasks was more complex as we were dea-\nling with uncharted territory. There were th-\nree rounds of annotation: \frst an exploratory\nphase for \fnding categories, then the bulk of\nthe annotation process, and \fnally a re\fne-\nment phase.Initial de\fnition of categories: We ai-\nmed to de\fne a suitable set of categories\nthat would be comprehensive enough to ca-\ntegorize the texts for our dataset, but kee-\nping in mind they were going to be used\nin the context of a machine learning com-\npetition. We started by sampling a set of\n200 tweets from the 2019 corpus and having\nthem annotated by four annotators (organi-\nzers of the competition) using the humor ca-\ntegories from (Berger, 2017). These are 45\ncategories grouped in four superclasses: lo-\ngic(e.g. absurd, analogy, mistakes), language\n(including exaggeration, wordplay or puns,\nsarcasm), identity (e.g. embarrassment, pa-\nrody, unmasking) and action (e.g. slapstick).\nFrom the beginning it was clear that this\nnumber of categories would be too large, and\nmany of them were not found in our dataset\n(for example, the action categories made no\nsense for the verbal humor in tweets) so the\nobjective was to narrow it down to a mana-\ngeable number of labels, and also to detect\ndi\u000berent categories that were not in this ori-\nginal set but could be present in the corpus.\nThe annotators were also asked to identify\nthe individuals/groups which were the target\nof the jokes in the tweets. We discussed and\niterated over the annotations obtained in this\n\frst approach until we reached an initial set\nof categories to use, which included 12 cate-\ngories for the mechanism (see Section 3.3 for\nthe de\fnitions) and 22 categories for the tar-\nget. The target categories were organized in a\ntree with 12 superclasses, so new labels could\nbe added as appropriate leaves of the tree.\nAnnotation of the corpus: Using this\ninitial set, we selected annotators to process\nthe corpus. Eleven annotators participated in\nthe annotation process for the training and\ndevelopment sets, they were all Computer\nScience students that had taken at least one\ncourse in NLP, and they were compensated\nwith course credit. Each annotator labeled a\ntotal of 600 tweets, 500 of these were uni-\nque to the annotator, while 100 were shared\nwith another annotator in order to calculate\nthe inter-annotator agreement. Their instruc-\ntions were to use the 12 categories for mecha-\nnism, but add new categories to the targets\ntree as necessary, as we knew the corpus could\ncontain many more targets.\nRe\fnement: Once the annotation process\nwas \fnished, a total of 58 target categories\nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea\n260Round 1 Round 2 Round 3\nCategories Subcategories Categories Subcategories Categories\nage children (2), teens, el-\nderly (1)age children (41), elderly (52),\nteens (25)age (121)\nbody shaming (9) body shaming (215) body shaming (224)\nmothers-in-law (1) family aunts (3), couples (87), ex (25),\nfathers (6), grandmothers (4),\nhusbands (1), mothers-in-law\n(52), mothers (45), orphans\n(1), widows (1), wives (9)family/relationships\n(234)\ngender men (3), women (15),\nhomosexuals (1)gender homosexuals (50), men (102),\ntransgender (5), women (329),\nothers (1)lgbt (57)\nmen (105)\nwomen (345)\nhealth alcoholics (4), illness\n(1), mental illness (2)health addictions (23), alcoholics\n(77), disability (12), illness\n(55), mental illness (21)health (70)\nsubstance use (104)\norigin ethnicity (4), race,\nimmigrants (2)origin ethnicity (64), immigrants (6),\nrace (17)ethnicity/origin (93)\nprofessions doctors (1), footba-\nllers (3), musicians\n(3), politicians (3),\nother professions (3)professions actors (7), bankers (2), boxers\n(3), builders (6), doctors (49),\nengineers (6), entertainment \f-\ngures (2), footballers (47), law-\nyers (13), musicians (52), nuns\n(2), politicians (37), sex wor-\nkers (4), teachers (11), other\nprofessions (76)professions (328)\nreligion jewish, jehovah wit-\nnessreligion atheists (1), christians (42),\njehovah witness (9), jewish (2),\nothers (2)religion (56)\nself-deprecating (20) self-deprecating (237) self-deprecating\n(257)\nself-\rattering (3)\nsexual\naggres-\nsorspaedophiles (1), ra-\npists (1)sexual\naggres-\nsorspaedophiles (3), rapists (9),\nothers (3)sexual aggressors\n(18)\nsocial\nstatuspoor (3), rich social\nstatuspoor (60), rich (5) social status (68)\norganizations\n(77)technology (63)\nhipsters (2) hipsters (11)\nideology communism (1)\nTable 1: Target categories (and subcategories) found in each round of annotation: in the \frst\nround of 200 tweets we found 22 categories, in the second round of 6,000 tweets we found 58\ncategories, in the \fnal annotation round we uni\fed and simpli\fed classes to get to 15 categories.\nThe numbers in parenthesis represent the number of instances found for each category.\nwere found in the corpus. Two more anno-\ntators (organizers of the competition) went\nthrough the annotations collecting and unif-\nying all the targets into a tree of categories.\nWe then analyzed the nodes of the tree loo-\nking for a set of categories that was managea-\nble but also contained the most representati-\nve ones. We ended up settling on a collection\nof 15 categories.\nTable 1 shows a summary of the categories\nand number of tweets found for each one of\nthem in the three rounds of annotation.\nThe \fnal step was annotating a subset of\nthe new test set. Two annotators (from the\norganizing team) took part in this, annota-ting 650 tweets each (100 tweets were sha-\nred for calculating inter-tagger agreement).\nIn this case, we considered the categories for\nmechanism and target as \fxed.\nThe average Cohen's kappa achieved for\ninter-annotator agreement of mechanism an-\nnotations in the training and dev sets was\n0.365. This agreement was a little better for\nthe test set, with 0.449. We believe the higher\ninter-annotator agreement in test than in\ntrain and dev is due to the expertise of the\nannotators, but in any case we can a\u000erm that\nit is a very di\u000ecult task.\nOn the other hand, we calculated agree-\nment for the annotations of targets (a multi-\nOverview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish\n261class task) as the F1 score between annota-\ntors (taking one annotator as gold and the\nother one as the candidate). This agreement\nis 0.375 on average for the train and dev sets,\nand slightly lower for the test set: 0.350. This\nway of calculating the agreement does not ta-\nke into account the large number of labels\nthere are, but at least it may give an idea of\nhow di\u000ecult the task is even for humans.\n2.3 Composition of the Corpus\nTable 2 shows a summary of the composition\nof the corpus split in train, development, and\ntest sets. We include the number of tweets\nthat are labeled with each of the mechanism\nand target categories as well.\nTraining Dev Test\nTweets 24000 6000 6000\nHumorous 9253 2342 3000\nMechanism and\ntarget labeled4800 1200 1200\nHaving at least\none target1629 399 400\nMechanism labels\nabsurd 566 142 136\nanalogy 319 84 53\nembarrassment 301 72 28\nexaggeration 476 103 75\ninsults 146 40 21\nirony 371 90 100\nmisunderstanding 416 100 94\nparody 255 59 65\nreference 578 121 85\nstereotype 230 68 35\nunmasking 441 130 69\nwordplay 701 191 439\nTarget labels\nage 105 16 15\nbody shaming 181 43 28\nethnicity/origin 69 24 41\nfamily/relationships 177 57 55\nhealth 58 12 24\nlgbt 40 17 13\nmen 92 13 23\nprofessions 263 65 63\nreligion 45 11 6\nself-deprecating 212 45 36\nsexual aggressors 13 5 8\nsocial status 52 16 8\nsubstance use 83 21 15\ntechnology 51 12 10\nwomen 287 58 74\nTable 2: Composition of the corpus.3 Tasks\nThe HAHA 2021 competition consisted of\nfour tasks. Two of them were analogous to\nthe tasks proposed in HAHA 2018 and 2019,\nand we proposed two novel tasks for this ite-\nration. We also created new baselines for the\n\frst two tasks that are stronger than the ones\nused in previous editions, aiming to increase\nthe challenge.\n3.1 Humor Detection\nGiven a tweet, the task of humor detection is\nto determine if its content is humorous or not\n(intended humor by the author; i.e. a joke).\nThe main metric for measuring performance\nfor this task is the F1 score of the `humorous'\nclass.\nIn previous years we used a simple random\nbaseline for this task, which was a very weak\nbaseline meant to encourage participation in\nthe task. This year we used a slightly stronger\nbaseline, but still one of the simplest machine\nlearning methods: we trained a Na\u007f \u0010ve Bayes\nclassi\fer with TF-IDF features. This method\nachieves an F1 of 0.6493 on the dev set, and\n0.6619 F1 on the test set.\n3.2 Humor Rating\nThe humor rating task is to predict a funni-\nness score value for a tweet on a 5-star ran-\nking, assuming it is humorous. The perfor-\nmance of this task is measured using the root\nmean squared error (RMSE) of the humor ra-\nting.\nIn previous years the baseline for this task\nassigned the average rating found in the trai-\nning corpus to all tweets. This year we trai-\nned a SVM regression model with TF-IDF\nfeatures { arguably the strongest of the ba-\nselines we used for this competition, beating\nthe top scores achieved in previous editions.\nThis method achieves 0.6532 RMSE on the\ndev set, and 0.6704 RMSE on the test set.\n3.3 Humor Logic Mechanism\nClassi\fcation\nFor a humorous tweet, this task is to predict\nthe mechanism by which the tweet conveys\nhumor from a prede\fned set of classes. In this\ntask, only one class per tweet is allowed. The\npossible categories for this task are the follo-\nwing:\n\u2022Absurd: Humor comes from a logical in-\nconsistency in the reasoning.\nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea\n262\u2022Analogy : It is a comparison between\ndissimilar elements.\n\u2022Embarrassment: In the punchline one\nof the participants shames or embarras-\nses another one.\n\u2022Exaggeration: There is a situation or\ncomparison that is exaggerated.\n\u2022Insults: There are insults to the charac-\nters in the joke or to real life people.\n\u2022Irony : They say something but mean\nthe opposite, or they describe a contra-\ndictory situation.\n\u2022Misunderstanding : Humor comes\nfrom a participant understanding a\nquestion or a situation wrong.\n\u2022Parody : The text is similar to another\nknown text or work (for example a song,\na saying, or a movie dialog) but it is mo-\ndi\fed to make it humorous.\n\u2022Reference: It describes a real life situa-\ntion, generally mundane, that the reader\nmight relate to or not, but when the\nreader does identify with the situation\nit results in a humorous e\u000bect3.\n\u2022Stereotype: Humor comes from using\na social group, ethnicity or profession to\nremark on a stereotypical characteristic.\n\u2022Unmasking : Humor comes from a cha-\nracter acting in a certain way and la-\nter showing that their intentions or cha-\nracteristics were di\u000berent than initially\nthought.\n\u2022Wordplay : Uses word ambiguity, made\nup words or combinations of words to\ngive a humorous sense.\nThe main metric for measuring perfor-\nmance in this task is the macro-averaged F1\nscore.\nThe baseline for this task is also a Na\u007f \u0010ve\nBayes model trained with TF-IDF features.\nThis method obtains a 0.1038 F1 score for\nthe dev set, and 0.1001 for the test set.\n3This is the only mechanism category that does\nnot correspond to at least one of the categories\nfrom (Berger, 2017), but it is a particular type of\nhumorous text that is very common in the dataset.3.4 Humor Target Classi\fcation\nFor a humorous tweet, the target classi\fca-\ntion task consists in predicting the target of\nthe joke based on its content (what/who it is\nmaking fun of) from a prede\fned set of clas-\nses. In this case, there may be many classes\nassociated to a tweet, and also tweets that\ndo not belong to any of the categories (it is a\nmulti-label classi\fcation). In this case, each\ntweet can be labeled with zero or more of the\nfollowing categories: age, body shaming,\nethnicity/origin ,family/relationships ,\nhealth, LGBT+, men, professions, re-\nligion, self-deprecating, sexual aggres-\nsors, social status ,substance use, tech-\nnology, and women. This task might be re-\nlated to other important NLP tasks such as\ndetection of o\u000bensive content or hate speech.\nTo measure performance in this task, we\nconsider the labels as pairs (tweet id, cate-\ngory), and calculate the macro-averaged F1\nscore of \fnding those exact pairs.\nThe baseline of this task is more elabo-\nrate. We \frst experimented with using dif-\nferent Na\u007f \u0010ve Bayes models for each target,\nbut they could capture absolutely none of\nthe targets in the corpus. We thus devised\nanother method: assigning the label X to a\ntweet if it contains one of the top words for\nlabel X in the training corpus. The collec-\ntion of top words was created by selecting the\n50th to 60th most frequent words for the la-\nbel (thus discarding the words that were too\ncommon, and the ones that were too rare).\nThis method obtained 0.0595 F1 score on the\ndev set, and 0.0527 on the test set.\n4 Competition\nThe competition ran between March 18 and\nJune 10, 2021 on the CodaLab4platform. Du-\nring that time, a total of 74 users registered\nto participate, and 18 of those users submit-\nted at least one system for the development\nor the evaluation phase.\n4.1 Phases\nThe competition consisted of three phases:\nDevelopment phase: from April 8 to May\n26. At the beginning of this phase, we relea-\nsed the training and development sets. Parti-\ncipants could train their systems and compa-\nre their results for the development set. Each\n4https://competitions.codalab.org/competitions/30090\nOverview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish\n263participant could submit up to 200 systems.\nThere were 276 submissions.\nEvaluation phase: from May 27 to June\n9. At the beginning of this phase, we released\nthe test set. Participants could run their al-\nready trained systems on the test tweets and\nsubmit their results. Each participant could\nsubmit only up to ten submissions. There we-\nre 140 submissions.\nPost-Evaluation phase: from June 10\nonward. This phase started after the compe-\ntition ended so anyone can o\u000ecially bench-\nmark their system. It could be used in the\nfuture to advance the state of the art in the-\nse tasks or to test alternative methods that\nthe users could not send to the evaluation\nphase, although the results obtained in this\nphase would not be part of the o\u000ecial results\nof the competition.\n4.2 Systems Descriptions\nAlmost all the systems submitted to the\ncompetition used neural networks for their\nsolutions, in most cases based on pre-trained\nneural language models such as BERT (De-\nvlin et al., 2018), GPT-2 (Radford et al.,\n2019), or BETO (Ca~ nete et al., 2020), a\nBERT-based model trained entirely with\nSpanish texts. Some teams also trained other\ntypes of models (for example SVM or Deci-\nsion Trees) in order to make comparisons,\nbut none of the participants that sent their\nsystem descriptions submitted any of these\nmodels. In what follows, we give a brief\ndescription of each system:\nJocoso (Grover and Goel, 2021), user\nTanishqGoel, experimented with an en-\nsemble of multiple transformer architectures,\n\fne-tuned on the humor dataset. They\nreached very good results in all four tasks,\nincluding the best result for Task 1.\nicc(Garc\u0013 \u0010a Subies, Betancur S\u0013 anchez,\nand Vaca, 2021) performed a \fne-tuning of\nBETO distinctly for each task, adjusting\nsome hyperparameters for Task 1 also, such\nas learning rate, batch size and dropout rate.\nTo preprocess the data they normalize every\nURL, username and laugh using a unique\ntoken in each case (\\[URL]\", \\[USER]\" and\n\\haha\").\nColBERT (Annamoradnejad and Zoghi,\n2021), user moradnejad, presented anadaptation of the ColBERT model to Spa-\nnish using BETO, feeding a neural network\nwhich has two parallel paths: One path\nmodels each sentence separately, and the\nother path models the whole text, capturing\nthe incoherence of the \fnal line (punchline)\nwith respect to the previous ones.\nBERT4EVER (Wang et al., 2021), user\nNeakail, used a model based on BERT,\ncontinuing its pre-training with the training\ndata from the task. For Tasks 3 and 4,\nthey used a pseudo-labeling technique,\nusing the model to predict the categories of\nthe unlabeled tweets and keeping the ones\nwith high con\fdence, creating 1940 more\nsilver-standard examples. Training a model\nwith this new data obtained the best results\nfor Tasks 3 and 4 in the competition.\nRoMa (Rodriguez, Ortega-Bueno, and\nRosso, 2021), user MJason, presented a\nneural network approach, combining Siamese\nNetworks, to obtain a representation for\neach tweet, and Reinforcement Learning,\nfor clustering tweets based on the learned\nrepresentation.\nUMUTeam (Garc\u0013 \u0010a-D\u0013 \u0010az and Valencia-\nGarc\u0013 \u0010a, 2021), user JAGD, approached\nthe four tasks using linguistic features\nand transformers. They use both the pre-\nprocessed data and the original one in\norder to obtain sentence embeddings, using\nfastText vectors and Spanish BERT, and\nlinguistic features, such as writing style or\nmisspellings, using UMUTextStats. They got\nthe best result for Task 2 in this competition.\nskblaz used a model called autoBOT, an\nautoML technique for text which combines\ndi\u000berent feature spaces ( \u0014Skrlj et al., 2021).\nThe system was run for 8 hours, the default\nneurosymbolic model was used. They report\nthis was one of the \frst non-English attempts\nwith autoBOT.\nkuiyongyi (Kui, 2021) built a system\nbased on Multilingual BERT and LSTM\nmodels for Tasks 1, 3 and 4, and a GPT-2\nbased model for Task 2.\nN&&N (Alsalman and Ennab, 2021),\nusersarasmadi , also used an adaptation of\nthe ColBERT model to create embeddings,\nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea\n264Team Username Task 1 Score Task 2 Score Task 3 Score Task 4 Score\nJocoso TanishqGoel 0.8850 (1) 0.6296 (3) 0.2916 (2) 0.3578 (2)\nicc icc 0.8716 (2) 0.6853 (9) 0.2522 (3) 0.3110 (4)\nColBERT moradnejad 0.8696 (3) 0.6246 (2) 0.2060 (7) 0.3099 (5)\nkuiyongyi kuiyongyi 0.8681 (4) 0.6797 (8) 0.2187 (5) 0.2836 (6)\nnoda risa jgcarrasco 0.8654 (5) - - -\nBERT4EVER Neakail 0.8645 (6) 0.6587 (4) 0.3396 (1) 0.4228 (1)\nRoMa Mjason 0.8583 (7) 1.1975 (11) - -\nUMUTeam JAGD 0.8544 (8) 0.6226 (1) 0.2087 (6) 0.3225 (3)\nskblaz skblaz 0.8156 (9) 0.6668 (6) 0.2355 (4) 0.2295 (7)\nhumBERTor sgp55 0.8115 (10) - - -\nRoBERToCarlos antoniorv6 0.7961 (11) 0.8602 (10) 0.0128 (10) 0.0000 (9)\nN&&N sarasmadi 0.7693 (12) - 0.0404 (9) -\nTECHSSN ayushnanda14 0.7679 (13) 0.6639 (5) - -\nKdeHumor kdehumor 0.7441 (14) 1.5164 (12) - -\nbaseline 0.6619 (15) 0.6704 (7) 0.1001 (8) 0.0527 (8)\nTable 3: Best result for each task for all teams in the competition. The numbers in parenthesis\nindicate the position of the team with respect of the other participants in that task.\nand used these embeddings as hidden layers\nin a neural network.\nTECHSSN (Nanda, Singh, and Gupta,\n2021), user ayushnanda14, created a model\nbased on a \fne-tuning of BERT adapted to\nTasks 1 and 2. They use the BERT encoding\nof the whole text and also individual senten-\nces, extracting features from all of them for\nthe \fnal classi\fcation.\nKdeHumor (Miraj and Aono, 2021)\nused a neural network approach for Tasks\n1 and 2. The network has three layers:\nan embeddings layer that uses pretrained\nSpanish word embeddings, a multi kernel\nCNN layer, and a BiLSTM layer.\nBesides these submissions, there were th-\nree more teams that participated in the com-\npetition (they are located around the middle\nof the table) but did not send any description\nof their system. The teams noda risa, hum-\nBERTor androBERTocarlos did not send\nany description of their systems, but are still\nincluded in Table 3.\n5 Results\nTable 3 shows the results for all the submit-\nting teams, including the top result for each\ntask for each team on the test set. The best\nsystem obtained 88.5 % F1 for humor detec-\ntion, a great improvement over the best re-\nsult in 2019 (82.1 % F1) and 2018 (79.7 %\nF1). However, we must take in considerationthat the test sets for the three editions we-\nre di\u000berent, so they are not directly compa-\nrable. The same happens for humor rating:\nthis year's top system got 0.6226 RMSE for\nTask 1, while in 2019 the best system achie-\nved 0.736, and in 2018 the best system achie-\nved 0.9784. The numbers for this task seem\nto be improving as well, again with the caveat\nthat the test sets were di\u000berent.\nThe humor mechanism and humor target\nclassi\fcation tasks were new this year, and in\nthis case the best system got 33.96 % macro-\naveraged F1 for the mechanism and 42.28 %\nfor the targets. Even though these are harder\ntasks, we consider many systems performed\nbetter than we expected, beating the baseli-\nnes by a large margin. Although there is still\nconsiderable room for improvement, we \fnd\nthese initial results encouraging to keep ad-\nvancing in this direction and pushing the li-\nmits in humor analysis.\n6 Conclusions\nWe presented the third edition of the HAHA\ntask at IberLEF, including two new subtasks\n{ humor mechanism and humor target classi-\n\fcation { in addition to the two tasks already\npresent in previous editions { humor detec-\ntion and humor rating. For this year's edi-\ntion, the existing dataset was extended with\na new test set, and also a subset was enriched\nwith annotations for the new challenges.\nFourteen teams participated in the task,\nmost of them used neural networks based on\npre-trained neural language models.\nOverview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish\n265Regarding Tasks 1 and 2, some partici-\npating teams achieved better results than in\nprevious editions, reaching 88.5 % F1 in task\n1 and 0.6226 RMSE in Task 2.\nFor Tasks 3 and 4, even if the results were\nnot very high, most of the teams were able\nto improve over the proposed baselines. We\nconsider that these are encouraging results,\nand we believe that with a larger corpus the\nlearning process could be improved.\nThe labeled datasets compiled for this\nchallenge are publicly available5. We hope\nthese datasets and the insights from the cu-\nrrent evaluation will encourage more research\non the challenging tasks of humor detection,\nrating, and analysis.\nReferences\nAlsalman, N. and N. Ennab. 2021. N&&N\nat HAHA@IberLEF2021: Determining the\nMechanism of Spanish Tweets using Col-\nBERT. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings, M\u0013 alaga,\nSpain, 9. CEUR-WS.\nAnnamoradnejad, I. and G. Zoghi. 2021.\nColBERT at HAHA 2021: Parallel Neu-\nral Networks for Rating Humor in Spanish\nTweets. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings, M\u0013 alaga,\nSpain, 9. CEUR-WS.\nAttardo, S., C. F. Hempelmann, and\nS. Di Maio. 2002. Script oppositions\nand logical mechanisms: Modeling incon-\ngruities and their resolutions. Humor,\n15(1):3{46.\nAttardo, S. and V. Raskin. 1991. Script\ntheory revis(it)ed: Joke similarity and joke\nrepresentation model. Humor: Internatio-\nnal Journal of Humor Research.\nBerger, A. A. 2017. An anatomy of humor .\nRoutledge.\nBuijzen, M. and P. M. Valkenburg. 2004. De-\nveloping a typology of humor in audiovi-\nsual media. Media psychology , 6(2):147{\n167.\nCastro, S., L. Chiruzzo, and A. Ros\u0013 a. 2018.\nOverview of the HAHA Task: Humor\nAnalysis based on Human Annotation at\n5https://github.com/pln-\fng-udelar/pln-inco-\nresources/tree/master/humor/haha2021IberEval 2018. In CEUR Workshop Pro-\nceedings, volume 2150, pages 187{194.\nCastro, S., L. Chiruzzo, A. Ros\u0013 a, D. Ga-\nrat, and G. Moncecchi. 2018. A\ncrowd-annotated Spanish corpus for hu-\nmor analysis. In Proceedings of the Sixth\nInternational Workshop on Natural Lan-\nguage Processing for Social Media , pages\n7{11, Melbourne, Australia, July. Associa-\ntion for Computational Linguistics.\nCastro, S., M. Cubero, D. Garat, and\nG. Moncecchi. 2016. Is this a joke? de-\ntecting humor in spanish tweets. In Ibero-\nAmerican Conference on Arti\fcial Intelli-\ngence, pages 139{150. Springer.\nCa~ nete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. P\u0013 erez. 2020. Spanish\npre-trained bert model and evaluation da-\nta. In PML4DC at ICLR 2020 .\nChiruzzo, L., S. Castro, M. Etcheverry,\nD. Garat, J. J. Prada, and A. Ros\u0013 a.\n2019. Overview of HAHA at IberLEF\n2019: Humor Analysis based on Human\nAnnotation. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF\n2019), CEUR Workshop Proceedings, Bil-\nbao, Spain, September. CEUR-WS.\nChiruzzo, L., S. Castro, and A. Ros\u0013 a. 2020.\nHAHA 2019 Dataset: A Corpus for Humor\nAnalysis in Spanish. In Proceedings of The\n12th Language Resources and Evaluation\nConference, pages 5106{5112.\nDevlin, J., M.-W. Chang, K. Lee, and K. Tou-\ntanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for lan-\nguage understanding. arXiv preprint ar-\nXiv:1810.04805.\nGarc\u0013 \u0010a-D\u0013 \u0010az, J. A. and R. Valencia-Garc\u0013 \u0010a.\n2021. UMUTeam at HAHA 2021: Linguis-\ntic Features and Transformers for Analy-\nsing Spanish Humor. The What, the How,\nand to Whom. In Proceedings of the Ibe-\nrian Languages Evaluation Forum (Iber-\nLEF 2021) , CEUR Workshop Procee-\ndings, M\u0013 alaga, Spain, 9. CEUR-WS.\nGarc\u0013 \u0010a Subies, G., D. Betancur S\u0013 anchez, and\nA. Vaca. 2021. BERT and SHAP for\nHumor Analysis based on Human Anno-\ntation. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings, M\u0013 alaga,\nSpain, 9. CEUR-WS.\nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea\n266Ghosh, A., G. Li, T. Veale, P. Rosso, E. Shu-\ntova, J. Barnden, and A. Reyes. 2015.\nSemeval-2015 task 11: Sentiment analysis\nof \fgurative language in twitter. In Pro-\nceedings of the 9th international workshop\non semantic evaluation (SemEval 2015),\npages 470{478.\nGrover, K. and T. Goel. 2021.\nHAHA@IberLEF2021: Humor Analysis\nusing Ensembles of Simple Transformers.\nInProceedings of the Iberian Langua-\nges Evaluation Forum (IberLEF 2021),\nCEUR Workshop Proceedings, M\u0013 alaga,\nSpain, 9. CEUR-WS.\nHossain, N., J. Krumm, M. Gamon, and\nH. Kautz. 2020. Semeval-2020 task 7:\nAssessing humor in edited news headlines.\narXiv preprint arXiv:2008.00304.\nKui, Y. 2021. Applying Pre-trained Model\nand Fine-tune to Conduct Humor Analy-\nsis on Spanish Tweets. In Proceedings of\nthe Iberian Languages Evaluation Forum\n(IberLEF 2021), CEUR Workshop Pro-\nceedings, M\u0013 alaga, Spain, 9. CEUR-WS.\nMeaney, J. 2020. Crossing the line: Whe-\nre do demographic variables \ft into hu-\nmor detection? In Proceedings of the 58th\nAnnual Meeting of the Association for\nComputational Linguistics: Student Re-\nsearch Workshop, pages 176{181.\nMeaney, J., S. Wilson, L. Chiruzzo, A. Lo-\npez, and W. Magdy. 2021. SemEval 2021\nTask 7: HaHackathon, Detecting and Ra-\nting Humor and O\u000bense. In 15th Interna-\ntional Workshop on Semantic Evaluation.\nMihalcea, R. and C. Strapparava. 2005. Ma-\nking computers laugh: Investigations in\nautomatic humor recognition. In Procee-\ndings of the Conference on Human Lan-\nguage Technology and Empirical Methods\nin Natural Language Processing, HLT '05,\npages 531{538, Stroudsburg, PA, USA.\nAssociation for Computational Linguis-\ntics.\nMiller, T., C. F. Hempelmann, and I. Gu-\nrevych. 2017. SemEval-2017 task 7:\nDetection and Interpretation of English\nPuns. In Proceedings of the 11th Interna-\ntional Workshop on Semantic Evaluation\n(SemEval-2017), pages 58{68.\nMiraj, R. and M. Aono. 2021. Humor Detec-\ntion in Spanish Tweets Using Neural Net-work. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings, M\u0013 alaga,\nSpain, 9. CEUR-WS.\nNanda, A., A. P. Singh, and A. Gupta. 2021.\nTECHSSN at HAHA @ IberLEF 2021:\nHumor Detection and Funniness Score\nPrediction using Deep Learning Techni-\nques. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings, M\u0013 alaga,\nSpain, 9. CEUR-WS.\nPotash, P., A. Romanov, and A. Rumshisky.\n2017. Semeval-2017 task 6:# hash-\ntagwars: Learning a sense of humor.\nInProceedings of the 11th Internatio-\nnal Workshop on Semantic Evaluation\n(SemEval-2017), pages 49{57.\nRadford, A., J. Wu, R. Child, D. Luan,\nD. Amodei, I. Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9.\nReyes, A., P. Rosso, A. Mart\u0013 \u0010, and M. Taul\u0013 e.\n2009. Caracter\u0013 \u0010sticas y rasgos afectivos\ndel humor: Un estudio de reconocimien-\nto autom\u0013 atico del humor en textos escola-\nres en catal\u0013 an. Procesamiento del lenguaje\nnatural , 43:235{243.\nRodriguez, M., R. Ortega-Bueno, and P. Ros-\nso. 2021. RoMa at HAHA-2021: Deep\nReinforcement Learning to Improve a\nTransformed-based Model for Humor De-\ntection. In Proceedings of the Iberian Lan-\nguages Evaluation Forum (IberLEF 2021) ,\nCEUR Workshop Proceedings, M\u0013 alaga,\nSpain, 9. CEUR-WS.\nSj\u007f obergh, J. and K. Araki. 2007. Recogni-\nzing humor without recognizing meaning.\nIn F. Masulli, S. Mitra, and G. Pasi, edi-\ntors, WILF , volume 4578 of Lecture No-\ntes in Computer Science, pages 469{476.\nSpringer.\n\u0014Skrlj, B., M. Martinc, N. Lavra\u0014 c, and S. Po-\nllak. 2021. autobot: evolving neuro-\nsymbolic representations for explainable\nlow resource text classi\fcation. Machine\nLearning, 110(5):989{1028.\nTsakona, V. 2009. Language and image in-\nteraction in cartoons: Towards a multimo-\ndal theory of humor. Journal of Pragma-\ntics, 41(6):1171{1188.\nOverview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish\n267Wang, L., X. Lin, N. Lin, Y. Fu, K. Wu,\nand J. Wu. 2021. Humor Analysis in\nSpanish Tweets with Multiple Strategies.\nInProceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2021), CEUR\nWorkshop Proceedings, M\u0013 alaga, Spain, 9.\nCEUR-WS.\nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea\n268Overview of the IDPT Task on Irony Detection in\nPortuguese at IberLEF 2021\nResumen de la tarea de detecci\u0013 on de iron\u0013 \u0010a en portugu\u0013 es\n(IDPT) en IberLEF 2021\nUlisses B. Corr^ ea2;3, Leonardo Coelho1,\nLeonardo Santos4,Larissa A. de Freitas1;2\n1Center for Technological Development (CDTec),\nFederal University of Pelotas (UFPel), Pelotas, RS, Brazil\n2Arti\fcial Intelligence Innovation Hub (H2IA),\nFederal University of Pelotas, Pelotas, RS, Brazil\n3Sul-Rio-Grandense Federal Institute of Education,\nScience, and Technology (IFSul),\nCharqueadas, RS, Brazil\n4University of S~ ao Paulo, S~ ao Paulo, SP, Brazil\nflarissa, lgcoelhog@inf.ufpel.edu.br,\nulissescorrea@ifsul.edu.br, leonardosantos@usp.br\nAbstract: This paper presents the Task on Irony Detection in Portuguese (IDPT),\nheld within Iberian Languages Evaluation Forum (IberLEF 2021). We asked the\nparticipants to develop systems capable of identifying irony in texts. We created\ntwo corpora containing tweets and news articles. Twelve teams registered to the\ntask, among which six submitted both predictions and technical reports. The best\nperforming system achieved a Balanced Accuracy (Bacc) value of 0.52 for tweets\n(Team PiLN) and 0.92 for news (Team BERT4EVER).\nKeywords: Irony Detection, Portuguese, Tweets, News.\nResumen: Este art\u0013 \u0010culo presenta la Tarea sobre Detecci\u0013 on de Iron\u0013 \u0010as en Portugu\u0013 es\n(IDPT), realizada en el IberLEF 2021. Les pedimos a los participantes que desarrol-\nlaran sistemas capaces de identi\fcar la iron\u0013 \u0010a en los textos. Creamos dos corpora que\ncontienen tweets y art\u0013 \u0010culos de noticias. Doce equipos se inscribieron en la tarea,\nentre los cuales seis presentaron predicciones e informes t\u0013 ecnicos. El sistema con\nmejor rendimiento logr\u0013 o un valor de precisi\u0013 on equilibrada (Bacc) de 0,52 para los\ntweets (Equipo PiLN) y 0,92 para las noticias (Equipo BERT4EVER).\nPalabras clave: Detecci\u0013 on de Iron\u0013 \u0010a, Portugu\u0013 es, Tweets, Noticias.\n1 Introduction\nThis is nothing new that, in recent decades,\na large part of human communication has\ntaken place on the internet. This way, so-\ncial networks are essential for disseminating\nopinions, positions, re\rections, debates, and\nmany types of manifestation. As a result,\nthese platforms are also a valuable source of\ninformation about public opinion. Therefore,\na target of interest for companies, advertis-\ning, politics, and research, as pointed out\nby (Pang and Lee, 2008). In this context,\nthere has been an increase in research inter-\nest in text mining on social networks in recent\nyears.\nSocial networks coexist with other meansof online communication and information. If,\non the one hand, the immediate communica-\ntion which characterizes social networks such\nas Twitter is an important way of disseminat-\ning opinions, the contents published by tra-\nditional journalism vehicles are also the sub-\nject of studies. Journalistic portals publish,\ndaily, thousands of news, reports, reviews,\nand analyses, among other materials, which\ncirculate quickly in society. All this diverse\ncontent is subject to investigation and anal-\nysis by researchers and other interested par-\nties.\nManifestations published on more robust\nplatforms such as news portals are broad-\ncast in natural language, as are texts on so-\ncial networks. In this sense, they include\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 269-276\nrecibido 05-07-2021 revisado 12-07-2021 aceptado 16-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-23\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalseveral linguistic resources and communica-\ntional strategies. Therefore, it is common to\n\fnd sensational statements conveyed in these\nspaces, even based on irony and other types\nof \fgurative language.\nAs a characteristic of human language,\ncommunication on social networks and\njournalistic portals presents a complexity\nthat challenges investigation techniques and\nmethods. Notably, for research in Arti\fcial\nIntelligence, the processing of \fgurative lan-\nguage represents a relevant challenge. Specif-\nically, the frequent use of irony in this genre\nhas important implications for tasks such as\nsentiment analysis and opinion mining that\naim to extract positive and negative opinions\nautomatically from texts.\nInterpreting ironic messages is a relatively\neasy task for humans. However, some de\nfacto acts involved in this communicational\noperation can confuse. Developing Natural\nLanguage Processing (NLP) resources that\naim to identify irony is crucial to improv-\ning several NLP tasks performance, such as\nSentiment Analysis and Hate Speech Detec-\ntion. Therefore, the IDPT { Irony Detec-\ntion in Portuguese is a task held within the\nIberian Languages Evaluation Forum (Iber-\nLEF 2021), a comparative evaluation cam-\npaign for Natural Language Processing Sys-\ntems in Iberian languages co-located with\ntheSociedad Espa~ nola de Procesamiento del\nLenguaje Natural (SEPLN) conference.\nThe IDPT task aims to challenge di\u000ber-\nent teams to propose techniques capable of\nidentifying ironic utterances automatically in\nnews corpora published in journalistic por-\ntals and tweets published in the microblog-\nging platform Twitter. The present paper\npresents an overview of the task. First, we\nbrie\ry present some theoretical re\rexions on\nirony concepts (Section 2) and describe the\nproposal of our task (Section 3). Section 4\npresents the corpora description and the an-\nnotation process. In Section 5, we describe\nthe evaluation measures. Participant systems\nand the results are discussed in Section 6. Fi-\nnally, the \fnal remarks are done in Section 7.\n2 On Irony\nEven in works that focus on system devel-\nopment, it is important to review concepts\nrelated to the linguistic phenomena explored\nin NLP research. In this section, we propose\nbrie\ry present some important re\rections onirony. It should be noted that theoretical in-\nsights are not our focus in this article.\nThe irony concept is usually understood as\na linguistic resource used with the purpose of\nexpressing the opposite to the literal mean-\ning of an utterance (Cignarella et al., 2018).\nAs discussed in Freitas (Freitas, dos Santos,\nand Deon, 2020), although many researchers\nproject their e\u000borts on irony studies, there is\nno consensus on the de\fnition of this linguis-\ntic phenomenon. Searle (Searle, 1969) and\nGrice (Grice, 1975) propose an irony de\fni-\ntion as an apparent violation of pragmatic\nprinciples. Kreuz and Glucksberg (Kreuz and\nGlucksberg, 1989) understand that the pres-\nence of irony conveys a pragmatic meaning\nby alluding to expectations (failures or not).\nAccording to Reyes (Reyes, Rosso, and Bus-\ncaldi, 2012), irony consists of a contradictory\nproperty in a given context or event.\nOther linguistic phenomena are often as-\nsociated with the concept of irony. It is com-\nmon to \fnd discussions about the di\u000berences\n(or not) between irony, sarcasm, satire, and\nother terms in several areas. Theorists such\nas Grice (Grice, 1975) and Sperber and Wil-\nson (Sperber and Wilson, 1981) propose dif-\nferent de\fnitions for each phenomenon, while\nothers consider sarcasm and irony the same\nphenomenon, making no distinctions. They\nare part of the second study group, such\nas those by Attardo (Attardo, 2000), Reyes\n(Reyes, Rosso, and Veale, 2013), and Hee\n(Van Hee, Lefever, and Hoste, 2016). Ac-\ncording to what Gibbs (Gibbs and Colston,\n2001) defends, he argues that sarcasm, com-\nbined with other linguistic resources such\nas humor, hyperbole, and rhetoric, can be\nconsidered a type of irony. Furthermore,\nMarchetti (Marchetti, Massaro, and Valle,\n2007) argues that irony is an `umbrella con-\ncept' that encompasses other concepts, such\nas sarcasm and satire.\nAlthough discussions in the \feld of phi-\nlosophy of language tend to di\u000berentiate phe-\nnomena, it is observed that, in Arti\fcial In-\ntelligence, the tendency is to group concepts.\nConsidering that NLP tools aimed at detect-\ning irony, in general, cannot consider the con-\ntext of production of an utterance, the area\nfocuses on identifying irony from elements in-\nternal to the text.\nUlisses B. Corr\u00eaa, Leonardo Coelho, Leonardo Santos, Larissa A. de Freitas\n2703 Task Description\nInferring ironic meanings is an easy task to\nhumans, yet some of the speech acts in-\nvolved in this operation might still cause\ncommunicational misunderstandings (Freitas\net al., 2014). Creating methods to under-\nstand ironic text can be a challenging task au-\ntomatically. Still, it is crucial to improve the\nperformance of other NLP's tasks, e.g., Senti-\nment Analysis (Gupta and Yang, 2017), and\nHate Speech Detection (Bosco et al., 2018).\nThis task aims to instigate participants to\napply their solutions for Irony Detection in\nPortuguese. Unfortunately, the availability\nofcorpora written in Portuguese is scarce,\nlimiting the amount of research done for this\nlanguage.\nThis task will contribute to the progress\nof Portuguese NLP, as there is a demand in\nthe area for the development of new methods\nand tools. Previous irony detection competi-\ntions, such as IDAT (Ghanem et al., 2019),\nIroSvA (Bueno et al., 2019), IronITA 2018\n(Cignarella et al., 2018), and SemEval 2018\nTask 3 (Hee, Lefever, and Hoste, 2018), in-\nspired us to develop a speci\fc task for Por-\ntuguese.\nThe task consists of automatically classify\nthe texts (tweets and news) for irony. We\npropose two independent subtasks:\n\u2022Subtask A: Irony detection in Por-\ntuguese tweets;\n\u2022Subtask B: Irony detection in Por-\ntuguese news.\nThe two subtasks have the same objective:\nsystems should determine whether a message\nis ironic or not according to a speci\fed con-\ntext (assigning a binary value 1 or 0).\nThis task is similar with previous tasks on\nirony detection at IDAT (Arabic) (Ghanem\net al., 2019), IronITA 2018 (Italian)\n(Cignarella et al., 2018), and SemEval 2018\nTask 3 (English) (Hee, Lefever, and Hoste,\n2018).\n4 Corpora Description and\nAnnotation Process\nThis section describes the corpora proposed\nfor evaluation, the annotation guidelines, and\nthe inter-annotator agreement.4.1 Datasets Description\nThe corpora proposed for evaluation contain\ntexts (tweets and news) about di\u000berent top-\nics written in Portuguese. In this task, we\nused corpora previously developed by Freitas\n(Freitas et al., 2014), Silva (Silva, 2018), and\nSchubert (Schubert and de Freitas, 2020) for\ntraining purposes.\nFreitas (Freitas et al., 2014) extracted\n2,779 tweets with \\Fim do mundo\" (\\End\nof the world\") expression, between December\n19th and 23rd. Silva (Silva, 2018) extracted\n12,700 tweets labeled with ironic hashtags\n(#ironia and #sarcasmo) and 2,700 tweets\nabout the economy, politics, and education\n- without ironic hashtags, between October\n8th and June 10th. Still, in this collection,\nwe removed all retweets. Schubert (Schubert\nand de Freitas, 2020) extracted ironic news\narticles from Sensacionalista1and The Piau\u0013 \u0010\nHerald2. Non-ironic news articles came from\nEstad~ ao3.\nTraining data was drawn from public\ndatasets of tweets4and news articles5. In\nsummary, this training data contain a set of\n15,212 tweets (12,736 ironic and 2,476 non-\nironic) and 18,494 news articles (7,222 ironic\nand 11,272 non-ironic).\nThe test corpora were created for this\ncompetition through manual annotation of\n300 tweets and 300 news articles.\nThe test dataset is composed of tweets\nwith the ironic hashtags #ironia or #sar-\ncasmo. The remaining tweets talk about\nthe reality show Big Brother Brasil6, referred\nthrough the hashtag #bbb.\nBoth sets of tweets were joined and shuf-\n\red in a single corpus . Then the corpus was\nsplit into three subsets, and each subset was\nassigned to three annotators.\nThe test dataset for news comprises 118\nironic news articles from the Di\u0013 ario de Bar-\nrelas7and 182 non-ironic news articles from\nthe R7 newspaper8. Di\u0013 ario de Barrelas is a\n\fctitious newspaper created to satirize the\nnews. However, the R7 newspaper is well\n1https://www.sensacionalista.com.br/\n2https://piaui.folha.uol.com.br/herald/\n3https://www.estadao.com.br/\n4https://github.com/fabio-ricardo/deteccao-\nironia\n5https://github.com/schuberty/PLNCrawler\n6https://gshow.globo.com/realities/bbb/\n7https://www.diariodebarrelas.com.br/category/noticias/\n8https://www.r7.com/\nOverview of the IDPT Task on Irony Detection in Portuguese at IberLEF 2021\n271known nationally and is a source of real news.\nThe extraction was divided into two steps,\nthe collection of news articles from the Di\u0013 ario\nde Barrelas and the collection of the latest\nnews articles from R7. Then both sets, ironic\nand non-ironic, were joined and shu\u000fed using\nthe Python Random package9.\nThe shu\u000fed dataset was split into three\nsets with 100 samples each. We did that to\ndistribute among annotators. Three volun-\nteers annotated each subset.\nTable 1 presents statistics about the size\nof the documents from news and tweets test\ncorpora .\nThe following section presents the Anno-\ntation Guidelines that instructed volunteers\nduring the manual annotation process of the\ntweets and news test sets.\n4.2 Annotation Guidelines\nIDPT proposed to the participants to clas-\nsify texts into two categories: ironic and non-\nironic.\nWe provided an Annotation Guide for a\nset of volunteers annotators (linguists and\ncomputer science students) that were respon-\nsible for manually classify the testing sam-\nples. In this guide, we developed a discussion\non irony concepts and gathered some exam-\nples (extracted from (Wick-Pedro and Vale,\n2020)) of irony-annotated tweets. The vol-\nunteers, as the resources developed by the\ntask participants, should classify the texts as\nIronic or Non-ironic.\n\u2022Non-ironic text: sentences that do not\ncontain linguistic mechanisms that alter-\nnate their meaning should be considered\nnon-ironic.\n1 Eu sou a favor da sa\u0013 \u0010da da atual\nPresidente. [I am in favor of the de-\nparture of the current President.]\n\u2022Ironic text: one must consider ironic\nthe text where there is an opposition of\nmeaning between what is intended and\nwritten.\n2 S~ ao muito nobres, a\fnal, a chapa\nusou dinheiro de corrup\u0018 c~ ao. [They\nare very noble, after all, the ticked\nhave used money from corruption.]\n9https://docs.python.org/3/library/random.html3 Chora petezada Chora Venezuela...\nminha morada (marquise ou\nviaduto) \u0013 e de luxo... t\u0013 a ok??\n[Cry Workers Party and Venezuela\nsupporters... my house (marquee\nor viaduct) is a luxury one... ok???]\nIn the sentence (1) the speaker expresses\nhis opinion without presenting, in the text,\nany elements that indicate a contradiction\nbetween the explicit opinion and the intended\nmessage.\nOn the other hand, we observe in sen-\ntences (2) and (3) elements that contradict\nthe explicit message and the intended one.\nThus, in (2), it is possible to infer an ironic\nmeaning because of the opposition between\n\\being noble\" and \\using money from cor-\nruption\". Since we know that being noble is\ngood quality and using money from corrup-\ntion is illegal, we can infer that the sentence\nis ironic. In (3), there are at least two ele-\nments that indicate irony: (i) the opposition\nbetween the idea of `living under a marquee\nor viaduct' and `living in a luxury house' and\n(ii) the use of three trite expressions used in\nBrazil by president Bolsonaro and his sup-\nporters (` petezada ' and `Venezuela' for refer-\nring to left-wing people, and ` ta ok? ').\nThe annotation procedure consists of\nmarking each of the statements as ironic or\nnon-ironic. We have not requested teams or\neven our volunteers' team to note the oppo-\nsition found.\n4.3 Inter-annotator Agreement\nEach subset of 100 samples was annotated\nby three annotators. Based on the annota-\ntions of each subset we assessed the inter-\nannotator agreement using the Fleiss Kappa\n(Fleiss, 1971). The value of Kappa for each\nsubset is:\n\u2022Tweets #0: 0.32\n\u2022Tweets #1: 0.36\n\u2022Tweets #2: 0.25\n\u2022News #0: 0.80\n\u2022News #1: 0.94\n\u2022News #2: 0.50\nAs one can see, based in Table 2 interpre-\ntation intervals, the inter-annotator agree-\nment for tweets subsets is considered fair. In\nUlisses B. Corr\u00eaa, Leonardo Coelho, Leonardo Santos, Larissa A. de Freitas\n272Characters Tokens\nMean Min Max Mean Min Max\nNewsIronic 1,650 60 2,548 270 10 435\nNon-ironic 1,813 305 10,362 299 49 1,810\nTweetsIronic 124 22 300 19 5 51\nNon-ironic 97 4 259 15 1 44\nTable 1: Documents Statistics per corpus .\nthe case of the news articles subsets, we ob-\ntained results between moderated andalmost\nperfect.\nValues of \u0014 Interpretation\n\u0014<0 poor\n0<\u0014< 0;2 slight\n0;21<\u0014< 0;4 fair\n0;41<\u0014< 0;6 moderate\n0;61<\u0014< 0;8 substantial\n0;81<\u0014< 1 almost perfect\nTable 2: Fleiss Kappa.\nFinally, we considered ironic or non-ironic\ninstances in which at least two annotators\nagreed, respectively. Considering this crite-\nrion, we obtained a corpus with 123 ironic\nand 177 non-ironic tweets and 115 ironic and\n185 non-ironic news.\n5 Evaluation Measures\nThe training set has been released on March\n30th, and participants had sixteen days to\ntrain their systems. The test set has been\nreleased on April 16th, and each participant\nhad twenty days to submit a maximum of\nthree runs.\nParticipating teams will received training\nand test datasets. The latter was sent with-\nout the label of the samples.\nWe evaluated the predictions sent by the\nparticipants using several metrics: Accuracy\n(Eq. 1), Precision (Eq. 2), Recall (Eq. 3),\nF1 (Eq. 4), and Bacc (Eq. 6). However,\ndue to the unbalanced datasets we choose the\nBalanced Accuracy to rank competitors.\nAccuracy =True Pos +True Neg\nTotal #Instances(1)\nPrecision =True Pos\nTrue Pos +True Neg(2)Recall =True Pos\nTrue Pos +False Neg(3)\nF1 = 2\u0002Precision:Recall\nPrecision +Recall(4)\nSpecificity =True Neg\nTrue Neg +False Pos(5)\nBacc =(Recall +Specificity )\n2(6)\n6 Participants Systems and\nDiscussion of the Results\nTwelve teams registered to the task, among\nwhich six submitted both predictions and\ntechnical reports. Participants are from\nuniversities and companies from four di\u000ber-\nent countries: Brazil, China, Portugal, and\nSpain.\nParticipants used either traditional ma-\nchine learning approaches (Support Vector\nMachine, MultiLayer Perceptron, Logistic\nRegression, Na\u007f \u0010ve Bayes, Random Forest,\nand others) and/or deep learning methods\n(Transformers).\nTables 3 and 4 present participants' re-\nsults for each dataset submitted run. The\nresults are ranked according to the Bacc.\nFor each system, best run is highlighted in\nbold. Team BERT4EVER, from China, used\ntransformers to achieve a Bacc of 0.92 for\nnews dataset. For the tweets dataset, Team\nPiLN, from Brazil, used super\fcial features\nand SVM to achieve Bacc of 0.52.\nBelow we summarize the proposed ap-\nproach of each team:\n\u2022BERT4EVER: the authors use the\nBERT model pre-trained by Souza et\nal. (Souza, Nogueira, and Lotufo, 2020)\nOverview of the IDPT Task on Irony Detection in Portuguese at IberLEF 2021\n273Bacc Accuracy F1 Precision Recall Team Run\n0.92 0.91 0.89 0.83 0.95 TeamBERT4EVER news 3.csv\n0.91 0.90 0.88 0.81 0.95 TeamBERT4EVER news 1.csv\n0.90 0.89 0.87 0.81 0.94 TeamBERT4EVER news 2.csv\n0.89 0.89 0.86 0.82 0.91 TeamSiDi-NLP news 1.csv\n0.83 0.82 0.78 0.71 0.87 TeamUFPR news 2.csv\n0.81 0.81 0.77 0.72 0.81 TeamUFPR news 1.csv\n0.80 0.77 0.76 0.64 0.93 TeamPiLN news 2.csv\n0.80 0.79 0.75 0.68 0.85 TeamCISUC news 3.csv\n0.78 0.79 0.73 0.72 0.74 TeamUFPR news 3.csv\n0.78 0.74 0.74 0.59 0.98 TeamGuillemGSubies news 2.csv\n0.78 0.73 0.73 0.59 0.98 TeamGuillemGSubies news 3.csv\n0.76 0.71 0.72 0.57 0.98 TeamGuillemGSubies news 1.csv\n0.71 0.74 0.63 0.69 0.58 TeamPiLN news 1.csv\n0.52 0.51 0.48 0.40 0.60 TeamCISUC news 2.csv\nTable 3: Participants results ranked in terms of Bacc for news dataset.\nBacc Accuracy F1 Precision Recall Team Run\n0.52 0.47 0.55 0.42 0.80 TeamPiLN tweets 2.csv\n0.51 0.46 0.55 0.41 0.80 TeamPiLN tweets 1.csv\n0.51 0.42 0.58 0.41 1.00 TeamCISUC tweets 1.csv\n0.50 0.42 0.58 0.41 0.99 TeamCISUC tweets 2.csv\n0.50 0.41 0.58 0.41 1.00 TeamUFPR tweets 1.csv\n0.50 0.41 0.58 0.41 0.99 TeamCISUC tweets 3.csv\n0.50 0.41 0.58 0.41 1.00 TeamGuillemGSubies tweets 3.csv\n0.50 0.41 0.58 0.41 1.00 TeamGuillemGSubies tweets 2.csv\n0.50 0.41 0.58 0.41 1.00 TeamGuillemGSubies tweets 1.csv\n0.50 0.41 0.58 0.41 1.00 TeamSiDi-NLP tweets 1.csv\n0.49 0.41 0.57 0.40 0.99 TeamUFPR tweets 2.csv\n0.49 0.41 0.57 0.40 0.98 TeamBERT4EVER tweets 3.csv\n0.49 0.40 0.57 0.40 0.99 TeamBERT4EVER tweets 2.csv\n0.48 0.40 0.56 0.40 0.93 TeamBERT4EVER tweets 1.csv\n0.42 0.38 0.46 0.36 0.64 TeamUFPR tweets 3.csv\nTable 4: Participants results ranked in terms of Bacc for tweets dataset.\nwith three di\u000berent strategies. Strategy\n1: \fne-tune the BERT model separately\nfor the training set in each \feld. Strat-\negy 2: adopt the Loss Weight strategy\nfor the training set in each \feld to solve\ndata imbalance. Strategy 3: combine\nboth datasets.\n\u2022PiLN: the authors use super\fcial fea-\ntures and Support Vector Machine\n(SVM); embeddings and MultiLayer\nPerceptron (MLP). In this work, the su-\nper\fcial features are linguistic features\nsuch as number of named entities, pres-\nence/absence of some symbols, expres-\nsions, number of emojis, frequent words,\namong others. And, the embedding usedis Distributed Bag of Words (DBOW).\n\u2022SiDi-NLP: the authors use BERT\nmodel pre-trained by Souza et al.\n(Souza, Nogueira, and Lotufo, 2020)\nto classify irony sentences and \fve ma-\nchine learning classi\fers { SVM, Logis-\ntic Regressor (LR), MLP, Random For-\nest (RF), and Na\u007f \u0010ve Bayes (NB).\n\u2022CISUC: the authors use three machine\nlearning classi\fers LR, NB, and RF.\n\u2022UFPR: the authors explore a total\nof nine strategies in the preprocess-\ning step, four on the feature extraction\nstep (CountVectorizer, T\fdfVectorizer,\nHashingVectorizer, and Word2Vec), and\nten algorithms in the learning step (RF,\nUlisses B. Corr\u00eaa, Leonardo Coelho, Leonardo Santos, Larissa A. de Freitas\n274MLP, SGD, linearSVC, SVC, decision-\nTree, perceptron, k-nearest neighbors,\nmultinomialNB, and gaussianNB).\n\u2022GuillemGSubies: the author use\nHashingVectorizer and RF to classify the\ndocuments.\nFrom the six systems presented, four use\nclassical machine learning methods. Just\ntwo systems use deep learning methods,\nBERT4EVER and SiDi-NLP, based on trans-\nformers architecture.\n7 Final Remarks\nMotivated by the necessity of improvements\nin Irony Detection task focused on Por-\ntuguese, we proposed a task within the Iber-\nLEF 2021. This paper overviews the \frst\ntask on irony detection in Portuguese to clas-\nsify tweets or news as ironic or non-ironic.\nThe datasets have been manually annotated,\nand the inter-annotator agreement for tweets\nsubsets is considered fair. In the case of the\nnews articles subsets, we obtained results be-\ntween moderated and almost perfect.\nTwelve teams registered to the task,\namong which six teams, from four countries,\nsubmitted both predictions and technical re-\nports.\nClassical feature-based models outper-\nformed deep learning methods when applied\nto the tweets IDPT dataset, achieving a Bacc\nof 0.52. It is important to note that several\nsystems performed with Bacc values around\n0.50 for the tweets task. Without a robust\nstatistical framework, we can not assure the\nsuperiority of any of them.\nThe deep learning methods, based on\nTransformers and BERT, performed bet-\nter when dealing with bigger inputs. The\nBERT4EVER Team achieved a Bacc of 0.92\nfor the news dataset, while approaches based\non classical methods performed, at best, with\nBacc of 0.83.\nAcknowledgments\nThe annotators for their essential work.\nThanks to Amanda Oliveira, Carolina\nGadelha, Felix Silva, Jo~ ao Eduardo Soares,\nJohnny Ferreira Birck, Luiz Ot\u0013 avio Alves\nHammes, and Rodrigo Rodrigues.\nGabriel Schubert and F\u0013 abio Ricardo\nAra\u0013 ujo da Silva by the corpora used to create\nthe training dataset.Johnny Ferreira Birck and Leonardo\nCoelho by the collection of the samples used\nin the test dataset.\nReferences\nAttardo, S. 2000. Irony as relevant inappro-\npriateness. Pragmatics, 32(6):793{826.\nBosco, C., F. Dell'Orletta, F. Poletto,\nM. Sanguinetti, and M. Tesconi. 2018.\nOverview of the EVALITA 2018 hate\nspeech detection task. In T. Caselli,\nN. Novielli, V. Patti, and P. Rosso, ed-\nitors, Proceedings of the Sixth Evaluation\nCampaign of Natural Language Processing\nand Speech Tools for Italian. Final Work-\nshop (EVALITA 2018) co-located with\nthe Fifth Italian Conference on Computa-\ntional Linguistics (CLiC-it 2018), Turin,\nItaly, December 12-13, 2018, volume 2263\nofCEUR Workshop Proceedings. CEUR-\nWS.org.\nBueno, R. O., F. M. R. Pardo, D. I. H.\nFar\u0013 \u0010as, P. Rosso, M. Montes-y-G\u0013 omez,\nand J. Medina-Pagola. 2019. Overview\nof the task on irony detection in span-\nish variants. In M. \u0013A. G. Cum-\nbreras, J. Gonzalo, E. M. C\u0013 amara,\nR. Mart\u0013 \u0010nez-Unanue, P. Rosso, J. Carrillo-\nde-Albornoz, S. Montalvo, L. Chiruzzo,\nS. Collovini, Y. Guti\u0013 errez, S. M. J.\nZafra, M. Krallinger, M. Montes-y-\nG\u0013 omez, R. Ortega-Bueno, and A. Ros\u0013 a,\neditors, Proceedings of the Iberian Lan-\nguages Evaluation Forum co-located with\n35th Conference of the Spanish Soci-\nety for Natural Language Processing,\nIberLEF@SEPLN 2019, Bilbao, Spain,\nSeptember 24th, 2019 , volume 2421 of\nCEUR Workshop Proceedings , pages 229{\n256. CEUR-WS.org.\nCignarella, A. T., S. Frenda, V. Basile,\nC. Bosco, V. Patti, and P. Rosso. 2018.\nOverview of the EVALITA 2018 task on\nirony detection in italian tweets (ironita).\nIn T. Caselli, N. Novielli, V. Patti,\nand P. Rosso, editors, Proceedings of\nthe Sixth Evaluation Campaign of Natu-\nral Language Processing and Speech Tools\nfor Italian. Final Workshop (EVALITA\n2018) co-located with the Fifth Italian\nConference on Computational Linguistics\n(CLiC-it 2018), Turin, Italy, December\n12-13, 2018, volume 2263 of CEUR Work-\nshop Proceedings. CEUR-WS.org.\nOverview of the IDPT Task on Irony Detection in Portuguese at IberLEF 2021\n275Fleiss, J. L. 1971. Measuring nominal scale\nagreement among many raters. Psycho-\nlogical Bulletin , 76(5):378{382.\nFreitas, L., L. dos Santos, and D. Deon.\n2020. Padr~ oes lingu\u0013 \u0010sticos para detec\u0018 c~ ao\nde ironia em m\u0013 ultiplos idiomas. Re-\nvista Eletr^ onica de Inicia\u0018 c~ ao Cient\u0013 \u0010\fca em\nComputa\u0018 c~ ao, 18(4), nov.\nFreitas, L. A., A. A. Vanin, D. N. Hogetop,\nM. N. Bochernitsan, and R. Vieira. 2014.\nPathways for irony detection in tweets.\nInProceedings of the 29th Annual ACM\nSymposium on Applied Computing , SAC\n'14, page 628{633, New York, NY, USA.\nAssociation for Computing Machinery.\nGhanem, B., J. Karoui, F. Benamara,\nV. Moriceau, and P. Rosso. 2019. Idat\nat \fre2019: Overview of the track on\nirony detection in arabic tweets. In Pro-\nceedings of the 11th Forum for Informa-\ntion Retrieval Evaluation , FIRE '19, page\n10{13, New York, NY, USA. Association\nfor Computing Machinery.\nGibbs, R. W. and H. L. Colston. 2001. The\nrisks and rewards of ironic communica-\ntion. IOS Press.\nGrice, H. P. 1975. Logic and conversation .\nAcademic Press.\nGupta, R. K. and Y. Yang. 2017. Crys-\ntalNest at SemEval-2017 task 4: Us-\ning sarcasm detection for enhancing\nsentiment classi\fcation and quanti\fca-\ntion. In Proceedings of the 11th Inter-\nnational Workshop on Semantic Evalua-\ntion (SemEval-2017), pages 626{633, Van-\ncouver, Canada, August. Association for\nComputational Linguistics.\nHee, C. V., E. Lefever, and V. Hoste. 2018.\nSemEval-2018 task 3: Irony detection in\nEnglish tweets. In Proceedings of The\n12th International Workshop on Seman-\ntic Evaluation, pages 39{50, New Orleans,\nLouisiana, June. Association for Compu-\ntational Linguistics.\nKreuz, R. J. and S. Glucksberg. 1989. How\nto be sarcastic: The choice reminder the-\nory of verbal irony. Experimental Psychol-\nogy, 118(4):374{386.\nMarchetti, A., D. Massaro, and A. Valle.\n2007. Non dicevo sul serio. Ri\ressioni su\nironia e psicologia. Franco Angeli.Pang, B. and L. Lee. 2008. Opinion min-\ning and sentiment analysis. Foundations\nand Trends\u00ae in Information Retrieval ,\n2(1{2):1{135.\nReyes, A., P. Rosso, and D. Buscaldi. 2012.\nFrom humor recognition to irony detec-\ntion: The \fgurative language of social me-\ndia. Data and Knowledge Engineering,\n74:2{12.\nReyes, A., P. Rosso, and T. Veale. 2013.\nA multidimensional approach for detect-\ning irony in twitter. Language Resources\nand Evaluation, 47(1):239{268.\nSchubert, G. and L. A. de Freitas. 2020.\nA constru\u0018 c~ ao de um corpus para de-\ntec\u0018 c~ ao de ironia e sarcasmo em portugu^ es.\nInAnais do XVII Encontro Nacional de\nIntelig^ encia Arti\fcial e Computacional ,\npages 709{717, Porto Alegre, RS, Brasil.\nSBC.\nSearle, J. R. 1969. Speech Acts: An Essay in\nthe Philosophy of Language. Cambridge\nUniversity Press.\nSilva, F. R. 2018. Detec\u0018 c~ ao de ironia\ne sarcasmo em l\u0013 \u0010ngua portuguesa:\numa abordagem utilizando deep\nlearning. https://github.com/fabio-\nricardo/deteccao-ironia. Acessado em:\n25-05-2019.\nSouza, F., R. Nogueira, and R. Lotufo. 2020.\nBERTimbau: pretrained BERT models\nfor Brazilian Portuguese. In 9th Brazil-\nian Conference on Intelligent Systems,\nBRACIS, Rio Grande do Sul, Brazil, Oc-\ntober 20-23.\nSperber, D. and D. Wilson. 1981. Irony and\nthe Use - Mention Distinction . Academic\nPress.\nVan Hee, C., E. Lefever, and V. Hoste. 2016.\nGuidelines for annotating irony in social\nmedia text, version 2.0. Technical report,\nLT3 Technical Report Series.\nWick-Pedro, G. and O. A. Vale. 2020. Co-\nmentcorpus: Description and analysis of\nirony in a corpus of opinion for brazil-\nian portuguese. Cadernos De Lingu\u0013 \u0010stica ,\n1(2):1{15.\nUlisses B. Corr\u00eaa, Leonardo Coelho, Leonardo Santos, Larissa A. de Freitas\n276Overview of ADoBo 2021: Automatic Detection ofUnassimilated Borrowings in the Spanish Press\nResumen de ADoBo 2021: detecci\u0013 on autom\u0013 atica de pr\u0013 estamos\nl\u0013 exicos no asimilados en la prensa espa~ nola\nElena \u0013Alvarez Mellado1, Luis Espinosa Anke2, Julio Gonzalo Arroyo1,\nConstantine Lignos3, Jordi Porta Zamorano4\n1NLP & IR group, UNED, Madrid, Spain\n2School of Computer Science and Informatics, Cardi\u000b University, Cardi\u000b, UK\n3Michtom School of Computer Science, Brandeis University, Massachusetts, USA\n4Centro de Estudios de la RAE, Madrid, Spain\nelena.alvarez@lsi.uned.es, espinosa-ankel@cardi\u000b.ac.uk,\njulio@lsi.uned.es, lignos@brandeis.edu, porta@rae.es\nAbstract: This paper summarizes the main \fndings of the ADoBo 2021 shared\ntask, proposed in the context of IberLef 2021. In this task, we invited participants to\ndetect lexical borrowings (coming mostly from English) in Spanish newswire texts.\nThis task was framed as a sequence classi\fcation problem using BIO encoding. We\nprovided participants with an annotated corpus of lexical borrowings which we split\ninto training, development and test splits. We received submissions from 4 teams\nwith 9 di\u000berent system runs overall. The results, which range from F1 scores of 37\nto 85, suggest that this is a challenging task, especially when out-of-domain or OOV\nwords are considered, and that traditional methods informed with lexicographic in-\nformation would bene\ft from taking advantage of current NLP trends.\nKeywords: Automatic detection of borrowings, loanword detection, linguistic bor-\nrowing, anglicisms.\nResumen: En este art\u0013 \u0010culo presentamos los resultados de ADoBo 2021, la tarea\ncompartida de IberLEF 2021 sobre detecci\u0013 on de pr\u0013 estamos l\u0013 exicos en la prensa\nespa~ nola. En esta tarea abordamos la detecci\u0013 on de pr\u0013 estamos como un problema\nde etiquetado de secuencias. A los participantes de la tarea se les proporcion\u0013 o un\ncorpus de prensa espa~ nola anotado con pr\u0013 estamos l\u0013 exicos no asimilados (mayoritari-\namente anglicismos) siguiendo el esquema BIO. Recibimos nueve sistemas distintos\nprovenientes de cuatro equipos diferentes. Los resultados obtenidos oscilan entre los\n37 y los 85 puntos de valor F1, lo que indica que la detecci\u0013 on de pr\u0013 estamos l\u0013 exicos es\nun problema no resuelto (sobre todo cuando se abordan pr\u0013 estamos no vistos anteri-\normente) y que el trabajo lexicogr\u0013 a\fco tradicional podr\u0013 \u0010a bene\fciarse de incorporar\nlas t\u0013 ecnicas actuales del PLN.\nPalabras clave: Pr\u0013 estamo l\u0013 exico, anglicismos, detecci\u0013 on autom\u0013 atica de pr\u0013 estamos.\n1 Introduction\nLexical borrowing is the process of import-\ning words from one language into another\n(Onysko, 2007; Poplack, Sanko\u000b, and Miller,\n1988), a phenomenon that occurs in all lan-\nguages. The task of automatically extracting\nlexical borrowings from text has proven to\nbe relevant in lexicographic work as well as\nfor NLP downstream tasks, such as parsing\n(Alex, 2008a), text-to-speech synthesis (Lei-dig, Schlippe, and Schultz, 2014) and ma-\nchine translation (Tsvetkov and Dyer, 2016).\nIn recent decades, English in particular\nhas produced numerous lexical borrowings\n(often called anglicisms ) in many European\nlanguages (Furiassi, Pulcini, and Gonz\u0013 alez,\n2012). Previous work estimated that a reader\nof French newspapers encounters a new lexi-\ncal borrowing every 1,000 words (Chesley and\nBaayen, 2010), English borrowings outnum-\nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021, pp. 277-285\nrecibido 05-07-2021 revisado 12-07-2021 aceptado 16-07-2021\nISSN 1135-5948. DOI 10.26342/2021-67-24\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Naturalbering all other borrowings combined (Ches-\nley, 2010). In Chilean newspapers, lexical\nborrowings account for approximately 30%\nof neologisms, 80% of those corresponding to\nanglicisms (Gerding et al., 2014). In Euro-\npean Spanish, it was estimated that angli-\ncisms could account for 2% of the vocabu-\nlary used in Spanish newspaper El Pa\u0013 \u0010s in\n1991 (Rodr\u0013 \u0010guez Gonz\u0013 alez, 2002), a num-\nber that is likely to be higher today. As\na result, the usage of lexical borrowings in\nSpanish (and particularly anglicisms) has at-\ntracted lots of attention, both in linguistic\nstudies and among the general public.\nFor ADoBo 2021, we proposed a shared\ntask on automatically detecting lexical bor-\nrowings in Spanish newswire, with a special\nfocus on unassimilated anglicisms. In this pa-\nper we describe the purpose and scope of the\nshared task, introduce the systems that par-\nticipated in it, and share the results obtained\nduring the competition.\n2 Related work\nSeveral projects have approached the task\nof extracting lexical borrowings in various\nEuropean languages, such as German (Alex,\n2008a; Alex, 2008b; Garley and Hocken-\nmaier, 2012; Leidig, Schlippe, and Schultz,\n2014), Italian (Furiassi and Ho\rand, 2007),\nFrench (Alex, 2008a; Chesley, 2010), Finnish\n(Mansikkaniemi and Kurimo, 2012), and\nNorwegian (Andersen, 2012; Losnegaard and\nLyse, 2012), with a particular focus on angli-\ncism extraction.\nDespite the interest in modeling anglicism\nusage, the problem of automatically extract-\ning lexical borrowings has been seldom ex-\nplored in the NLP literature for Iberian lan-\nguages in general and for Spanish in particu-\nlar, with only a few recent exceptions (Seri-\ngos, 2017; \u0013Alvarez-Mellado, 2020).\n3 Lexical borrowing: scope of the\nphenomenon\nThe concept of linguistic borrowing covers a\nwide range of linguistic phenomena, but is\ngenerally understood as the process of intro-\nducing words, elements or patterns of one\nlanguage (the donor language) into another\nlanguage (the recipient language) (Haugen,\n1950; Weinreich, 1963). In that sense, lex-\nical borrowing is somewhat similar to lin-\nguistic code-switching (the process of us-\ning two languages interchangeably in thesame discourse that is common among bilin-\ngual speakers), and in fact both phenomena\nhave been sometimes described as a contin-\nuum with a fuzzy frontier between the two\n(Clyne, Clyne, and Michael, 2003). Con-\nsequently, disagreement on what a borrow-\ning is (and is not) exists (G\u0013 omez Capuz,\n1997) and various classi\fcations and typolo-\ngies for characterizing borrowing usage have\nbeen proposed, both for borrowings in gen-\neral (Thomason and Kaufman, 1992; Ma-\ntras and Sakel, 2007; Haspelmath and Tad-\nmor, 2009) and for anglicism usage in Span-\nish in particular (Pratt, 1980; Lorenzo, 1996;\nG\u0013 omez Capuz, 1997; Rodr\u0013 \u0010guez Gonz\u0013 alez,\n1999; N\u0013 u~ nez Nogueroles, 2018).\n4 Task description\nFor the ADoBo shared task we have fo-\ncused on unassimilated lexical borrowings,\nwords from another language that are used\nin Spanish without orthographic modi\fcation\nand that have not (yet) been integrated into\nthe recipient language|for example, run-\nning, smartwatch, in\ruencer ,holding, look,\nhype, prime time and lawfare .\n4.1 Motivation for the task\nThe task of extracting unassimilated lexical\nborrowings is a more challenging undertak-\ning than it might appear to be at \frst. To\nbegin with, lexical borrowings can be either\nsingle or multitoken expressions (e.g., prime\ntime, tie break ormachine learning ). Second,\nlinguistic assimilation is a diachronic process\nand, as a result, what constitutes an unassim-\nilated borrowing is not clear-cut. For exam-\nple, words like barorclubwere unassimilated\nlexical borrowings in Spanish at some point\nin the past, but have been around for so long\nin the Spanish language that the process of\nphonological and morphological adaptation is\nnow complete and they cannot be considered\nunassimilated borrowings anymore. On the\nother hand, realia words, that is, culture-\nspeci\fc elements whose name entered via the\nlanguage of origin decades ago (like jazz or\nwhisky ) cannot be considered unassimilated\nanymore, despite their orthography not hav-\ning been adapted into Spanish conventions.\nAll these subtleties make the annotation of\nlexical borrowings non-trivial. Consequently,\nin prior work on anglicism extraction from\nSpanish text, plain dictionary lookup pro-\nduced very limited results with F1 scores of\nElena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano\n278Team System Type Prec. Rec. F1 Ref. Pred. Corr.\nALL 88.81 81.56 85.03 1,285 1,180 1,048\nMarrouviere (1) ENG 90.70 82.65 86.49 1,239 1,129 1,024\nOTHER 47.06 52.17 49.48 46 51 24\nALL 88.77 81.17 84.80 1,285 1,175 1,043\nVersae (2) ENG 90.31 82.73 86.35 1,239 1,135 1,025\nOTHER 45.00 39.13 41.86 46 40 18\nALL 89.40 66.30 76.14 1,285 953 852\nMarrouviere (3) ENG 90.98 67.55 77.54 1239 920 837\nOTHER 45.45 32.61 37.97 46 33 15\nALL 92.28 61.40 73.74 1,285 855 789\nMarrouviere (4) ENG 93.43 63.12 75.34 1,239 837 782\nOTHER 38.89 15.22 21.88 46 18 7\nALL 62.76 46.30 53.29 1,285 948 595\nVersae (5) ENG 62.97 47.62 54.23 1,239 937 590\nOTHER 45.45 10.87 17.54 46 11 5\nALL 65.15 37.82 47.86 1,285 746 486\nMgrafu (6) ENG 65.31 38.90 48.76 1,239 738 482\nOTHER 50.0 8.69 14.81 46 8 4\nALL 75.27 27.47 40.25 1,285 469 353\nBERT4EVER (7) ENG 75.43 28.25 41.10 1,239 464 350\nOTHER 60.00 6.52 11.76 46 5 3\nALL 76.29 25.29 37.99 1,285 426 325\nBERT4EVER (8) ENG 76.48 25.99 38.80 1,239 421 322\nOTHER 60.00 6.52 11.76 46 5 3\nALL 76.44 24.75 37.39 1,285 416 318\nBERT4EVER (9) ENG 76.64 25.42 38.18 1,239 411 315\nOTHER 60.00 6.52 11.76 46 5 3\nTable 1: Results on the test set. For each label, precision, recall and F1 score are provided,\nalong with the reference number of borrowings, the predicted number of borrowings and the\nnumber of correct predictions.\nSet Tokens ENG OTHER Unique\nTrain 231,126 1,493 28 380\nDev. 82,578 306 49 316\nTest 58,997 1,239 46 987\nTotal 372,701 3,038 123 1,683\nTable 2: Corpus split and counts.\n47 (Serigos, 2017) and 26 ( \u0013Alvarez-Mellado,\n2020). In fact, whether a given expression is\na borrowing or not cannot always be deter-\nmined by plain dictionary lookup; after all,\nan expression such as social media is an an-\nglicism in Spanish, even when both social and\nmedia also happen to be Spanish words that\nare registered in regular dictionaries. This\njusti\fes the need for a more NLP-heavy ap-\nproach to the task, which has already proven\nto be promising. Previous work on borrow-\ning extraction using a CRF model with hand-\ncrafted features produced an F1 score of 86\non a corpus of Spanish headlines ( \u0013Alvarez-\nMellado, 2020).Finally, although there are some al-\nready well-established shared tasks on mixed-\nlanguage settings, they have focused exclu-\nsively on code-switched data (Solorio et al.,\n2014; Molina et al., 2016; Aguilar et al.,\n2018), which is close to borrowing but dif-\nferent in scope and nature (see Section 3),\nand no speci\fc venue exists on borrowing de-\ntection in NLP so far. To the best of our\nknowledge, ADoBo is the \frst shared task\nspeci\fcally devoted to linguistic borrowing.\n4.2 Dataset\nA corpus of newspaper articles written in\nSpanish was distributed to the task partici-\npants. The corpus articles were sourced from\nvarious Spanish newspapers and online media\nbased in Spain. The articles were annotated\nwith unassimilated lexical borrowings.\nGiven that lexical borrowings can be mul-\ntiword expressions (such as best seller, big\ndata) and that those units should be treated\nas one borrowing and not as two independent\nborrowings, BIO encoding was used to denote\nOverview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press\n279Team System Type Prec. Rec. F1 Ref. Pred. Corr.\nALL 73.66 82.49 77.83 1,285 1,439 1,060\nMarrouviere (1) ENG 76.31 83.45 79.72 1,239 1,355 1,034\nOTHER 30.95 56.52 40.00 46 84 26\nALL 81.49 63.04 71.08 1,285 994 810\nMarrouviere (4) ENG 82.70 64.81 72.67 1,239 971 803\nOTHER 30.43 15.22 20.29 46 23 7\nALL 72.66 67.63 70.05 1,285 1,196 869\nMarrouviere (3) ENG 75.49 68.85 72.01 1,239 1,130 853\nOTHER 24.24 34.78 28.57 46 66 16\nALL 59.57 82.33 69.13 1,285 1,776 1,058\nVersae (2) ENG 61.34 84.02 70.91 1,239 1,697 1041\nOTHER 21.52 36.96 27.20 46 79 17\nALL 42.27 48.48 45.16 1,285 1,474 623\nVersae (5) ENG 42.37 49.72 45.75 1,239 1,454 616\nOTHER 35.00 15.22 21.21 46 20 7\nALL 52.17 39.22 44.78 1,285 966 504\nMgrafu (6) ENG 52.30 40.36 45.56 1,239 956 500\nOTHER 40.00 8.69 14.29 46 10 4\nALL 70.29 28.72 40.77 1,285 525 369\nBERT4EVER (7) ENG 70.38 29.54 41.61 1,239 520 366\nOTHER 60.00 6.52 11.76 46 5 3\nALL 69.92 26.23 38.14 1,285 482 337\nBERT4EVER (8) ENG 70.02 26.96 38.93 1,239 477 334\nOTHER 60.00 6.52 11.76 46 5 3\nALL 70.49 25.84 37.81 1,285 471 332\nBERT4EVER (9) ENG 70.60 26.55 38.59 1,239 466 329\nOTHER 60.00 6.52 11.76 46 5 3\nTable 3: Results on the lower-cased version of the test set.\nthe boundaries of each span.\nTwo classes were used for borrowings: ENG\nfor English borrowings, and OTHER for lexi-\ncal borrowings from other languages. Tokens\nthat were not part of a borrowing were anno-\ntated with the \\outside\" tag ( O). Only unas-\nsimilated lexical borrowings were considered\nborrowings. This means that borrowings\nthat have already gone through orthographi-\ncal adaption (such f\u0013 utbol orhackear ) were not\nconsidered borrowings and were therefore an-\nnotated as O. Annotation guidelines were also\nmade available for participants.\nThe data was distributed in CoNLL for-\nmat. An additional collection of documents\nthat was not evaluated (the background set)\nwas released as a part of the test set. This\nwas done to encourage scalability to larger\ndata collections and to ensure that partici-\npating teams were not be able to easily per-\nform manual examination of the evaluated\npart of the test set.\nThe dataset contained a high number of\nunique borrowings and OOV words, and\nthere was minimal overlap between splits.\nThis enabled a more rigorous evaluation ofsystem performance, as it helped us better\nassess the generalizing abilities of the partic-\nipants' models. Table 2 contains the number\nof tokens and borrowing spans per type in\neach split.\n4.3 Evaluation metrics\nThe evaluation metrics used for the task was\nthe standard precision, recall and F1 over\nspans:\n\u2022Precision: The percentage of borrowings\nin the system's output that are correctly\nrecognized and classi\fed.\n\u2022Recall: The percentage of borrowings in\nthe test set that were correctly recog-\nnized and classi\fed.\n\u2022F1-measure: The harmonic mean of Pre-\ncision and Recall.\nF1-measure was used as the o\u000ecial eval-\nuation score for the \fnal ranking of the par-\nticipating teams. Evaluation was done ex-\nclusively at the span level. This means that\nonly exact matches were considered, and no\nElena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano\n280Team System Type Prec. Rec. F1 Ref. Pred. Corr.\nALL 90.35 82.33 86.16 1,285 1,171 1,058\nMarrouviere (1) ENG 91.18 83.45 87.15 1,239 1,134 1,034\nOTHER 64.86 52.17 57.83 46 37 24\nALL 88.71 80.08 84.17 1,285 1,160 1,029\nVersae (2) ENG 90.19 81.60 85.68 1,239 1,121 1,011\nOTHER 46.15 39.13 42.35 46 39 18\nALL 90.84 66.38 76.71 1,285 939 853\nMarrouviere (3) ENG 91.09 67.64 77.63 1,239 920 838\nOTHER 78.95 32.61 46.15 46 19 15\nALL 91.39 60.31 72.67 1,285 848 775\nMarrouviere (4) ENG 92.75 61.99 74.31 1,239 828 768\nOTHER 35.00 15.22 21.21 46 20 7\nALL 62.76 46.30 53.29 1,285 948 595\nVersae (5) ENG 62.97 47.62 54.23 1,239 937 590\nOTHER 45.45 10.87 17.54 46 11 5\nALL 66.81 36.50 47.21 1,285 702 469\nMgrafu (6) ENG 67.00 37.53 48.11 1,239 694 465\nOTHER 50.0 8.69 14.81 46 8 4\nALL 78.37 25.37 38.33 1,285 416 326\nBERT4EVER (7) ENG 78.40 26.07 39.13 1,239 412 323\nOTHER 75.00 6.52 12.00 46 4 3\nALL 79.03 22.88 35.49 1,285 372 294\nBERT4EVER (8) ENG 79.08 23.49 36.22 1,239 368 291\nOTHER 75.00 6.52 12.00 46 4 3\nALL 79.34 22.41 34.95 1,285 363 288\nBERT4EVER (9) ENG 79.39 23.00 35.67 1,239 359 285\nOTHER 75.00 6.52 12.00 46 4 3\nTable 4: Results on the unquoted version of the test set.\ncredit was given to partial matches. For ex-\nample, given the multitoken borrowing late\nnight, the entire phrase would have to be cor-\nrectly labeled in order to count as a true pos-\nitive. This makes the evaluation more rigor-\nous, as it avoids the overly-generous scores\nthat can sometimes result from token level\nevaluation. A model that can only detect En-\nglish function words would detect onandthe\ninon the rocks orbyinstand by and still get\na generous result on a token-level evaluation.\n4.4 Resource limitation for model\ntraining\nThe following limitations were established for\nparticipants during training:\n\u2022No additional human annotation was al-\nlowed for training. Given that the main\npurpose of the shared task was to evalu-\nate how di\u000berent models perform for the\ntask of borrowing detection, using ex-\nternal data annotated with borrowings\nwould prevent a fair evaluation of di\u000ber-\nent model approaches.\n\u2022Although the usage of regular lexiconsand linguistic resources was accepted,\nno automatically-compiled lexicons of\nborrowings (such as those produced by\nalready-existing models that perform\nborrowing extraction) were allowed. The\nreason for this limitation was that we\nwere interested in evaluating how dif-\nferent approaches to borrowing detec-\ntion performed when dealing with pre-\nviously unseen borrowings, and models\nthat piggyback on already-existing sys-\ntems's output would prevent that.\n5 System descriptions\nWe received nine submissions from four dif-\nferent teams. However, only two teams sub-\nmitted system descriptions. As a result, we\nhave no description whatsoever for two of the\nparticipating systems, including the one that\nobtained the best results. We provide a brief\nsummary of the two participating systems for\nwhich we received a submission, and refer the\nreader to their respective task description pa-\npers for further details.\nOverview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press\n281Team System Type Prec. Rec. F1 Ref. Pred. Corr.\nALL 78.04 82.96 80.42 1,285 1,366 1066\nMarrouviere (1) ENG 78.67 83.94 81.22 1,239 1,322 1040\nOTHER 59.09 56.52 57.78 46 44 26\nALL 77.96 67.70 72.47 1,285 1,116 870\nMarrouviere (3) ENG 78.28 68.93 73.30 1,239 1,091 854\nOTHER 64.00 34.78 45.07 46 25 16\nALL 81.14 61.95 70.26 1,285 981 796\nMarrouviere (4) ENG 82.36 63.68 71.83 1,239 958 789\nOTHER 30.43 15.22 20.29 46 23 7\nALL 60.07 81.48 69.15 1,285 1,743 1,047\nVersae (2) ENG 61.76 83.05 70.84 1,239 1,666 1,029\nOTHER 23.38 39.13 29.27 46 77 18\nALL 42.41 48.48 45.24 1,285 1,469 623\nVersae (5) ENG 42.48 49.72 45.82 1,239 1,450 616\nOTHER 36.84 15.22 21.54 46 19 7\nALL 54.56 37.74 44.62 1,285 889 485\nMgrafu (6) ENG 54.60 38.82 45.38 1,239 881 481\nOTHER 50.0 8.69 14.81 46 8 4\nALL 72.96 26.46 38.83 1,285 466 340\nBERT4EVER (7) ENG 72.79 27.20 39.60 1,239 463 337\nOTHER 100 6.52 12.24 46 3 3\nALL 72.75 23.89 35.97 1,285 422 307\nBERT4EVER (8) ENG 72.55 24.54 36.67 1,239 419 304\nOTHER 100 6.52 12.24 46 3 3\nALL 73.17 23.35 35.40 1,285 410 300\nBERT4EVER (9) ENG 72.97 23.97 36.09 1,239 407 297\nOTHER 100 6.52 12.24 46 3 3\nTable 5: Results on the unquoted and lower-cased version of the test set.\n5.1 BERT4EVER team: CRF\nmodel with data augmentation\nThe BERT4EVER team submitted a sys-\ntem to ADoBo based on combining several\nCRF models trained on di\u000berent portions of\nthe task's training data. The models were\nused to label a freely-available open corpus\nin Spanish, and individual models were then\nre-trained on the output. Results suggest\nthat this strategy improves two F1 points\non the test set when compared to a trained-\non-task-data-only baseline. The paper com-\nbines two well-known items in the ML tool-\nbox, namely CRFs and data augmentation,\nand shows that bootstrapping an additional\ndataset is indeed useful.\n5.2 Versae team: using STILTs\nThe Versae team submitted a system\nthat experimented with using STILTs|\nsupplementary training on intermediate\nlabel-data tasks (Phang, F\u0013 evry, and Bow-\nman, 2019)|for the ADoBo task. They\nexperimented with training using part of\nspeech, named entity recognition, code-\nswitching, and language identi\fcationdatasets, but found that models trained\nin this way consistently perform worse\nthan \fne-tuning multilingual language\nmodels. The Versae team also explored\nwhich multilingual language models per-\nform best, evaluating multilingual BERT,\nRoBERTa, and models trained on small sets\nof languages.\n6 Results\nResults of the task were computed using\nSeqScore1(Palen-Michel, Holley, and Lig-\nnos, 2021), a Python package for evaluating\nsequence labeling tasks, con\fgured to emu-\nlate the conlleval evaluation script. Scores\nare summarized in Table 1. F1 ranged from\n37.29 to 85.03, with the Marrouviere team\nscoring highest (F1=85.03, P=88.81 and\nR=81.56), close to the next-highest scores\nfrom the Versae team (F1=84.80, P=88.77\nand R=81.17).\nIn order to get a better understanding of\nthe systems that took part in the shared task,\nwe performed some experiments on the out-\nput that was submitted by participants.\n1https://github.com/bltlab/seqscore\nElena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano\n2826.1 Combining outputs\nIn order to assess the complementarity of the\nsubmitted systems, an experiment was car-\nried out combining their outputs. The com-\nbination consisted of the union of all detected\nterms. Since the number of systems is not\nvery high, all combinations of systems were\nexplored. In terms of F1 score, the best per-\nforming combination was (1), (2), and (4),\nwith F1=87.83, P=87.83, and R=89.26, a\nresult that outperforms the scores obtained\nseparately by each individual system.\n6.2 Removing ortho-typographic\ncues\nThree variations of the test set were included\nin the background set (the additional collec-\ntion of documents released along with the\ntest set):\n1. A lowercase version, where all uppercase\nletters in the original test set were trans-\nformed to lowercase.\n2. A no-quotation-mark version, where all\nquotation marks in the original test set\n(\\ \" ` ' \u001c \u001d) were removed.\n3. A lowercase no-quotation-mark version,\nwhere all uppercase letters where trans-\nformed to lowercase AND all quotation\nmarks were removed.\nNone of these versions were used to rank\nthe systems but to observe the systems dif-\nference in performance on di\u000berent textual\ncharacteristics. The rationale for these ex-\nperiments was to assess how well systems per-\nformed if certain orthotypographic cues that\nusually appear along with borrowings (such\nas quotation marks) were removed. After all,\na borrowing is still a borrowing regardless of\nwhether it is written with or without quo-\ntation marks and it would be of little use to\nhave a model that systematically labeled any-\nthing between quotation marks as a borrow-\ning, or that only detected borrowings if they\nare written between quotation marks.\nSimilarly, many of the foreign words that\nappear in newswire are usually proper names,\nwhere the uppercase can serve as cue to dis-\ntinguish them from borrowings. Given that\nspeakers are capable of distinguishing bor-\nrowings from proper names in oral settings|\nwhere no case distinction exists|and that\nthese cues are not present in other textualgenres (e.g. social media), we were interested\nin assessing how well the models performed\nwhen no case cue was available.\nResults for these experiments are pre-\nsented in Tables 3, 4 and 5. Focusing on the\nbest two performing systems, we observe a\ndrop of global F1 due to a consistent drop on\nprecision not compensated with the a slightly\nincrease of recall for the lowercased versions\nof the test set. In general, the drop in sys-\ntem (2) is more pronounced than in system\n(1), which causes its repositioning in the cor-\nresponding rankings. For the unquoted ver-\nsion of the test set, system (1) increases its\nF1 and system (2) decrements it slightly. Not\nhaving information on system (1), we can not\nattribute any of the di\u000berences to any char-\nacteristics of the systems.\n7 Conclusions\nIn this paper we have presented the results of\nthe ADoBo shared task on extracting unas-\nsimilated lexical borrowings from Spanish\nnewswire. We have introduced the motiva-\ntion for this topic, we have described the\nscope and nature of the proposed task, we\nhave shared the obtained results and have\nsummarized the main \fndings. Participants\nresults ranged from F1 scores of 37 to 85.\nThese scores show that this is not a triv-\nial task and that lexical borrowing detection\nis an open problem that requires further re-\nsearch.\nOur goal with this shared task was to raise\nawareness about a topic that, although highly\nrelevant in the linguistics literature, has been\nmostly neglected within NLP. Although the\nparticipation for this \frst edition was mod-\nest (nine systems submitted from four di\u000ber-\nent teams), the response was positive and it\nseems to indicate that there exists a moder-\nate population within the community that is\ninterested in borrowing as an NLP task. In\nfact, a post-task survey distributed among\nregistered participants showed that 85% of\nrespondents were interested in seeing future\neditions around this phenomenon, particu-\nlarly on languages other than Spanish and\nincluding both semantic and diachronic bor-\nrowings.\nReferences\nAguilar, G., F. AlGhamdi, V. Soto,\nM. Diab, J. Hirschberg, and T. Solorio.\n2018. Named entity recognition on code-\nOverview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press\n283switched data: Overview of the CALCS\n2018 shared task. In Proceedings of\nthe Third Workshop on Computational\nApproaches to Linguistic Code-Switching ,\npages 138{147, Melbourne, Australia,\nJuly. Association for Computational Lin-\nguistics.\nAlex, B. 2008a. Automatic detection of En-\nglish inclusions in mixed-lingual data with\nan application to parsing . Ph.D. thesis,\nUniversity of Edinburgh.\nAlex, B. 2008b. Comparing corpus-based\nto web-based lookup techniques for auto-\nmatic English inclusion detection. In Pro-\nceedings of the Sixth International Con-\nference on Language Resources and Eval-\nuation (LREC'08), Marrakech, Morocco,\nMay. European Language Resources As-\nsociation (ELRA).\n\u0013Alvarez-Mellado, E. 2020. L\u0013 azaro: An ex-\ntractor of emergent anglicisms in Spanish\nnewswire. Master's thesis, Brandeis Uni-\nversity.\nAndersen, G. 2012. Semi-automatic ap-\nproaches to anglicism detection in Norwe-\ngian corpus data. In C. Furiassi, V. Pul-\ncini, and F. Rodr\u0013 \u0010guez Gonz\u0013 alez, editors,\nThe anglicization of European lexis . pages\n111{130.\nChesley, P. 2010. Lexical borrowings in\nFrench: Anglicisms as a separate phe-\nnomenon. Journal of French Language\nStudies, 20(3):231{251.\nChesley, P. and R. H. Baayen. 2010. Predict-\ning new words from newer words: Lex-\nical borrowings in French. Linguistics,\n48(6):1343.\nClyne, M., M. G. Clyne, and C. Michael.\n2003. Dynamics of language contact: En-\nglish and immigrant languages. Cam-\nbridge University Press.\nFuriassi, C. and K. Ho\rand. 2007. The\nretrieval of false anglicisms in newspaper\ntexts. In Corpus Linguistics 25 Years On .\nBrill Rodopi, pages 347{363.\nFuriassi, C., V. Pulcini, and F. R. Gonz\u0013 alez.\n2012. The anglicization of European lexis .\nJohn Benjamins Publishing.\nGarley, M. and J. Hockenmaier. 2012. Beef-\nmoves: Dissemination, diversity, and dy-\nnamics of English borrowings in a Ger-\nman hip hop forum. In Proceedings ofthe 50th Annual Meeting of the Associa-\ntion for Computational Linguistics (Vol-\nume 2: Short Papers) , pages 135{139,\nJeju Island, Korea, July. Association for\nComputational Linguistics.\nGerding, C., M. Fuentes, L. G\u0013 omez, and\nG. Kotz. 2014. Anglicism: An ac-\ntive word-formation mechanism in Span-\nish. Colombian Applied Linguistics Jour-\nnal, 16(1):40{54.\nG\u0013 omez Capuz, J. 1997. Towards a typo-\nlogical classi\fcation of linguistic borrow-\ning (illustrated with anglicisms in romance\nlanguages). Revista alicantina de estudios\ningleses, 10:81{94.\nHaspelmath, M. and U. Tadmor. 2009.\nLoanwords in the world's languages: a\ncomparative handbook. Walter de Gruyter.\nHaugen, E. 1950. The analysis of linguistic\nborrowing. Language, 26(2):210{231.\nLeidig, S., T. Schlippe, and T. Schultz. 2014.\nAutomatic detection of anglicisms for the\npronunciation dictionary generation: a\ncase study on our German IT corpus. In\nSpoken Language Technologies for Under-\nResourced Languages .\nLorenzo, E. 1996. Anglicismos hisp\u0013 anicos .\nBiblioteca rom\u0013 anica hisp\u0013 anica: Estudios\ny ensayos. Gredos.\nLosnegaard, G. S. and G. I. Lyse. 2012. A\ndata-driven approach to anglicism identi\f-\ncation in Norwegian. In G. Andersen, edi-\ntor, Exploring Newspaper Language: Us-\ning the web to create and investigate a\nlarge corpus of modern Norwegian. John\nBenjamins Publishing, pages 131{154.\nMansikkaniemi, A. and M. Kurimo. 2012.\nUnsupervised vocabulary adaptation for\nmorph-based language models. In Pro-\nceedings of the NAACL-HLT 2012 Work-\nshop: Will We Ever Really Replace the N-\ngram Model? On the Future of Language\nModeling for HLT , pages 37{40. Associa-\ntion for Computational Linguistics.\nMatras, Y. and J. Sakel. 2007. Grammat-\nical borrowing in cross-linguistic perspec-\ntive, volume 38. Walter de Gruyter.\nMolina, G., F. AlGhamdi, M. Ghoneim,\nA. Hawwari, N. Rey-Villamizar, M. Diab,\nand T. Solorio. 2016. Overview for the\nElena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta Zamorano\n284second shared task on language identi\fca-\ntion in code-switched data. In Proceedings\nof the Second Workshop on Computational\nApproaches to Code Switching, pages 40{\n49, Austin, Texas, November. Association\nfor Computational Linguistics.\nN\u0013 u~ nez Nogueroles, E. E. 2018. A compre-\nhensive de\fnition and typology of angli-\ncisms in present-day Spanish. Epos: Re-\nvista de \flolog\u0013 \u0010a, (34):211{237.\nOnysko, A. 2007. Anglicisms in German:\nBorrowing, lexical productivity, and writ-\nten codeswitching, volume 23. Walter de\nGruyter.\nPalen-Michel, C., N. Holley, and C. Lignos.\n2021. SeqScore. https://github.com/\nbltlab/seqscore.\nPhang, J., T. F\u0013 evry, and S. R. Bowman.\n2019. Sentence encoders on stilts: Supple-\nmentary training on intermediate labeled-\ndata tasks. arXiv preprint 1811.01088.\nPoplack, S., D. Sanko\u000b, and C. Miller. 1988.\nThe social correlates and linguistic pro-\ncesses of lexical borrowing and assimila-\ntion. Linguistics, 26(1):47{104.\nPratt, C. 1980. El anglicismo en el espa~ nol\npeninsular contempor\u0013 aneo , volume 308.\nGredos.\nRodr\u0013 \u0010guez Gonz\u0013 alez, F. 1999. Anglicisms in\ncontemporary Spanish. An overview. At-\nlantis, 21(1/2):103{139.\nRodr\u0013 \u0010guez Gonz\u0013 alez, F. 2002. Spanish. In\nM. G\u007f orlach, editor, English in Europe. Ox-\nford University Press, chapter 7, pages\n128{150.\nSerigos, J. R. L. 2017. Applying corpus\nand computational methods to loanword\nresearch: new approaches to Anglicisms in\nSpanish. Ph.D. thesis, The University of\nTexas at Austin.\nSolorio, T., E. Blair, S. Maharjan,\nS. Bethard, M. Diab, M. Ghoneim,\nA. Hawwari, F. AlGhamdi, J. Hirschberg,\nA. Chang, and P. Fung. 2014. Overview\nfor the \frst shared task on language\nidenti\fcation in code-switched data.\nInProceedings of the First Workshop\non Computational Approaches to Code\nSwitching, pages 62{72, Doha, Qatar,\nOctober. Association for Computational\nLinguistics.Thomason, S. G. and T. Kaufman. 1992.\nLanguage contact, creolization, and ge-\nnetic linguistics. Univ of California Press.\nTsvetkov, Y. and C. Dyer. 2016. Cross-\nlingual bridges with models of lexical bor-\nrowing. Journal of Arti\fcial Intelligence\nResearch, 55:63{93.\nWeinreich, U. 1963. Languages in contact\n(1953). The Hague: Mouton .\nOverview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press\n285  \n \n \n \n \n \n \n \n \nInformaci\u00f3n General   \n  \n Informaci\u00f3n para los Autores  \nFormato de los Trabajos  \n\u2022 La longitud m\u00e1xima admitida para las contribuciones ser\u00e1 de 10 p\u00e1ginas DIN A4 (210 x 297 \nmm.), incluidas referencias y figuras.  \n\u2022 Los art\u00edculos pueden estar escritos en ingl\u00e9s o espa\u00f1ol. El t\u00edtulo, resumen y palabras clave \ndeben escribirse en ambas lenguas.  \n\u2022 El formato ser\u00e1 en Word \u00f3 LaTeX  \nEnv\u00edo  de los Trabajos  \n\u2022 El env\u00edo de los trabajos se realizar\u00e1 electr\u00f3nicamente a trav\u00e9s de la p\u00e1gina web de la Sociedad \nEspa\u00f1ola para el Procesamiento del  Lenguaje Natural ( http://www.sepln.org ) \n\u2022 Para los trabajos con formato LaTeX s e mandar\u00e1 el archivo PDF junto a todos los fuentes \nnecesarios para compilaci\u00f3n LaTex  \n\u2022 Para los trabajos con formato Word  se mandar\u00e1 el archi vo PDF junto al DOC o RTF  \n\u2022 Para m\u00e1s informaci\u00f3n http://www.sepln.org/la -revista/informacion -para-autores  \n \n  \n Informaci\u00f3n Adicional  \nFunciones del Consejo de Redacci\u00f3n\nLas funciones del Consejo de Redacci\u00f3n o Editorial de la revista SEPLN son las siguientes:  \n\u2022Controlar la selecci\u00f3n y tomar las decisiones en la publicaci\u00f3n de los contenidos que han de\nconformar cada n\u00famero de la revista\n\u2022Pol\u00edtica editorial\n\u2022Preparaci\u00f3n de cada n\u00famero\n\u2022Relaci\u00f3n con los evaluadores y autores\n\u2022Relaci\u00f3n con el comit\u00e9 cient\u00edfico\nEl consejo de redacci\u00f3n est\u00e1 formado por los siguientes miembros  \nL. Alfonso Ure\u00f1a L\u00f3pez (Director)\nUniversidad de Ja\u00e9n  \nlaurena@ujaen.es  \nPatricio Mart\u00ednez Barco (Secretario)  \nUniversidad de Alicante  \npatricio@dlsi.ua.es  \nManuel Palomar Sanz  \nUniversidad de Alicante  \nmpalomar@dlsi.ua.es  \nFelisa Verdejo Ma\u00edllo  \nUNED  \nfelisa@lsi.uned.es  \nFunciones del Consejo Asesor\nLas funciones del Consejo Asesor o Cient\u00edfico de la revista SEPLN son las siguientes:  \n\u2022Marcar, orientar y redireccionar la pol\u00edtica cient\u00edfica de la revista y las l\u00edneas de investigaci\u00f3n\na potenciar\n\u2022Representaci\u00f3n\n\u2022Impulso a la difusi\u00f3n internacional\n\u2022Capacidad de atracci\u00f3n de autores\n\u2022Evaluaci\u00f3n\n\u2022Composici\u00f3n\n\u2022Prestigio\n\u2022Alta especializaci\u00f3n\n\u2022Internacionalidad\nEl Consejo Asesor est\u00e1 formado por los siguientes miembros:  \nManuel de Buenaga  Universidad de Alcal\u00e1 (Espa\u00f1a)  \nSylviane Cardey -Greenfield  Centre de recherche en linguistique et traitement automatique des \nlangues (Francia)  \nJos\u00e9 Camacho Collados  Cardiff University  (Reino Unido)  \nIrene Castell\u00f3n  Universidad de Barcelona (Espa\u00f1a)  \nArantza D\u00edaz de Ilarraza  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nAntonio Ferr\u00e1ndez  Universidad de Alicante (Espa\u00f1a)  \nKoldo Gojenola  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nXavier G\u00f3mez Guinovart  Universidad de Vigo (Espa\u00f1a)  \nJos\u00e9 Miguel Go\u00f1i  Universidad Polit\u00e9cnica de Madrid (Espa\u00f1a)  \nRam\u00f3n L\u00f3pez -C\u00f3zar Delgado  Univers idad de Granada (Espa\u00f1a)  \nMariana Lara Neves  German Federal Institute for Risk Assessment  (Alemania)  \nProcesamiento del Lenguaje Natural, Revista n\u00ba 67, septiembre de 2021\n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural  \n  Elena Lloret  Universidad de Alicante  (Espa\u00f1a)  \nBernardo Magnini  Fondazione Bruno Kessler (Italia)  \nNuno J. Mamede  Instituto de Engenharia de Sistemas e Comput adores (Portugal)  \nM. Teresa Mart\u00edn Valdivia  Universidad de Ja\u00e9n (Espa\u00f1a)  \nPatricio Mart\u00ednez -Barco  Universidad de Alicante (Espa\u00f1a)  \nEugenio Mart\u00ednez C\u00e1mara  Universidad de Granada (Espa\u00f1a)  \nPaloma Mart\u00ednez Fern\u00e1ndez  Universidad Carlos III (Espa\u00f1a)  \nRaquel Mart\u00ednez Unanue  Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a)  \nLeonel Ruiz Miyares  Centro de Ling\u00fc\u00edstica Aplicada de Santiago de Cuba (Cuba)  \nRuslan Mitkov  University of Wolverhampton (Reino Unido)  \nManuel Montes y G\u00f3mez  Instituto Nacional de Astrof\u00ed sica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico)  \nLlu\u00eds Padr\u00f3  Universidad Polit\u00e9cnica de Catalu\u00f1a (Espa\u00f1a)  \nManuel Palomar  Universidad de Alicante (Espa\u00f1a)  \nFerr\u00e1n Pla  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nGerman Rigau  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nHoracio Sa ggion  Universidad Pompeu Fabra  (Espa\u00f1a)  \nPaolo Rosso  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nEmilio Sanch\u00eds  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nKepa Sarasola  Universidad del Pa\u00eds Vasco (Espa\u00f1a)  \nEncarna Segarra  Universidad Polit\u00e9cnica de Valencia (Espa\u00f1a)  \nThamar Solorio  University of Houston (Estados Unidos de Am\u00e9rica)  \nMaite Taboada  Simon Fraser University (Canad\u00e1)  \nMariona Taul\u00e9  Universidad de Barcelona  \nJuan-Manuel Torres -Moreno  Laboratoire Informatique d\u2019Avignon / Universit \u00e9 d\u2019Avignon \n(Francia)  \nJos\u00e9 Antonio Troyano Jim\u00e9nez  Universidad de Sevilla (Espa\u00f1a)  \nL. Alfonso Ure\u00f1a L\u00f3pez  Universidad de Ja\u00e9n (Espa\u00f1a)  \nRafael Valencia Garc\u00eda  Universidad de Murcia (Espa\u00f1a)  \nRen\u00e9 Venegas Vel\u00e1sques  Pontificia Universidad Cat\u00f3lica de Valpara\u00eds o (Chile)  \nFelisa Verdejo Ma\u00edllo  Universidad Nacional de Educaci\u00f3n a Distancia (Espa\u00f1a)  \nManuel Vilares  Universidad de la Coru\u00f1a (Espa\u00f1a)  \nLuis Villase\u00f1or -Pineda  Instituto Nacional de Astrof\u00edsica, \u00d3ptica y Electr\u00f3nica (M\u00e9xico)  \n \nCartas al director  \nSociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural  \nDepartamento de Inform\u00e1tica. Universidad de Ja\u00e9n  \nCampus Las Lagunillas, Edificio A3. Despacho 127. 23071 Ja\u00e9n  \nsecretaria.sepln@ujaen.es  \n \nM\u00e1s informaci\u00f3n  \nPara m\u00e1s informaci\u00f3n sobre la Sociedad Esp a\u00f1ola del Procesamiento del Lenguaje Natural puede \nconsultar la p\u00e1gina web http://www.sepln.org . \nSi desea inscribirse como socio de la Sociedad Espa\u00f1ola del Procesamiento del Lenguaje Natural \npuede realizarlo a trav\u00e9s del formulario web que se encuentra en esta direcci\u00f3n \nhttp://www.sepln.org/sepln/la -sociedad  \nLos n\u00fameros anteriores de la revista se encuentran disponibles en la revista electr\u00f3nica: \nhttp://journal.sepln.org/sepln/ojs/ojs/index.php/pln/issue/archive  \nLas funciones del Consejo de Redacci\u00f3n est\u00e1n disponibles en Internet a trav\u00e9s de \nhttp://www.sepln.org/la -revista/consejo -de-redaccion  \nLas funciones del Consejo Asesor est\u00e1n disponibles Internet a trav\u00e9s de la p\u00e1gina \nhttp://www.sepln.org/la -revista/ consejo -asesor  \nLa inscripci\u00f3n como nuevo socio de la SEPLN se puede realizar a trav\u00e9s de la p\u00e1gina \nhttp://www.sepln.org/sepln/inscripcion -para-nuevos -socios  \n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje NaturalIberLEF 2021: Res\u00famenes de las tareas de evaluaci\u00f3n  \nOverview of FakeDeS at IberLEF 2021: Fake News Detection in Spanish Shared Task  \nHelena G\u00f3mez -Adorno, Juan Pablo Posadas -Dur\u00e1n, Gemma Bel Enguix, Claudia Porto Capetillo  ...... 223 \nOverview of the eHealth Knowledge Discovery Challenge at IberLEF 2021  \nAlejandro Piad -Morffis, Suilan Estevez -Velarde, Yoan Gutierrez, Yudivian Almeida -Cruz, Andr\u00e9s \nMontoyo, Rafael Mu\u00f1oz  ................................ ................................ ................................ .............................  233 \nNLP applied to occupational health: MEDDOPROF shared task at IberLEF 2021 on automatic \nrecognition, classification and normalization of professions and occupations from medical texts  \nSalvador Lima -L\u00f3pez, Eul\u00e0lia Farr\u00e9 -Maduell, Antonio Miranda -Escalada, Vicent Briv\u00e1 -Iglesias, Martin \nKrallinger  ................................ ................................ ................................ ................................ ...................  243 \nOverview of HAHA at IberLEF 2021: Detecting, Rat ing and Analyzing Humor in Spanish  \nLuis Chiruzzo, Santiago Castro, Santiago G\u00f3ngora, Aiala Ros\u00e1, J. A. Meaney, Rada Mihalcea  ............  257 \nOverview of the IDPT Task on Irony Detection in Portuguese at IberLEF 2021  \nUlisses B. Corr\u00eaa, Leonardo Coelho, Leonardo Santos,  Larissa A. de Freitas  ................................ ........  269 \nOverview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press  \nElena \u00c1lvarez Mellado, Luis Espinosa Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta \nZamorano  ................................ ................................ ................................ ................................ ...................  277 \nInformaci\u00f3n General  \nInformaci\u00f3n para los autores  ................................ ................................ ................................ ......................  289 \nInformaci\u00f3n adicional  ................................ ................................ ................................ ................................  291 \n\u00a9 2021 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural", "language": "PDF", "image": "PDF", "pagetype": "PDF", "links": "PDF"}