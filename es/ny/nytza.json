{"title": "PDF", "author": "PDF", "url": "http://matema.ujaen.es/jnavas/web_recursos/archivos/weka%20master%20recursos%20naturales/apuntesAD.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "id": "PDF", "license": "PDF", "body": "PDF", "comments": "PDF", "commentsbody": "PDF", "raw_text": "PDF", "text": " \n    \n \nT\u00c9CNICAS  DE AN\u00c1LISIS  DE DATOS \n \nAPLICACIONES PR\u00c1CTICAS UTILIZANDO MICROSOFT \nEXCEL Y WEKA  \n  \n \n \n \n \n  \nJos\u00e9 Manuel Molina L\u00f3pez \nJes\u00fas Garc\u00eda Herrero 2006 \n \n \nPR\u00d3LOGO \n \nEstos apuntes pretenden dar una visi\u00f3n gener al de las t\u00e9cnicas de an\u00e1lisis de \ndatos y de las aplicaciones que las implementan, permitiendo entender los \nconceptos y algoritmos sobre los que se basan las t\u00e9cnicas as\u00ed como el \nresultado de su aplicaci\u00f3n sobre diversas fuentes de ficheros. \n Estos apuntes son una recolecci\u00f3n de informaci\u00f3n de muy variadas fuentes, \np\u00e1ginas de intenet, art\u00edculos etc.. todas ellas aparecen citadas. De entre todas \nellas cabe resaltar el trabajo fin de ca rrera de David S\u00e1nchez titulado \u201cData \nMining mediante Sistemas Clasificadores Gen\u00e9ticos. An\u00e1lisis comparativo con \nlas t\u00e9cnicas cl\u00e1sicas implementadas en WEKA\u201d, en la titulaci\u00f3n de Ingenier\u00eda \nInform\u00e1tica (Julio 2003) donde se rea liza un gran esfuerzo por explicar el \nfuncionamiento interno de la herramient a WEKA y de d\u00f3nde se ha extra\u00eddo la \ninformaci\u00f3n acerca de la s clases y el c\u00f3digo que implementa los algoritmos \npara estos apuntes. As\u00ed tambi\u00e9n resulta ne cesario resaltar la tesis doctoral de \nF\u00e9lix Chamorro, ya que el cap\u00edtulo 2 (e l estado del arte) se pormenorizan todas \nlas t\u00e9cnicas de an\u00e1lisis de datos y que ha  sido utilizado para la elaboraci\u00f3n de \nestos apuntes. \n Esperamos que estos apuntes sean de utilidad para los alumnos que se \nacerquen al an\u00e1lisis de datos y en par ticular para aquellos que tengan inter\u00e9s \nen aplicar los conocimientos te\u00f3ric os en el campo de la pr\u00e1ctica. \n  Jos\u00e9 Manuel Molina L\u00f3pez    Jes\u00fas Garc\u00eda Herrero    \u00cdndice \nT\u00e9cnicas de An\u00e1lisis de Datos  i  \n  \n\u00cdndice \n   \nCAP\u00cdTULO 1. INTRODUCCI\u00d3N 1  \n1.1.  KDD  Y MINER\u00cdA DE DATOS  1  \n1.1.2.  EL PROCESO DE KDD 3  \n1.1.3.  MINER\u00cdA DE DATOS  5  \n1.1.4.  TECNOLOG\u00cdAS DE APOYO  6  \n1.1.5.  \u00c1REAS DE APLICACI\u00d3N  9  \n1.1.6.  TENDENCIAS DE LA MINER\u00cdA DE DATOS  13  \n1.2.  MINER\u00cdA DE DATOS Y ALMACENAMIENTO DE DATOS  14  \n1.2.1.  ARQUITECTURA , MODELADO , DISE\u00d1O , Y ASPECTOS DE LA ADMINISTRACI\u00d3N  14  \n1.2.2.  DATA MINING Y FUNCIONES DE BASES DE DATOS  16  \n1.2.3.  DATA  WAREHOUSE 17  \n1.2.4.  DATA  WAREHOUSE  Y DATA  MINING 21  \n1.3.  HERRAMIENTAS COMERCIALES DE AN\u00c1LISIS DE DATOS  22  \n1.4.  ARQUITECTURA SOFTWARE PARA DATA MINING  33  \n1.4.2.  ARQUITECTURA FUNCIONAL  35  \n1.4.3.  ARQUITECTURA DEL SISTEMA  36  \n1.4.4.  EL DATA MINING EN LA ARQUITECTURA DEL SISTEMA  38  \nCAP\u00cdTULO 2. AN\u00c1LISIS ESTAD\u00cdSTICO MEDIANTE EXCEL\n 41  \n2.1.  AN\u00c1LISIS DE UNA VARIABLE . ESTAD\u00cdSTICA DESCRIPTIVA E \nINFERENCIA  43    \u00cdndice \nT\u00e9cnicas de An\u00e1lisis de Datos  ii 2.2.  T\u00c9CNICAS DE EVALUACI\u00d3N DE HIP\u00d3TESIS  57  \n2.2.1.  AN\u00c1LISIS DE RELACIONES ENTRE ATRIBUTOS  57  \n2.2.2.  RELACI\u00d3N ENTRE VARIABLES NOMINALES -NOMINALES  57  \n2.2.3.  RELACIONES NUM\u00c9RICAS -NOMINALES  59  \n2.2.3.1. Comparaci\u00f3n de dos medias 60  \n2.2.3.2. An\u00e1lisis de la varianza 61  \n2.2.4.  RELACIONES NUM\u00c9RICAS -NUM\u00c9RICAS : 64  \n2.2.4.1. Regres i\u00f3n lineal 64  \n2.2.5.  EVALUACI\u00d3N DEL MODELO DE REGRESI\u00d3N  65  \n2.2.5.1. Medidas  de Calidad 65  \n2.2.5.2. Test de Hip\u00f3tesis sobre modelo de regresi\u00f3n 66  \n2.3.  EJEMPLOS DE APLICACI\u00d3N DE T\u00c9CNICAS DE EVALUACI\u00d3N DE \nHIP\u00d3TESIS  67  \n2.3.1.  EJEMPLOS DE VALIDACI\u00d3N DE HIP\u00d3TESIS  67  \n2.4.  T\u00c9CNICAS CL\u00c1SICAS DE CLASIFICACI\u00d3N Y PREDICCI\u00d3N  76  \n2.4.1.  CLASIFICACI\u00d3N BAYESIANA : 80  \n2.4.2.  REGRESI\u00d3N LINEAL  90  \nCAP\u00cdTULO 3. T\u00c9CNICAS DE MINER\u00cdA DE DATOS  \nBASADAS EN APRENDIZAJE AUTOM\u00c1TICO 96  \n3.1.  T\u00c9CNICAS DE MINER\u00cdA DE DATOS  96  \n3.2.  CLUSTERING .  (\u201cSEGMENTACI\u00d3N \u201d) 98  \n3.2.1.  CLUSTERING NUM\u00c9RICO (K-MEDIAS ) 99  \n3.2.2.  CLUSTERING CONCEPTUAL (COBWEB) 100  \n3.2.3.  CLUSTERING PROBABIL\u00cdSTICO (EM) 104  \n3.3.  REGLAS DE ASOCIACI\u00d3N  107  \n3.4.  LA PREDICCI\u00d3N  110  \n3.4.1.  REGRESI\u00d3N NO LINEAL . 110  \n3.4.2.  \u00c1RBOLES DE PREDICCI\u00d3N  111  \n3.4.3.  ESTIMADOR DE N\u00daCLEOS  115  \n3.5.  LA CLASIFICACI\u00d3N  120  \n3.5.1.  TABLA DE DECISI\u00d3N  121  \n3.5.2.  \u00c1RBOLES DE DECISI\u00d3N  123  \n3.5.3.  REGLAS DE CLASIFICACI\u00d3N  135    \u00cdndice \nT\u00e9cnicas de An\u00e1lisis de Datos  iii 3.5.4.  CLASIFICACI\u00d3N BAYESIANA  140  \n3.5.5.  APRENDIZAJE BASADO EN EJEMPLARES  145  \n3.5.6.  REDES DE NEURONAS  153  \n3.5.7.  L\u00d3GICA BORROSA (\u201cFUZZY LOGIC \u201d) 157  \n3.5.8.  T\u00c9CNICAS GEN\u00c9TICAS : ALGORITMOS GEN\u00c9TICOS (\u201cGENETIC ALGORITHMS \u201d) 157  \nCAP\u00cdTULO 4. T\u00c9CNICAS DE AN\u00c1LISIS DE DATOS EN \nWEKA 159  \nINTRODUCCI\u00d3N  159  \nPREPARACI\u00d3N DE LOS DATOS  160  \nMUESTRA DE DATOS  160  \nOBJETIVOS DEL AN\u00c1LISIS  161  \nEJECUCI\u00d3N DE WEKA 162  \nPREPROCESADO DE LOS DATOS  164  \nCARACTER\u00cdSTICAS DE LOS ATRIBUTOS  165  \nTRABAJO CON FILTROS . PREPARACI\u00d3N DE FICHEROS DE MUESTRA  167  \nFiltros de atributos 168  \nFiltros de instancias 172  \nVISUALIZACI\u00d3N  173  \nREPRESENTACI\u00d3N 2D DE LOS DATOS  173  \nFILTRADO \u201cGR\u00c1FICO \u201d DE LOS DATOS  177  \nASOCIACI\u00d3N  178  \nAGRUPAMIENTO  183  \nAGRUPAMIENTO NUM\u00c9RICO  184  \nAGRUPAMIENTO SIMB\u00d3LICO  189  \nCLASIFICACI\u00d3N  191  \nMODOS DE EVALUACI\u00d3N DEL CLASIFICADOR  192  \nSELECCI\u00d3N Y CONFIGURACI\u00d3N DE CLASIFICADORES  195  \nPREDICCI\u00d3N NUM\u00c9RICA  203  \nAPRENDIZAJE DEL MODELO Y APLICACI\u00d3N A NUEVOS DATOS . 209  \nSELECCI\u00d3N DE ATRIBUTOS  211    \u00cdndice \nT\u00e9cnicas de An\u00e1lisis de Datos  iv CAP\u00cdTULO 5. IMPLEMENTACI\u00d3N DE LAS T\u00c9CNICAS DE \nAN\u00c1LISIS DE DATOS EN WEKA 215  \n5.1.  UTILIZACI\u00d3N DE LAS CLASES DE WEKA  EN PROGRAMAS \nINDEPENDIENTES  215  \n5.2.  TABLA DE DECISI\u00d3N EN WEKA 215  \n5.3.  ID3 EN WEKA 216  \n5.4.  C4.5  EN WEKA  (J48) 216  \n5.5.  \u00c1RBOL DE DECISI\u00d3N DE UN SOLO NIVEL EN WEKA 219  \n5.6.  1R EN WEKA 220  \n5.7.  PRISM  EN WEKA 221  \n5.8.  PART  EN WEKA 221  \n5.9.  NAIVE  BAYESIANO EN WEKA 222  \n5.10.  VFI EN WEKA 223  \n5.11.  KNN  EN WEKA  (IBK) 224  \n5.12.  K* EN WEKA 226  \n5.13.  REDES DE NEURONAS EN WEKA 227  \n5.14.  REGRESI\u00d3N LINEAL EN WEKA 228  \n5.15.  REGRESI\u00d3N LINEAL PONDERADA LOCALMENTE EN WEKA 230  \n5.16.  M5 EN WEKA 231  \n5.17.  KERNEL DENSITY EN WEKA 232  \n5.18.  K-MEANS EN WEKA 234  \n5.19.  COBWEB  EN WEKA 234  \n5.20.  EM EN WEKA 235  \n5.21.  ASOCIACI\u00d3N A PRIORI EN WEKA 236  \nCAP\u00cdTULO 6. EJEMPLOS SOBRE CASOS DE ESTUDIO 239    \u00cdndice \nT\u00e9cnicas de An\u00e1lisis de Datos  v BIBLIOGRAF\u00cdA 240  Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 1 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n  \nCap\u00edtulo 1. Introducci\u00f3n \nEn este texto se estudia uno de los campos que m\u00e1s se est\u00e1n estudiando en \nestos d\u00edas: La extracci\u00f3n de conocimiento a partir de fuentes masivas de datos. \nPara ello se emplean las denominadas t\u00e9cnicas de miner\u00eda de datos, que son algoritmos capaces de obtener relaciones entre distintos atributos o conceptos \npara ayudar, por ejemplo, a la toma de decisiones. \nAdem\u00e1s de las t\u00e9cnicas estad\u00edsticas se estudian las t\u00e9cnicas de Miner\u00eda de \nDatos [Data Mining] basadas en t\u00e9cnic as de aprendizaje autom\u00e1tico que se \nimplementan en una herramienta de miner\u00eda de datos de libre distribuci\u00f3n: WEKA. Esta herramienta permite, a partir de ficheros de texto en un formato determinado, utilizar distintos tipos de t\u00e9cnicas para extraer informaci\u00f3n. \nA continuaci\u00f3n se definen los conceptos fundamentales emplea dos en el texto: \nKDD y, sobretodo, miner\u00eda de datos, as\u00ed como sus principales caracter\u00edsticas. \nPosteriormente se comenta la estructura del proyecto. \n1.1.  KDD y Miner\u00eda de Datos \nHoy en d\u00eda, la canti dad de datos que ha sido alma cenada en las bases de \ndatos excede nuestra habilidad para reducir y analizar los datos sin el uso de \nt\u00e9cnicas de an\u00e1lisis automatizadas. Muchas bases de datos comerciales transaccionales y cient\u00edficas crecen a una proporci\u00f3n fenomenal.  \nKDD [Knowledge Discovery in Databases] [PSF91] es el proceso completo de \nextracci\u00f3n de informaci\u00f3n,  que se encarga adem\u00e1s de la preparaci\u00f3n de los \ndatos y de la interpretaci\u00f3n de los resultados obtenidos. KDD se ha definido como \u201cel proceso no trivia l de identificaci\u00f3n en los datos de patrones v\u00e1lidos, \nnuevos, potencialmente \u00fatiles, y finalmente comprensibles\u201d [FAYY96]. Se trata \nde interpretar grandes cantidades de datos y encontrar relaciones o patrones. \nPara conseguirlo har\u00e1n falta t\u00e9cnicas de aprendizaje autom \u00e1tico [Machine \nLearning] [MBK98], estad\u00edstica [MIT97,  DEGR86], bases de datos [CODD70], \nt\u00e9cnicas de representaci\u00f3n del conocim iento, razonamiento basado en casos \n[CBR, Case Based Reasoning], ra zonamiento aproximado, adquisici\u00f3n de \nconocimiento, redes de neuronas y visua lizaci\u00f3n de datos. Tareas comunes en \nKDD son la inducci\u00f3n de reglas, los probl emas de clasificaci\u00f3n y clustering, el \nreconocimiento de patrones, el model ado predictivo, la detecci\u00f3n de \ndependencias, etc.  \nKDD es un campo creciente: hay muc has metodolog\u00edas del descubrimiento del \nconocimiento en uso y bajo desarrollo . Algunas de estas t\u00e9cnicas son \ngen\u00e9ricas, mientras otros son de dominio espec\u00edfico.  Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 2 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Los datos recogen un conjunto de hechos  (una base de datos) y los patrones \nson expresiones que describen un subconjunto de los datos (un modelo aplicable a ese subconjunto). KDD involu cra un proceso iterativo e interactivo \nde b\u00fasqueda de modelos, patrones o par \u00e1metros. Los patrones descubiertos \nhan de ser v\u00e1lidos, novedosos para el sist ema (para el usuar io siempre que \nsea posible) y potenc ialmente \u00fatiles.  \nSe han de definir medidas cuantitativas para los patrones obtenidos (precisi\u00f3n, \nutilidad, beneficio obtenido...) . Se debe establecer al guna medida de inter\u00e9s \n[interestingness] que considere la validez , utilidad y simplicidad de los patrones \nobtenidos mediante algun a de las t\u00e9cnicas de Miner\u00ed a de Datos. El objetivo \nfinal de todo esto es incorporar el conocimiento obtenido en alg\u00fan sistema real, \ntomar decisiones a partir de los resultados alcanzados o, simp lemente, registrar \nla informaci\u00f3n conseguida y suminist r\u00e1rsela a quien est\u00e9 interesado.  \nHa llegado un momento en el que disp onemos de tanta informaci\u00f3n que nos \nvemos incapaces de sacarle provecho. Lo s datos tal cual se almacenan [raw \ndata] no suelen proporcionar beneficios directos. Su va lor real reside en la \ninformaci\u00f3n que podamos extraer de el los: informaci\u00f3n que nos ayude a tomar \ndecisiones o a mejorar nuestra comprens i\u00f3n de los fen\u00f3menos que nos rodean.  \nSe requiere de grandes  cantidades de datos que  proporcionen informaci\u00f3n \nsuficiente para derivar un conocimi ento adicional. Dado que se requieren \ngrandes cantidades de datos, es esencial el proceso de la eficiencia. La \nexactitud es requerida para asegurar que el descubrimient o del conocimiento \nes v\u00e1lido. Los resultados deber\u00e1n se r presentados de una manera entendible \npara el ser humano. Una de las prem isas mayores de KDD es que el \nconocimiento es descubierto usando t\u00e9 cnicas de aprendizaje inteligente que \nvan examinando los datos a trav\u00e9s de procesos autom atizados. Para que una \nt\u00e9cnica sea considerada \u00fatil para el descubrimiento del co nocimiento, \u00e9ste \ndebe ser interesante; es decir, debe tener  un valor potencial para el usuario. \nKDD proporciona la capacidad para descubr ir informaci\u00f3n nueva y significativa \nusando los datos existentes . KDD se define como: \"The nontrivial process of \nidentifying valid, novel, potentially useful, and ultimately understandable \npatterns in data\" en Fayyad, Piatetsky-Shapiro & Sm yth: \"From data mining to \nknowledge discovery: An overview\" Ad vances in Knowledge Discovery and \nData Mining (AAAI / MIT Press, 1996)  y se puede resumir en la Figura 1. \n Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 3 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 1.1: Esquema del proceso de KDD \nKDD r\u00e1pidamente excede la capacidad humana para analizar grandes \ncantidades de datos. La cantidad de da tos que requieren procesamiento y \nan\u00e1lisis en grandes bases de datos ex ceden las capacidades humanas y la \ndificultad de transformar los datos c on precisi\u00f3n es un co nocimiento que va \nm\u00e1s all\u00e1 de los l\u00edmites de las bases de dat os tradicionales. Por consiguiente, la \nutilizaci\u00f3n plena de los datos almacenado s depende del uso de t\u00e9cnicas del \ndescubrimiento del conocimiento. \nLa utilidad de aplicaciones futuras en KDD es de largo al cance. KDD puede \nusarse como un medio de recuperaci\u00f3 n de informaci\u00f3n, de  la misma manera \nque los agentes inteligentes realizan la recuperaci\u00f3n de informaci\u00f3n en el Web. \nNuevos modelos o tendencias en los datos podr\u00e1n descubrirse usando estas \nt\u00e9cnicas. KDD tambi\u00e9n puede usarse  como una base para las interfaces \ninteligentes del ma\u00f1ana,  agregando un componente del  descubrimiento del \nconocimiento a un sistema de bases de datos o integrando KDD con las hojas \nde c\u00e1lculo y visualizaciones.  \n1.1.2. El proceso de KDD \nEl proceso de KDD se inicia con la ident ificaci\u00f3n de los datos. Para ello hay que \nimaginar qu\u00e9 datos se necesitan,  d\u00f3nde se pueden encontrar y c\u00f3mo \nconseguirlos. Una vez que se dispone de datos, se deben se leccionar aquellos \nque sean \u00fatiles para los objetivos propuesto s. Se preparan, poni\u00e9ndolos en un \nformato adecuado. \nUna vez se tienen los datos adecuados se procede a la miner\u00eda de datos, \nproceso en el que se seleccionar\u00e1n la s herramientas y t\u00e9cnicas adecuadas \npara lograr los objetivos pretendidos. Y tras este pr oceso llega el an\u00e1lisis de \nresultados, con lo que se obti ene el conocimiento pretendido. \nEn la figura 1.2 se muestra la met odolog\u00eda que debe seguirse para obtener \nconocimiento a partir de los datos que se encuentran en la base de datos. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 4 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 1.2: Metodolog\u00eda para el descubrimiento de conocimiento en bases de datos. \nKDD es un proceso interactivo e iter ativo, que involucra numerosos pasos e \nincluye muchas decisiones que deben ser tomadas por el usuario, y se estructura en las siguientes etapas [FAYY96]: \n\u2022 Comprensi\u00f3n del dominio de la aplicac i\u00f3n, del conocimiento relevante y \nde los objetivos del usuario final.  \n\u2022 Creaci\u00f3n del conjunto de datos: consiste  en la selecci\u00f3n del conjunto de \ndatos, o del subconjunto de variabl es o muestra de datos, sobre los \ncuales se va a realizar el descubrimiento.  \n\u2022 Limpieza y preprocesamiento de los datos: Se compone de las \noperaciones, tales como: recolecci\u00f3n de la informaci\u00f3n necesaria sobre \nla cual se va a realizar el proceso,  decidir las estrategi as sobre la forma \nen que se van a manejar los cam pos de los datos no disponibles, \nestimaci\u00f3n del tiempo de la informaci\u00f3n y sus posibles cambios.  \n\u2022 Reducci\u00f3n de los datos y proyecci\u00f3n: Encontrar las caracter\u00edsticas m\u00e1s \nsignificativas para representar lo s datos, dependiendo del objetivo del \nproceso. En este paso se pueden ut ilizar m\u00e9todos de transformaci\u00f3n \npara reducir el n\u00famero efectivo de variables a ser consideradas o para \nencontrar otras representaciones de los datos.  \n\u2022 Elegir la tarea de Miner\u00eda de Datos: De cidir si el objetivo del proceso de \nKDD es: Regresi\u00f3n, Clasificac i\u00f3n, Agrupamiento, etc.  \n\u2022 Elecci\u00f3n del algoritmo(s) de Miner\u00eda de Datos: Selecci\u00f3n del m\u00e9todo(s) a \nser utilizado para buscar los patrones  en los datos. Incluye adem\u00e1s la \ndecisi\u00f3n sobre que modelos y par\u00e1metros pueden ser los m\u00e1s apropiados.  \n\u2022 Miner\u00eda de Datos: Consiste en la b\u00fasqueda de los patrones de inter\u00e9s en \nuna determinada forma de representac i\u00f3n o sobre un conjunto de Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 5 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda representaciones, utilizando para ello m\u00e9 todos de clasificaci\u00f3n, reglas o \n\u00e1rboles, regresi\u00f3n, agrupaci\u00f3n, etc.  \n\u2022 Interpretaci\u00f3n de los patrones encontrados. Dependiendo de los \nresultados, a veces se hace necesar io regresar a uno de los pasos \nanteriores.  \n\u2022 Consolidaci\u00f3n del conocimiento descubierto: consiste en la \nincorporaci\u00f3n de este conocimiento al funcionamiento del sistema, o \nsimplemente documentaci\u00f3n e informaci\u00f3n a las partes interesadas.  \nEl proceso de KDD puede involucrar vari as iteraciones y puede contener ciclos \nentre dos de cualquiera de los pasos. La mayor\u00eda de los trabajos que se han \nrealizado sobre KDD se c entran en la etapa de miner\u00eda . Sin embargo, los otros \npasos se consideran impor tantes para el \u00e9xito del KDD. Por eso aunque la \nMiner\u00eda de Datos es una parte del pr oceso completo de KDD [FAYY96], en \nbuena parte de la literatura los t\u00e9rminos Miner\u00eda de Dato s y KDD se identifican \ncomo si fueran lo mismo. \nEn la figura 1.3 se muestra el esfuer zo que requiere cada fa se del proceso de \nKDD.  \n0%10%20%30%40%50%60%70%Esfuerzo (%)\nEntendimiento del\nDominioPreparaci\u00f3n de\nlos DatosData Mining Interpretaci\u00f3n y\nConsolidaci\u00f3n del\nConocimiento\nFase\n \nFigura 1.3: Esfuerzo requerido por cada fase del proceso de KDD. \nComo se observa en la figura 1.3, gran parte del esfuerzo del proceso de KDD \nrecae sobre la fase de preparaci\u00f3n de lo s datos, fase crucial para tener \u00e9xito \ncomo ya se coment\u00f3 anteriormente. \n1.1.3. Miner\u00eda de Datos \nMiner\u00eda de Datos es un t\u00e9rmino gen \u00e9rico que engloba resultados de \ninvestigaci\u00f3n, t\u00e9cnicas y herramientas usadas para extraer informaci\u00f3n \u00fatil de \ngrandes bases de datos. Si bien Miner\u00eda de Datos es una parte del proceso completo de KDD, en buena parte de la lit eratura los t\u00e9rminos Miner\u00eda de Datos \ny KDD se identifican como si fueran lo mismo. Concretamente, el t\u00e9rmino Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 6 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Miner\u00eda de Datos es us ado com\u00fanmente por los es tad\u00edsticos, analistas de \ndatos, y por la comunidad de administ radores de sistemas inform\u00e1ticos como \ntodo el proceso del descubr imiento, mientras que el t\u00e9rmino KDD es utilizado \nm\u00e1s por los especialistas en Inteligencia Artificial. \nEl an\u00e1lisis de la informaci\u00f3n recopil ada (por ejemplo, en un experimento \ncient\u00edfico) es habitual que sea un proces o completamente m anual (basado por \nlo general en t\u00e9cnicas est ad\u00edsticas). Sin embargo, cu ando la cantidad de datos \nde los que disponemos aumenta la resolu ci\u00f3n manual del pr oblema se hace \nintratable. Aqu\u00ed es donde entra en juego el conjunto de t\u00e9cnicas de an\u00e1lisis \nautom\u00e1tico al que nos referimos al habl ar de Miner\u00eda de Datos o KDD.  \nHasta ahora, los mayores \u00e9xitos en Mi ner\u00eda de Datos se pueden atribuir directa \no indirectamente a avances en bases de datos (un campo en el que los \nordenadores superan a los humanos). No obstante, muchos problemas de \nrepresentaci\u00f3n del conocimiento y de reducci\u00f3n de la complejidad de la \nb\u00fasqueda necesaria (usando conocimiento a priori) est\u00e1n a\u00fan por resolver. Ah\u00ed \nreside el inter\u00e9s que ha despertado el tema entre investigadores de todo el \nmundo.  \nA continuaci\u00f3n se presentan varias def iniciones de Miner\u00eda de Datos (MD): \n\u2022 \u201cMD es la extracci\u00f3n no trivial de informaci\u00f3n impl\u00edcita, desconocida \npreviamente, y potencialmente \u00fatil desde los datos\u201d [PSF91]. \n\u2022 \u201cMD es el proceso de extracci\u00f3n y refinamiento de conocimiento \u00fatil \ndesde grandes bases de datos\u201d [SLK96]. \n\u2022 \u201cMD es el proceso de extra cci\u00f3n de informaci\u00f3n previamente \ndesconocida, v\u00e1lida y procesable desde grandes bases de datos para \nluego ser utilizada en la toma de decisiones\u201d [CHSVZ]. \n\u2022 \"MD es la exploraci\u00f3n y an\u00e1lisis, a trav\u00e9s de medios autom\u00e1ticos y \nsemiautom\u00e1ticos, de grandes cantidades de datos con el fin de descubrir \npatrones y reglas signif icativos\" [BERR97].  \n\u2022 \"MD es el proceso de planteamient o de distintas cons ultas y extracci\u00f3n \nde informaci\u00f3n \u00fatil, patrones y t endencias previamente desconocidas \ndesde grandes cantidades de datos  posiblemente almacenados en \nbases de datos\u201d [THUR99]. \n\u2022 \u201cMD es el proceso de descubrir modelos en los datos\u201d [WF00]. \n1.1.4. Tecnolog\u00edas de Apoyo \nPara el estudio de la Miner\u00eda de Datos se ha tomado la perspectiva orientada a \ndatos, por dos razones. Pr imero porque la mayor\u00eda de  los trabajos en Miner\u00eda \nde Datos est\u00e1n enfocados hacia el data warehouse  que proporciona el apoyo a \nla Miner\u00eda de Datos organizando y es tructurando los datos. Adem\u00e1s, otras \ntecnolog\u00edas de apoyo a la miner\u00eda datos han sido utilizadas desde hace tiempo Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 7 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda y la integraci\u00f3n de estas tecnolog\u00edas  con la administr aci\u00f3n de datos ha \ncontribuido mucho a mejora r la Miner\u00eda de Datos. \nLas m\u00e1s importantes entre estas tecnol og\u00edas son los m\u00e9todos estad\u00edsticos \n[DEGR86] y el aprendizaje  autom\u00e1tico [MIT97]. Los m\u00e9todos estad\u00edsticos han \nproducido varios paquetes estad\u00edsticos [THUR99]  para computar sumas, \npromedios, y distribuciones, que han ido in tegr\u00e1ndose con las bases de datos a \nexplorar. El aprendizaje autom\u00e1tico c onsiste en la obtenci\u00f3n de reglas de \naprendizaje y modelos de los datos, para lo cual a menudo se necesita la \nayuda de la estad\u00edstica. Por esta ra z\u00f3n, los m\u00e9todos estad\u00edsticos y el \naprendizaje autom\u00e1tico son los dos co mponentes m\u00e1s  importantes de la \nMiner\u00eda de Datos. Adem\u00e1s existen otras tecnolog\u00edas, entre las que se incluyen \nvisualizaci\u00f3n, procesami ento paralelo, y apoyo a la toma de decisiones. Las \nt\u00e9cnicas de visualizaci\u00f3n ayudan a presen tar los datos para facilitar la Miner\u00eda \nde Datos. Las t\u00e9cnicas procesamiento paralelo ayudan a mejorar el rendimiento \nde la Miner\u00eda de Datos. Los sistemas de apoyo a la toma de decisiones ayudan \na discriminar los resultados y proporcionan los resultados esenciales para llevar \na cabo las funciones de direcci\u00f3n. \nRazonamiento estad\u00edstico \nLas t\u00e9cnicas y m\u00e9todos estad\u00edsticas  del razonamiento han sido utilizados \ndurante varias d\u00e9cadas, siendo los \u00fanicos medios de anal izar los datos en el \npasado. Numerosos paquetes [THUR99] est\u00e1n ahora disponibles para computar promedios, sumas, y diferent es distribuciones para diferentes \naplicaciones. Por ejemplo, la oficin a del censo usa an\u00e1lisis y m\u00e9todos \nestad\u00edsticos para analizar  la poblaci\u00f3n en un pa\u00eds.  M\u00e1s recientemente, las \nt\u00e9cnicas estad\u00edsticas del razonamient o est\u00e1n jugando un papel importante en la \nMiner\u00eda de Datos. Algunos paquetes estad\u00edsticos que han sido utilizados \ndurante mucho tiempo, se han integrado con las diferentes bases de datos, y se est\u00e1n comercializ\u00e1ndose en la actual idad como productos para la Miner\u00eda de \nDatos.  \nLa estad\u00edstica juega un impor tante papel en el an\u00e1lisis de los datos, e incluso \ntambi\u00e9n en el aprendizaje autom\u00e1tico. Debido a esto, no se puede estudiar la \nMiner\u00eda de Datos sin un buen conocimiento de la estad\u00edstica.  \nVisualizaci\u00f3n \nLas tecnolog\u00edas de la visualizaci\u00f3n muestran gr\u00e1ficamente los datos en las \nbases de datos. Se ha investigado much o sobre la visualizaci\u00f3n y el campo ha \nadelantado un gran trecho sobre todo con la incorporaci\u00f3n de la inform\u00e1tica \nmultimedia. Por ejemplo, los datos en las bases de datos ser\u00e1n filas y filas de valores num\u00e9ricos, y las herramientas de visualizaci\u00f3n toman estos datos y \ntrazan con ellos alg\u00fan tipo de gr\u00e1fico. Los modelos de visualizaci\u00f3n pueden ser \nbidimensionales, tridimensionales o in cluso multidimensio nales. Se han \ndesarrollado varias herramientas de visual izaci\u00f3n para integrarse con las bases \nde datos, y algunos trabajos sobre este tema est\u00e1n recogidos en [VIS95].  \nAs\u00ed, las herramientas de visualizaci\u00f3n ay udan de forma interactiva a la Miner\u00eda \nde Datos, aunque hay pocos tr abajos sobre la integrac i\u00f3n de las herramientas Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 8 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda de Miner\u00eda de Datos y de visualizaci\u00f3 n. Algunas ideas preliminares se \npresentaron en el IEEE Databases and Visualization Workshop  de 1995 \n(v\u00e9ase, por ejemplo, [VIS95]). Sin em bargo, se han realizado m\u00e1s progresos \nque se pueden encontrar en [VIS97], aunque queda todav\u00eda mucho trabajo por hacer en este tema. \nProcesamiento paralelo \nEl procesamiento paralelo es una t\u00e9cn ica que ha sido utilizado durante mucho \ntiempo. El \u00e1rea se ha des arrollado significativamente, desde sistemas con un \n\u00fanico procesador hasta sistemas mu ltiprocesador. Los sistemas de \nmultiprocesamiento pueden estar formados por sistemas distribuidos o por sistemas centralizados de multiproces adores con memoria compartida, o con \nmultiprocesadores sin memoria compartida. Hay muchos trabajos sobre la utilizaci\u00f3n de las arquitecturas paralelas pa ra el procesamiento de las bases de \ndatos (v\u00e9ase, por ejemplo, [IEEE89] ). A pesar de haberse realizado \nconsiderable trabajo sobre el tema, es tos sistemas no fueron comercializados \nhasta el desarrollo del data warehouse , ya que muchos de los data warehouses  \nemplean el procesamiento paralelo para ac elerar el proceso de las consultas. \nEn un sistema de bases de datos paralelas, se ejecutan varias operaciones y \nfunciones en paralelo. A pes ar de que la investigaci\u00f3n en sistemas de bases de \ndatos en paralelo empez\u00f3 en los a\u00f1os setenta, estos sistemas se han \nempezado a utilizar para las aplicaciones  comerciales recientemente, debido \nen parte a la explosi\u00f3n del data warehouse  y de las tecnolog\u00edas de Miner\u00eda de \nDatos d\u00f3nde el rendimiento de los algoritmos de consulta es cr\u00edtico. Para \nescalar las t\u00e9cnicas de Miner\u00eda de Datos se necesita hardware y software apropiado, por lo que los fabricantes de bases de datos  est\u00e1n empleando \nordenadores con procesamiento paralel o para llevar a cabo la Miner\u00eda de \nDatos.  \nApoyo a la toma de decisiones \nLos sistemas de apoyo a la toma de decis iones son las herramientas que usan \nlos directivos para tomar decisiones efic aces, y se basan en la teor\u00eda de la \ndecisi\u00f3n. Se puede consider ar a las herramientas de Miner\u00eda de Datos como \ntipos especiales de herramientas de apo yo a la toma de decisiones. Las \nherramientas de apoyo a la toma de decisiones pertenecen a una amplia categor\u00eda (v\u00e9ase, por ejemplo, [DECI]). \nEn general, las herramientas de apoyo a la toma de decisiones podr\u00edan \nutilizarse tambi\u00e9n como herramientas par a eliminar los resultados innecesarios \ne irrelevantes obtenidos de la Mi ner\u00eda de Datos. Tambi\u00e9n pueden ser \nconsideradas de este tipo, herramientas tales como las hojas de c\u00e1lculo, sistemas expertos, sistem as de hipertexto, sistemas de gesti\u00f3n de informaci\u00f3n \nde web, y cualquier otro sistema que ayude a analistas y gestores a manejar \neficazmente grandes cant idades de datos e informa ci\u00f3n. Recientemente ha \naparecido un \u00e1rea nueva ll amada gesti\u00f3n del conocimiento. La gesti\u00f3n del \nconocimiento trata de manejar  eficazmente los datos,  la informaci\u00f3n, y el \nconocimiento de una organizaci\u00f3n [MORE98a].  Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 9 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Se puede pensar que el apoyo a la toma de decisiones es una tecnolog\u00eda que \nse solapa con la Miner\u00eda de Datos, almacenamiento de datos, gesti\u00f3n del conocimiento, aprendizaje autom\u00e1tico, es tad\u00edstica, y otra s tecnolog\u00edas que \nayudan gestionar el conocimiento de una organizaci\u00f3n y los datos.  \nAprendizaje autom\u00e1tico \nEl aprendizaje autom\u00e1t ico, en muchos casos, consiste fundamentalmente en el \naprendizaje de reglas a partir de los datos [MIT97], y por eso muchas de las t\u00e9cnicas de aprendizaje autom\u00e1tico son ut ilizadas en la actualidad en la Miner\u00eda \nde Datos.  \nEl aprendizaje autom\u00e1tic o aparece continuamente en la realizaci\u00f3n de \naprendizaje  computacional desde la exper iencia. Como Mitchell describe en su \nexcelente texto sobre aprendizaje au tom\u00e1tico [MIT97], el aprendizaje \nautom\u00e1tico consiste en aprender de las ex periencias del pasado con respecto a \nalguna medida de rendimient o. Por ejemplo, en las aplicaciones de los juegos \nde computadora, el aprendi zaje autom\u00e1tico podr\u00eda ser aprender a jugar un \njuego de ajedrez, desde las experiencia s del pasado que podr \u00edan ser juegos \nque el ordenador juega contra s\u00ed mismo, con respecto a alguna medida de \nrendimiento, como ganar un cierto n\u00famero de partidas. \nSe han desarrollado distintas t\u00e9cnicas en el aprendizaje autom\u00e1t ico, incluyendo \nel aprendizaje conceptual donde se apr ende los conceptos desde diferentes \nejemplos de entrenamiento,  las redes de neuronas, los algoritmos gen\u00e9ticos, \nlos \u00e1rboles de decisi\u00f3n, y la programaci\u00f3n de la l \u00f3gica inductiva. Se han \nrealizado diferentes est udios te\u00f3ricos sobre el aprendizaje autom\u00e1tico, que \nintentan determinar la complejidad y c apacidad de las diferentes t\u00e9cnicas de \naprendizaje autom\u00e1tico [MIT97]. \nLos investigadores del aprendizaje aut om\u00e1tico  han agr upado las t\u00e9cnicas en \ntres categor\u00edas [THUR99]. La primera es  el aprendizaje activo que se ocupa de \nla interacci\u00f3n y realizaci\u00f3n de las consul tas durante el aprendizaje, la segunda \nes el aprendizaje desde el conocimiento anterior, y la tercera es el aprendizaje \nincremental. Hay alguna superposici\u00f3n entre los tres m\u00e9todos. Durante un \nseminario sobre aprendizaje autom\u00e1ti co [DARP98] fueron estudiados los \nproblemas y desaf\u00edos en aprendizaje autom\u00e1tico y sus relaciones con la Miner\u00eda de Datos. Hay todav\u00eda mucha in vestigaci\u00f3n que realiz ar en este \u00e1rea, \nsobre todo en la integraci\u00f3n del apr endizaje autom\u00e1tico con las diferentes \nt\u00e9cnicas de gesti\u00f3n de datos. Tal invest igaci\u00f3n mejorar\u00e1 significativamente el \n\u00e1rea de Miner\u00eda de Datos. Algunos de lo s algoritmos m\u00e1s conocidos de \naprendizaje autom\u00e1tico  se enc uentran en [QUIN93, MBK98]. \n1.1.5. \u00c1reas de Aplicaci\u00f3n \nEn este punto se presentan las principa les \u00e1reas y sectores empresariales en \nlas que se puede aplicar la miner\u00eda de datos. \nMarketing Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 10 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Actualmente con la generaci\u00f3n de los puntos de ventas informatizados y \nconectados a un ordenador ce ntral, y el constante uso de las tarjetas de \ncr\u00e9ditos se genera gran cantidad de info rmaci\u00f3n que hay que analizar. Con ello \nse puede emplear la miner\u00eda de datos para: \n\u2022 Identificar patrones de compra de los clientes: Determinar c\u00f3mo \ncompran, a partir de sus principales ca racter\u00edsticas, conocer el grado de \ninter\u00e9s sobre tipos de productos, si  compran determinados productos en \ndeterminados momentos,... \n\u2022 Segmentaci\u00f3n de clientes: Consiste en la agrupaci\u00f3n de los clientes con \ncaracter\u00edsticas similares, por ejem plo demogr\u00e1ficas. Es una importante \nherramienta en la estrategia de mark eting que permite realizar ofertas \nacordes a diferentes tipos de co mportamiento de los consumidores. \n\u2022 Predecir respuestas a campa\u00f1as de mailing : Estas campa\u00f1as son caras \ny pueden llegar a ser molestas para los clientes a los que no le interesan \nel tipo de producto pr omocionado por lo que es importante limitarlas a \nlos individuos con una alta probabili dad de interesarse por el producto. \nEst\u00e1 por ello muy relacionada con la segmentaci\u00f3n de clientes. \n\u2022 An\u00e1lisis de cestas de la compra  [market-basket analysis]: Consiste en \ndescubrir relaciones entre productos,  esto es, determinar qu\u00e9 productos \nsuelen comprarse junto con otros,  con el fin de distribuirlos \nadecuadamente. \nCompa\u00f1\u00edas de Seguros \nEn el sector de las compa\u00f1\u00edas de seguros y la salud privada, se pueden \nemplear las t\u00e9cnicas de miner\u00eda  de datos, por ejemplo para: \n\u2022 An\u00e1lisis de procedimientos m\u00e9di cos solicitados conjuntamente. \n\u2022 Predecir qu\u00e9 clientes compran nuevas p\u00f3lizas. \n\u2022 Identificar patrones de comporta miento para clientes con riesgo. \n\u2022 Identificar compor tamiento fraudulento. \nBanca \nEn el sector bancario la informaci \u00f3n que puede almacenarse es, adem\u00e1s de las \ncuentas de los clientes, la relativa a la utilizaci\u00f3n de las tarjetas de cr\u00e9dito, que \npuede permitir conocer h\u00e1bitos y patrones de comportamiento de los usuarios. \nEsta informaci\u00f3n puede aplicarse para: \n\u2022 Detectar patrones de uso fraudul ento de tarjetas de cr\u00e9dito. \n\u2022 Identificar clientes leales: Es im portante para las compa\u00f1\u00edas de cualquier \nsector mantener los clientes. Y es que hay estudios que demuestran que Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 11 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda es cuatro veces m\u00e1s caros obtener nuevos clientes que mantener los \nexistentes. \n\u2022 Predecir clientes con probabilid ad de cambiar su afiliaci\u00f3n. \n\u2022 Determinar gasto en tarjeta de cr\u00e9dito por grupos. \n\u2022 Encontrar correlaciones entre indicadores financieros. \n\u2022 Identificar reglas de mercado de valores a partir de hist\u00f3ricos:  \n \nTelecomunicaciones \nEn el sector de las telecomunicaci ones se puede almacenar informaci\u00f3n \ninteresante sobre las llamadas realizadas, tal como el destino, la duraci\u00f3n, la \nfecha,... en que se realiza la llamada, por ejemplo para: \n\u2022 Detecci\u00f3n de fraude telef\u00f3nico: Mediant e por ejemplo el agrupamiento o \nclustering  se pueden detectar patrones  en los datos que permitan \ndetectar fraudes. \nMedicina \nTambi\u00e9n en el campo m\u00e9dico se alma cena gran cantidad de informaci\u00f3n, sobre \nlos pacientes, tal como enfermedades  pasadas, tratamientos impuestos, \npruebas realizadas, evoluci\u00f3n,... \nSe pueden emplear t\u00e9cnicas  de miner\u00eda de datos con esta informaci\u00f3n, por \nejemplo, para: \n\u2022 Identificaci\u00f3n de terapias m\u00e9dica s satisfactorias para diferentes \nenfermedades. \n\u2022 Asociaci\u00f3n de s\u00edntomas y clasific aci\u00f3n diferencial de patolog\u00edas. \n\u2022 Estudio de factores ( gen\u00e9ticos, precedentes, h\u00e1bitos,  alimenticios,...) de \nriesgo para la salud en distintas patolog\u00edas. \n\u2022 Segmentaci\u00f3n de pacientes para una atenci\u00f3n m\u00e1s inteligente seg\u00fan su \ngrupo. \n\u2022 Estudios epidemiol\u00f3gicos, an\u00e1lisis  de rendimientos de campa\u00f1as de \ninformaci\u00f3n, prevenci\u00f3n, sustituci\u00f3n de f\u00e1rmacos,... \n\u2022 Identificaci\u00f3n de terapias m\u00e9di cas y tratamientos err\u00f3neos para \ndeterminadas enfermedades. \nIndustria farmac\u00e9utica Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 12 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En el sector qu\u00edmico y farmac\u00e9utico se almacenan gran cantidad de \ninformaci\u00f3n: \n\u2022 Bases de datos de dominio p\u00fablic o conteniendo informaci\u00f3n sobre \nestructuras y propiedades de componentes qu\u00edmicos. \n\u2022 Resultados de universidades y l aboratorios publicadas en revistas \nt\u00e9cnicas. \n\u2022 Datos generados en la realizaci\u00f3n de los experimentos. \n\u2022 Datos propios de la empresa. \nLos datos son almacenados en diferentes  categor\u00edas y a cada categor\u00eda se le \naplica un diferente trato. Se podr\u00edan realizar, entre otras, las siguientes \noperaciones con la informaci\u00f3n obtenida: \n\u2022 Clustering  de mol\u00e9culas: Consiste en el  agrupamiento de mol\u00e9culas que \npresentan un cierto nivel de similitud, con lo que se pueden descubrir \nimportantes propiedades qu\u00edmicas. \n\u2022 B\u00fasqueda de todas las mol\u00e9culas que contienen un patr\u00f3n espec\u00edfico: \nSe podr\u00eda introducir una subestruct ura (un patr\u00f3n), devolviendo el \nsistema todas las mol\u00e9culas que son similares a dicha estructura. \n\u2022 B\u00fasqueda de todas las mol\u00e9culas que  vincula un camino espec\u00edfico \nhacia una mol\u00e9cula objetivo: Rea lizar una b\u00fasqueda exhaustiva puede \nser impracticable, por lo que se p ueden usar restricciones en el espacio \nde b\u00fasqueda. \n\u2022 Predicci\u00f3n de resultado de experiment os de una nueva mol\u00e9cula a partir \nde los datos almacenados: A trav\u00e9s de determinadas t\u00e9cnicas de \ninteligencia artificial es posible  predecir los resultados a nuevos \nexperimentos a partir de los datos, c on el consiguiente ahorro de tiempo \ny dinero. \nBiolog\u00eda \nCon la finalizaci\u00f3n en los pr\u00f3ximos a\u00f1os del Proyecto Genoma Humano y el \nalmacenamiento de toda la informaci\u00f3n que est\u00e1 generando en bases de datos \naccesibles por Internet, el siguiente reto consiste en  descubrir c\u00f3mo funcionan \nnuestros genes y su influencia en la sal ud. Existen nuevas tecnolog\u00edas (chips \nde ADN, prote\u00f3mica, gen\u00f3mic a funcional, variablidad gen\u00e9tica individual) que \nest\u00e1n posibilitando el desa rrollo de una \u201cnueva biolog\u00eda\u201d que permite extraer \nconocimiento biom\u00e9dicos a partir de bas es de datos experimentales en el \nentorno de un ordenador b\u00e1sicam ente mediante t\u00e9cnicas de miner\u00eda de datos y \nvisualizaci\u00f3n. Estos trabajos form an parte de los desarrollos de la \nBioinform\u00e1tica. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 13 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 1.1.6. Tendencias de la Miner\u00eda de Datos \nEl inter\u00e9s que despierta la Miner\u00eda de Da tos para el an\u00e1lisis de la informaci\u00f3n \nespecialmente en el \u00e1rea comercial ha ce que se busquen nuevas aplicaciones \nbasadas en esta tecnolog\u00eda. Algunas de las principales nuevas aplicaciones \nbasadas en la Miner\u00eda de Datos se presentan a continuaci\u00f3n. \nMiner\u00eda de Textos \nLa Miner\u00eda de Textos [Text Mining] surge ante el problema cada vez m\u00e1s \napremiante de extraer informaci\u00f3n aut om\u00e1ticamente a partir de masas de \ntextos. Se trata as\u00ed de extraer informa ci\u00f3n de datos no estructurados: texto \nplano. \nExisten varias aproximaciones a la representaci\u00f3n de la  informaci\u00f3n no \nestructurada [HH96]: \n\u2022 \u201cBag of Words\u201d: Cada palabra constitu ye una posici\u00f3n de un vector y el \nvalor corresponde con el n\u00famero de veces que ha aparecido. \n\u2022 N-gramas o frases: Permite tener en cuenta el orden de las palabras. \nTrata mejor frases negativas \u201c ... excepto ... \u201d, \u201c... pero no .... \u201d, que \ntomar\u00edan en otro caso las palabras que le siguen como relevantes. \n\u2022 Representaci\u00f3n relacional (primer orden): Permite detectar patrones m\u00e1s \ncomplejos (si la palabra X est\u00e1 a la izqui erda de la palabra Y en la \nmisma frase...). \n\u2022 Categor\u00edas de conceptos. \nCasi todos se enfrentan con el \u201cv ocabulary problem\u201d [FUR87]: Tienen \nproblemas con la sinonimia, la  polisemia, los lemas, etc. \nUn ejemplo de aplicaci\u00f3n basada en Miner\u00eda de Textos es la generaci\u00f3n \nautom\u00e1tica de \u00edndices en documentos. Ot ras m\u00e1s complicadas consistir\u00edan en \nescanear completamente un texto y mo strar un mapa en el que las partes m\u00e1s \nrelacionadas, o los documentos m\u00e1s relacionados se coloquen cerca unos de \notros. En este caso se tratar\u00eda de ana lizar las palabras en el contexto en que se \nencuentren. \nEn cualquier caso, aunque a\u00fan no se ha avanzado mucho en el \u00e1rea de \nMiner\u00eda de Textos, ya hay productos co merciales que emplean esta tecnolog\u00eda \ncon diferentes prop\u00f3sitos. \nMiner\u00eda de datos Web \nLa Miner\u00eda de datos Web [Web Mining] es una tecnolog\u00eda usada para descubrir \nconocimiento interesante en todos los as pectos relacionados a la Web. Es uno \nde los mayores retos. El enorme volum en de datos en la Web generado por la \nexplosi\u00f3n de usuarios y el desarrollo de  librer\u00edas digitales hace que la \nextracci\u00f3n de la informaci\u00f3n \u00fatil sea un gran problema. Cuando el usuario Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 14 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda navega por la web se encuentra frec uentemente saturado por los datos. La \nintegraci\u00f3n de herramientas de miner\u00eda de datos puede ayudar a la extracci\u00f3n \nde la informaci\u00f3n \u00fatil.  \nLa Miner\u00eda de datos Web se puede clasif icar en tres grupos distintos no \ndisjuntos, dependiendo del tipo  de informaci\u00f3n que se quiera extraer, o de los \nobjetivos [KB00]: \n\u2022 Miner\u00eda del Contenido  de la Web [Web Content Mining]: Extraer \ninformaci\u00f3n del contenido de los documentos en la web. Se puede \nclasificar a su vez en: \no Text Mining: Si los documentos son textuales (planos). \no Hypertext Mining: Si los doc umentos contienen enlaces a s\u00ed \nmismos o a otros documentos \no Markup Mining: Si los documentos son semiestructurados (con \nmarcas). \no Multimedia Mining: Para  im\u00e1genes, audio, v\u00eddeo,... \n\u2022 Miner\u00eda de la Estructura de la W eb [Web Structure Mining]: Se intenta \ndescubrir un modelo a partir de  la tipolog\u00eda de enlaces de la red. Este \nmodelo puede ser \u00fatil para clas ificar o agrupar documentos. \n\u2022 Miner\u00eda del Uso de la Web [Web Usage Mining]: Se intenta extraer \ninformaci\u00f3n (h\u00e1bitos, preferencias, etc. de los usuarios o contenidos y \nrelevancia de documentos) a partir de las sesiones y comportamiento de los usuarios navegantes \n1.2.  Miner\u00eda de Datos y Almacenamiento de Datos \nComo se ha enfatizado repetidamente, los datos son cr\u00edticos para hacer data \nmining. Por consiguiente, se necesit an sistemas de bases de datos para \nmanejar los datos a los que aplicar dat a mining eficazmente. Estos sistemas \npodr\u00edan ser sistemas de data warehous e o sistemas de bases de datos.  \n1.2.1. Arquitectura, Modelado, Dise\u00f1o, y Aspectos de la \nAdministraci\u00f3n \nLas t\u00e9cnicas de data mining exist en desde hace alg\u00fan tiempo. \u00bfPor qu\u00e9 \nentonces data mining se ha hecho tan popular ahora? La principal raz\u00f3n es que \nahora con los sistemas de bases de datos  se pueden representar, almacenar y \nrecuperar los datos, y reforzar caracter\u00eds ticas como la integridad y seguridad.  \n Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 15 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ahora que se tienen los datos guarda dos en las bases de datos y quiz\u00e1s \nnormalizados y estructurados, \u00bfC\u00f3mo se puede hacer data mining? Un \nenfoque es reforzar un SGBD con una he rramienta de data mining. Se puede \ncomprar un SGBD comercial y una herra mienta de data mining comercial que \ntenga construidas las interfaces par a el SGBD y se puede aplicar la \nherramienta a los datos administrados por el SGBD. A pesar de que este \nenfoque tiene ventajas y prom ueve las arquitecturas abiertas, hay algunos \ninconvenientes. Podr\u00eda haber algunos problemas de rendimiento cuando se usa \nun SGBD de prop\u00f3sito general para data mining. \nEl otro enfoque es una integraci\u00f3n fuer te del SGBD con las herramientas de \ndata mining. El n\u00facleo de la base de datos tiene las herramientas de data mining incorporadas dentro de \u00e9l. Se puede decir que este tipo de SGBD es un \nMining SGBD (SGBD de data mining). Seg \u00fan esto las diferentes funciones del \nSGBD como el procesamiento de consul tas y la gesti\u00f3n del almacenamiento \nson influenciadas por las t\u00e9 cnicas de data mining. Por ejemplo, los algoritmos \nde optimizaci\u00f3n pueden ser modificados po r las t\u00e9cnicas de data mining. Se ha \ninvestigado mucho sobre la integraci \u00f3n de data mining y el n\u00facleo del SGBD \n(v\u00e9ase  [TSUR98]). \nMining SGBD tambi\u00e9n significar\u00eda la elim inaci\u00f3n de funciones innecesarias de \nun SGBD y el protagonismo de las caract er\u00edsticas clave. Por ejemplo, el \nprocesamiento de transacciones es una funci\u00f3n soportada por la mayor\u00eda de \nlos SGBD comerciales. Sin embargo, data mining normalment e no se dirige a \nlos datos transaccionales sino a los dat os de apoyo a la toma de decisiones. \nEstos datos no pueden ser datos q ue se actualicen a menudo por \ntransacciones. As\u00ed que, podr\u00edan eliminar se funciones como la gesti\u00f3n de \ntransacciones en un Mining SGBD, y se  podr\u00eda dar m\u00e1s importancia a las \ncaracter\u00edsticas adicionales que proporcion en integridad y calidad a los datos.  \nEn el general, en el caso de un Mining SGBD, la agregaci\u00f3n de  una \nherramienta de data mining influir\u00e1 sobre las diferentes funciones del SGBD \ncomo: el procesamiento de consultas, la  gesti\u00f3n del almacenamiento, la gesti\u00f3n \nde transacciones, la gesti\u00f3n de metadata (di ccionario de datos), la gesti\u00f3n de la \nseguridad y de la integridad.  \nEl tipo de modelado de los datos us ado puede tener alg\u00fan impacto en data \nmining. Muchos de los datos que ser\u00e1n utilizados se guardan en bases de datos relacionales. Sin embargo, ac tualmente cada vez m\u00e1s se guardan los \ndatos en bases de datos no relacionales tales como ba ses de datos orientadas \na objetos, bases de datos objeto-relaciona les y bases de datos multimedia. Hay \npoca informaci\u00f3n sobre data minig en ba ses de datos orientadas a objetos, \naunque si hay algunos trabaj os sobre data mining en  las bases de datos \nmultimedia. En las bases de datos orient adas a objetos prim ero se extraen las \nrelaciones entre los objetos y se guardan en una base de datos relacional, y \ndespu\u00e9s las herramientas de data mining se aplican a la base de datos relacional. \nEl dise\u00f1o de la base de datos juega un papel fundamental en la aplicaci\u00f3n de \ndata mining. Por ejemplo, en el caso de data warehousing,  se han propuesto \ndiferentes enfoques en el modelo y s ubsiguiente dise\u00f1o del almac\u00e9n. \u00c9stos Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 16 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda incluyen modelos multidimensionales de datos y modelos del procesamiento \nanal\u00edtico en l\u00ednea. Se han propuesto va rios esquemas como el esquema en \nestrella para el almacenamiento de los datos. Como se ha mencionado, la \norganizaci\u00f3n eficaz de los datos es cr\u00edti ca para data mining. Por consiguiente \ntambi\u00e9n, tales modelos y esquemas  son importantes para data mining \nLa administraci\u00f3n de las bases de datos tambi\u00e9n resulta influida por la \nrealizaci\u00f3n de data mining. Si se int egra data mining un SGBD, aparecen las \nsiguientes cuestiones \u00bfCon q u\u00e9 frecuencia ser\u00e1 aplica do data mining a la base \nde datos? \u00bfPuede ser usado data mining pa ra analizar la auditoria de datos? \n\u00bfComo influir\u00e1 en data mining la act ualizaci\u00f3n frecuente de los datos? \u00c9stas \ninteresantes preguntas tendr\u00e1n res puestas cuando se obtenga m\u00e1s informaci\u00f3n \nsobre la integraci\u00f3n de data mining con las funciones del SGBD. \n1.2.2. Data mining y Funciones de Bases de datos \nEn el caso de integraci\u00f3n fuerte entre  el SGBD y data mi ning hay un fuerte \nimpacto sobre las diferentes funci ones del sistema de bases de datos. Por \nejemplo, en el procesamiento de consul tas. Se han realizado trabajos para \nexaminar lenguajes de consultas como SQL y determinar si se necesitan \nextensiones para soportar data mining (v \u00e9ase por ejemplo [ACM96a]). Si hay \nestructuras adicionales y consultas que son complejas, entonces el optimizador de consultas tiene que ser adaptado para manejar esos casos. Estrechamente \nrelacionado con la optimizaci\u00f3n de cons ultas esta la eficiencia de las \nestructuras de almacenamiento, \u00edndices, y m\u00e9todos de acceso. Pueden ser necesarios mecanismos especiales  para apoyar data mining  en el \nprocesamiento de consultas. \nEn el caso de gesti\u00f3n de transacciones, la realizaci\u00f3n de data mining puede \ntener poco impacto, puesto que data mining se hace normalmente en los datos \nde apoyo a la toma de decisiones y no  en los datos transaccionales. Sin \nembargo hay casos d\u00f3nde se analizan los datos transaccionales para \nanomal\u00edas como en los casos de tarjetas de cr\u00e9dito y de tarjetas de tel\u00e9fono. A \nveces las compa\u00f1\u00edas de tarjetas de cr\u00e9d ito o de tel\u00e9fono han notificado sobre \nusos an\u00f3malos de tarjetas de cr\u00e9dito o de tel\u00e9fono. Esto normalmente se hace \nanalizando los datos transaccionales. Tambi \u00e9n se podr\u00eda aplicar data mining a \nestos datos. \nEn el caso de metadata, se podr\u00eda aplic ar data mining a me tadata para extraer \nla informaci\u00f3n \u00fatil en casos d\u00f3nde lo s datos no sean analizables. \u00c9sta puede \nser la situaci\u00f3n para datos no estr ucturados cuyo metadata deba ser \nestructurado. Por otro lado, los met adata podr\u00edan ser un recurso muy \u00fatil para \nuna herramienta de data mining. Metadata podr\u00eda dar  informaci\u00f3n adicional \npara ayudar con el proceso de data mining. \nLa seguridad, integridad, ca lidad del datos, y toleranc ia a fallos son influidas \npor data mining. En el caso de s eguridad, data mining podr\u00eda suponer una \namenaza importante para la seguridad y privacidad.  \n Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 17 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por otro lado data mining pueden usarse  para descubrir las intrusiones as\u00ed \ncomo para analizar la auditoria de datos. En el caso de auditoria, la cantidad de \ndatos sobre los que se apl ica  data mining es grand e. Se pueden aplicar las \nherramientas de data mining a los datos para descubrir los modelos anormales. \nPor ejemplo, si un empleado hace un exce sivo n\u00famero de viajes a un pa\u00eds \ndeterminado y este hecho es conocid o, proponiendo al gunas preguntas. La \nsiguiente pregunta a realizar es si el empleado tiene asociaciones con ciertas \npersonas de ese pa\u00eds. Si la  respuesta es positiva, entonces la conducta del \nempleado se marca.  \nComo ya se ha mencionado data mining tiene muchas aplicaciones en el \ndescubrimiento de la intrusi\u00f3n y analiz ando amenazas a las bases de datos. Se \npuede usar data mining para descubrir modelos de intrusiones y amenazas. \n\u00c9sta es un \u00e1rea emergente y se llama Informaci\u00f3n de Confianza. No s\u00f3lo es importante tener datos de calidad, tambi \u00e9n es importante recuperarse de fallos \nmaliciosos o de otro tipo, y proteger los datos de amenaz as o intrusiones. \nAunque la investigaci\u00f3n en esta \u00e1r ea simplemente est\u00e1 empezando, se \nesperan grandes progresos. \nEn el caso de calidad e in tegridad de los datos, se podr\u00edan aplicar las t\u00e9cnicas \nde data mining para descubrir datos malo s y mejorar la calidad de los datos. \nData mining tambi\u00e9n pueden usarse par a analizar la seguridad de los datos \npara varios sistemas como sistemas de control de circulaci\u00f3n a\u00e9rea, sistemas \nnuclear, y sistemas de armamento.  \n1.2.3. DATA WAREHOUSE \nUn data warehouse es un  tipo especial de base de dat os. Al parecer, el \nt\u00e9rmino se origin\u00f3 a finales de lo s ochenta [DEVL88], [INMO88], aunque el \nconcepto es m\u00e1s antiguo. La referenc ia [INMO93] define un data warehouse \ncomo \"un almac\u00e9n de datos orientado a un tema, integrado, no vol\u00e1til y variante \nen el tiempo, que soporta decisiones de administraci\u00f3n\" (donde el t\u00e9rmino no vol\u00e1til significa que una vez que los datos han sido insertados, no pueden ser \ncambiados, aunque s\u00ed pueden ser borrados). Los data warehouses surgieron por dos razones: primero, la necesida d de proporcionar una  fuente \u00fanica de \ndatos limpia y consistente para prop\u00f3s itos de apoyo para la toma de \ndecisiones; segundo, la necesidad de ha cerlo sin afectar a los sistemas \noperacionales. \nPor definici\u00f3n, las cargas de trabajo del data warehouse est\u00e1n destinadas para \nel apoyo a la toma de decisiones y por lo tanto, tienen consultas intensivas (con \nactividades ocasionales de inserci\u00f3n por lotes); asimismo, los propios data warehouses tienden a ser bastante grandes  (a menudo mayores que 500GB y \ncon una tasa de crecimiento de ha sta el 50 por ciento anual). Por \nconsecuencia, es dif\u00edcil -aunque no im posible- perfeccionar el rendimiento. \nTambi\u00e9n puede ser un problema la esca labilidad. Contribuy en a ese problema \n(a) los errores de dise\u00f1o de la base de datos, (b) el uso ineficiente de los \noperadores relacionales, (e) la debilidad  en la implementac i\u00f3n del modelo \nrelacional del DBMS, (d) la fa lta de escalabili dad del propio DBMS y (e) los \nerrores de dise\u00f1o arquitect\u00f3nico que li mitan la capacidad e imposibilitan la \nescalabilidad de la plataforma. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 18 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n\u2022 DATA MARTS \nLos usuarios a menudo realizaban amplias operaciones de informes y an\u00e1lisis \nde datos sobre un subconjunto rela tivamente peque\u00f1o de todo el data \nwarehouse. Asimismo, era muy probable qu e los usuarios repitieran las mismas \noperaciones sobre el mismo subconj unto de datos cada vez que era \nactualizado. Adem\u00e1s, algunas de esas actividades -por ej emplo, an\u00e1lisis de \npron\u00f3sticos, simulaci\u00f3n, modelado de dat os de negocios del tipo \"qu\u00e9 pasar\u00eda \nsi...\"- involucraban la creaci\u00f3n de nuevos esquemas y datos con \nactualizaciones posteriores a esos nuevos datos. \nLa ejecuci\u00f3n repetida de tale s operaciones sobre el mismo subconjunto de todo \nel almac\u00e9n no era muy eficiente; por lo tanto, pareci\u00f3 buena idea construir \nalg\u00fan tipo de \"almac\u00e9n\" lim itado de prop\u00f3sito general q ue estuviera hecho a la \nmedida de ese prop\u00f3sito. Adem\u00e1s, en al gunos casos ser\u00eda posible extraer y \npreparar los datos requeridos  directamente a partir de las fuentes locales, lo \nque proporcionaba un acceso m\u00e1s r\u00e1pido a los datos que si tuvieran que ser \nsincronizados con los dem\u00e1s datos ca rgados en todo el data warehouse. \nDichas consideraciones condujeron al concepto de data marts. \nDe hecho, hay alguna controversia s obre la definici\u00f3n precisa del t\u00e9rmino data \nmart. Se puede definir como \"un almac\u00e9n de  datos especializado, orientado a \nun tema, integrado, vol\u00e1til y variante en el tiempo para apoyar un subconjunto espec\u00edfico de decisiones de administraci \u00f3n\". La principal diferencia entre un \ndata mart y un data warehouse es que el data mart es especializado y vol\u00e1til. \nEspecializado quiere decir que contiene datos par a dar apoyo (solamente) a un \n\u00e1rea espec\u00edfica de an\u00e1lisis de negocios; por vol\u00e1til se entiende que los usuarios \npueden actualizar los datos e incluso, posiblemente, crear nuevos datos (es \ndecir, nuevas tablas) para alg\u00fan prop\u00f3sito. \nHay tres enfoques principales par a la creaci\u00f3n de un data mart: \n\u2022 Los datos pueden ser simplemente ex tra\u00eddos del data warehouse; se \nsigue un enfoque de \"divide y vencer\u00e1s\" sobre la carga de trabajo \ngeneral de apoyo para la toma de decis iones, a fin de lograr un mejor \nrendimiento y escalabilidad. Por lo  general, los datos extra\u00eddos son \ncargados en una base de datos que tiene un esquema f\u00edsico que se parece mucho al subconjunto aplicab le del data warehouse; sin \nembargo, puede ser simplificado de alguna manera gracias a la \nnaturaleza especializada del data mart. \n\u2022 A pesar del hecho de que el data warehouse pretende proporcionar un \n\"punto de control \u00fanico\", un data mart puede ser creado en forma \nindependiente (es decir, no por medio de la extracci\u00f3n a partir del data \nwarehouse). Dicho enfoque puede se r adecuado si el data warehouse \nes inaccesible por alguna causa: ra zones financieras, operacionales o \nincluso pol\u00edticas (o puede ser que ni si quiera exista todav\u00eda el data \nwarehouse). Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 19 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Algunas instalaciones han segui do un enfoque de \"primero el data \nmart\", donde los data marts son creados conforme van siendo \nnecesarios y el data warehouse general  es creado, finalmente, como \nuna consolidaci\u00f3n de los diversos data marts. \nLos \u00faltimos dos enfoques sufren posible s problemas de desacople sem\u00e1ntico. \nLos data marts independientes son par ticularmente susceptibles a tales \nproblemas, debido a que no hay forma obvia de verificar los desacoples sem\u00e1nticos cuando las bases de datos son dise\u00f1adas en forma independiente. \nPor lo general, la consolidaci\u00f3n de data marts en data wa rehouses falla, a \nmenos que (a) se construya primero un esquema l\u00f3gico \u00fanico para el data \nwarehouse y (b) los esquemas para los data marts individuales se deriven \ndespu\u00e9s a partir del esquema del data warehouse.  \nUn aspecto importante en el  dise\u00f1o de data marts: es la granularidad de la \nbase de datos. Donde granularidad se refiere al nivel m\u00e1s bajo de agregaci\u00f3n \nde datos que se mantendr\u00e1 en la base de da tos. Ahora bien, la mayor\u00eda de las \naplicaciones de apoyo para la toma de dec isiones requerir\u00e1n tarde o temprano \nacceso a datos detallados y por lo tanto,  la decisi\u00f3n ser\u00e1 f\u00e1cil para el data \nwarehouse. Para un data mart puede ser m\u00e1s dif\u00edcil. La extracci\u00f3n de grandes \ncantidades de datos detallados del data wa rehouse, y su almacenamiento en el \ndata mart, puede ser muy ineficiente si es e nivel de detalle no se necesita con \nmucha frecuencia. Por otro lado, en algunas ocasiones es dif\u00edcil establecer definitivamente cu\u00e1l es el nivel m\u00e1 s bajo de agregaci\u00f3n que en realidad se \nnecesita. En dichos casos, los datos detallados pueden ser accedidos \ndirectamente desde el data warehouse cuando se necesiten, manteniendo en \nel data mart los datos que de al guna manera ya fueron agregados.  \n\u2022 APLICACIONES DE LOS DATA WAREHOUSE \nLa explotaci\u00f3n del Data Warehouse pu ede realizarse mediante diversas \nt\u00e9cnicas: \n\u2022 Query & Reporting \n\u2022 On-line analytical processing (OLAP) \n\u2022 Executive Information System (EIS) \n\u2022 Decision Support Systems (DSS) \n\u2022 Visualizaci\u00f3n de la informaci\u00f3n \n\u2022 Data Mining \u00f3 Mi ner\u00eda de Datos, etc.  \nSe llaman sistemas OLAP (On Line Analyt ical Processing) a aquellos sistemas \nque deben: \n\u2022 Soportar requerimientos complejos de an\u00e1lisis \n\u2022 Analizar datos desde diferentes perspectivas Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 20 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Soportar an\u00e1lisis complejos contra un volumen ingente de datos \nLa funcionalidad de los sistemas OLAP se caracteriza por ser un an\u00e1lisis \nmultidimensional de datos mediante navega ci\u00f3n del usuario por los mismos de \nmodo asistido. \nExisten dos arquitecturas diferent es para los sistemas OLAP: OLAP \nmultidimensional (MD-OLAP) y OLAP relacionales (ROLAP). \nLa arquitectura MD-OLAP usa bases de datos multidimensionales, la \narquitectura ROLAP implanta OLAP s obre bases de datos relacionales \nLa arquitectura MD-OLAP requiere unos c\u00e1 lculos intensivos de compilaci\u00f3n. \nLee de datos precompilados, y tie ne capacidades limitadas de crear \nagregaciones din\u00e1micamente o de hallar ra tios que no se hayan precalculado y \nalmacenado previamente. \nLa arquitectura ROLAP, accede a los datos almacenados en un Data \nWarehouse para proporcionar los an\u00e1lisis OLAP. La premisa de los sistemas \nROLAP es que las capacidades OLAP se s oportan mejor contra las bases de \ndatos relacionales. \nLos usuarios finales ejecutan sus an\u00e1lisis multidimensionales a trav\u00e9s del motor \nROLAP, que transforma din\u00e1micamente su s consultas a consultas SQL. Se \nejecutan estas consultas SQL en las bases de datos relacionales, y sus resultados se relacionan mediant e tablas cruzadas y conjuntos \nmultidimensionales para devolver los resu ltados a los usuario s. ROLAP es una \narquitectura flexible y general, que crece para dar soporte a amplios \nrequerimientos OLAP. El MOLAP es una soluci\u00f3n particular, adecuada para \nsoluciones departamentales con unos vol\u00famenes de informaci\u00f3n y n\u00famero de \ndimensiones m\u00e1s modestos. \nUna cuesti\u00f3n t\u00edpica de un sistema OLAP o DSS podr\u00eda ser: \u201c\u00bfCompraron m\u00e1s \nmonovol\u00famenes en 1998 los habitantes de l norte de Espa\u00f1a, o los del sur?\u201d \nSin embargo, un sistema data mining en este escenario podr\u00eda ser interrogado \nas\u00ed:  \n\u201cQuiero un modelo que identifique las caracter\u00edsticas predictivas m\u00e1s \nimportantes de las personas que compran monovolumenes...\u201d \n\u2022 QUERY & REPORTING \nLas consultas o informes libres trabajan t anto sobre el detalle como sobre las \nagregaciones de la informaci\u00f3n. \nRealizar este tipo de explotaci\u00f3n en un almac\u00e9n de datos supone una \noptimizaci\u00f3n del tradiciona l entorno de informes (repor ting), dado que el Data \nWarehouse mantiene una estructura y una tecnolog\u00eda mucho m\u00e1s apropiada \npara este tipo de solicitudes. \nLos sistemas de \"Query & Reporting\", no basados en almacenes de datos se \ncaracterizan por la complejidad de las consultas, los alt\u00edsimos tiempos de Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 21 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda respuesta y la interferencia con otros procesos inform\u00e1ticos que compartan su \nentorno. \n1.2.4. DATA WAREHOUSE Y DATA MINING \nData warehouse  almacena los datos de las bases de datos heterog\u00e9neas para \nque los usuarios consulten s\u00f3lo un \u00fani co aspecto. Las respuestas que un \nusuario consigue a una consulta dependen de los vol\u00famenes del data \nwarehouse. El data warehous e  en general no intenta extraer la informaci\u00f3n de \nlos datos almacenados. Data warehouse  estructura y organiza los datos para \nsuportar funciones de administraci\u00f3n, dat a mining intenta extraer la informaci\u00f3n \n\u00fatil, as\u00ed como predecir las tendencias de los datos. La Figur a 3 10 ilustra la \nrelaci\u00f3n entre el data warehouse  y data mining. Observe que no es necesario \nconstruir un data warehouse para hacer data mining, ya que tambi\u00e9n puede \naplicarse data mining a las bases de da tos. Sin embargo, un data warehouse  \nestructura los datos de tal manera que fa cilita data mining, por lo que en \nmuchos casos es muy deseable tener un almac\u00e9n del datos para llevar a cabo \ndata mining.. \n\u00bfD\u00f3nde acaba data warehouse y donde em pieza data mining? \u00bfHay una \ndiferencia clara entre data warehouse y data mining? La respues ta es subjetiva. \nHay ciertas preguntas que los data war ehouse pueden contes tar. Adem\u00e1s, los \ndata warehouse disponen de capacida des para el apoyo a la toma de \ndecisiones. Algunos data warehouse llev an a cabo predicciones y tendencias. \nEn este caso los data warehouse lle van a cabo algunas de las funciones de \ndata mining. En el general, en el caso de un data warehouse la respuesta est\u00e1 en la base de datos. El data warehouse tiene que disponer de optimizaci\u00f3n de \nconsultas y t\u00e9cnicas de acceso par a obtener respuestas. Por ejemplo, \nconsidere preguntas como \u00bf\"Cu\u00e1ntos aut om\u00f3viles rojos compraron los m\u00e9dicos \nen 1990 en Nueva York \"? La respuesta est\u00e1 en la base de datos. Sin embargo, para una pregunta como \"  \u00bfC u\u00e1ntos autom\u00f3viles rojos comprar\u00e1n \nlos m\u00e9dicos en 2005 en Nueva York \"? la respuesta no puede estar en la base de datos. Bas\u00e1ndose en los patrones de comp ra de los m\u00e9dicos en Nueva York \ny sus proyecciones del sueldo, se podr\u00eda predecir la respuesta a esta pregunta. \nEsencialmente, un warehouse organiza los datos eficazmente para realizar \ndata mining sobre ellos. La pregunta es entonces \u00bfEs imprescindible tener un \nwarehouse para hacer data mining? La re spuesta es que es muy interesante \ntener un warehouse, pero esto no signifi ca que sea impre scindible. Podr\u00eda \nusarse un buen SGBD para gestionar una base de datos eficazmente. \nTambi\u00e9n, a menudo con un warehouse no se tienen datos transaccionales. Por lo tanto, los datos no pueden ser actuales, y los resultados obtenidos desde data mining tampoco lo ser\u00e1n. Si se necesita la informaci\u00f3n actualizada, \nentonces se podr\u00eda hacer data mining sobre una base de datos administrada por un SGBD que tambi\u00e9n tenga cara cter\u00edsticas de procesamiento de \ntransacciones. Hacer data mining sobre datos que se actualizan a menudo es \nun desaf\u00edo. T\u00edpicamente data mining se ha usado sobre los datos de apoyo a la \ntoma de decisiones. Por consiguiente hay  varios problemas que necesitan ser \ninvestigados extensamente, antes de que se pueda llevar a cabo lo que se \nconoce como data mining en tiempo real. De momento al menos, es cr\u00edtico Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 22 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda disponer de un buen data warehouse para llevar a cabo un buen data mining \npara funciones de apoyo a la toma de decisiones. Observe que tambi\u00e9n se \npodr\u00eda tener una herramienta integrada para llevar a cabo las funciones de data \nwarehouse y data mining. Una herramienta de este tipo ser\u00e1 conocida como \ndata warehouse miner. \n \n1.3.  Herramientas Comerciales de An\u00e1lisis de \nDatos \nKnowledgeSeeker  de Angoss Softwa re International, Toronto, Canada \nPuntos Clave: \n\u2022 Herramienta interactiva de clasificaci\u00f3n. \n\u2022 Basada en los algoritmos de \u00e1rbol es de decisi\u00f3n CHAID y XAID. \n\u2022 Se ejecuta sobre plataformas Windows y UNIX \nVentajas: \n\u2022 Representaci\u00f3n flexible de \u00e1rboles de decisi\u00f3n. \n\u2022 Provee caracter\u00edsticas para permitir la  identificaci\u00f3n de la relevancia de \nlos resultados en los negocios. \n\u2022 El API permite usar los resultados del an\u00e1lisis en aplicaciones \npersonalizadas. \nAspectos a tener en cuenta: \n\u2022 Solo soporta \u00e1rboles de decisi\u00f3n \n\u2022 Poco soporte para la transformaci\u00f3n de datos. \n\u2022 El soporte para predicci\u00f3n se limit a a la exportaci\u00f3n de las reglas \ngeneradas. \nCuando usarla: \n\u2022 Si se necesita una herramient a que permita adelantar una visi\u00f3n \ninstant\u00e1nea general de sus datos. \n\u2022 Si necesita una herramienta interactiva para explorar sus datos. \n\u2022 No est\u00e1 indicada si se necesit a una herramienta que soporte predicci\u00f3n \ndesde dentro de sus datos. \n Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 23 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda DataCruncher de DataMind, San Mateo, CA, USA \nPuntos Clave: \n\u2022 Herramienta de Data Mining para clasificaci\u00f3n y clustering \n\u2022 Basada en Tecnolog\u00eda de agentes  de redes (ANT Agent Network \nTechnology) \n\u2022 La aplicaci\u00f3n servidor se ejec uta sobre UNIX y Windows NT; la \naplicaci\u00f3n cliente en todas las plataformas Windows. \nVentajas: \n\u2022 F\u00e1cil de usar, ya que los modelos necesitan pocas adaptaciones. \n\u2022 Agent Network Technology puede ser utilizada para clasificaci\u00f3n, \npredicci\u00f3n y clustering no supervisado. \n\u2022 Resultados vers\u00e1tiles, que permit en una minuciosa valoraci\u00f3n de los \nmodelos y de sus resultados \nAspectos a tener en cuenta: \n\u2022 Se necesita familiarizarse con la tecnolog\u00eda para comprender los \nresultados. \n\u2022 Est\u00e1 basada en una t\u00e9cnica propietaria \n\u2022 Tiene soporte limitado para la transformaci\u00f3n de datos. \nCuando usarla: \n\u2022 Si se necesita una herramienta client e-servidor con una interface f\u00e1cil de \nusar. \n\u2022 Si se necesita valorar para cada caso  la bondad  de la predicci\u00f3n de los \nmodelos. \n\u2022 Si quiere invertir alg\u00fan esfuerzo en hacer un completo uso del an\u00e1lisis \nde resultados. \n \nIntelligent Miner de IBM, Armonk, NY, USA \nPuntos Clave: \n\u2022 Soporta m\u00faltiples operaciones de data minino en un entrono cliente-\nservidor Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 24 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Utiliza redes de neuronas, \u00e1rboles de inducci\u00f3n y varias t\u00e9cnicas \nestad\u00edsticas. \n\u2022 Trabaja sobre clientes Windows, OS /2 y X-Windows, y servidores AIX \n(incluyendoSP2), OS/400 y OS/390. \nVentajas: \n\u2022 Buen soporte para an\u00e1lisis de asociaciones y clustering (incluyendo \nvisualizaci\u00f3n de clustering), adem\u00e1s  de clasificaci\u00f3n y predicci\u00f3n. \n\u2022 Optimizada para data minino en grandes bases de datos(del orden de \ngigabytes) ya que se aprovecha de la  plataforma de procesamiento \nparalelo PS2 de IBM. \n\u2022 Tiene un entorno de trabajo int egrado con caracter\u00edsticas muy \ninteresantes tanto para usuarios expertos como no especialistas. \nAspectos a tener en cuenta: \n\u2022 Algunos problemas que ten\u00eda han si do resueltos con la nueva interface \nque ha sido desarrollada completamente en Java. \n\u2022 Solo trabaja sobre plataf ormas IBM, y el acceso a los datos se limita a \nlas bases de datos DB2 y a ficheros planos. \n\u2022 Inicialmente la mayor\u00eda de los proy ectos requerir\u00e1n entr adas importantes \ndesde los servicios de soporte y consultor\u00eda de IBM \nCuando usarla: \n\u2022 Deber\u00eda ir a una tienda de IBM para obs ervar la funcionalidad del data \nmining integrado en su entorno de soporte a las decisiones \n\u2022 Para grandes proyectos de data mi ning, en particular cuando los datos \nest\u00e1n contenidos en DB2. \n\u2022 Si se desan utilizar varias operaciones de data mining, tales como \nclasificaci\u00f3n, clustering y an\u00e1lisis de asociaciones. \n\u2022 Para realizar an\u00e1lisis de cesta de la compra con varios gigabytes de \ndatos. \n\u2022 Si interesa utilizar los servicios de consultor\u00eda de IBM.  \n \nClamentine de Integral Solutions, Basingstoks, UK \nPuntos Clave: Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 25 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Herramienta con un entrono de trabajo que soporta todo el proceso de \ndata mining \n\u2022 Ofrece \u00e1rboles de decisi\u00f3n, redes de neuronas, generaci\u00f3n de reglas de \nasociaci\u00f3n y caracter\u00edsticas de visualizaci\u00f3n. \n\u2022 Se ejecuta sobre VM S, UNIX o Windows NT. \nVentajas: \n\u2022 Interface gr\u00e1fica intuitiva para programaci\u00f3n visual. \n\u2022 Las t\u00e9cnicas de data mining pueden complementarse combin\u00e1ndose \nentre si. \n\u2022 Visi\u00f3n interactiva de las relaciones ent re las variables a trav\u00e9s de grafos \nde red. \nAspectos a tener en cuenta: \n\u2022 No soporta Windows nativo. \n\u2022 Es necesario familiarizarse con la herramienta para conseguir una \n\u00f3ptima utilizaci\u00f3n de su s funcionalidades. \n\u2022 No est\u00e1 optimizada para arquitecturas en paralelo. \nCuando usarla: \n\u2022 Si se necesita una herramienta que c ubra por completo el rango de los \nprocesos de data mining. \n\u2022 Si se desean combinar herramient as y modelos para construir los \nprocesos de data mining que exijan tales requisitos. \n\u2022 Si se desea desarrollar el modelo en C. \n\u2022 Si se necesitan grandes capacidades anal\u00edticas y de gesti\u00f3n de datos sin \nrequerir un extenso an\u00e1lisis de datos  ni experiencia en tecnolog\u00edas \ninform\u00e1ticas. \n \nAlice de Isoft SA, Gif sur Yvette, Francia. \nPuntos Clave: \n\u2022 Herramienta de escritorio para data minino interactivo. \n\u2022 Se basa en tecnolog\u00eda de \u00e1rboles de decisi\u00f3n. \n\u2022 Se ejecuta sobre plataformas Windows. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 26 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ventajas: \n\u2022 La representaci\u00f3n altamente intera ctiva permite guiar el an\u00e1lisis. \n\u2022 La opci\u00f3n de generar gr\u00e1ficos provee una visi\u00f3n general de los datos en \ntodas las etapas del proceso de Data Mining. \n\u2022 Se trata de una herramienta econ\u00f3m ica valida para usuarios que \ncomienzan a realizar data mining. \nAspectos a tener en cuenta: \n\u2022 No tiene opciones para desarrollar modelos. \n\u2022 Peque\u00f1o soporte para  transformaci\u00f3n de datos. \n\u2022 No genera conjuntos de reglas opt imizadas desde los \u00e1rboles de \ndecisi\u00f3n. \nCuando usarla: \n\u2022 Si se desea usar data mining para buscar patrones y relaciones en los \ndatos. \n\u2022 Si se quiere tener la posibilidad de dirigir el an\u00e1lisis interactivamente. \n\u2022 Si no se es un experto en data mining y se desea realizar el an\u00e1lisis. \n\u2022 Si se quiere entender los patr ones que se encuentran en la base de \ndatos y no se desea construir modelos predictivos. \n \nDecisi\u00f3n Series, de NeoVista Software Cupertino CA, USA. \nPuntos Clave: \n\u2022 Herramientas para m\u00faltiples oper aciones de data mining para el \ndesarrollo de modelos basados en servidores. \n\u2022 Proporciones algoritmos de redes de neuronas, \u00e1rboles y reglas de \ninducci\u00f3n, clustering y an\u00e1lisis de asociaciones.  \n\u2022 Trabaja sobre sistemas UNIX mono o multi-procesadores de HP y Sun. \nAccede s\u00f3lo a ficheros planos , aunque posiblemente las \u00faltimas \nversiones ya trabajaran contra bases de datos relacionales. \nVentajas: \n\u2022 Soporta un gran rango de operaci ones y algoritmos de data mining, la \nmayor\u00eda de los cuales han sido altam ente optimizados para obtener altos \nrendimientos. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 27 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Est\u00e1 optimizado para plataformas que trabajan en paralelo con grandes \nconjuntos de datos. \n\u2022 Ofrece una considerable flexibilidad  para construir modelos de alto \nrendimiento para aplicaciones de usuario final embebidas. \nAspectos a tener en cuenta: \n\u2022 Las herramientas de desarrollo gr\u00e1fico son bastante b\u00e1sicas. \n\u2022 Poco soporte para la exploraci\u00f3n de datos. \n\u2022 La mayor\u00eda de los clientes necesitaran un considerable soporte de \nconsultas para generar aplicaciones y ejecutarlas. Es necesario tener \nconocimientos de an\u00e1lisis de dat os y de utilizaci\u00f3n de UNIX para \ndesarrollar las aplicaciones. \nCuando usarla: \n\u2022 Si se desean construir aplicaciones  con alto rendimiento de modelos de \ndata mining embebidos que utilizan ent ornos con multiprocesadores. \n\u2022 Si se quiere tener un absoluto cont rol sobre todos los elementos de los \nprocesos de construcci\u00f3n de modelos. \n\u2022 Si se necesitan combinar operac iones y tecnicas de data mining \nalternativas en aplicaciones complejas. \n\u2022 Si se quiere trabajar con una so luci\u00f3n que puede comunicar una \naplicaci\u00f3n data minino para enlazar con sus necesidades. \n \nPilot Discovery Server de Pilo t Software, Cambridge MA, USA. \nPuntos Clave: \n\u2022 Herramienta para clasificaci\u00f3n y predicci\u00f3n. \n\u2022 Basada en la tecnolog\u00eda de \u00e1rboles de decisi\u00f3n CART. \n\u2022 Trabaja sobre UNIX y Windows NT \nVentajas: \n\u2022 Buena representaci\u00f3n del an\u00e1lisis de resultados \n\u2022 Es f\u00e1cil de usar y de entender. \n\u2022 Muy integrada con sistemas gestores de bases de datos relacionales. \nAspectos a tener en cuenta: Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 28 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Solamente indicada para clientes de los programas para soporte a la \ntoma de decisiones de Pilot. \n\u2022 Solamente cubre un especifico sect or del espectro del data mining. \n\u2022 S\u00f3lo trabaja con datos almacenados en  bases de datos relacionales. \nCuando usarla: \n\u2022 Si se desea optimizar las campa\u00f1as de marketing. \n\u2022 Si se necesita interpretar f\u00e1cilmente los resultados sin realizar un gran \nrefinamiento de los modelos. \n\u2022 Solo si se est\u00e1n utilizando los pr ogramas para soporte a la toma de \ndecisiones de Pilot. \n\u2022 No est\u00e1 indicada si se quieren resolver los problemas utilizando \ndiferentes t\u00e9cnicas. \n \nSAS Solution for Data Mining de SAS Institute, Cary, NC, USA \nPuntos Clave: \n\u2022 Un gran n\u00famero de herramientas de selecci\u00f3n, exploraci\u00f3n y an\u00e1lisis de \ndatos para entornos cliente-servidor. \n\u2022 Las opciones de data mining incl uyen: aplicaciones de redes de \nneuronas, de \u00e1rboles de decisi\u00f3n y herramientas de estad\u00edstica. \n\u2022 Aplicaciones portables para un gr an n\u00famero de entornos PC, UNIX y \nmainframes. \nVentajas: \n\u2022 SAS ofrece data warehouse y an\u00e1lisis de datos. \n\u2022 Conjuntos extensibles de herramient as de manipulaci\u00f3n y visualizaci\u00f3n \nde datos. \n\u2022 SAS tiene una gran experiencia en  herramientas estad\u00edsticas y de \nan\u00e1lisis de datos. \nAspectos a tener en cuenta: \n\u2022 La oferta para hacer data mining es una mezcolanza de todas las \nt\u00e9cnicas SAS existentes. \n\u2022 Integraci\u00f3n con la programaci\u00f3n en 4GL. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 29 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 No soporta el an\u00e1lisis de asociaciones.  \nCuando usarla: \n\u2022 Si ya se utiliza SAS para almacenar , administrar y analizar los datos. \n\u2022 Si se va a utilizar SAS para la construcci\u00f3n del data warehouse. \n\u2022 Si es necesaria una alta funcionalidad en la manipulaci\u00f3n de datos. \n\u2022 Si se es experto en estad\u00edstica y se quieren utilizar las funciones \nestad\u00edsticas de SAS. \n  \nMineSet, de Silicon Graphi cs, Mountain View, CA, USA \nPuntos Clave: \n\u2022 Paquete de herramientas para Data mining y visualizaci\u00f3n. \n\u2022 Proporciona algoritmos par a la generaci\u00f3n de reglas  para clasificaci\u00f3n y \nasociaciones. \n\u2022 Trabaja sobre plataformas SGI bajo IRIS. \nVentajas: \n\u2022 Ofrece herramientas de visualizaci\u00f3n para los datos y los modelos \ngenerados. \n\u2022 Suporta muchas operaciones de data mining. \n\u2022 El gestor de herramientas act\u00faa como un punto central de control y \npermite el acceso y transformaci\u00f3n de los datos. \nAspectos a considerar: \n\u2022 Requiere un servidor SGI. \n\u2022 La gran cantidad de opciones y par\u00e1 metros puede provocar confusi\u00f3n en \nusuarios noveles. \n\u2022 Las herramientas de visualizaci \u00f3n necesitan mucha preparaci\u00f3n y \npersonalizaci\u00f3n de los datos par a producir buenos resultados. \nCuando usarla: \n\u2022 Si se quieren detectar patrones por visualizaci\u00f3n. \n\u2022 Si se quieren construir aplicaciones que representen los resultados de \ndata mining a trav\u00e9s de visualizaci\u00f3n. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 30 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Si se dispone de equipos de Silicon Graphics \n\u2022 Esta indicada para VARs que quieran desarrollar soluciones \npersonalizadas de data mining usando MineSet. \n \nSPSS, de SPSS, Chicago IL, USA \nPuntos Clave: \n\u2022 Herramientas de escritorio para clasif icaci\u00f3n y predicci\u00f3n, clustering, y \nun gran rango de operaciones estad\u00edsticas. \n\u2022 Proporciona una herramienta de redes de neuronas adem\u00e1s de \nproductos de an\u00e1lisis  estad\u00edstico. \n\u2022 SPSS para Windows y Neural Connec tion son productos que trabajan en \nmodo monopuesto en plataformas Windows. \nVentajas: \n\u2022 Las funciones de an\u00e1lisis estad\u00edstic o complejo son accesibles a trav\u00e9s \nde una interface de usuario muy bien dise\u00f1ada.  \n\u2022 Neural Connection ofrece un amplio  rango de opciones y funciones a \ntrav\u00e9s un entorno de desarrollo muy f\u00e1cil de usar. \n\u2022 El lenguaje de scripts permite una gr an personalizaci\u00f3n del  entorno y el \ndesarrollo de aplicaciones estad\u00edsticas aisladas. \nAspectos a considerar: \n\u2022 Para analistas de datos y estad\u00edsticos, m\u00e1s que para usuarios finales. \n\u2022 SPSS CHAID carece de la funcionalidad  de otros productos de escritorio \nde \u00e1rboles de decisi\u00f3n. \n\u2022 Neural Connection es un producto aisl ado: la base de la integraci\u00f3n con \nSPSS es a trav\u00e9s de transferencia  de datos, que se limita a la \nimportaci\u00f3n de 32.000 registros. \nCuando usarla: \n\u2022 Si se necesita un an\u00e1lisis complejo  combinando estad\u00edstica con \u00e1rboles \nde decisi\u00f3n y redes de neuronas. \n\u2022 Si se disponen de grandes conocimientos  estad\u00edsticos y se quiere utilizar \ndata mining basado en IA. \n\u2022 Si se necesita verificaci\u00f3n estad\u00edstica de los resultados encontrados. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 31 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Si es preciso construir aplicac iones de an\u00e1lisis departamental para \nescritorio. \n\u2022 Si tiene un presupuesto ajustado. \n  \nSyllogic Data Mining Tool, de Sy llogic, Houten, The Netherlands \nPuntos Clave: \n\u2022 Herramienta con entorno de trabajo mu lti-estrat\u00e9gico con interface \nvisual. \n\u2022 Soporta an\u00e1lisis de \u00e1rboles de decis i\u00f3n, clasificaci\u00f3n k-vecino m\u00e1s \npr\u00f3ximo, y an\u00e1lisis de clusteri ng y asociaciones por k-means. \n\u2022 Trabaja sobre Windows NT y en es taciones UNIX con uno o varios \nprocesadores \nVentajas: \n\u2022 La interface visual permite a los usuarios construir proyectos de data \nmining enlazando objetos. \n\u2022 La versi\u00f3n est\u00e1 optimizada para entornos masivam ente paralelos y \nvalidos para grandes bases de datos. \n\u2022 La empresa tambi\u00e9n ofrece un gran n\u00famero de servicios de consultar\u00eda \nen las \u00e1reas de datawarehousing y data mining.  \nAspectos a considerar: \n\u2022 La interface y la presentaci\u00f3n de resultados necesita algunos \nrefinamientos para ser utilizada por usuarios finales. \n\u2022 DMT/MP no soportan el mism o rango de operaciones que DMT \nCuando usarla: \n\u2022 Si se necesita servicio de consultor\u00eda a la vez que se desarrolla el \nproyecto de data mining con un entorno de datawarehousing. \n\u2022 Si se necesita utilizar gran n\u00fam ero de operaciones de data mining. \n\u2022 Si se quiere utilizar una herramient a similar en el escritorio y en el \nentorno MP.  \n \nDarwin de Thinking Machines, Bedford MA, USA Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 32 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Puntos Clave: \n\u2022 Herramientas de desarrollo de data mining de tipo cliente-servidor para \nla construcci\u00f3n de modelos de clasificaci\u00f3n y predicci\u00f3n. \n\u2022 La construcci\u00f3n de modelos utiliz a algoritmos de redes de neuronas, \n\u00e1rboles de inducci\u00f3n y k-vecino m\u00e1s pr\u00f3ximo. \n\u2022 Trabaja sobre plataformas Sun de Solaris, AIX de IBM y SP2, con \nclientes Motif. Tambi\u00e9n existen ve rsiones cliente que trabajan sobre \nWindows. \nVentajas: \n\u2022 Ofrecen buena cobertura al proceso completo de descubrimiento del \nconocimiento. \n\u2022 Pone el \u00e9nfasis en el desarrollo  de modelos predictivos de alto \nrendimiento. \n\u2022 Proporciona escalabilidad par a soportar paralelizaci\u00f3n. \nAspectos a considerar: \n\u2022 Mejor para analistas de datos y desarrolladores de aplicaciones que \npara los usuarios de negocio. \n\u2022 Es preciso familiarizarse con las diferentes opciones de Darwin para \ncada tipo de modelo si se quiere obtener el mejor resultado de la \nherramienta. \n\u2022 No soporta an\u00e1lisis no supervisa do de clustering o de asociaciones. \nCuando usarla: \n\u2022 En la construcci\u00f3n de aplicaciones  de data mining para gesti\u00f3n de \nrelaciones entre clientes. \n\u2022 Si se necesita una herramienta que ponga mucho \u00e9nfasis en modelado \npor clasificaci\u00f3n y predictivos. \n\u2022 Si se dispone de una gran  comp leja base de datos que precise la \npotencia de una plataforma con multiprocesadores. \n\u2022 Si se necesita observar la creaci\u00f3n de los modelos de data mining, \nDarwin proporciona m\u00faltiples al goritmos y varias opciones de \nrefinamiento. \n\u2022 Si se quiere usar las herramientas de data mining para auxi liar la gesti\u00f3n \nde redes Thinking Machina ti ene objetivos muy expl\u00edc itos en este sector \ny ya colabora con Cabletron. Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 33 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 1.4.  Arquitectura Software para Data Mining \nAnteriormente se han discutido diferent es tecnolog\u00edas para data mining. Se \nnecesita el apoyo arquitect\u00f3nico para inte grar estas tecnolog\u00edas. La Figura 1.4 \nmuestra una pir\u00e1mide que presenta la estructura de c\u00f3mo las diferentes tecnolog\u00edas encajan entre si. Como se mu estra en esta figura, en el nivel m\u00e1s \nbajo se encuentra las comunicaciones y sistemas. A conti nuaci\u00f3n aparece el \nsoporte del middleware. Esto va seguido  por la gesti\u00f3n de la bases de datos y \nel data warehouse. Despu\u00e9s aparecen la s diferentes tecnolog\u00edas de data \nmining. Finalmente, se tienen los sistemas de apoyo a la toma de decisiones \nque usan los resultados de data mining y ayudan a que los usuarios tomen las \ndecisiones eficazmente. Estos usuarios pueden ser administradores, analistas, \nprogramadores, y cualquier otro usuar io del sistema de informaci\u00f3n.  \nCuando se construyen sistemas, las diferentes tecnolog\u00edas involucradas \npueden no encajar exactamente en la pir\u00e1 mide tal como se ha mostrado. Por \nejemplo, se podr\u00eda saltar la fase de data warehouse y se  podr\u00eda ir directamente \na la herramienta de data mining. Uno de los problemas importantes, en este \npunto, son las interfaces entre los diferent es sistemas. En la actualidad no se \ntiene bien definida cual quiera de las  interfaces no rmales excepto en el caso de \nalgunos de los lenguajes est\u00e1ndar de definici\u00f3n de in terfaz que surgen de los \ndiferentes grupos como el Object Management Group. Si n embargo, cuando \nestas tecnolog\u00edas vayan madurando, se  ir\u00e1n desarrollando los est\u00e1ndares para \nlas interfaces. \n \n \nFigura 1.4: Pir\u00e1mide para Data mining \nYa se ha estudiado c\u00f3mo las diferent es tecnolog\u00edas trabajan juntas. Por \nejemplo, una posibilidad es  la mostrada en la Figur a 1.5 donde se integran \nm\u00faltiples bases de datos a trav\u00e9s de alg\u00fan middleware y como consecuencia \nforman un data warehouse que se explora a continuaci\u00f3n. Los componentes de  \ndata mining tambi\u00e9n se integran en este  escenario para aplicar data mining a Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 34 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda las bases de datos directam ente. Algunos de estos probl emas se discutir\u00e1n en \nla secci\u00f3n de la arquitectura del sistema. \n \n \nFigura 1.5: Arquitectura de data  mining \nLa figura 1.6 ilustra una vista tridimensiona l de las tecnolog\u00edas de data mining. \nEn el centro se encuentra la tecnolog\u00ed a para la integraci\u00f3n. \u00c9sta es la \ntecnolog\u00eda del middleware tal como la ges ti\u00f3n distribuida orie ntada al objeto y \ntambi\u00e9n la tecnolog\u00eda web para la int egraci\u00f3n y acceso a trav\u00e9s de web.  \n \n \nFigura 1.6: Visi\u00f3n en tres dimensiones Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 35 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nEn una primera dimensi\u00f3n tenemos t odas las tecnolog\u00edas b\u00e1sicas de datos \ncomo multimedia, bases de datos relacionales y orientadas a objetos, y bases de datos distribuidas, heterog\u00e9neas y de he rencia. En la segunda dimensi\u00f3n \ntenemos las tecnolog\u00edas para realizar da ta mining. Aqu\u00ed se ha incluido el \nwarehousing as\u00ed como el aprendizaje autom \u00e1tico, tal como la programaci\u00f3n de \nla l\u00f3gica inductiva, y el razonamient o estad\u00edstico. La tercera dimensi\u00f3n \ncomprende tecnolog\u00edas como el proces amiento paralelo, la  visualizaci\u00f3n, \ngesti\u00f3n de metadatos (diccionario de dat os), y el acceso seguro que son \nimportantes para llevar a cabo data mining.  \n1.4.2. Arquitectura Funcional  \nA continuaci\u00f3n se describen los compo nentes funcionales de data mining. \nAnteriormente se discutieron los com ponentes funcionales  de un sistema de \ngesti\u00f3n de bases de datos. En adici\u00f3n, se mostro una arquitectura en la que la \nherramienta de data mining er a uno de los m\u00f3dulos de l SGBD. Un SGBD con \nestas caracter\u00edsticas ser\u00e1 un SGBD  Mining. Un SGBD Mining se puede \norganizar de varias maneras. Un enfoque alte rnativo se ilustra en Figura 4. En \neste enfoque se considera data mining como una extensi\u00f3n del procesador de \nconsultas. Es decir, podr\u00edan extender se los m\u00f3dulos del procesador de \nconsultas como el optimizador de cons ultas para ocuparse de data mining. Esto \nes una vista de alto nivel como se ilustra en la Figura 1.7. Observe que en este diagrama se ha omitido al gestor de la s transacciones, ya que data mining se \nusa principalmente en el proces amiento anal\u00edtico en l\u00ednea (OLTP). \n \nFigura 1.7: Data mining como parte del procesador de consultas \nLa pregunta es: \u00bfCu\u00e1les son los componentes de la herramienta de data \nmining? Como se ilustra en la Figur a 1.8, una herramienta de data mining \npodr\u00eda tener los siguientes componentes : un componente de aprendizaje de \nexperiencia que usa varios conjunt os de entrenamiento y aprende varias \nestrategias, un componente analizador de datos que analiza los datos en base \na lo que tiene que aprender, y un compo nente productor de resultados que \nrealiza la clasificaci\u00f3n, el  clustering, y otras tareas como las asociaciones. Hay Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 36 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda interacci\u00f3n entre los tres component es. Por ejemplo, el componente que \nproduce los resultados entrega los re sultados obtenidos al componente de \nentrenamiento para ver si este co mponente tiene que ser adaptado. El \ncomponente de entrenamiento da la in formaci\u00f3n al componente analizador de \ndatos. El componente de analizador de dat os da la informaci\u00f3n al componente \nproductor de los resultados. \n \nFigura 1.8: Las Funciones de data mining \nObserve que no se han incluido componente s tales como el preprocesador de \ndatos y el podador (refinador) de los resu ltados en los m\u00f3dulos  de data mining. \nEstos componentes tambi\u00e9n son necesario s para completar el proceso entero. \nEl preprocesador de datos formatea los datos. De alguna forma el data \nwarehouse puede hacer esta funci\u00f3n. El componente de poda o recorte de  \nresultados puede extraer s\u00f3lo la informaci\u00f3n \u00fatil. Esto  podr\u00eda llevarse a cabo \npor un sistema de apoyo a la toma de decisiones. Todos estos pasos se \nintegrar\u00e1n en el proceso de data mining. \n1.4.3. Arquitectura del Sistema \nAlgunas de las arquitecturas que se ha n discutido anteriormente as\u00ed como la \nobservada en la Figura 1.5 pueden cons iderarse como una arquitectura del \nsistema para data mining.  Una arquitectura del sistema consiste en \ncomponentes como los middleware y otro s componentes del sistema como el \nsistema de bases de datos y el sistem a de data warehouse para data mining. \nLos middleware que se ilustran en Figura 1.5 podr\u00ed an basarse en diferentes \ntecnolog\u00edas. Un sistema middleware mu y popular es el qu e se basa en una \narquitectura cliente-servidor. \nEn efecto, muchos de los sistemas de bases de datos se basan en la \narquitectura cliente-servidor. Middlewar e tambi\u00e9n incluye de facto est\u00e1ndares \ncomo el  Open DataBase Connectivity  Connectivity (ODBC) de Microsoft o \nsistemas distribuidos basados en objetos. \nEn [THUR97] se proporciona una discusi \u00f3n detallada de tecnolog\u00edas cliente-\nservidor. En particular se discute el par adigma de cliente-servidor as\u00ed como una \napreciaci\u00f3n global de ODBC  y los sistemas de gesti\u00f3 n distribuida de objetos \ncomo el Object Manegement Group\u2019s (O MG) Common Object Request Broquer Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 37 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Architecture (CORBA). Aqu\u00ed se discute data mining con respecto al paradigma \ndel cliente-servidor.  \nLa mayor\u00eda de los vendedores de sistem as de bases de datos han migrado a \nuna arquitectura llamada arquitectura de c liente-servidor. Con este enfoque, \nm\u00faltiples clientes acceden a los diferent es servidores de las bases de datos a \ntrav\u00e9s de alguna red. Una visi\u00f3n de al to nivel de la comunicaci\u00f3n cliente-\nservidor de se ilustra en la Figura 1.9. El  objetivo \u00faltimo es comunicar m\u00faltiples \nclientes vendedores con m\u00faltiples serv idores vendedores de una manera \ntransparente. \n \nFigura 1.9: La Arquitectura cliente-servidor de Basada en la Interoperabilidad \nEn orden a facilitar la comunicaci\u00f3n ent re m\u00faltiples clientes y servidores, se \nhan propuesto varios est\u00e1ndares. Un ejem plo es la Organizaci\u00f3n Internacional \nde Est\u00e1ndares (ISO), el est\u00e1ndar Remote  Database Access (RDA). Esta norma \nprovee una interfaz gen\u00e9rica para la comunicaci\u00f3n entre un cliente y un \nservidor. Microsoft ODBC tambi\u00e9n ha aumentado su popularidad para la \ncomunicaci\u00f3n de los clientes con los se rvidores. El CORBA de OMG mantiene \nlas especificaciones para las comunicaci ones cliente-servid or basadas en la \ntecnolog\u00eda orientada a objetos. Aqu\u00ed, una posibilidad es enc apsular las bases \nde datos servidoras como objetos y dist ribuir las peticiones  apropiadas de los \nclientes y acceder los serv idores a trav\u00e9s de un Obje ct Request Broker (ORB). \nOtros est\u00e1ndares incluyen el DRDA de IB M (Distribuited Relational Database \nAccess - el Acceso de la base de datos relacional Distribuida ) y el SQL Access \nGroup (ahora parte del Open Group); Call Level Interface la Interfaz de Nivel de \nLlamada  (CLI). Se han publicado varios lib ros sobre computaci\u00f3n cliente-\nservidor y administraci\u00f3n de datos. Dos buenas referencias son [ORFA94] y [ORFA96]. Tambi\u00e9n se estudian en detal le algunos de estos problemas en \n[THUR97]. \nUn sistema de middleware que est\u00e1 aum entando su popularidad para conectar \nsistemas heterog\u00e9neos es el CORBA de OMG. Como se declara en [OMG95], \nhay tres componentes principales en COR BA. Uno es el mo delo orientado a \nobjetos, el segundo es Object Request Broker el Corredor de Demanda de \nObjeto  (ORB) a trav\u00e9s del cual los clientes  y servidores se comunican entre s\u00ed, \ny el tercero es Interface Definition Language el Lenguaje de Definici\u00f3n de \nInterfaces  (IDL) qu\u00e9 espec\u00edfica las interfac es para la comunicaci\u00f3n cliente-\nservidor. La Figura 1.10 ilustra la comuni caci\u00f3n cliente-servidor a trav\u00e9s de Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 38 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ORB. Aqu\u00ed, los clientes y servidores est\u00e1n encapsulados como objetos. Los dos \nobjetos comunican entonces  entre s\u00ed. La comunicaci\u00f3n  se hace mediante ORB. \nAdem\u00e1s, las interfaces deben ajustarse a IDL. \n \nFigura 1.10: La interoperabilidad a trav\u00e9s del ORB \n1.4.4. El Data Mining en la Arquitectura del Sistema \nConsidere la arquitectura de la Figura 8.  En este ejemplo, la herramienta de \ndata mining podr\u00eda usarse como un serv idor, los sistemas de administraci\u00f3n de \nbases de datos podr\u00edan ser otro servidor , mientras el data warehouse ser\u00eda un \ntercer servidor. El cliente emite las peticiones al sistema de base de datos, al \nwarehouse, y al componente de data mining como se ilustra en la figura 1.11. \n \n \nFigura 1.11: Data mining basado en Cliente-Servidor \nTambi\u00e9n se podr\u00eda usar un ORB para data mi ning. En este caso  la herramienta \nde data mining se encapsula como un obj eto. El sistema de bases de datos y \nwarehouse tambi\u00e9n son objetos. Esto se ilustra en la Figura 1.12. El desaf\u00edo \naqu\u00ed es definir IDLs para varios objetos. \nObs\u00e9rvese que la tecnolog\u00eda cliente-serv idor no desarrolla algoritmos para la \nadministraci\u00f3n de datos, para warehousin g, o para la realizaci\u00f3n de data Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 39 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda mining. Esto significa que todav\u00eda se neces itan los algoritmos para realizar data \nmining, warehousing, y administraci\u00f3n de la base de datos. La tecnolog\u00eda \ncliente-servidor y, en particular, la te cnolog\u00eda de administraci\u00f3n distribuida de \nobjetos como CORBA, es la que facilita  la \u00ednteroperaci\u00f3n entre los diferentes \ncomponentes. Por ejemplo, el sistem a data mining, el sistema de base de \ndatos, y warehose comunican entre s\u00ed y con los clientes a trav\u00e9s del ORB. \n \nFigura 1.12: Data mining mediante ORB \nLa arquitectura a tres niveles se ha hecho muy popular (vea la discusi\u00f3n en \n[THUR971). En esta arquitectura, el c liente es un cliente ligero y realiza un \nprocesamiento m\u00ednimo, el servidor hac e las funciones de adm inistraci\u00f3n de la \nbase de datos, y el nivel intermedio lleva a cabo varias funciones de proceso de \nnegocio. En el caso de data mining, se podr\u00eda utilizar tambi\u00e9n una arquitectura \nde tres niveles donde la herramienta de data mining se pone en el nivel \nintermedio. La herramienta de data mi ning podr\u00eda desarrollarse como una \ncolecci\u00f3n de componentes. Estos component es podr\u00edan estar basados en la \ntecnolog\u00eda orientada al objeto. Desarro llando los m\u00f3dulos de data mining como \nuna colecci\u00f3n de componentes, se podr\u00edan desarrollar herramientas gen\u00e9ricas \ny entonces se podr\u00eda personalizarlas para las aplicaciones especializadas. \nOtra ventaja de desarrollar un sistema de data mining como una colecci\u00f3n de \ncomponentes es que se podr\u00edan co mprar los componentes a vendedores \ndiferentes y despu\u00e9s ensamblarlos par a formar un sistema. Adem\u00e1s, los \ncomponentes podr\u00edan ser reutilizados. Por ahora asumiremos que los m\u00f3dulos \nson el integrador de los dat os fuente, la herramienta de data mining, el podador \n(discriminador) de los resultados, y el  generador de informes. Entonces cada \nuno de estos m\u00f3dulos puede encapsularse como un objeto y se podr\u00eda usar \nORB\u2019s para integrar estos objetos dife rentes. Como resultado, se puede usar \nun enfoque plug-and-play en el desarrollo  de herramientas de data mining. \nTambi\u00e9n se podr\u00eda descomponer la he rramienta de data mining en m\u00faltiples \nm\u00f3dulos y encapsular estos m\u00f3dulos como  objetos. Por ejemplo, considere los \nm\u00f3dulos de la herramienta de data mining ilustrados  en la Figura 5. Estos Cap\u00edtulo 1  Introducci\u00f3n \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 40 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda m\u00f3dulos son parte del m\u00f3dulo de la herramienta de data mining y pueden ser \nencapsulados como objetos e integrados a trav\u00e9s de un ORB. Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 41 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n  \nCap\u00edtulo 2. An\u00e1lisis \nEstad\u00edstico mediante Excel \nIntroducci\u00f3n. M\u00e9todos cl\u00e1sicos de an\u00e1lisis de datos  \nDescripci\u00f3n de datos. Estad\u00edsticos de una variable Generalizaci\u00f3n. Distribuciones de probabili dad e intervalos de     confianza \nContrastes de hip\u00f3tesis. Tipos Relaciones entre atributos Nominales- Num\u00e9ricos: Tests de comparaci\u00f3n de medias (muestras \ndependientes e independientes) y an\u00e1lisis de varianza. \nNominales-Nominales: Tablas de Contingencia. Tests de independencia y \ncomparaci\u00f3n de proporciones. \nNum\u00e9ricos - Num\u00e9ricos: An\u00e1lisis de Regresi\u00f3n Aplicaci\u00f3n de t\u00e9cnicas estad\u00edsticas a la clasificaci\u00f3n. T\u00e9cnicas cl\u00e1sicas de \nclasificaci\u00f3n y predicci\u00f3n \nClasificaci\u00f3n mediante regresi\u00f3n num\u00e9rica Clasificador bayesiano  \nEvaluaci\u00f3n de Hip\u00f3tesis  \n Objetivo: se pretende validar o rechazar ideas preconcebidas a partir \ndel an\u00e1lisis de los datos disponible s, generalizando la s conclusiones \n     Pasos:  1. Generaci\u00f3n de hip\u00f3tesis  2. Determinar qu\u00e9 datos son neces arios. Recolectar y preparar \n 3. Evaluaci\u00f3n de hip\u00f3tesis para aceptar o rechazar  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 42 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n Variables (Atributos) \nUnidades (Ejemplos) \nTiempo Matriz de datos\nv1v2                                                                                          \nvM 1 \n2 \nn t1 \n \nTipos de variables  \n\u2022 nominales o categ\u00f3ricas (incluyendo \nordinales) \n\u2022 num\u00e9ricas  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 43 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n2.1.  An\u00e1lisis de una variable. Estad\u00edstica \nDescriptiva e Inferencia \n \n\u2022 Estad\u00edsticos: resumen (describen) toda la informaci\u00f3n contenida en una \nmuestra de datos :  \n\u2022 Variables continuas \n\u2022 medidas centrales (me dia, moda, mediana) \n\u2022 medidas de dispersi\u00f3n (rango, varianza, desviaci\u00f3n \nest\u00e1ndar, percentiles) \n\u2022 medidas de forma (histograma) \n\u2022 Variables nominales \n\u2022 frecuencias relativas (probabilidades), moda \n\u2022 media y varianza de probabilidad estimada \n\u2022 Muestra : yi; i =1\u2026n;   toma valo res en un rango continuo/discreto \n Estad\u00edsticos de variable continua \n \n\u2022 Media  (esperanza) muestral: promedio de todos los valores \n\u2211\n===n\niiyny y media\n11)(  \n \n\u2022 Moda : valor que aparece m\u00e1s veces \n\u2022 Mediana : valor que deja el mismo n\u00famero de casos a ambos lados \n( ) ( )i i i y N y Ny y mediana \u2265 = \u2264 =k j y casos \u00ba y casos \u00ba| )(  \n \n\u2022 equivale a ordenar el vector de datos y tomar el valor central \n\u2022 menos sensible frente a valores extremos poco probables Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 44 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Recorrido (rango):  \n max(yi)-min(yi)  \n \n\u2022 Varianza: promedio de desviaciones con respecto a valor medio  \n\uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\u2212\u2212= \u2212\u2212= \u2211 \u2211\n= =n\niin\nii yn ynyyny Var\n12 2\n12\n11) (11)(  \n        \n \n\u2022 Desviaci\u00f3n est\u00e1ndar  (t\u00edpica): ra\u00edz cuadrada de la varianza \n)( )( y Var y desvy= =\u03c3  \n \nmedia, sigma\n-4-202468101214\n0 1 02 03 04 0\nmuestravalorDatos\nvalor medio\nvalor medio+sigma\nvalor medio - sigma\n \nHistograma \nEstimaci\u00f3n de la distribuci\u00f3n de densidad de probabilidad: frecuencia \nrelativa de valores de yi por unidad de intervalo \nla suma total de frecuencias absolutas es el n\u00famero de datos la suma de frecuencias relativas es 1   Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 45 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nHistograma acumulado \nSuma de frecuencias relativas de casos inferiores al valor en abscisas \n(acumulaci\u00f3n de histograma normalizado): \n Estimaci\u00f3n de Prob(Y<=yi)  en el extremo superior debe ser 1  \n \nEjemplo: histograma de variable uniforme intervalos  N\u00ba de casos en intervalo histograma normal\n020406080100120140\n-3 -2,4 -1,8 -1,2 -0,6 0 0,6 1,2 1,8 2,4 3\nyfrecuencia absoluta\nacumulado\n00,10,20,30,40,50,60,70,80,91\n-3 -2,4 -1,8 -1,2 -0,6 0 0,6 1,2 1,8 2,4 3\nintervalos  Valores \nacumulados  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 46 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n \nCuantiles del histograma \n \n\u2022 Cuantil:  valores que dividen el reco rrido de datos en k partes de la \nmisma frecuencia (percentiles: 100 par tes, cuartiles: 4 partes, etc.) \n\u2022 Ejemplo: cuartiles \n histograma\n020406080100120140\n0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1histograma\nacumulado\n00,20,40,60,811,2\n00 , 5 1acumuladoCap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 47 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n \nPercentiles e histograma acumulado \n\u2022 Percentil p: valor que deja debajo al p% de los individuos, y al \n(100-p)% por encima: se entra en eje vertical del histograma \nacumulado \n - percentil 50: median a (por definici\u00f3n) \n - percentiles 25, 75: cuartiles. Abarcan al 50% de los individuos   \n   (recorrido inter-cuart\u00edlico)  - con distribuci\u00f3n normal tipificada   - percentiles 25, 75: [-0.674, 0.674] \n  - percentiles 2.5, 97.5: [-1.96, 1.96] \nacumulado\n00,10,20,30,40,50,60,70,80,91\n-3 -2,4 -1,8 -1,2 -0,6 0 0,6 1,2 1,8 2,4 3\n \n Cuartil 1 frecuencia\n020406080\n0123456789 1 0\ncalificaci\u00f3nalumnosCalificaci\u00f3n\n2,8\n0,6\n5\n3,1\n3,94,9\n10\n6,55\n...porcentaje cuartiles\n0,25 1,4\n0,5 2,725\n0,75 4\n17 , 7\nCuartil 2 Cuartil 3 Recorrido inter-cuart\u00edlico: \n[1.4, 4]: contiene 50% datos Cuartil 4 Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 48 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nEstad\u00edsticos de variable nominal \n\u2022 yi nominal: toma valores de un conj unto discreto (categor\u00edas): {vi1, \u2026, \nviki} \n\u2022 Distribuci\u00f3n de frecuencias de cada valor   \n \n\u2211\n=====\nik\njjki ki\nn nnn pnn pnn p\n11 21 1\n)%/(100)%/(100)%/(100\n#  \n\u2022 Moda : valor que aparece m\u00e1s veces \n)(maxjn\nj \nEjemplo variable nominal y num\u00e9rica \n \n Edad Sexo\n23 M\n25 M\n18 H\n37 M45 H\n62 H\n43 M\n40 H\n60 M54 H\n28 H\n18 H\n54 M\n29 H42 M\n26 M\n32 M\n41 M\n37 M36 H\n53 H\n21 M\n24 H\n21 H\n45 M\n64 H22 M\n61 M\n37 M\n66 M0102030405060\nHM\nsexoporcentaje\n020406080100120\n18 25 35 45 55 65\nedadporcentajefrecuencia\nacumuladaCap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 49 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Media y varianza de frecuencias estimadas \n\u2022 C\u00e1lculo de cada frecuencia \n\u2022 para una categor\u00eda dada: m casos de n \n   p=m/n \n\u2022 puede verse como asignar: vi=1 c ada ejemplo en la categor\u00eda   \n                                                 vi=0 en el resto \n\u2211\n==n\niivnp\n11 \n \n\u2022 Varianza de p: \n) 1() 1( ) (1)(2\n1\np pp p pvnp Var\npn\nii\n\u2212 =\u2212 = \u2212 =\u2211\n=\n\u03c3 \n\u2022 caso m\u00e1xima varianza: p=0.5  \n  Generalizaci\u00f3n de la muestra a la poblaci\u00f3n \n\u2022 Los estad\u00edsticos resumen (describen) toda la  informaci\u00f3n contenida en \nuna muestra (estad\u00edstica descriptiva) \n\u2022 Para generalizar las conclusiones, es deseable formular razonamientos \nsobre la poblaci\u00f3n que genera la muestra \n\u2022 Paso de los estad\u00edsticos (y i) a los estimadores (Yi) \n\u2022 Uso de distribuciones te\u00f3ricas de probabilidad para caracterizar \nlos estimadores \n\u2022 Cuantificaci\u00f3n de la probabilid ad de los resultados (nunca se \ngarantiza con certeza absoluta) \n\u2022 Puede hacerse an\u00e1lisis contrari o: deducci\u00f3n de propiedades de la \nmuestra a partir de la poblac i\u00f3n (inter\u00e9s te\u00f3rico) \nDistribuciones de probabilidad Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 50 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Modelo que representa la tendenci a de un histograma con muchos datos \ny cajas peque\u00f1as \n\u2022 Funci\u00f3n distribuci\u00f3n de probabilidad de X: FX(x) \n\u221e<<\u221e\u2212 \u2264 = x xXPxFX ); ( )(  \n \n\u2022 Funci\u00f3n densidad de probabilidad de X: fX(x) \n\u222b \u222b=\u2264\u2264 =\u221e<<\u221e\u2212 =\n\u221e\u2212b\naXx\nX XX\nX\ndxxf bXaP dxxf xFxdxx dFxf\n)( ) (;)( )(;)()(\n \n \nDistribuci\u00f3n Normal \n\u2022 Curva de gran inter\u00e9s por explic ar datos en muchas situaciones \n\u2022 Aplicada por primera vez como distribuc i\u00f3n por A. Quetelet (1830)     \n\uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\u2212 =2\n21exp\n21)( z zf\n\u03c0 \n \n\u2022 distribuci\u00f3n sim\u00e9trica: co incide media y mediana en 0 \n\u2022 se dispone del valor de la distribuci\u00f3n de probabilidad: \u00e1rea bajo la curva \nde fZ(z) para cualquier valor:  \nTipificar o estandarizar  variables: Se mide el des plazamiento respecto a la \nmedia en unidades de desviaci\u00f3n t\u00edpica: \nii\niyyz\u03c3\u2212= \n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 51 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n \nDistribuci\u00f3n Normal e Intervalos de Confianza \n \n\u2022 Ej.: se sabe conocen par\u00e1metros de poblaci\u00f3n con dist ribuci\u00f3n normal: \nmedia: m= 115; desviaci\u00f3n t\u00edpica:s= 20 \n\u2022 \u00bfcasos inferiores a 70? z=(70-115)/20, F(z)=0,012 \n\u2022 \u00bfcasos superiores a 150? z=(150-115)/20, 1-F(z)=0,04 \n\u2022 \u00bfen intervalo 90-130? F((130- 115)/20)-F((90-115)/20)=0,667 \n\u2022 \u00bfqu\u00e9 intervalos sim\u00e9trico ti enen el 80%, 95% de los casos \n(intervalos de confianza)? z=F-1(a/2); y=m \u00b1zs \n\u2022 80%: z0.1=1,28; 115 \u00b1 z0.1*20=[89.3,  140.6] \n\u2022 95%: z0.025=1,96; 115 \u00b1 z0.025*20=[75.8,  154.2] \n \nInferencia \n\u2022 Objetivo : dado un estad\u00edsticos de una muestr a sacada al azar, razonar \nacerca del verdadero par \u00e1metro de la poblaci\u00f3n \n\u2022 Se basa en la estimaci\u00f3n de par\u00e1me tros y contraste de hip\u00f3tesis con \nc\u00e1lculo de probabilidades \n\u2022  muestra aleatoria y repr esentativa (estratificaci\u00f3n) \n\u2022  elementos independientes zF Z(z)\n-3 0,001349967\n-2,5 0,00620968\n-2 0,022750062\n-1,5 0,066807229\n-1 0,15865526\n-0,5 0,308537533\n00 , 5\n0,5 0,691462467\n1 0,84134474\n1,5 0,933192771\n2 0,977249938\n2,5 0,99379032\n3 0,998650033\n- - - - - 0 1 3 0 zf(z) \nz0F(z 0)\nUna cola (unilateral) -3 -1f(z) F(z 0)\n-2 0 zf(z) F(z 0)\nSim\u00e9trico dos colas \n(bilateral) Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 52 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Paso de la poblaci\u00f3n a una muestra aleatoria \n\u2022 Dada una poblaci\u00f3n con media y varianza: \n\u2022 Se toma una muestra aleatoria (n casos) de la poblaci\u00f3n: yi, \ni=1,\u2026,n \n\u2022 C\u00f3mo se distribuyen los estad\u00edsticos de la muestra? A su vez son \nVAs \n Distribuci\u00f3n de la media muestral \nY yn\niin\niin\nii\nnYVarny Varny VarY yEnyEyny\n\u03c3 \u03c31);(1)(1)()(1)(1\n1211\n= = == ==\n\u2211\u2211\u2211\n===\n \n \n\u2022 Qu\u00e9 distribuci\u00f3n sigue?  Teorema del L\u00edmite Central: \n\u201cUna muestra suficientemente grande de una poblaci\u00f3n con distribuci\u00f3n \narbitraria tendr\u00e1 estad\u00edstico media con distribuci\u00f3n normal\u201d \n\u2022 Consecuencia: intervalo de confianza de la media a partir de dist. \nNormal \nYnz Yy \u03c31\u00b1 = \n\u2022 Mayor \u201cNormalidad\u201d: tama\u00f1o de las m uestras, distribuci\u00f3n pob. parecida \na normal \n \nEjemplo l\u00edmite central \n\u2022 Poblaci\u00f3n: 1000 individuos, 400 mujeres, 600 hombres  \n49.0) 1( ;4.0 = \u2212 = = P P P \u03c3  \n \n\u2022 Muestras de 10 individuos Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 53 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 155.0) 1(\n101;4.0 )(;10\n1\n= \u2212 ====\u2211\n=\nP PPpEy p\npii\n\u03c3 \n\u2022 Intervalo de confianza al 95% (con distribuci\u00f3n normal): \n\u2022 Influye: \n\u2022  intervalo de confianza (z): \u201cgarant\u00eda\u201d de no equivocarnos \n\u2022  tama\u00f1o de muestra (n) \n\u2022  variabilidad de poblaci\u00f3n (p) \n155.0)P1(P\n101;4.0P)p(E;y101p\np10\n1ii\n= \u2212 =\u03c3==\u2211 =\n=\n \n]7.0,1.0[ 96.1 = \u00b1p P \u03c3  \n \n051015202530\n0 0 , 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 9 1 \n\u2022 Si las muestras fueran de 50 individuos:  \n069.0) 1(\n501;50150\n1\n= \u2212 ==\u2211\n=\nP Py p\npii\n\u03c3 \n]54.0,26.0[ 96.1 = \u00b1p P \u03c3  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 54 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n Ejemplo de aplicac i\u00f3n para decisi\u00f3n \n\u2022 Para determinar el intervalo de conf ianza del estimador al 95% se aplica \nel argumento del muestreo \u201cdado la vuelta\u201d: \npy\npP EjzyY\n\u03c3\u03c3\u03b1\n96.1 :2/\n\u00b1=\u00b1\u2208\n \n \n\u2022 Ejemplo: Un supermercado se plant ea extender su horario a s\u00e1bado por \nla tarde. Necesita un m\u00ednimo del 10%  de sus clientes para cubrir costes. \nCon una muestra de 1500 personas se obtiene que hay un 8% de \nclientes interesados \u00bfQu\u00e9 hacer? \n%]37.9 %,63.6[1500/)08.01(*08.0 96.1 08.0 96.1p Pp = \u2212 \u00b1 =\u03c3 \u00b1=\n \n \n\u2022 Con una confianza del 95% podemos decir que los clientes dispuestos a \ncomprar el s\u00e1bado por la tarde no contiene al deseado 10%. \n Contrastes de hip\u00f3tesis \n\u2022 Contrastar es medir la probabili dad de que el estad\u00ed stico obtenido en \nuna muestra sea fruto del azar \n\u2022 Formulaci\u00f3n del modelo e hip\u00f3tesis : se conoce la distribuci\u00f3n del \nestad\u00edstico bajo condiciones hip\u00f3tesis \n\u2022 Hip\u00f3tesis nula ( H0): es lo que dudamos y quer emos contrastar: Ej: \u00bfEl \nporcentaje total es 10%?, la medi a de los ingresos es superior a 5? \n\u2022 Bajo H0, el estad\u00edstico sigue el modelo, y la diferencia observada \nes \u00fanicamente fruto del azar \n\u2022 Hip\u00f3tesis alternativas: alternativas  que permiten rechazar la hip\u00f3tesis \nnula: prob. distinta de 10% , media menor a 5, etc. \n\u2022 Rechazar hip\u00f3tesis H0: hay evidencia para negar H0 \n\u2022 No rechazable: no hay evidencia estad\u00edstica para hacerlo (no \nimplica demostrar su veracidad) Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 55 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nContrastes con normal y varianza conocida  \nContraste de dos colas (bilateral ): deja la mitad a cada lado, a/2 \n\u2022 Ej: Hip\u00f3tesis nula H0: P=10%  \n]115.0 085.0[p; 1500/)1.01(*1.0p \u2208 \u2212 =\u03c3  \n \n\u2022 Hip\u00f3tesis alternativa: \n%10\u2260P  \n \n \n\u2022  \n\u2022 Regi\u00f3n cr\u00edtica: -1,96<z<1.96 \n \nContraste de una cola (unilateral): deja a un solo lado a \n\u2022 Ej: Hip\u00f3tesis nula H0: \n087.0 65.1 = \u2212>p Pp \u03c3  \n \n\u2022 Hip\u00f3tesis alternativa: P<10% \n z0.025=1.96 \nz0.05=1.65 0.085 -3 3p 0.1 0.115 \n-3 3p 0.1 0.087 Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 56 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n\u2022 Regi\u00f3n cr\u00edtica: z>1.65 \n Contraste con varianza estimada \n\u2022 La variable (yi-y)/s no es exacta mente la normal tipificada (s es \nestimada): \n\u2022 Distribuci\u00f3n t-Student : par\u00e1metro grados de libertad:n-1  \n\u2022 se ensanchan los intervalos de confianza (s\u00f3lo si pocos datos) \n \n \n\u03c3 ,\u00b5 conocida \nestad\u00edstico \n)1,0(\n/N\nny\u2192\u2212\n\u03c3\u00b5 \nInt. confianza \nn zy /2/\u03c3\u03b1\u00b1  \u03c3 , \u00b5 conocida \nestad\u00edstico \n)1,0(\n/1\u2212\u2192\u2212\nnt\nny\n\u03c3\u00b5 \nInt. confianza \nn tyn/1,2/\u03c3\u03b1 \u2212 \u00b1  \n \n \nEjemplo de Intervalos con t-Student  Los valores del pH de una piscina en 10 det erminaciones son: 6,8; 6,78; 6,77; \n6,8; 6,78; 6,8; 6,82, 6,81; 6,8 y 6, 79. Utilizando normal y t-Student, hallar: \n\u2022 Intervalo de confianza 95% para media poblacional \n\u2022 Intervalo de confianza 65% para media poblacional -5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.4 Student \n(N=9)Student \n(N=50)Student \n(N=100) Normal\nProb[X>z] z z\n0,10% 4,30 3,26 3,17 3,09\n0,50% 3,25 2,68 2,63 2,58\n1% 2,82 2,40 2,36 2,33\n2,50% 2,26 2,01 1,98 1,96\n5% 1,83 1,68 1,66 1,64\n10% 1,38 1,30 1,29 1,2820% 0,88 0,85 0,85 0,84Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 57 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Contrastar hip\u00f3tesis nula de que la media poblacional es  6,8 con niveles \nde significaci\u00f3n a =0,05 y a=0,35 \n \nnormal: \n media 95%: [6,765, 6,825], media 65%: [6,781, 6,809] \nt-Student: \n media 95%: [6,761, 6,829], media 65%: [6,780, 6,801] \n \n2.2.  T\u00e9cnicas de Evaluaci\u00f3n de hip\u00f3tesis \n2.2.1. An\u00e1lisis de relaciones entre atributos \nEl objetivo del an\u00e1lisis entre  los atributos que definen los datos es ver el tipo de \ninterrelaci\u00f3n o dependencia que existe entre los valores de dichos atributos. \nEste an\u00e1lisis se lleva a cabo haciendo uso de los datos disponibles para tener \u201cevidencia estad\u00edstica\u201d que permita valid ar o refutar hip\u00f3t esis que pretendan \nexplicar las relaciones.  \nLa herramienta o t\u00e9cnica que permite lleva r a cabo este tipo de an\u00e1lisis es \nel denominado tests de hip\u00f3tesis, que se define de manera distinta en funci\u00f3n \ndel tipo de atributos con los que este mos trabajando. De esta manera en \nfunci\u00f3n del tipo de atributo tenemos: \n\u2022 Nominales-nominales : En este caso los dos atributos toman valores de un \nconjunto de posibles valores (por ejempl o: Norte, Sur, Este y Oeste). La \nrelaci\u00f3n entre las variables se obti ene mediante las tablas de contingencia. \n\u2022 Nominales-num\u00e9ricos : En este caso uno de los atributos toma valores de \nun conjunto de posibles valores y otro to ma valores num\u00e9ricos. La relaci\u00f3n \nentre los atributos se obtiene mediant e la comparaci\u00f3n de medias y el \nan\u00e1lisis de varianza. \n\u2022 Num\u00e9ricos-num\u00e9ricos : En caso los dos atributos toman valores \nnum\u00e9ricos. La relaci\u00f3n entre los dos atributos se obtiene mediante el \nan\u00e1lisis de regresi\u00f3n y covarianza. \nEn la secci\u00f3n \u00a1Error! No se encuentra el origen de la referencia.  se \ncontemplan m\u00e1s casos de contrastes de hip\u00f3tesis. \n2.2.2. Relaci\u00f3n entre variables nominales-nominales \nEl objetivo es analizar la interrelaci\u00f3n (dependencia) entre los valores de variables nominales. En este caso la herramienta de an\u00e1lisis para dos variables \nes la denominada tabla de contingencia . En esta tabla se calcula la Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 58 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda distribuci\u00f3n de los casos (las frecuenc ias de aparici\u00f3n) par a las distintas \ncombinaciones de valores de las dos variables, como se oberva en la figura siguiente. \n   Variable 2  totales 1 \n  valor 1 valor 2 ... valor p2   \nvalor 1 n 11 n 12 ... n 1p2 t1 \nvalor 2 n 21 n 22 ... n 2p2 t2 \n... ... ... ... ... ... valor p1 n\np11 n p12 ... n p1p2 tp1 Variable 1 totales 2 t'1 t'2 ... t'p2 t \n \nFigura 1: Tabla de contingencia.  \n \nA partir de la tabla de contingencia  podemos calcular  las probabilidades \nmarginales de los valores de la vari able 1 como Pi=ti /t, que representa la \nprobabilidad de que la variable 1 tome el valor i. Del mismo modo podemos \ncalcular las probabilidades para la  variable 2 como Pj=t\u2019j/t.  \nA partir de las probabilidades margi nales podemos calcular los casos \n\u201cesperados\u201d, bajo la hip\u00f3tesis a cues tionar de independencia entre variables. \nPara calcular el valor esperado se mult iplica el n\u00famero total de casos por la \nprobabilidad de que la variable 1 tome el valor i y la variable 2 tome el valor j, \nes decir Eij=t(ti/t)(t\u2019j/t)= tit\u2019j/t. Obs\u00e9rvese que \u00fanicame nte bajo la hip\u00f3tesis de \nindependencia podemos calc ular la probabilidad conj unta como un producto de \nprobabilidades. \nLa t\u00e9cnica de an\u00e1lisis estad\u00edstico que se aplica para la relaci\u00f3n entre dos \nvariables nominales es el contraste Chi-2.  Las caracter\u00edsticas de este test son: \n\u2022 Es aplicable en an\u00e1lisis bi-variable (normalmente clase vs atributo) \n\u2022 Determina si es rechazable la hip\u00f3tesis de que dos variables son \nindependientes \n\u2022 Bajo hip\u00f3tesis H0 se determinan los casos en el supuesto de \nvariables independientes. Los va lores esperados se determinan \ncon probabilidades marginales de las categor\u00edas: Eij=tPi Pj \n(valores esperados) \n\u2022 El estad\u00edstico Chi-cuadrado mide la diferencia entre los valores \nesperados y los valores observados, por lo que su expresi\u00f3n es:  \n\u2211\u2211\n==\u2212 =1\n12\n12 2/) (p\nip\njij ij ij E OE \u03c7    (1) \nLa expresi\u00f3n anterior, \u03c72, bajo las condiciones de H0 sigue una \ndistribuci\u00f3n conocida denominada distribuc i\u00f3n Chi-cuadrado, caracterizada por Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 59 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda el par\u00e1metro grados de libertad  que es el (n\u00ba de filas-1)(n\u00ba de columnas \u20131) . \nCuando no se cumple la hip\u00f3tesis H0 las variables son dependientes.  \nPor lo tanto se formula un test de hip\u00f3tesis para determinar el valor de \nChi-cuadrado para esa hip\u00f3tesis. La distribuci\u00f3n Chi-Cuadradado est\u00e1 \ntabulada: \nprobabilidad chi2 \nsupera estad\u00edsticovalor \nestad\ngrados de libertad 56789 1 0 1 1\n1 0,025 0,014 0,008 0,005 0,003 0,002 0,001\n2 0,082 0,050 0,030 0,018 0,011 0,007 0,0043 0,172 0,112 0,072 0,046 0,029 0,019 0,0124 0,287 0,199 0,136 0,092 0,061 0,040 0,0275 0,416 0,306 0,221 0,156 0,109 0,075 0,051\n6 0,544 0,423 0,321 0,238 0,174 0,125 0,088\n7 0,660 0,540 0,429 0,333 0,253 0,189 0,139\n  \nY el test lo que calcula es  la probabilidad de que la diferencia entre el \nvalor observado y el valor esperado supere un cierto umbral. \n \n \nFigura 2: Representaci\u00f3n Gr\u00e1fica del  test Chi-Cuadrado. \n \n2.2.3. Relaciones num\u00e9ricas-nominales \nLas t\u00e9cnicas para establecer posibles relaciones entre dos variables una de \nellas num\u00e9rica y la otra nominal (o ent re dos nominales si trabajamos con \nproporciones) se utiliza la t\u00e9cnica de la comparaci\u00f3n de medias y proporciones. \nEsta t\u00e9cnica mide la relaci\u00f3n entre  variables num\u00e9ricas y nominales, o \nnominales y nominales (proporciones),  determinando si es rechazable la \nhip\u00f3tesis de que las diferencias de medi as o proporciones condicionadas a las \netiquetas de la variable nominal son debid as al azar. Es decir que se calcula el \nimpacto de la variable nomi nal sobre la continua. \nExisten dos tipos de an\u00e1lisis s eg\u00fan si tenemos dos medias o \nproporciones o un n\u00famero mayor de dos. Si tenemos dos medias o \nproporciones se calcula la significatividad de la difere ncia. Si tenemos m\u00e1s de \ndos valores distintos se realiza un an\u00e1lisis de varianza. \u03b1\n\u03c72Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 60 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 2.2.3.1. Comparaci\u00f3n de dos medias \nEn este caso tenemos dos subpobl aciones, una para cada grupo, cada una con \nsu media y varianza. Las hip\u00f3tesis que podemos establecer son: \n\u2022 H0: la diferencia de medias en la poblaci\u00f3n es nula D=0 \n\u2022 Hip\u00f3tesis alternativa A: las medias son distintas: D!=0 \n\u2022 Hip\u00f3tesis alternativa B: la media de 1 es mayor que 2: D>0 \n\u2022 Hip\u00f3tesis alternativa C: la media de 1 es mayor que 2: D<0 \nComo vemos, no hay una \u00fanica posibil idad de hip\u00f3tesis alternativa sino \nvarias, con diferentes intervalos de re chazo en funci\u00f3n de la informaci\u00f3n que \ntengamos a priori. Adem\u00e1s, para la com paraci\u00f3n de las variables num\u00e9ricas de \ndos clases, las situaciones posibles q ue podemos encontrar nos dentro de la \nmuestra total son: \n\u2022 Muestras independientes: conjuntos distintos \n\u2022 Muestras dependientes: es decir  las muestras pertenecen al \nmismo conjunto, con dos variables a comparar en cada ejemplo \n \nCuando el n\u00famero de muestras es  muy elevado para cada grupo, las \nmuestras siguen una distribuci\u00f3n normal po r lo que las hip\u00f3tes is anteriormente \nexpuestas se eval\u00faan m ediante los valores de una gaussiana est\u00e1ndar. De \nesta manera se calcular\u00eda la media de la diferencia y su varianza y se aplicar\u00eda \nal c\u00e1lculo de probabilidades  de una gaussiana est\u00e1ndar. En el caso de la \nhip\u00f3tesis A se utilizar\u00edan las dos cola s de la gaussiana y en el caso de la \nhip\u00f3tesis B utilizar\u00edamos una \u00fanica cola , como se observa en la siguiente \nfigura. \n \nFigura 3:  Representaci\u00f3n Gr\u00e1fica de compraci\u00f3n de dos medias \nmedianteuna gaussiana. -3 \u03b1/2=0.025 \u03b1/2=0.025 \nz=\u22121.96  z=+1.96  \n- 3\u03b1=0.05 \nz=\u22121.65  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 61 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Cuando las muestras s on peque\u00f1as no es v\u00e1lida la hip\u00f3tesis de \nnormalidad de los estad\u00edsticos de medias y el test se realiza considerando una \ndistribuci\u00f3n t-Student: \n/2,GL yt\u03b1 \u03c3 \u00b1      ( 2 )   \nEl proceso para el c\u00e1lculo cuando la s muestras son independientes (test no \npareado) es: \n\u2022 En cada muestra (tama\u00f1os n1, n2) obtenemos las medias y varianzas:  \n2 1 2 1 , ,,y y yy \u03c3 \u03c3     (3) \n\u2022 Se calcula la diferencia:  \n2 1y y d \u2212 =      ( 4 )  \n\u2022 Varianza de la diferencia:  \n22\n2\n12\n1 2\nn ny y\nd\u03c3 \u03c3\u03c3 + =      ( 5 )  \n\u2022 Los grados de libertad de la t-Student se eval\u00faan seg\u00fan la varianza: \n\u2022 Distinta varianza ( heteroscedasticidad ): gl=min(n1, n2)  \n\u2022 Misma varianza ( homoscedasticidad ): gl=n1+n2-2 \nEl proceso de c\u00e1lculo cuando las m uestras dependientes (test pareado), \nse fundamenta en que se dispone de la di ferencia en cada uno de los ejemplos \ny no en que tenemos dos variables (eje mplo: cambio en el tiempo de una \nvariable para todos los ejem plos d1, d2, ..., dn): di=d1i- d2i. En este caso todo \nes equivalente al caso anter ior pero lo c\u00e1lculos son: \n\u03c3 \u03c3 \u03c3\nnddndnddn\niin\nii1;) (11;1\n12 2\n1= \u2212\u2212= = \u2211 \u2211\n= =   (6) \n2.2.3.2. An\u00e1lisis de la varianza \nEsta t\u00e9cnica tambi\u00e9n mide la relaci\u00f3n entre variables num\u00e9ricas y nominales, \npero en este caso se descompone la variabilidad del resultado en varios \ncomponentes: \n\u2022 Efectos de factores repres entados por otras variables \n\u2022 Efectos de error experimental \nLa t\u00e9cnica del an\u00e1lisis de la varianza  simple (ANOVA) considera un solo \nfactor con varios niveles nominales.  Para cada nivel se tiene una serie de Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 62 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda observaciones y el modelo: Yij=ui+uij, representa ruido con la misma varianza \npor nivel, donde i var\u00eda entre  1 y el n\u00famero de niveles (variable nominal) y j \nvar\u00eda entre 1 y el n\u00famero de datos por ni vel. Adem\u00e1s de esta t\u00e9cnica existe la \nt\u00e9cnica MANOVA que es un modelo multif actorial de la varianza. En este \nmodelo se definen I niveles, cada uno de ellos representado por un conjunto de \nmuestras, como se puede observar en la  siguiente figura, y donde cada nivel \nest\u00e1 represntado por una media y una varianza. \n \n \nFigura 4:  Niveles de la t\u00e9cnica MANOVA. \n \n \nFigura 5:  Represntaci\u00f3n Gr\u00e1fica de los Niveles de la t\u00e9cnica MANOVA. \n \nEl an\u00e1lisis MANOVA eval\u00faa las siguientes variables: \n\u2022 N\u00famero total de elementos: Factor B 1 2 ... r\nFactor A\nX111 X121 ... X 1r1\nX112 X122 ... X 1r2\n1 ... ... ... ...\nX11n11 X12n12 ... X1rn1r\nX211 X221 ... X 2r1\nX212 X222 ... X 2r2\n2 ... ... ... ...\nX21n21 X22n22 ... X2rn2r\n... ... ... ...\nXt11 Xt21 ... X tr1\nXt12 Xt22 ... X tr2\nt ... ... ... ...\nXt1nt1 Xt2nt2 ... Xtrntr\ny \n1Y2Y3YVariaci\u00f3n \nNE Variaci\u00f3n  ECap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 63 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2211\n==I\niin n\n1     (7) \n\u2022 Media por nivel:  \n\u2211\n==I\niij\nii Y\nnY\n11     ( 8 )  \n\u2022 Media total:  \n\u2211\u2211\n===I\niin\njiji\nY\nnY\n11     ( 9 )  \n\u2022 Relaci\u00f3n entre \u201ccuadrados\u201d:  \n) ( ) ( ) (\n12\n12\n1YYn YY YYI\nii i iI\niin\njijI\niin\njiji i\n\u2212 + \u2212 = \u2212 \u2211 \u2211\u2211 \u2211\u2211\n= == ==  (10) \nY realiza una estimaci\u00f3n de varianzas de la siguiente manera \n\u2022 Varianza inter-grupo ( between ) (I-1 grados de libertad):  \n2\n1) (11YYnISI\nii i b \u2212\u2212= \u2211\n=    ( 1 1 )  \n\u2022 Varianza intra-grupo ( within ) (n-I grados de libertad): \n2\n1) (1\niI\niin\njij w YYInSi\u2211\u2211\n==\u2212\u2212=     ( 1 2 )  \n\u2022 Varianza total (n-1 grados de libertad): \n2\n1) (11YYnSI\niin\njiji\u2211\u2211\n==\u2212\u2212=     ( 1 3 )  \nLa hip\u00f3tesis que planteamos o la  pregunta que queremos responder es: \n\u00bfEs significativamente mayor que la un idad la relaci\u00f3n entre la varianza \nintergrupo e intragrupo, f=Sb/Sw?. Por lo  tanto debemos realizar un contraste \nde hip\u00f3tesis de cociente de varianzas maestrales, que sigue una distribuci\u00f3n F \nde Fisher-Snedecor : F(x, I-1,n-I), como se ve en la figura siguiente. \n \nFigura 6:  Representaci\u00f3n de la F-Fisher-Snedecor. \u03b1F \nRango: [0,20] Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 64 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nEste test permite rechazar o no la hip\u00f3tesis de que el cociente entre \nvarianzas estimadas se deba al azar. Por lo tanto  \n2.2.4. Relaciones num\u00e9ricas-num\u00e9ricas: \n2.2.4.1. Regresi\u00f3n lineal \nLa regresi\u00f3n lineal permite identificar relaciones entre variables num\u00e9ricas y \nconstruir modelos de regresi\u00f3n: 1 va riable salida y m\u00faltiples entradas \nnum\u00e9ricas. Se consideran relaciones de una variable de sa lida (dependiente) \ncon m\u00faltiples variables de entrada (indepe ndientes). Este problema se puede \nrepresentar de la siguiente manera: \nDada la muestra de datos: )} , ( ),...,, (),, {(2 2 1 1 n ny X y X y XG G G\n donde \ns dimensione I con  vectores:XG\n, se busca estimar una funci\u00f3n que mejor \n\u201cexplique\u201d los datos: \n)g(y\u02c6               :(.)\nX XR R gI\nG G\n= \uf8e7\u2192\uf8e7\uf8e7\u2192\uf8e7    ( 1 4 )  \nEl procedimiento de resoluci\u00f3n para estimar dicha funci\u00f3n es el \nprocedimiento de m\u00ednimos cuadrados que es tima el vector de coeficientes que \nminimiza error:  \nt\nIt\nItI\nppp i i\nx x X a aa AX A xa a Xgy\n] 1[ ;] [)(*)( )( \u02c6\n1 1 010\n\"G\n\"GGG G\n= == + = = \u2211\n=\n  (15) \n \nEl objetivo es que dadas N muestr as, el procedimiento debe determinar \ncoeficientes que minimicen el error de predicci\u00f3n global  \n2\n1] )([\u2211\n=\u2212 =n\njj jy XgG\n\u03b5      ( 1 6 )  \nEste es un problema cl\u00e1sico de mini mizaci\u00f3n de funci\u00f3n cuadr\u00e1tica cuya \nsoluci\u00f3n es \u00fanica. La formulaci\u00f3n gen\u00e9ric a matricial del problema se puede \nexpresar como: Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 65 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda AHA\nx xx xx x\nXgXg\nyy\ng\nyy\ny\nN\nINII\nN N NGG\n\"####\"\"\nG#G\n# #G*\n111\n) ()(\n\u02c6\u02c6\n\u02c6;\n12 2\n11 1\n1 1 1 1\n=\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=\n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=\n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=\n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n= (17) \nPor lo que la soluci\u00f3n de m\u00ednimos cuadrados es: 1ttAH HH y\u2212\uf8ee\uf8f9=\uf8f0\uf8fbG G \n2.2.5. Evaluaci\u00f3n del modelo de regresi\u00f3n \nLa evaluaci\u00f3n del modelo rea liza el an\u00e1lisis de validez del modelo asumido, es \ndecir se van a calcular una serie de medida s de \u201cparecido\u201d entre  la variable de \nsalida estimada mediant e la funci\u00f3n y los valores de  la variable de salida real, \nide esta manera analizarem os la nfluencia de las va riables de entrada en el \nc\u00e1lculo de la variable de salida (si ex iste o no una relaci \u00f3n lineal entre las \nvariables de entrada que permita determi nar la variable de salida). Estas \nmedidas son: el Factor de Correlaci\u00f3n (que muestra si existe la relaci\u00f3n lineal), \nel error de predicci\u00f3n (diferencia entre  la predicha y la real) y el error en \ncoeficientes. \n2.2.5.1. Medidas de Calidad \nEl factor de correlaci\u00f3n se eval\u00faa como: \n() ()\n\u2211 \u2211\u2211 \u2211\u2211\n= == ==\n= =\u2212 = \u2212 == \u2212 \u2212 =\nN\njjN\njjn\njj yn\njj yn\njj j\nyy\ny\nNy y\nNyy y S y y Sy Vary Varyy Covy yy y\nSSyy Corr\n1 112\n12\n\u02c61\u02c6\n1,\u02c61\u02c6; ;\u02c6 \u02c6)()\u02c6(),\u02c6() )(\u02c6 \u02c6(1),\u02c6(\n (18) \nEn general, se puede hacer factores  de correlaci\u00f3n entre cualquier par \nde variables num\u00e9ricas: indica el grado de re laci\u00f3n lineal existent e. Para ello se \ncalcula la matriz de covarianzas (o la  de correlaciones que es la misma pero \nnormalizada) de la siguiente manera: \n()() ( )( )\n() ( )\n() ( )11 2 1 2\n12 2\n1\n1var cov , cov ,\ncov , var 1\u02c6 \u02c6\ncov , varnt\ni X\ni\nIIx xx xx\nxx xCXn\nxx x\u00b5\n=\uf8ee \uf8f9\n\uf8ef \uf8fa\n\uf8ef \uf8fa \u221e= \u2212 =\uf8ef \uf8fa\n\uf8ef \uf8fa\n\uf8f0 \uf8fb\u2211\"\nG #\n#%\n\" (19) \ndonde \u2211\n==n\niiX\nn 11\u02c6GG\u00b5  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 66 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El error de predicci\u00f3n se eval\u00faa como: () ( )2\n12\n1\u02c6 \u2211 \u2211\n= == \u2212 =n\njjn\njj jyy Error \u03b5 bajo la \nhip\u00f3tesis de que los datos yi tengan la misma varianza sy, sean \nindependientes, y que el modelo lineal sea adecuado el error puede calcularse \ncomo: 2)1(y n Error \u03c3\u2212\u2248 . \nEl error en coeficientes se eval\u00faa a partir de la expresi\u00f3n que permite \nencontrar los coeficientes yt t\nAt tH HH yH HH A \u03b5 \u03b5G GG G1 1] [ ; ] [\u2212 \u2212= = . La \nrelaci\u00f3n entre los errores en predicci\u00f3n y en coeficientes estimados se eval\u00faa: \n1 2\n222\n] [10\n\u2212=\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n= HH Ct\ny\nAAA\nA\nI\u03c3\n\u03c3\u03c3\u03c3\n\"\"#% ## #\"\"\n  (20) \nPor lo que el error en los coeficient es depende de el error en y, sy2 y el \nrecorrido de datos X, es decir la matriz H. \n2.2.5.2. Test de Hip\u00f3tesis sobre modelo de regresi\u00f3n \nEstos valores permiten analizar la \u201ccali dad\u201d del modelo mediante los test de \nhip\u00f3tesis: hip\u00f3tesis de significatividad de par\u00e1metros (gaussiana o t-Student) y \nla hip\u00f3tesis de ausencia de relaci\u00f3n (F de Fisher-Snedecor). \nPara evaluar la significatividad de  par\u00e1metros, partimos de varianzas de \npar\u00e1metros {s2A1,\u2026s2AF} y los propios valores estimados, y nos preguntamos \nsi son significativ os los par\u00e1metros: ? ,...,\n11\nFAF\nAA A\n\u03c3 \u03c3. Este test puede resolverse \nmediante una gaussiana est\u00e1nda r si tenemos gran cantidad de datos, o bien, si \nhay pocos datos: en vez de estad\u00edstica normal, una t-Student con n-F-1 grados \nde libertad. Tambi\u00e9n podemos extender el modelo y analizarlo: ej: dependencia \ncuadr\u00e1tica, ver si son significativos nuevos t\u00e9rminos \nPara analizar la validez del modelo debemos realizar un an\u00e1lisis de la \nvarianza que permite rechazar o no la hi p\u00f3tesis de que no existe relaci\u00f3n entre \nvariables (relaci\u00f3n debida al azar, correlaci\u00f3n  nula). Para ello a partir del valor:  \n() ()()2\n12\n12\n1\u02c6 \u02c6 \u2211 \u2211 \u2211\n= = =\u2212 + \u2212 = \u2212N\njjN\njjN\njj yy yy yy    (21) \ncalculamos el estad\u00edstico : \n)1 /(/\n\u2212\u2212=In SRISEF  que sigue una distribuci\u00f3n: F de \nSnedecor: F(n1, n2), donde los grados  de libertad son: I, n-I-1 \n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 67 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 2.3.  Ejemplos de aplicaci\u00f3n de t\u00e9cnicas de \nevaluaci\u00f3n de hip\u00f3tesis \nCon un objetivo meramente ilustra tivo, en esta secci\u00f3n se sugieren \nalgunas aplicaciones de las t\u00e9cnicas de  contraste de hip\u00f3tesis y miner\u00eda de \ndatos presentadas en otras secciones. So n ejemplos que se relacionar\u00edan con \nel objetivo final de este proyecto de anal izar y describir relaciones de inter\u00e9s y \nmodelos subyacentes en datos del dominio  del tr\u00e1fico a\u00e9reo.  Hay que tener en \ncuenta, que son ejemplos sugeridos que quedar\u00edan sujetos a su validaci\u00f3n \nmediante la generaci\u00f3n de los datos apropiados, sujeto a una metodolog\u00eda \napropiada de preparaci\u00f3n, inte rpretaci\u00f3n y validaci\u00f3n. \n2.3.1. Ejemplos de Validaci\u00f3n de Hip\u00f3tesis \nPara ilustrar la t\u00e9cnica de contrast e de hip\u00f3tesis para independencia entre \nvariables de tipo nominal, supongamos que partimos de los datos de la tabla \nsiguiente: \n \nEn esta tabla se representan dos variabl es nominales: retardo y tipo de avi\u00f3n. \nLa variable retardo puede tomar 4 valores:  nulo, medio, alto y muy alto. La \nvariable tipo de avi\u00f3n puede tomar 3 valore s: Ligero, Mediano y Pesado. En la \ntabla aparecen el n\u00famero de aviones de cada tipo en funci\u00f3n del retardo que \nsufren. Es decir, aparece la distribuci \u00f3n observada para el n\u00famero de aviones \nde cada tipo que sufre una determinada categor\u00eda de retardo. \nSi en la tabla anterior consideramos \u00fanicamente los valores totales de las \nvariables tipo de avi\u00f3n y retardo, podem os calcular la pr obabilidad de cada \ncategor\u00eda dividiendo del total marginal por el n\u00famero total de casos. Adem\u00e1s, \nen el caso hipot\u00e9tico de que fueran la s dos variables independientes, la \nprobabilidad conjunta de cada casilla ser\u00eda el producto  de estas probabilidades, \ny multiplicada por el n\u00famero total de ca sos tendr\u00edamos el valor esperado en \ncada casilla. Eij= t(ti/t)(t\u2019j/t) \nAs\u00ed, por ejemplo, para la combinaci\u00f3n av i\u00f3n ligero y retardo nulo, tendr\u00edamos: \n74.51 934934117\n934413\n, = == = nulo retardo ligero tipoN  \nRepitiendo el mismo proceso para el  resto de casillas, tenemos: \nCap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 68 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nPor lo tanto a partir de dichos datos pode mos plantearnos la hip\u00f3tesis nula H0: \nlas variables retardo y categor\u00eda son i ndependientes. Calculando el estad\u00edstico \nque acumula las desviaciones  cuadr\u00e1ticas divididas por los valores esperados \ntenemos: \n\u2211\u2211\n==\u2212 =1\n12\n12 2/) (p\nip\njij ij ij E O E \u03c7    (22) \n \ny evaluamos la probabilidad del estad\u00edstic o mediante la funci\u00f3n Chi-cuadrado. \nTomando 3x2 grados de liber tad, tenemos que el valor de corte al 95% para \nrechazar ser\u00eda de 12.59 (ver siguiente Figura).  \n \nFigura 7:  Test Chi-Cuadrado. \n \nSin embargo, con los valores observados, tenemos que la desviaci\u00f3n es 44,91, \nque para una distribuci\u00f3n Chi-cuadrado de 6 grados de libertad tiene una \nprobabilidad de aparecer de 4, 87e-8, lo que nos permite rechazar con mucha \nevidencia la hip\u00f3tesis de independencia y concluir una clara dependencia entre \nlas variables. \nEl ejemplo siguiente aplica la misma t\u00e9cnica para determinar la \ninterdependencia entre la intenci\u00f3n de voto y el sexo en una poblaci\u00f3n dada: \n\u03b1\n\u03c72Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 69 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nRelaciones num\u00e9ricas-num\u00e9r icas: regresi\u00f3n lineal \n\u2022 Permite identificar relaciones en tre variables num\u00e9ricas y construir \nmodelos de regresi\u00f3n: 1 variable sa lida y m\u00faltiples entradas num\u00e9ricas \n\u2022 Se consideran relaciones de una variable de salida (dependiente) con \nm\u00faltiples variables de entrada (independientes) \nEjemplo: regresi\u00f3n lineal de 1 variable   \nA\u00f1o Renta Consumo consumo E\n1970 1959,75 1751,87 1683,473374\n1971 2239,09 1986,35 1942,433251972 2623,84 2327,9 2299,11261\n1973 3176,06 2600,1 2811,043671\n1974 3921,6 3550,7 3502,190468\n1975 4624,7 4101,7 4153,993607\n1976 5566,02 5012,6 5026,63666\n1977 6977,84 6360,2 6335,452914\n1978 8542,51 7990,13 7785,967518\n1979 9949,9 9053,5 9090,676976\n1980 11447,5 10695,4 10479,014881981 13123,04 12093,8 12032,31062\n1982 15069,5 12906,27 13836,76054\n1983 16801,6 15720,1 15442,48976\n1984 18523,5 17309,7 17038,76316\n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 70 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda a1 a0\n0,927041871 -133,296932Estimaci\u00f3n Lineal\n \nnta a a ConsumoE Re*1 0+ =  \ndependencia consumo\n02000400060008000100001200014000160001800020000\n0 5000 10000 15000 20000\nrentaconsumosConsumo\nconsumo E\n \n \nEjemplo: regresi\u00f3n lineal de 2 variables  \nx1 x2 y Valor\nSuperficie Antig \u00fcedad Valor predicho\n310 20 106.287 Euro s 109.180 Euros\n333 12 107.784 Euro s 112.283 Euros\n356 33 113.024 Euro s 108.993 Euros\n379 43 112.275 Euro s 108.128 Euros\n402 53 104.042 Euro s 107.262 Euros\n425 23 126.497 Euro s 115.215 Euros\n448 99 94.311 Euro s 99.800 Euros\n471 34 106.961 Euro s 115.469 Euros\n494 23 122.006 Euro s 119.233 Euros\n517 55 126.497 Euro s 113.518 Euros\n540 22 111.527 Euro s 122.132 Euros Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 71 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda a2 a1 a0\n-220,444829 58,2271936 95538,7217Estimaci\u00f3n Lineal\n \nAntig\u00fcedad*2a Superficie*1a0a Valor + + =  \n020000400006000080000100000120000140000valor (euros)\n10 20 30 40 50 60 70 80 90 100 110\n310\n333\n356\n379\n402\n425\n448\n471\n494\n517\n540\nantig\u00fcedad (a)superficie (m2)valores predichos\n020000400006000080000100000120000140000\n10 30 50 70 90\n310\n356\n402\n448\n494\n540\n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 72 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Evaluaci\u00f3n del modelo de regresi\u00f3n \nAn\u00e1lisis de validez del modelo asumido: \n\u2022 Medidas de \u201cparecido\u201d entre variable de salida estimada y real, influencia \nde variables de entrada \n\u2013 Factor de Correlaci\u00f3n \n\u2013 Error de predicci\u00f3n \n\u2013 Error en coeficientes \n \n\u2022 An\u00e1lisis de \u201ccalidad\u201d del modelo \n\u2013 Hip\u00f3tesis de significativi dad de par\u00e1metros: t-Student \n\u2013 Hip\u00f3tesis de ausencia de relaci\u00f3n: F de Fisher-Snedecor \n Factor de correlaci\u00f3n Factor de correlaci\u00f3n entre datos y predicciones: \n() ()\n\u2211 \u2211\u2211 \u2211\u2211\n= == ==\n= =\u2212 = \u2212 == \u2212 \u2212 =\nN\njjN\njjn\njj yn\njj yn\njj j\nyy\ny\nNy y\nNyy y S y y Sy Vary Varyy Covy yy y\nSSyy Corr\n1 112\n12\n\u02c61\u02c6\n1,\u02c61\u02c6; ;\u02c6 \u02c6)()\u02c6(),\u02c6() )(\u02c6 \u02c6(1),\u02c6(\n \n \n  En general, se puede hacer factores de correlaci\u00f3n entre cualquier par de \nvariables num\u00e9ricas: indica el  grado de relaci\u00f3n lineal existente \n Matriz de Covarianza Muestra de vectores aleatorios: \n\u2022 Matriz de covarianzas: \n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 73 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2211\n==n\niiX\nn 11\u02c6GG\u00b5 \n\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n= \u2212 \u2212 =\u2211\n=\n) var( ), cov() var( ), cov(), cov( ), cov( ) var(\n)\u02c6)(\u02c6(1\u02c6\n11 2 12 1 2 1 1\n1\nI In\nit\ni i X\nx xxx xxxx xx x\nX XnC\n\"# % #\"\nGGGG\nG \u00b5 \u00b5\n \n  \n\u2022 La matriz de correlaciones es similar, normalizada \n \n \nError de Predicci\u00f3n \n() ( )2\n12\n1\u02c6 \u2211 \u2211\n= == \u2212 =n\njjn\njj jyy Error \u03b5\n \n bajo la hip\u00f3tesis de que los datos yi  tengan la misma varianza sy, sean \nindependientes, y que el modelo lineal sea adecuado: \n2)1(y n Error \u03c3\u2212\u2248 \nError en coeficientes? Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 74 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda yt t\nAt tH HH yH HH A \u03b5 \u03b5G GG G1 1] [ ; ] [\u2212 \u2212= =  \nrelaci\u00f3n errores en predicci\u00f3n y en coeficientes estimados: \n \n1 2\n222\n] [10\n\u2212=\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n= HH Ct\ny\nAAA\nA\nI\u03c3\n\u03c3\u03c3\u03c3\n\"\"#% ## #\"\"\n \n \nEl error en los coeficientes depende de  \n\u2022 error en y, sy2 \n\u2022 recorrido de datos X: matriz H \n  \n \n \nSignificatividad de par\u00e1metros \n\u2022 Dadas las  varianzas de par\u00e1metros {s2A1,\u2026s2AF} y los propios valores \nestimados, son significativos los par\u00e1metros? xy\n5,33 8,15\n5,65 7,847,27 9,338,05 10,078,66 11,608,80 11,48\n8,89 11,89\n8,98 11,129,35 12,019,82 12,01xy\n1,32 3,671,68 4,664,69 7,574,99 7,486,98 9,668,80 11,51\n10,01 12,02\n15,01 17,4717,10 19,8219,67 21,94\n0,005,0010,0015,0020,0025,00\n0,00 5,00 10,00 15,00 20,00 25,000,005,0010,0015,0020,00\n0,00 5,00 10,00 15,00 20,00Rango: [5,10] \n\u03c3y=1 \n\u03c3A0=0.6 \n\u03c3A1=0.07 \n \n  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 75 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ? ,...,\n11\nFAF\nAA A\n\u03c3 \u03c3 \n \n \n \n \n \n\u2022 Si hay pocos datos: en vez de estad\u00ed stica normal, t-Student con n-F-1 \ngrados de libertad \n\u2022 Posibilidad de extender el m odelo y analizarlo: ej: dependencia \ncuadr\u00e1tica, ver si son significativos nuevos t\u00e9rminos \n Validez del modelo: an\u00e1lisis de varianza \n\u2022 Permite rechazar o no la hip\u00f3tesis de que no existe relaci\u00f3n entre \nvariables (relaci\u00f3n debida al azar, correlaci\u00f3n nula) \n() ()()2\n12\n12\n1\u02c6 \u02c6 \u2211 \u2211 \u2211\n= = =\u2212 + \u2212 = \u2212N\njjN\njjN\njj yy yy yy\n \n \n \n\u2022 Estad\u00edstico  \n)1 /(/\n\u2212\u2212=InSRISEF \n distribuci\u00f3n: F de Snedecor: F(n1, n2)  \n grados de libertad: I, n-I-1 -4 -3 -2 -1 0 1 x N(0,1) \u03b1/2 Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 76 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n   \n2.4.  T\u00e9cnicas cl\u00e1sicas de clasificaci\u00f3n y \npredicci\u00f3n \n \n\u2022 Modelado de datos con atributos num\u00e9ricos para su aplicaci\u00f3n a \nClasificaci\u00f3n. Generalizaci\u00f3n \n\u2022 Datos representados como vect ores de atributos num\u00e9ricos: \npatrones \n\u2022 Clases: {C1, ..., CM} \n\u2022 Muestras:E= \n} ...,  ,..., ,..., , ...,,{)( )(\n1)2( )2(\n1)1( )1(\n12 1M\nnM\nn nMX X X XX XGGGGGG\n \n\u2013 Tama\u00f1o:  \n\u2211\n==M\njjn n\n1 \n\u2022 Para cada clase, Ci, hay ni pat rones, cada uno con I atributos:   para \ncada clase Ci: \n} ...,, {)( )(\n1i\nni\niX XGG\n \ni\ni\nIji\nj\ni\nj n j\nxx\nX ,...,1 ;\n)()(\n1\n)(=\n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=#G \n \n)g(C\u02c6               } ,...,{ :(.)1\nX XC C C R gMI\nG G\n= \uf8e7\u2192\uf8e7= \uf8e7\u2192\uf8e7\n \n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 77 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Funci\u00f3n discriminante de cada clase:  \n \n \n \n   \n\u2022 Propiedad deseable para el dise\u00f1o de gi(.): sobre el conjunto de \nentrenamiento, cada patr\u00f3n de la clase Ci tiene un valor m\u00e1ximo con el \ndiscriminante gi(.):  \nii\nj k\nM ki\nj i n j Xg Xg ,...,1 )}, ({ max ) ()(\n,...,1)(=\u2200 =\n=G G\n \n \nFronteras de decisi\u00f3n  )(1XgG\n)(2XgG\n)(XgMGXGMax(.) C\u02c6Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 78 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n \n  \n lineales Xgij :)(G\n051015202530\n0 5 10 15 20 25 30\nX1X2++\n++\n+ +\n+++\n++ ++ +\n123\ng13\ng12g23\ns cuadratica Xgij :)(G\n051015202530\n0 5 10 15 20 25 30\nX1X2++\n++\n+ +\n+++\n++++ +\n123g13g12 g23\ng12Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 79 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nClasificaci\u00f3n con Regresi\u00f3n Lineal: 1 \n\u2022 Para cada clase se define la funci\u00f3n de pertenencia gi:  \n\uf8f3\uf8f2\uf8f1\n\u2209\u2208=ii\niCXCXXg GGG\n;0;1)(  \n \n\u2022 Se construye una funci\u00f3n lineal que \u201caproxime\u201d gi: \n()\n()\n()\n()it\ni it\ni i\ntI\nntti\nnti\ni i yH HH A\nXXXX\ny\nIiG G\nG##GG##G\n##\nG1\n)()1(\n1)()(\n1\n] [          ;\n1111\nH                 \n0011\n\u2212=\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n= \n \n \n\u2022 Hay que \u201caprender\u201d M funciones gi \n \n \n\u2022 Otra opci\u00f3n: para cada par de cl ases, funci\u00f3n frontera gij:  \n\uf8f4\uf8f3\uf8f4\uf8f2\uf8f1\n\u2208 \u2212\u2208 +=\nji\nijCXCXXg GGG\n;1;1)( \n \n\u2022 Funciones lineales para todos los pares: Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 80 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ()\n()\n()\n()it\nij ijt\nij ij\ntj\nntjti\nnti\nij ij yH HH A\nXXXX\ny\nji G G\nG##GG##G\n##\nG1\n)()(\n1)()(\n1\n] [          ;\n1111\nH                 \n1111\n\u2212=\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n\u2212\u2212++\n= \n\u2022 Hay que \u201caprender\u201d M(M-1)/2 pares gij fronteras posibles \n \n2.4.1. Clasificaci\u00f3n bayesiana \naplicaci\u00f3n de modelos estad\u00edsticos  \n\u2022 Clasificaci\u00f3n con modelo de estructura probabil\u00edstica conocida \n    Clases: {C1, ..., CM}. Se conoce a priori: \n\u2013 Probabilidades de clase: P(Ci) \n\u2013 Distribuciones de probabilidad condicionadas (par\u00e1metros \nconstantes) \n)(), ,..., ()| ,..., ( )| ,...,(\n1 11 1\nii I Ii I I I i I X\nCPCx X x XPCx X x XP Cx xF\n\u2264 \u2264= \u2264 \u2264 = G\n \n \n \n\u2013 densidad \nIi I X\ni I Xx xCx xFCx xf\u2202 \u2202\u2202=...)| ,...,()| ,...,(\n11\n1G\nG \n \n \nEj.: distribuci\u00f3n normal multivariada Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 81 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n\u2022 Par\u00e1metros: vector de medias y matriz covarianzas \n()\n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=\n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n=\uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\u2212 \u2212 \u2212 =\u2212\n22\n1 11\n2/\n2 11 21\n;) ( ) (21exp\n21)(\nF n FF\nx xx xxxx xx x\nnt\nn\nSxS x\nSxf\n\u03c3 \u03c3 \u03c3\u03c3 \u03c3 \u03c3\n\u00b5\u00b5\n\u00b5\u00b5 \u00b5\n\u03c0\n\"####\"\n#GGGGG G\n \n \n\u2022 Ejemplo \n\uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\n\u2212\u2212= \uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\u2212=2166 21;530S \u00b5G\n \n \n \n  Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 82 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nTeorema de Bayes ap licado a clasificaci\u00f3n  \n)()()|()|(\nXfCpCXfXCPi i\ni GGG\n= \n\u2022 Probabilidad a posteriori: es la probabilidad de que el patr\u00f3n tenga clase \nCi: \n)|( X CPiG\n \n \n\u2022 Probabilidad a priori: P(Ci) es la probabilidad total de cada clase \n\u2022 Verosimilitud: \n)|(iCXfG\n \n \n\u2022   : es la distribuci\u00f3n de Ci aplicada a  \n\u2022 Densidad total: \n) () |( ...)()|( )(1 1 M M CP CXf CPCXf XfG GG\n++ =  \n \nCriterio de clasificaci\u00f3n MAP: \n{} { } )()|( )|( )(i i i CpCXf\nim\u00e1ximo XCP\nim\u00e1ximo X ClaseG G G\n= = \n \n\u2013 funci\u00f3n discriminante de Ci: propor cional a su prob a posteriori: \n \n)()|( )(i i i CpCXf XgGG\n=\n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 83 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2013 la clase es la de aquella que maximiza el discriminante \n  Clasificaci\u00f3n bayesiana y distrib. normal  \n\u2022 Distribuciones condicionales gaussi anas. Para cada clase Ci hay una \nfunci\u00f3n discriminante de par\u00e1m etros mij, sij, j=1...I \n()2\n12\n2 12//) (21\n... 2)(log))|()( log()(ijF\niij j\nFi i ini\ni i i xCPCxfCP xg \u03c3 \u00b5\n\u03c3 \u03c3\u03c3 \u03c0\u2211\n=\u2212 \u2212 = =G G\n \n\u2022 Par\u00e1metros de distribuci\u00f3n condicionada a cada clase \n\u2022 Regiones de decisi\u00f3n:   \n\u2013 Funciones cuadr\u00e1ticas (hip\u00e9rbolas) dadas por diferencias: \n)( )( )( xg xg xgj i ijGGG\u2212 =  \n \n\u2013 Si  son iguales, y diagonales: regiones lineales (caso particular) \n      Resumen clasificador  bayesiano num\u00e9rico \n \n\u2022 Algoritmo: \n\u2022 Estimar par\u00e1metros de cada clase Ci (entrenamiento) Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 84 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda i ii\nni\ni C X XC\ni, } ...,,{:)( )(\n1 \u00b5\uf8e7\u2192\uf8e7GG\n \n\u2211\n==in\nji\nj i xn 1)( 1\u02c6GG\u00b5 \n\u2211\n=\u2212 =in\nji i\nii xnC\n12) (1\u00b5G \n\u2022 Estimar probabilidad de cada clase \n\u2211\n== =M\niii\ni n nNnCP\n1; )(\u02c6 \n\u2022 Obtener regiones de decisi\u00f3n: gij(.) \nClasificaci\u00f3n Bayesiana con Atributos Nominales Atributos nominales con valores discretos \n\u2013 Ai={V1,...,Vni}: atributo con ni valores posibles \n\u2013 Pasamos de densidades a probabi lidades: probabilidad a priori: \np(Ai=Vj|Ck)? \n\u2013 Estimaci\u00f3n \u201ccontando\u201d el n\u00famero de casos: \nkj i k\nC  clase de e de \u00baV Acon  C  clase de e de \u00ba)| (jemplos njemplos nCV Apk j i== =  \n \n\u2022 Simplificaci\u00f3n: independencia de at ributos (\u201cNaive Bayes\u201d): la \nprobabilidad conjunta de varios at ributos se pone como producto \n)| (*...*| (*)| ( )|() ,..., , (\n2 2 1 12 2 1 1\nk I I k k k iI I i\nCV Ap CV Ap CV Ap CXpV A V AV A X\n= = = == = = =\n \n\u2022 Clasificaci\u00f3n:  \n)()(*)| (*...*)| (*)| ()()(*)|()|(\n2 2 1 1\nik k F F k kik k i\ni k\nXpCp CV Ap CVAp CVApXpCp CXpXCp\n= = == =\n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 85 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nEjemplo con atri butos nominales \n  \n \n \n\u2022 Ej.: (salario=poco, cliente=si, edad=adulto, hijos=tres) \n)(/ 0141.0)(/20/8*8/4*8/3*8/3*8/4)(/)(*)| (*)| (*)| (*)| ()| ()(/ 0083.0)(/20/12*12/3*12/6*12/8*12/2)(/)(*)| (*)| (*)| (*)| ()|(\nXip XipXp NOp NO treshp NO adultoep NOsicp NO pocospX NOpXip XipXp SIp SI treshp SI adultoep SIsicp SI pocospXSIp\niiii\n== = = = ==== = = = ==\n \n Atributos sin valores \n\u2022 Si el ejemplo a clasificar no tiene un atributo, simplemente se omite. \n\u2013 Ej.: ( salario=poco, cliente=si, edad=?, hijos=3 )  \n SALARIO CLIENTE EDAD HIJOS CR\u00c9DITO\nPoco S\u00ed Joven Uno NO \nMucho Si Joven Uno SI \nMucho Si Joven Uno SI \nPoco Si Joven Uno NO \nMucho Si Joven Dos SI \nPoco Si Joven Dos NO \nMucho Si Adulto Dos SI \nMucho Si Adulto Dos SI \nPoco No Adulto Dos NO \nMucho Si Adulto Dos SI \nMedio No Adulto Tres NO \nMucho Si Adulto Dos SI \nMedio Si Adulto Dos SI \nMedio No Adulto Tres NO \nMedio No Adulto Dos SI \nMucho No Mayor Tres NO \nPoco No Mayor Tres SI \nPoco No Mayor Tres SI \nMucho No Mayor Tres NO \nMucho No Mayor Tres SI \n p(SI)   = 12/20 \np(NO) = 8/20  \n \nSalarioCr\u00e9dito No S\u00ed \nPoco 4/8 2/12 \nMucho 2/8 8/12 \nMedio 2/8 2/12 \n \nClienteCr\u00e9dito No S\u00ed \nS\u00ed 3/8 8/12 \nNo 5/8 4/12 \n \nEdad Cr\u00e9dito No S\u00ed \nJoven 3/8 3/12 \nAdulto 3/8 6/12 \nMayor 2/8 3/12 \n \nHijos Cr\u00e9dito No S\u00ed \nUno 2/8 2/12 \nDos 2/8 7/12 \nTres 4/8 3/12 Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 86 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda )(/ 0375.0)(/20/8*8/4*8/3*8/4)(/)(*)| (*)| (*)| ()| ()(/ 0167.0)(/20/12*12/3*12/8*12/2)(/)(*)| (*)| (*)| ()|(\nXip XipXp NOp NO treshp NOsicp NO pocospX NOpXip XipXp SIp SI treshp SIsicp SI pocospXSIp\niiii\n== = = ==== = = ==\n \n \n   \n\u2022 Si hay faltas en la muestra de entrenamiento, no cuentan en la \nestimaci\u00f3n de probabilidades de ese atributo \n Faltas en atributo EDAD    \n \n \nAtributos no representados. Ley m  \n\u2022 Problema: con muestra poco repres entativa, puede ocurrir que en \nalguna clase, un valor de atri buto no aparezca: p(Ai=Vj|Ck)=0 SALARIO CLIENTE EDAD HIJOS CR\u00c9DITO \nPoco S\u00ed Joven Uno NO \nMucho Si Joven Uno SI \nMucho Si Joven Uno SI \nPoco Si ? Uno NO \nMucho Si ? Dos SI \nPoco Si ? Dos NO \nMucho Si ? Dos SI \nMucho Si Adulto Dos SI \nPoco No Adulto Dos NO \nMucho Si Adulto Dos SI \nMedio No Adulto Tres NO \nMucho Si Adulto Dos SI \nMedio Si Adulto Dos SI \nMedio No Adulto Tres NO \nMedio No Adulto Dos SI \nMucho No Mayor Tres NO \nPoco No Mayor Tres SI \nPoco No Mayor Tres SI \nMucho No Mayor Tres NO \nMucho No Mayor Tres SI \n  \nSalarioCr\u00e9dito No S\u00ed \nPoco 4/8 2/12 \nMucho 2/8 8/12 \nMedio 2/8 2/12 \n \nClienteCr\u00e9dito No S\u00ed \nS\u00ed 3/8 8/12 \nNo 5/8 4/12 \n \nEdad Cr\u00e9dito No S\u00ed \nJoven 1/6 2/10 \nAdulto 3/6 5/10 \nMayor 2/6 3/10 p(SI)   = \n12/20 \np(NO) = 8/20  \n \nHijos Cr\u00e9dito No S\u00ed \nUno 2/8 2/12 \nDos 2/8 7/12 \nTres 4/8 3/12 Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 87 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2013 Cualquier ejemplo X con Ai=Vj generar\u00e1 P(Ck|X)=0, \nindependientemente de lo s otros atributos! \n\u2022 Se suele modificar la estimaci\u00f3n de las probabilidades a priori con un \nfactor que elimina los ceros.  \n\u2013 Ej.: P(Edad|Cr\u00e9dito=NO)= \n\uf8fe\uf8fd\uf8fc\n\uf8f3\uf8f2\uf8f1\n82: ,83: ,83: Mayor Adulto Joven  \n \n\u2013 Ley m:  \n\uf8fe\uf8fd\uf8fc\n\uf8f3\uf8f2\uf8f1\n++\n++\n++ \u00b5\u00b5\n\u00b5\u00b5\n\u00b5\u00b5\n83/ 2: ,83/ 3: ,83/ 3: Mayor Adulto Joven  \n \n\u2013 A veces simplemente se inicializ an las cuentas a 1 en vez de 0: \n\uf8fe\uf8fd\uf8fc\n\uf8f3\uf8f2\uf8f1\n++\n++\n++\n3812: ,3813: ,3813: Mayor Adulto Joven \n \n Atributos mixtos \n\u2022 Independencia de atributos (\u201cNaive Bayes\u201d) \n)C| V A(p*...*)C|V A(p*)C|V A(p)C|X(p\nk F F k 2 2 k 1 1k i\n= = ==\n \n\u2013 Atributos discretos: probabilidade s a priori con cada clase Ck \nkj i k\nC  clase de e de \u00baV Acon  C  clase de e de \u00ba\n)| (\njemplos njemplos n\nCV Apk j i=\n= =  \n \n\u2013 Atributos continuos: densidades de clase Ck: normales de \npar\u00e1metros mk, sk Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 88 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\n\uf8f0\uf8ee\n\u03c3\u00b5\u2212\n\u2212\n\u03c3\u03c0= \u2192 =\n2\nik2\nikj\nikkj A kj i\n) V(\n21exp\n21)C|V(f)C|VA(pi\n \n \nEjemplo con atributos mixtos \n \n \n   \n\u2022 Ej.: (salario=700, cliente=si, edad=adulto, hijos=3) \n   SALARIO CLIENTE EDAD HIJOS CR\u00c9DITO \n525 S\u00ed Joven 1 NO \n2000 Si Joven 1 SI \n2500 Si Joven 1 SI \n470 Si Joven 1 NO \n3000 Si Joven 2 SI \n510 Si Joven 2 NO \n2800 Si Adulto 2 SI \n2700 Si Adulto 2 SI \n550 No Adulto 2 NO \n2600 Si Adulto 2 SI \n1100 No Adulto 3 NO \n2300 Si Adulto 2 SI \n1200 Si Adulto 2 SI \n900 No Adulto 3 NO \n800 No Adulto 2 SI \n800 No Mayor 3 NO \n1300 No Mayor 3 SI \n1100 No Mayor 3 SI \n1000 No Mayor 3 NO \n4000 No Mayor 3 SI \n p(SI)   = \n12/20 \np(NO) = 8/20\n \nHijos Cr\u00e9dito No S\u00ed \nMedia 2.25 2.08 \nDesv Est\u00e1ndar 0.89 0.67  \nEdad Cr\u00e9dito No S\u00ed \nJoven 3/8 3/12 \nAdulto 3/8 6/12 \nMayor 2/8 3/12  \nClienteCr\u00e9dito No S\u00ed \nS\u00ed 3/8 8/12 \nNo 5/8 4/12  \nSalarioCr\u00e9dito No S\u00ed \nMedia 732 2192\nDesv Est\u00e1ndar 249 942 Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 89 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda )(/5 81.2)(/1*20/8*89.0)25.23(\n21exp\n89.021*8/3*8/3*249)732 700(\n21exp\n24921)(/)(*)|3(*)| (*)| (*)|700()| ()(/6 61.5)(/1*20/12*67.0)08.23(\n21exp\n67.021*12/6*12/8*942) 2192 700(\n21exp\n94221)(/)(*)|3(*)| (*)| (*)|700()|(\n22\n2222\n22\nXip eXPXp NOp NO hf NO adultoep NOsicp NO sfX NOpXip eXPXp SIp SI hf SI adultoep SIsicp SI sfXSIp\nii H Siii H Si\n\u2212 == \uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee \u2212\u2212 \uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee \u2212\u2212= = = = ==\u2212 == \uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee \u2212\u2212 \uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee \u2212\u2212= = = = ==\n\u03c0 \u03c0\u03c0 \u03c0\n \n Clasificaci\u00f3n con costes \n\u2022 MAP proporciona clasificaci\u00f3n con m\u00ednima prob. de Error \n\u2013 Coste de decisi\u00f3n           :  prob. Error total=  \n\u2022 Con frecuencia los costes son as im\u00e9tricos, y unos errores son m\u00e1s \ngraves que otros. Matriz de costes  \n \n \n \n \n\u2022 Costes de cada decisi\u00f3n. Criterio  de m\u00ednimo coste medio: dada una \ndecisi\u00f3n, promedio los costes de cada equivocaci\u00f3n y su coste: \n)|( )|( )|( cos)|( )|( )|( cos)|( )|( )|( cos\n2 23 1 13 33 32 1 12 23 31 2 21 1\nXCpc XCpc XDteXCpc XCpc XDteXCpc XCpc XDte\nG G GG G GG G G\n+ =+ =+ =\n \uf8f7\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\uf8ec\n\uf8ed\uf8eb\n000\n32 3123 2113 12\nc cc cc c Clase  \nreal Clasificado comoCap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 90 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ejemplo de clasificaci\u00f3n con costes \n \n\u2022 Clasificaci\u00f3n de setas con dos atributos, (X, Y) y tres categor\u00edas: \nVenenosa, Mal sabor, comestible : {V, MS, C } \n \n \n \n       \n2.4.2. Regresi\u00f3n Lineal \nLa regresi\u00f3n lineal [DOB90] es la forma m\u00e1s simple de regresi\u00f3n, ya que en \nella se modelan los datos usando una l\u00ednea recta. Se caracteriza, por tanto, por la utilizaci\u00f3n de dos variables, una aleatoria, y (llamada variable respuesta), que es \nfunci\u00f3n lineal de otra variable aleatoria, x (llamada variable predictora), form\u00e1ndose la \necuaci\u00f3n 2.13. -30 -20 -10 0 10 20 30 40 50-50-40-30-20-100102030\n-30 -20 -10 0 10 20 30 40 50-50-40-30-20-100102030\uf8f7\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\uf8ec\n\uf8ed\uf8eb\n0 1 110 0 11000 10000 Clase  Clasificado \nV MS C \nV \nMS \nC []\n[]\n[] \uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee= \u2212 =\uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\n\u2212\u2212= =\uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\n\u2212\u2212= \u2212 \u2212=\n51 4545 51;20 20 :71 4040 71;55 :71 5050 71;5 5 :\n3 32 21 1\nC MSC CC V\nttt\n\u00b5\u00b5\u00b5\nVC \nMS\nVC \nMS\nM\u00ednimo \nerror M\u00ednimo \ncoste Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 91 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda bxay +=  Ec. 2.13 \nEn esta ecuaci\u00f3n la variaci\u00f3n de y se asume que es constante, y a y b son los \ncoeficientes de regresi\u00f3n que especifican la intersecci\u00f3n con el eje de ordenadas, y la \npendiente de la recta,  respectivamente. Estos coeficientes se calculan utilizando el \nm\u00e9todo de los m\u00ednimos cuadrados [PTVF96] que minimizan el error entre los datos reales y la estimaci\u00f3n de la l\u00ednea. Dados s ejemplos de datos en forma de puntos (x\n1, \ny1), (x2, x2),..., (xs, ys), entonces los coeficientes de la regresi\u00f3n pueden estimarse \nseg\u00fan el m\u00e9todo de los m\u00ednimos cuadrados con las ecuaciones 2.14 y 2.15. \n2\nxxy\nSSb=  Ec. 2.14 \nbx-ya=  Ec. 2.15 \nEn la ecuaci\u00f3n 2.14, Sxy es la covarianza de x e y, y Sx2 la varianza de x. \nTambi\u00e9n es necesario saber cu\u00e1n buena es la recta de regresi\u00f3n construida. Para ello, se emplea el coeficiente de regresi\u00f3n (ecuaci\u00f3n 2.16), que es una medida del ajuste de la muestra. \n2\ny2\nx2\nxy 2\nSSSR=  Ec. 2.16 \nEl valor de R2 debe estar entre 0 y 1. Si se acerca a 0 la recta de regresi\u00f3n no \ntiene un buen ajuste, mientras que si se acerca a 1 el ajuste es \u201cperfecto\u201d. Los \ncoeficientes a y b a menudo proporcionan buenas aproximaciones a otras ecuaciones \nde regresi\u00f3n complicadas. \n \nEn el ejemplo siguiente, para una muestra de 35 marcas de cerveza, se estudia \nla relaci\u00f3n entre el grado de alcohol de las cervezas y su contenido cal\u00f3rico. y se representa un peque\u00f1o conjunto de datos.  \n \n \nFigura 2.1: Regresi\u00f3n lineal simple.  \n Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 92 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El eje vertical muestra el n\u00famero de calor\u00edas (por cada tercio de litro) y el \nhorizontal el contenido de alcohol (expresado en porcentaje). La nube de puntos es la \nrepresentaci\u00f3n  de los datos de la muestra, y la recta es el resultado de la regresi\u00f3n \nlineal aplicando el  ajuste de los m\u00ednimos cuadrados. En los siguientes apartados se mostrar\u00e1n dos tipos de regresiones que ampl\u00edan la regresi\u00f3n lineal simple. \n \n\u2022 Regresi\u00f3n Lineal M\u00faltiple \nLa regresi\u00f3n Lineal M\u00faltiple [PTVF96] es una extensi\u00f3n de regresi\u00f3n lineal que \ninvolucra m\u00e1s de una variable predictora, y permite que la variable respuesta y sea \nplanteada como una funci\u00f3n lineal de un vector multidimensional. El modelo de regresi\u00f3n m\u00faltiple para n variables predictoras ser\u00eda como el que se muestra en la \necuaci\u00f3n 2.17. \nnn 22 11 0 xb xbxbby ++ + + = ...  Ec. 2.17 \nPara encontrar los coeficientes bi se plantea el modelo en t\u00e9rminos de \nmatrices, como se muestra en la ecuaci\u00f3n 2.18. \n\uf8f7\uf8f7\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\uf8ec\uf8ec\n\uf8ed\uf8eb=\nmn m11n 211n 11\nz zz zz z\nZ\n\"# #\"\"\n; \n\uf8f7\uf8f7\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\uf8ec\uf8ec\n\uf8ed\uf8eb=\nm21\nyyy\nY#; \n\uf8f7\uf8f7\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\uf8ec\uf8ec\n\uf8ed\uf8eb=\nn21\nbbb\nB# Ec. 2.18 \nEn la matriz Z, las filas representan los m ejemplos disponibles para calcular la \nregresi\u00f3n, y las columnas los n atributos que formar\u00e1n parte de la regresi\u00f3n. De esta \nforma, zij ser\u00e1 el valor que toma en el ejemplo i el atributo j. El vector Y est\u00e1 formado \npor los valores de la variable dependiente para cada uno de los ejemplos, y el vector B \nes el que se desea calcular, ya que se corresponde con los par\u00e1metros desconocidos \nnecesarios para construir la regresi\u00f3n lineal m\u00faltiple. Representando con XT la matriz \ntraspuesta de X y con X-1 la inversa de la matriz X, se calcular\u00e1 el vector B mediante la \necuaci\u00f3n 2.19. \n() YZZZ BT1T\u2212=  Ec. 2.19 \nPara determinar si la recta de regresi\u00f3n lineal m\u00faltiple est\u00e1 bien ajustada, se \nemplea el mismo concepto que en el caso de la regresi\u00f3n lineal simple: el coeficiente de regresi\u00f3n. En este caso, se utilizar\u00e1 la ecuaci\u00f3n 2.20. \n() ( )\n()\u2211=\u2212\u2212=m\n1i2\niT T\n2\nyyZB-Y ZB-Y1 R  Ec. 2.20 \nAl igual que en el caso de la regresi\u00f3n simple, el valor de R2 debe estar entre 0 \ny 1, siendo 1 el indicador de ajuste perfecto.  \n \nUna vez explicado el modo b\u00e1sico por el que se puede obtener una recta de \nregresi\u00f3n m\u00faltiple para un conjunto de ejemplos de entrenamiento, a continuaci\u00f3n se muestra, en la figura 2.11, un ejemplo concreto en el que se muestra el proceso. Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 93 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 2.2: Ejemplo de obtenci\u00f3n de una Regresi\u00f3n Lineal M\u00faltiple.  \n \nTal y como se muestra en la figura 2.11, en un primer momento se obtienen, a \npartir de los ejemplos de entrenamiento, las matrices Z e Y, siendo el objetivo la matriz \nB. En el segundo paso se calcula los valores de dicha matriz, que ser\u00e1n los \ncoeficientes en la regresi\u00f3n. Por \u00faltimo, en un tercer paso se comprueba si la recta generada tiene un buen ajuste o no. En este caso, como se muestra en la misma figura, el ajuste es magn\u00edfico, dado que el valor de R\n2 es muy cercano a 1. Por \u00faltimo, \nen este ejemplo no se ha considerado el t\u00e9rmino independiente, pero para que se \nobtuviese bastar\u00eda con a\u00f1adir una nueva columna a la matriz Z con todos los valores a \n1. \n \n \nSelecci\u00f3n de Variables \n \nAdem\u00e1s del proceso anterior para la generaci\u00f3n de la regresi\u00f3n lineal, se suele \nrealizar un procedimiento estad\u00edstico que seleccione las mejores variables predictoras, ya que no todas tienen la misma importancia, y reducir su n\u00famero har\u00e1 que \ncomputacionalmente mejore el tiempo de respuesta del modelo. Los procesos que se \nsiguen para la selecci\u00f3n de variables predictoras son b\u00e1sicamente dos: eliminaci\u00f3n \nhacia atr\u00e1s  [backward elimination], consistente en obtener la regresi\u00f3n lineal para \ntodos los par\u00e1metros e ir eliminando uno a uno los menos importantes; y selecci\u00f3n \nhacia delante  [fordward selection], que consiste en generar una regresi\u00f3n lineal simple \n(con el mejor par\u00e1metro, esto es, el m\u00e1s correlacionado con la variable a predecir) e ir \na\u00f1adiendo par\u00e1metros al modelo. Hay un gran n\u00famero de estad\u00edsticos que permiten \nseleccionar los par\u00e1metros, y a modo de ejemplo se comentar\u00e1 el basado en el criterio Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 94 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda de informaci\u00f3n Akaike  [AKA73], que se basa en la teor\u00eda de la informaci\u00f3n y cuya \nformulaci\u00f3n se muestra en la ecuaci\u00f3n 2.21. \n()2pLlog2 AIC + \u00d7\u2212=  Ec. 2.21 \nEn esta ecuaci\u00f3n L es la verosimilitud  [likelihood] y p el n\u00famero de variables \npredictorias. Aplicado a la regresi\u00f3n, el resultado ser\u00eda el que se muestra en las \necuaciones 2.22 y 2.23. \n( )2p MSElogm AIC + \u00d7=  Ec. 2.22 \n( )\nmyy\nMSEm\n1i2\ni i \u2211=\u2212\n=\u02c6\n Ec. 2.23 \nEn la ecuaci\u00f3n 2.22, m es el n\u00famero de ejemplos disponibles, y MSE  es el \nerror cuadr\u00e1tico medio [mean squared error] del modelo, tal y como se define en la \necuaci\u00f3n 2.23. En esta ecuaci\u00f3n yi  es el valor de la clase para el ejemplo i e iy\u02c6 el \nvalor que la regresi\u00f3n lineal da al ejemplo i. En la pr\u00e1ctica algunas herramientas no \nutilizan exactamente la ecuaci\u00f3n 2.22, sino una aproximaci\u00f3n de dicha ecuaci\u00f3n. \n \n\u2022 Regresi\u00f3n Lineal Ponderada Localmente \n \nOtro m\u00e9todo de predicci\u00f3n num\u00e9rica es la regresi\u00f3n lineal ponderada \nlocalmente [Locally weighted linear regresi\u00f3n]. Con este m\u00e9todo se generan modelos \nlocales durante el proceso de predicci\u00f3n dando m\u00e1s peso a aquellos ejemplares de entrenamiento m\u00e1s cercanos al que hay que predecir. Dicho de otro modo, la construcci\u00f3n del clasificador consiste en el almacenamiento de los ejemplos de \nentrenamiento, mientras que el proceso de validaci\u00f3n o de clasificaci\u00f3n de un ejemplo \nde test consiste en la generaci\u00f3n de una regresi\u00f3n lineal espec\u00edfica, esto es, una \nregresi\u00f3n lineal en la que se da m\u00e1s peso a aquellos ejemplos de entrenamiento cercanos al ejemplo a clasificar. De esta forma, este tipo de regresi\u00f3n est\u00e1 \u00edntimamente relacionado con los algoritmos basados en ejemplares. Para utilizar este tipo de regresi\u00f3n es necesario decidir un esquema de ponderaci\u00f3n para los ejemplos \nde entrenamiento, esto es, decidir cu\u00e1nto peso se le va a dar a cada ejemplo de \nentrenamiento para la clasificaci\u00f3n de un ejemplo de test. Una medida usual es ponderar el ejemplo de entrenamiento con la inversa de la distancia eucl\u00eddea entre dicho ejemplo y el de test, tal y como se muestra en ecuaci\u00f3n 2.24. \nijid11\u03c9+=  Ec. 2.24 \nEn esta ecuaci\u00f3n \u03c9i es el peso que se le otorgar\u00e1 al ejemplo de entrenamiento \ni para clasificar al ejemplo j, y dij ser\u00e1 la distancia eucl\u00eddea de i con respecto a j. \n \nM\u00e1s cr\u00edtico que la elecci\u00f3n del m\u00e9todo para ponderar es el \u201cpar\u00e1metro de \nsuavizado\u201d que se utilizar\u00e1 para escalar la funci\u00f3n de distancia, esto es, la distancia \nser\u00e1 multiplicada por la inversa de este par\u00e1metro. Si este par\u00e1metro es muy peque\u00f1o s\u00f3lo los ejemplos muy cercanos recibir\u00e1n un gran peso, mientras que si es demasiado grande los ejemplos muy lejanos podr\u00edan tener peso. Un modo de asignar un valor a este par\u00e1metro es d\u00e1ndole el valor de la distancia del k-\u00e9simo vecino m\u00e1s cercano al Cap\u00edtulo  2  An\u00e1lisis Estad\u00edstico Mediante Excel \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 95 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ejemplo a clasificar. El valor de k depender\u00e1 del ruido  de los datos. Cuanto m\u00e1s ruido, \nm\u00e1s grande deber\u00e1 ser k. Una ventaja de este m\u00e9todo de estimaci\u00f3n es que es capaz \nde aproximar funciones no lineales. Adem\u00e1s, se puede actualizar el clasificador \n(modelo incremental), dado que \u00fanicamente ser\u00eda necesario a\u00f1adirlo al conjunto de entrenamiento. Sin embargo, como el resto de algoritmos basado en ejemplares, es lento. \n \n   Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 96 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nCap\u00edtulo 3. T\u00e9cnicas de \nMiner\u00eda de Datos basadas \nen Aprendizaje Autom\u00e1tico \n \n \n3.1.  T\u00e9cnicas de Miner\u00eda de Datos \nComo ya se ha comentado, las t\u00e9cnicas de Miner\u00eda de Datos (una etapa dentro \ndel proceso completo de KDD [FAYY96]) intentan obtener patrones o modelos a partir de los datos recopilados. Decidir si los modelos obtenidos son \u00fatiles o no suele requerir una valoraci\u00f3n subjetiva por parte del usuario. Las t\u00e9cnicas de Miner\u00eda de Datos se clasifican en dos grandes categor\u00edas: supervisadas o predictivas y no \nsupervisadas o descriptivas [WI98].    \n \n    \n   Num\u00e9rico \n  Clustering Conceptual \n   Probabilistico \n No supervisadas   \n    \n  Asociaci\u00f3n A Priori \n    \n    \n    \nT\u00e9cnicas   Regresi\u00f3n \n  Predicci\u00f3n \u00c1rboles de Predicci\u00f3n \n   Estimador de N\u00facleos Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 97 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda     \n    \n Supervisadas  Tabla de Decisi\u00f3n \n   \u00c1rboles de Decisi\u00f3n \n   Inducci\u00f3n de Reglas \n   Bayesiana \n  Clasificaci\u00f3n Basado en Ejemplares \n   Redes de Neuronas \n   L\u00f3gica Borrosa \n   T\u00e9cnicas Gen\u00e9ticas \n    \nFigura 3.1: T\u00e9cnicas de la Miner\u00eda de Datos \nUna t\u00e9cnica constituye el enfoque conceptual para extraer la informaci\u00f3n de los \ndatos, y, en general es implementada por varios algoritmos. Cada algoritmo \nrepresenta, en la pr\u00e1ctica, la manera de desarrollar una determinada t\u00e9cnica paso a paso, de forma que es preciso un entendimiento de alto nivel de los algoritmos para saber cual es la t\u00e9cnica m\u00e1s apropiada para cada problema. Asimismo es preciso entender los par\u00e1metros y las caracter\u00edsticas de los algoritmos para preparar los datos a analizar.  \n \nLas predicciones se utilizan para prever  el comportamiento futuro de alg\u00fan tipo \nde entidad mientras que una descripci\u00f3n puede ayudar a su comprensi\u00f3n. De hecho, los modelos predictivos pueden ser descriptivos (hasta donde sean comprensibles por personas) y los modelos descriptivos pueden emplearse para realizar predicciones. De \nesta forma, hay algoritmos o t\u00e9cnicas que pueden servir para distintos prop\u00f3sitos, por \nlo que la figura anterior \u00fanicamente representa para qu\u00e9 prop\u00f3sito son m\u00e1s utilizadas las t\u00e9cnicas. Por ejemplo, las redes de neuronas pueden servir para predicci\u00f3n, clasificaci\u00f3n e incluso para aprendizaje no supervisado. \n \nEl aprendizaje inductivo no supervisado estudia el aprendizaje sin la ayuda del \nmaestr o; es decir, se aborda el aprendizaje sin supervisi\u00f3n, que trata de ordenar los \nejemplos en una jerarqu\u00eda seg\u00fan las regularidades en la distribuci\u00f3n de los pares atributo-valor sin la gu\u00eda del atributo especial clase. \u00c9ste es el proceder de los \nsistemas que realizan clustering conceptual y de los que se dice tambi\u00e9n que \nadquieren nuevos conceptos. Otra posibilidad contemplada para estos sistemas es la \nde sintetizar conocimiento cualitativo o cuant itativo, objetivo de los sistemas que llevan \na cabo tareas de descubrimiento.  \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 98 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En el aprendizaje inductivo supervisado existe un atributo especial, \nnormalmente denominado clase, presente en todos los ejemplos que especifica si el \nejemplo pertenece o no a un cierto concepto, que ser\u00e1 el objetivo del aprendizaje. El \natributo clase normalmente toma los valores + y -, que significan la pertenencia o no del ejemplo al concepto que se trata de aprender; es decir, que el ejemplo ejemplifica positivamente al concepto -pertenece al conc epto- o bien lo ejemplifica negativamente \n-que no pertenece al concepto. Mediante una generalizaci\u00f3n del papel del atributo clase, cualquier atributo puede desempe\u00f1ar ese papel, convirti\u00e9ndose la clasificaci\u00f3n \nde los ejemplos seg\u00fan los valores del atributo en cuesti\u00f3n, en el objeto del \naprendizaje. Expresado en una forma breve, el objetivo del aprendizaje supervisado es: a partir de un conjunto de ejemplos, denominados de entrenamiento, de un cierto dominio D de ellos, construir criterios para determinar el valor del atributo clase en un \nejemplo cualquiera del dominio. Esos criterios est\u00e1n basados en los valores de uno o \nvarios de los otros pares (atributo; valor) que intervienen en la definici\u00f3n de los \nejemplos. Es sencillo transmitir esa idea al caso en el que el atributo que juega el papel de la clase sea uno cualquiera o con m\u00e1s de dos valores. Dentro de este tipo de aprendizaje se pueden distinguir dos grandes grupos de t\u00e9cnicas: la predicci\u00f3n y la clasificaci\u00f3n [WK91]. A continuaci\u00f3n se presentan las principales t\u00e9cnicas (supervisadas y no supervisadas) de miner\u00eda de datos \n \n3.2.  Clustering.  (\u201cSegmentaci\u00f3n\u201d) \nTambi\u00e9n llamada agrupamiento, permite la identificaci\u00f3n de tipolog\u00edas o grupos \ndonde los elementos guardan gran similitud entre s\u00ed y muchas diferencias con los de otros grupos. As\u00ed se puede segmentar el colectivo de clientes, el conjunto de valores e \u00edndices financieros, el espectro de observaciones astron\u00f3micas, el conjunto de zonas \nforestales, el conjunto de empleados y de sucu rsales u oficinas, etc. La segmentaci\u00f3n \nest\u00e1 teniendo mucho inter\u00e9s desde hace ya tiempo dadas las importantes ventajas que aporta al permitir el tratamiento de grandes colectivos de forma pseudoparticularizada, \nen el m\u00e1s id\u00f3neo punto de equilibrio entre el tratamiento individualizado y aquel totalmente masificado. \n \nLas herramientas de segmentaci\u00f3n se basan en t\u00e9cnicas de car\u00e1cter \nestad\u00edstico, de empleo de algoritmos matem\u00e1ticos, de generaci\u00f3n de reglas y de redes neuronales para el tratamiento de registros. Para otro tipo de elementos a agrupar o segmentar, como texto y documentos, se usan t\u00e9cnicas de reconocimiento de conceptos. Esta t\u00e9cnica suele servir de punto de partida para despu\u00e9s hacer un \nan\u00e1lisis de clasificaci\u00f3n sobre los clusters . \n \nLa principal caracter\u00edstica de esta t\u00e9cnica es la utilizaci\u00f3n de una medida de \nsimilaridad que, en general, est\u00e1 basada en los atributos que describen a los objetos, y se define usualmente por proximidad en un espacio multidimensional. Para datos \nnum\u00e9ricos, suele ser preciso preparar los datos antes de realizar data mining sobre \nellos, de manera que en primer lugar se someten a un proceso de estandarizaci\u00f3n. Una de las t\u00e9cnicas empleadas para conseguir la normalizaci\u00f3n de los datos es utilizar la medida z ( z-score ) que elimina las unidades de los datos. Esta medida, z, es la que \nse muestra en la ecuaci\u00f3n 2.1, donde \u00b5\nf es la media de la variable f y \u03c3f la desviaci\u00f3n \nt\u00edpica de la misma. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 99 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ff if\nif\u03c3\u00b5 x\nz\u2212\n=  Ec. 2.1 \nEntre las medidas de similaridad destaca la distancia eucl\u00eddea, ecuaci\u00f3n 2.2. \n()\u2211 \u2212 =\n=n\n1l2\njl il j i x x x d(x ),  Ec. 2.2 \nHay varios algoritmos de clustering . A continuaci\u00f3n se exponen los m\u00e1s \nconocidos. \n  \n3.2.1. Clustering Num\u00e9rico (k-medias) \nUno de los algoritmos m\u00e1s utilizados para hacer clustering es el k-medias (k-\nmeans) [MAC67], que se caracteriza por su sencillez. En primer lugar se debe especificar por adelantado cuantos clusters se van a  crear, \u00e9ste es el par\u00e1metro k, para lo cual se seleccionan k elementos aleatoriamente, que  representaran el centro o \nmedia de cada cluster. A continuaci\u00f3n cada una de las instancias, ejemplos, es \nasignada al centro del cluster m\u00e1s cercano de acuerdo con  la distancia Euclidea que le separa de \u00e9l. Para cada uno de los clusters as\u00ed construidos se calcula el centroide de todas sus instancias. Estos centroides son tomados como los nuevos centros de sus respectivos clusters. Finalmente se repite el proceso completo con los nuevos centros de los clusters. La iteraci\u00f3n contin\u00faa hasta que se repite la asignaci\u00f3n de los \nmismos ejemplos a los mismos clusters, ya que los puntos centrales de los clusters se \nhan estabilizado y permanecer\u00e1n invariables despu\u00e9s de cada iteraci\u00f3n. El algoritmo de k-medias es el siguiente: \n \n \n1. Elegir k ejemplos que act\u00faan como semillas ( k n\u00famero de \nclusters). \n2. Para cada ejemplo, a\u00f1adir ejemplo a la clase m\u00e1s similar. \n3. Calcular el centroide de cada clase, que pasan a ser las nuevas \nsemillas \n4. Si no se llega a un criterio de convergencia (por ejemplo, dos \niteraciones no cambian las clasificaciones de los ejemplos), volver \na 2. \n \nFigura 3.2: Pseudoc\u00f3digo del algoritmo de k-medias. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 100 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Para obtener los centroides, se calcula la media [mean] o la moda [mode] \nseg\u00fan se trate de atributos num\u00e9ricos o simb\u00f3licos. A continuaci\u00f3n, en la figura 2.3, se \nmuestra un ejemplo de clustering con el algoritmo k-medias. \n \nEn este caso se parte de un total de nueve ejemplos o instancias, se configura \nel algoritmo para que obtenga 3 clusters, y se inicializan aleatoriamente los centroides de los clusters a un ejemplo determinado. Una vez inicializados los datos, se comienza el bucle del algoritmo. En cada una de las gr\u00e1ficas inferiores se muestra un paso por el \nalgoritmo. Cada uno de los ejemplos se representa con un tono de color diferente que \nindica la pertenencia del ejemplo a un cluster determinado, mientras que los centroides siguen mostr\u00e1ndose como c\u00edrculos de mayor tama\u00f1o y sin relleno. Por ultimo el proceso de clustering finaliza en el paso 3, ya que en la siguiente pasada del algoritmo (realmente har\u00eda cuatro pasadas, si se configurara as\u00ed) ning\u00fan ejemplo cambiar\u00eda de \ncluster. \n \n \nFigura 3.3: Ejemplo de clustering con k-medias. \n \n \n3.2.2. Clustering Conceptual (COBWEB) \nEl algoritmo de k-medias se encuentra con un problema cuando los atributos no \nson num\u00e9ricos, ya que en ese caso la distancia entre ejemplares no est\u00e1 tan clara. Para resolver este problema Michalski [MS83] presenta la noci\u00f3n de clustering conceptual, que utiliza para justificar la necesidad de un clustering cualitativo frente al \nclustering cuantitativo, basado en la vecindad entre los elementos de la poblaci\u00f3n. En \neste tipo de clustering una partici\u00f3n de los datos es buena si cada clase tiene una Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 101 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda buena interpretaci\u00f3n conceptual (modelo cognitivo de jerarqu\u00edas). Una de las \nprincipales motivaciones de la categorizaci\u00f3n de un conjunto de ejemplos, que \nb\u00e1sicamente supone la formaci\u00f3n de conceptos, es la predicci\u00f3n de caracter\u00edsticas de \nlas categor\u00edas que heredar\u00e1n sus subcategor\u00edas. Esta conjetura es la base de COBWEB [FIS87]. A semejanza de los humanos, COBWEB forma los conceptos por agrupaci\u00f3n de ejemplos con atributos similares. Representa los clusters como una distribuci\u00f3n de probabilidad sobre el espacio de los valores de los atributos, generando un \u00e1rbol de clasificaci\u00f3n jer\u00e1rquica en el que los nodos intermedios definen \nsubconceptos. El objetivo de COBWEB es hallar un conjunto de clases o clusters \n(subconjuntos de ejemplos) que maximice la utilidad de la categor\u00eda (partici\u00f3n del conjunto de ejemplos cuyos miembros son clases). La descripci\u00f3n probabil\u00edstica se basa en dos conceptos: \n \n\u2022 Predicibilidad: Probabilidad condicional de que un suceso tenga un cierto \natributo dada la clase, P(A i=Vij|Ck). El mayor de estos valores corresponde al \nvalor del atributo m\u00e1s predecible y es el de los miembros de la clase (alta similaridad entre los elementos de la clase). \n\u2022 Previsibilidad: Probabilidad condicional de que un ejemplo sea una instancia de \nuna cierta clase, dado el valor de un atributo particular, P(C\nk|Ai=Vij). Un valor \nalto indica que pocos ejemplos de las otras clases comparten este valor del \natributo, y el valor del atributo de mayor probabilidad es el de los miembros de \nla clase (baja similaridad interclase). \n \nEstas dos medidas, combinadas mediante el teorema de Bayes, \nproporcionan una funci\u00f3n que eval\u00faa la utilidad de una categor\u00eda (CU), que se muestra en la ecuaci\u00f3n 2.3. \n() () ( )\nnV AP C|V AP CP\nCUn\n1ki j2\nij i\nij2\nk ij i k \u2211 \uf8fa\n\uf8fb\uf8f9\n\uf8ef\n\uf8f0\uf8ee\u2211\u2211 = \u2212 \u2211\u2211 =\n== Ec. 2.3 \nEn esta ecuaci\u00f3n n es el n\u00famero de clases y las sumas se extienden a todos \nlos atributos Ai y sus valores Vij en cada una de las n clases Ck. La divisi\u00f3n por n sirve \npara incentivar tener clusters con m\u00e1s de un elemento. La utilidad de la categor\u00eda mide el valor esperado de valores de atributos que pueden ser adivinados a partir de la partici\u00f3n sobre los valores que se pueden adivinar sin esa partici\u00f3n. Si la partici\u00f3n no \nayuda en esto, entonces no es una buena partici\u00f3n. El \u00e1rbol resultante de este \nalgoritmo cabe denominarse organizaci\u00f3n probabil\u00edstica o jer\u00e1rquica de conceptos. En la figura 2.4 se muestra un ejemplo de \u00e1rbol que se podr\u00eda generar mediante COBWEB. En la construcci\u00f3n del \u00e1rbol, incrementalmente se incorpora cada ejemplo al mismo, donde cada nodo es un concepto probabil\u00edstico que representa una clase de objetos. COBWEB desciende por el \u00e1rbol buscando el mejor lugar o nodo para cada \nejemplo. Esto se basa en medir en cu\u00e1l se tiene la mayor ganancia de utilidad de \ncategor\u00eda. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 102 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nFigura 3.4: Ejemplo de \u00e1rbol generado por COBWEB. \n \nSin embargo, no se puede garantizar que se genere este \u00e1rbol, dado que el \nalgoritmo es sensible al orden en que se introduzcan los ejemplos. En cuanto a las etiquetas de los nodos, \u00e9stas fueron puestas a posteriori, coherentes con los valores de los atributos que determinan el nodo. Cuando COBWEB incorpora un nuevo ejemplo en el nodo de clasificaci\u00f3n, desciende a lo largo del camino apropiado, \nactualizando las cuentas de cada nodo, y llevando a cabo por medio de los diferentes \noperadores, una de las siguientes acciones: \n\u2022 Incorporaci\u00f3n: A\u00f1adir un nuevo ejemplo a un nodo ya existente. \n\u2022 Creaci\u00f3n de una nueva disyunci\u00f3n: Crear una nueva clase. \n\u2022 Uni\u00f3n: Combinar dos clases en una sola. \n\u2022 Divisi\u00f3n: Dividir una clase existente en varias clases. \n \nLa b\u00fasqueda, que se realiza en el espacio de conceptos, es por medio de un \nheur\u00edstico basado en el m\u00e9todo de escalada gracias a los operadores de uni\u00f3n y \ndivisi\u00f3n. En la figura 2.5 se muestra el resultado de aplicar cada una de estas operaciones. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 103 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 3.5: Operaciones de COBWEB. \n \n \n1. Nuevo Ejemplo: Lee un ejemplo e. Si no hay m\u00e1s ejemplos, \nterminar. \n2. Actualiza ra\u00edz. Actualiza el c\u00e1lculo de la ra\u00edz. \n3. Si la ra\u00edz es hoja, entonces: Expandir en dos nodos hijos y \nacomodar en cada uno de ellos un ejemplo; volver a 1. \n4. Avanzar hasta el siguiente nivel: Aplicar la funci\u00f3n de \nevaluaci\u00f3n a varias opciones para determinar, mediante la \nf\u00f3rmula de utilidad de una categor\u00eda, el mejor (m\u00e1xima CU) lugar \ndonde incorporar el ejemplo en el nivel siguiente de la jerarqu\u00eda. En las opciones que se evaluar\u00e1n se considerar\u00e1 \u00fanicamente el nodo actual y sus hijos y se elegir\u00e1 la mejor\nopci\u00f3n de las siguientes: \na. A\u00f1adir e a un nodo que existe (al mejor hijo) y, si esta \nopci\u00f3n resulta ganadora, comenzar de nuevo el proceso de avance hacia el siguiente nivel en ese nodo hijo. \nb. Crear un nuevo nodo conteniendo \u00fanicamente a e y, si esta \nopci\u00f3n resulta ganadora, volver a 1. \nc. Juntar los dos mejores nodos hijos con eincorporado al \nnuevo nodo combinado y, si esta opci\u00f3n resulta ganadora, comenzar el nuevo proceso de avanzar hacia el siguiente nivel en ese nuevo nodo. \nd. Dividir el mejor nodo, reemplazando este nodo con sus \nhijos y, si esta opci\u00f3n resulta ganadora, aplicar la funci\u00f3n de evaluaci\u00f3n para incorporar een los nodos \noriginados por la divisi\u00f3n. \n \nFigura 3.6: Algoritmo de COBWEB.  Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 104 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nEl algoritmo se puede extender a valores num\u00e9ricos usando distribuciones \ngaussianas, ecuaci\u00f3n 2.4. De esta forma, el sumatorio de probabilidades es ahora \ncomo se muestra en la ecuaci\u00f3n 2.5. \n()()\n22\u03c32\u00b5x\ne\n\u03c32\u03c01xf\u2212\u2212\n=  Ec. 2.4 \n() () \u222b = \u2194 \u2211 =\u221e+\n\u221e\u2212\nii2\nij2\nij i\u03c0\u03c321dxxf VAP  Ec. 2.5 \nPor lo que la ecuaci\u00f3n de la utilidad de la categor\u00eda quedar\u00eda como se muestra \nen la ecuaci\u00f3n 2.6. \n() \u2211 \u2211 =\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n= ii ikn\n1kk\u03c31\n\u03c31\n\u03c021CPk1CU -  Ec. 2.6 \n \n3.2.3. Clustering Probabil\u00edstico (EM) \nLos algoritmos de clustering estudiados hasta el momento presentan ciertos \ndefectos entre los que destacan la dependencia que tiene el resultado del orden de los ejemplos y la tendencia de estos algoritmos al sobreajuste [overfitting]. Una aproximaci\u00f3n estad\u00edstica al problema del cl ustering resuelve estos problemas. Desde \neste punto de vista, lo que se busca es el grupo de clusters m\u00e1s probables dados los \ndatos. Ahora los ejemplos tienen ciertas probabilidades de pertenecer a un cluster. La base de este tipo de clustering se encuentra en un modelo estad\u00edstico llamado mezcla de distribuciones [finite mixtures]. Cada distribuci\u00f3n representa la probabilidad de que un objeto tenga un conjunto particular de pares atributo-valor, si se supiera  que es \nmiembro de ese cluster. Se tienen k distribuciones de probabilidad que representan los \nk clusters. La mezcla m\u00e1s sencilla se tiene cuando los atributos son num\u00e9ricos con \ndistribuciones gaussianas. Cada distribuci\u00f3n (normal) se caracteriza por dos par\u00e1metros: la media ( \u00b5) y la varianza ( \u03c3\n2). Adem\u00e1s, cada distribuci\u00f3n tendr\u00e1 cierta \nprobabilidad de aparici\u00f3n p, que vendr\u00e1 determinada por la proporci\u00f3n de ejemplos \nque pertenecen a dicho cluster respecto del n\u00famero total de ejemplos. En ese caso, si \nhay k clusters, habr\u00e1 que calcular un total de 3k-1 par\u00e1metros: las k medias, k \nvarianzas y k-1 probabilidades de la distribuci\u00f3n dado que la suma de probabilidades \ndebe ser 1, con lo que conocidas k-1 se puede determinar la k-\u00e9sima . \n \nSi se conociera el cluster al que pertenece, en un principio, cada uno de los \nejemplos de entrenamiento ser\u00eda muy sencillo obtener los 3k-1 par\u00e1metros necesarios \npara definir totalmente las distribuciones de dichos clusters, ya que simplemente se aplicar\u00edan las ecuaciones de la media y de la varianza para cada uno de los clusters. Adem\u00e1s, para calcular la probabilidad de cada una de las distribuciones \u00fanicamente se dividir\u00eda el n\u00famero de ejemplos de entrenamiento que pertenecen al cluster en cuesti\u00f3n entre el n\u00famero total de ejemplos de entrenamiento. Una vez obtenidos estos \npar\u00e1metros, si se deseara calcular la probabilidad de pertenencia de un determinado \nejemplo de test a cada cluster, simplemente se aplicar\u00eda el teorema de Bayes, ecuaci\u00f3n 2.54 a cada problema concreto, con lo que quedar\u00eda la ecuaci\u00f3n 2.7. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 105 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda P(x)p\u03c3\u00b5f(x;\nP(x)A)P(A)|P(xx)|P(AA A A ),= =  Ec. 2.7 \nEn esta ecuaci\u00f3n A es un cluster del sistema, x el ejemplo de test, pA la \nprobabilidad del cluster A y f(x;\u00b5 A,\u03c3A) la funci\u00f3n de la distribuci\u00f3n normal del cluster A, \nque se expresa con la ecuaci\u00f3n 2.4. Sin embargo, el problema es que no se sabe de qu\u00e9 distribuci\u00f3n viene cada dato y se desconocen los par\u00e1metros de las distribuciones. Por ello se adopta el procedimiento empleado por el algoritmo de clustering k-medias, \ny se itera. \n \nEl algoritmo EM ( Expectation Maximization ) empieza adivinando  los par\u00e1metros \nde las distribuciones (dicho de otro modo, se empieza adivinando  las probabilidades \nde que un objeto pertenezca a una clase) y, a continuaci\u00f3n, los utiliza para calcular las probabilidades de que cada objeto pertenezca a un cluster y usa esas probabilidades \npara re-estimar los par\u00e1metros de las probabilidades, hasta converger. Este algoritmo \nrecibe su nombre de los dos pasos en los que se basa cada iteraci\u00f3n: el c\u00e1lculo de las probabilidades de los grupos o los valores esperados de los grupos, mediante la ecuaci\u00f3n 2.7, denominado expectation ; y el c\u00e1lculo de los valores de los par\u00e1metros \nde las distribuciones, denominado maximization , en el que se maximiza la verosimilitud \nde las distribuciones dados los datos. \n \nPara estimar los par\u00e1metros de las distribuciones se tiene que considerar que \nse conocen \u00fanicamente las probabilidades de pertenencia a cada cluster, y no los clusters en s\u00ed. Estas probabilidades act\u00faan como pesos, con lo que el c\u00e1lculo de la media y la varianza se realiza con las ecuaciones 2.8 y 2.9 respectivamente. \n\u2211\u2211=\n==\nN\n1i iN\n1i ii\nAwxw\u00b5  Ec. 2.8 \n( )\n\u2211\u2211 \u2212=\n==\nN\n1i iN\n1i ii 2\nAw\u00b5xw\u03c3  Ec. 2.9 \nDonde N es el n\u00famero total de ejemplos del conjunto de entrenamiento y wi es \nla probabilidad de que el ejemplo i pertenezca al cluster A. La cuesti\u00f3n es determinar \ncu\u00e1ndo se finaliza el procedimiento, es decir en que momento se dejan de realizar \niteraciones. En el algoritmo k-medias  se finalizaba cuando ning\u00fan ejemplo de \nentrenamiento cambiaba de cluster en una iteraci\u00f3n, alcanz\u00e1ndose as\u00ed un \u201cpunto fijo\u201d [fixed point]. En el algoritmo EM es un poco m\u00e1s complicado, dado que el algoritmo tiende a converger pero nunca se llega a ning\u00fan punto fijo. Sin embargo, se puede ver \ncu\u00e1nto se acerca calculando la verosimilitud  [likelihood] general de los datos con esos \npar\u00e1metros, multiplicando las probabilidades de los ejemplos, tal y como se muestra en la ecuaci\u00f3n 2.10. \n() \u220f\n=\uf8f7\uf8f8\uf8f6\uf8ec\uf8ed\uf8eb\u2211N\n1icluster\nji jsjxPp |   Ec. 2.10 \nEn esta ecuaci\u00f3n j representa cada uno de los clusters del sistema, y pj la \nprobabilidad de dicho cluster. La verosimilitud  es una medida de lo \u201cbueno\u201d que es el \nclustering, y se incrementa con cada iteraci\u00f3n del algoritmo EM. Se seguir\u00e1 iterando hasta que dicha medida se incremente un valor despreciable. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 106 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Aunque EM garantiza la convergencia, \u00e9sta puede ser a un m\u00e1ximo local, por \nlo que se recomienda repetir el proceso varias veces, con diferentes par\u00e1metros \niniciales para las distribuciones. Tras estas repeticiones, se pueden comparar las \nmedidas de verosimilitud  obtenidas y escoger la mayor de todas ellas. En la figura 2.7 \nse muestra un ejemplo de clustering probabil\u00edstico con el algoritmo EM. \n \n \nFigura 3.7: Ejemplo de clustering con EM.  \n \nEn este experimento se introducen un total de doscientos ejemplos que \nconstituyen dos distribuciones desconocidas para el algoritmo. Lo \u00fanico que conoce el \nalgoritmo es que hay dos clusters, dado que este dato se introduce como par\u00e1metro \nde entrada. En la iteraci\u00f3n 0 se inicializan los par\u00e1metros de los clusters a 0 (media, \ndesviaci\u00f3n t\u00edpica y probabilidad). En las siguientes iteraciones estos par\u00e1metros van tomando forma hasta finalizar en la iteraci\u00f3n 11, iteraci\u00f3n en la que finaliza el proceso, \npor el incremento de la medida de verosimilitud , tan s\u00f3lo del orden de 10\n-4. \n \n\u2022 Extensiones al algoritmo EM \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 107 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El modelo puede extenderse desde un atributo num\u00e9rico como se ha visto \nhasta el momento, hasta m\u00faltiples atributos, asumiendo independencia entre atributos. \nLas probabilidades de cada atributo se multiplican entre s\u00ed para obtener una \nprobabilidad conjunta para la instancia, tal y como se hace en el algoritmo naive  \nBayesiano. Tambi\u00e9n puede haber atributos correlacionados, en cuyo caso se puede modelar con una distribuci\u00f3n normal bivariable, en donde se utiliza una matriz de covarianza. En este caso el n\u00famero de par\u00e1metros crece seg\u00fan el cuadrado del n\u00famero de atributos que se consideren correlacionados entre s\u00ed, ya que se debe \nconstruir una matriz de covarianza. Esta escalabilidad en el n\u00famero de par\u00e1metros \ntiene serias consecuencias de sobreajuste. \n \nEn el caso de un atributo nominal con v posibles valores, se caracteriza \nmediante v valores num\u00e9ricos que representan la probabilidad de cada valor. Se \nnecesitar\u00e1n otros kv valores num\u00e9ricos, que ser\u00e1n las probabilidades condicionadas \nde cada posible valor del atributo con respecto a cada cluster. En cuanto a los valores desconocidos, se puede optar por varias soluciones: ignorarlo en el productorio de probabilidades; a\u00f1adir un nuevo valor a los posibles, s\u00f3lo en el caso de atributos nominales; o tomar la media o la moda del atributo, seg\u00fan se trate de atributos num\u00e9ricos o nominales. Por \u00faltimo, aunque se puede especificar el n\u00famero de \nclusters, tambi\u00e9n es posible dejar que sea el algoritmo el que determine \nautom\u00e1ticamente cu\u00e1l es el n\u00famero de clusters mediante validaci\u00f3n cruzada. \n \n3.3.  Reglas de Asociaci\u00f3n \nEste tipo de t\u00e9cnicas se emplea para establecer las posibles relaciones o \ncorrelaciones entre distintas acciones o sucesos aparentemente independientes; \npudiendo reconocer como la ocurrencia de un suceso o acci\u00f3n puede inducir o generar \nla aparici\u00f3n de otros [AIS93b]. Son utilizadas cuando el objetivo es realizar an\u00e1lisis \nexploratorios , buscando relaciones dentro del conjunto de datos. Las asociaciones \nidentificadas pueden usarse para predecir comportamientos, y permiten descubrir correlaciones y co-ocurrencias de eventos [AS94, AS94a, AS94b]. Debido a sus \ncaracter\u00edsticas, estas t\u00e9cnicas tienen una gran aplicaci\u00f3n pr\u00e1ctica en muchos campos \ncomo, por ejemplo, el comercial ya que son especialmente interesantes a la hora de comprender los h\u00e1bitos de compra de los clientes y constituyen un pilar b\u00e1sico en la concepci\u00f3n de las ofertas y ventas cruzada, as\u00ed como del \"merchandising\" [RMS98]. En otros entornos como el sanitario, estas herramientas se emplean para identificar factores de riesgo en la aparici\u00f3n o complicaci\u00f3n de enfermedades. Para su utilizaci\u00f3n \nes necesario disponer de informaci\u00f3n de cada uno de los sucesos llevados a cabo por \nun mismo individuo o cliente en un determinado per\u00edodo temporal. Por lo general esta forma de extracci\u00f3n de conocimiento se fundamenta en t\u00e9cnicas estad\u00edsticas [CHY96], como los an\u00e1lisis de correlaci\u00f3n y de variaci\u00f3n [BMS97]. Uno de los algoritmos mas utilizado es el algoritmo A priori, que se presenta a continuaci\u00f3n. \n \nAlgoritmo A Priori \n \nLa generaci\u00f3n de reglas de asociaci\u00f3n se logra bas\u00e1ndose en un \nprocedimiento de covering . Las reglas de asociaci\u00f3n son parecidas, en su forma, a las \nreglas de clasificaci\u00f3n, si bien en su lado derecho puede aparecer cualquier par o Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 108 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda pares atributo-valor . De manera que para encontrar ese tipo de reglas es preciso \nconsiderar cada posible combinaci\u00f3n de pares atributo-valor  del lado derecho. Para \nevaluar las reglas se emplean la medida del soporte [support], ecuaci\u00f3n 2.11, que \nindica el n\u00famero de casos, ejemplos, que cubre la regla y la confianza [confidence], ecuaci\u00f3n 2.12, que indica el n\u00famero de casos que predice la regla correctamente, y que viene expresado como el cociente entre el n\u00famero de casos en que se cumple la regla y el n\u00famero de casos en que se aplica, ya que se cumplen las premisas. \n() ( )BAP B A soporte \u2229 = \u21d2  Ec. 2.11 \n() ( )( )\n()APBAPA|BP B A confianza\u2229= = \u21d2  Ec. 2.12 \nLas reglas que interesan son \u00fanicamente aquellas que tienen su valor de \nsoporte muy alto, por lo que se buscan, independientemente de en qu\u00e9 lado aparezcan, pares atributo-valor  que cubran una gran cantidad de ejemplos. A cada par \natributo-valor  se le denomina item, mientras que a un conjunto de items se les \ndenomina item-sets . Por supuesto, para la formaci\u00f3n de item-sets  no se pueden unir \nitems  referidos al mismo atributo pero con distinto valor, dado que eso nunca se podr\u00eda \nproducir en un ejemplo. Se buscan item-sets  con un m\u00e1ximo soporte, para lo que se \ncomienza con item-sets  con un \u00fanico item. Se eliminan los item-sets  cuyo valor de \nsoporte sea inferior al m\u00ednimo establecido, y se combinan el resto formando item-sets  \ncon dos items . A su vez se eliminan aquellos nuevos item-sets  que no cumplan con la \ncondici\u00f3n del soporte, y al resto se le a\u00f1adir\u00e1 un nuevo item, formando item-sets  con \ntres items . El proceso continuar\u00e1 hasta que ya no se puedan formar item-sets  con un \nitem m\u00e1s. Adem\u00e1s, para generar los item-sets  de un determinado nivel, s\u00f3lo es \nnecesario emplear los item-sets  del nivel inferior (con n-1 coincidencias, siendo n el \nn\u00famero de items  del nivel). Una vez se han obtenido todos los item-sets , se pasar\u00e1 a \nla generaci\u00f3n de reglas. Se tomar\u00e1 cada item-set  y se formar\u00e1n reglas que cumplan \ncon la condici\u00f3n de confianza . Debe tenerse en cuenta que un item-set  puede dar \nlugar a m\u00e1s de una regla de asociaci\u00f3n, al igual que un item-set  tambi\u00e9n puede no dar \nlugar a ninguna regla.  \n \nUn ejemplo t\u00edpico de reglas de asociaci\u00f3n es el an\u00e1lisis de la cesta de la \ncompra [market-basket analysis]. B\u00e1sicamente consiste en encontrar asociaciones entre los productos que habitualmente compran  los clientes, para utilizarlas en el desarrollo de las estrategias mercadot\u00e9cnicas. En la figura 2.8 se muestra un ejemplo \nsencillo de obtenci\u00f3n de reglas de asociaci\u00f3n aplicado a este campo. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 109 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 3.8: Ejemplo de obtenci\u00f3n de reglas de asociaci\u00f3n A Priori.  \n \nEn esta imagen se muestra c\u00f3mo se forman los item-sets  a partir de los item-\nsets del nivel inferior, y c\u00f3mo posteriormente se obtienen las reglas de asociaci\u00f3n a \npartir de los item-sets  seleccionados. Las reglas en negrita son las que se obtendr\u00edan, \ndado que cumplen con la confianza m\u00ednima requerida. El proceso de obtenci\u00f3n de las reglas de asociaci\u00f3n que se coment\u00f3 anteriormente se basa en el algoritmo que se muestran en la figura 2.9 (A priori, Agrawal et al. 94). \n  \n \n \n \n \n1. Genera todos los items-sets con un elemento. Usa \u00e9stos para \ngenerar los de dos elementos y as\u00ed sucesivamente. Se toman todos los posibles pares que cumplen con las medidas m\u00ednimas del \nsoporte. Esto permite ir eliminando posibles combinaciones ya \nque no todas se tienen que considerar. \n2. Genera las reglas revisando que cumplan con el criterio m\u00ednimo \nde confianza. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 110 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.9: Algoritmo de obtenci\u00f3n de reglas de asociaci\u00f3n A Priori. \n \nUna observaci\u00f3n interesante es que si una conjunci\u00f3n de consecuentes de una \nregla cumple con los niveles m\u00ednimos de soporte y confianza, sus subconjuntos \n(consecuentes) tambi\u00e9n los cumplen. Por el contrario, si alg\u00fan item no los cumple, no \ntiene caso considerar sus superconjuntos. Esto da una forma de ir construyendo reglas, con un solo consecuente, y a partir de ellas construir de dos consecuentes y as\u00ed sucesivamente. \n \n3.4.  La predicci\u00f3n \nEs el proceso que intenta determinar los valores de una o varias variables, a \npartir de un conjunto de datos. La predicci\u00f3n de valores continuos puede planificarse por las t\u00e9cnicas estad\u00edsticas de regresi\u00f3n [JAM85, DEV95, AGR96]. Por ejemplo, para predecir el sueldo de un graduado de la universidad con 10 a\u00f1os de experiencia de trabajo, o las ventas potenciales de un nuevo producto dado su precio. Se pueden resolver muchos problemas por medio de la regresi\u00f3n lineal, y puede conseguirse \ntodav\u00eda m\u00e1s aplicando las transformaciones a las variables para que un problema no \nlineal pueda convertirse a uno lineal. A continuaci\u00f3n se presenta una introducci\u00f3n intuitiva de las ideas de regresi\u00f3n lineal, m\u00faltiple, y no lineal, as\u00ed como la generalizaci\u00f3n a los modelos lineales. \nM\u00e1s adelante, dentro de la clasificaci\u00f3n, se estudiar\u00e1n varias t\u00e9cnicas de data \nmining que pueden servir para predicci\u00f3n num\u00e9rica. De entre todas ellas las m\u00e1s \nimportantes se presentaran en la clasificaci\u00f3n bayesiana, la basada en ejemplares y las redes de neuronas. A continuaci\u00f3n se mostrar\u00e1n un conjunto de t\u00e9cnicas que espec\u00edficamente sirven para la predicci\u00f3n.  \n3.4.1. Regresi\u00f3n no lineal.  \nEn muchas ocasiones los datos no muestran una dependencia lineal [FRI91]. \nEsto es lo que sucede si, por ejemplo, la variable respuesta depende de las variables independientes seg\u00fan una funci\u00f3n polin\u00f3mica, dando lugar a una regresi\u00f3n polin\u00f3mica que puede planearse agregando las condiciones polin\u00f3micas al modelo lineal b\u00e1sico. De est\u00e1 forma y aplicando ciertas transformaciones a las variables, se puede convertir \nel modelo no lineal en uno lineal que puede resolverse entonces por el m\u00e9todo de \nm\u00ednimos cuadrados. Por ejemplo consid\u00e9rese una relaci\u00f3n polin\u00f3mica c\u00fabica dada por: \ny = a + b\n1x + b 2 x2 + b 3 x3. Ec. 2.25 \n \nPara convertir esta ecuaci\u00f3n a la forma lineal, se definen las nuevas variables: \nx1=  x          x 2 = x2       x 3 =x3 Ec. 2.26 \nCon lo que la ecuaci\u00f3n anterior puede convertirse entonces a la forma lineal \naplicando los cambios de variables,  y resultando la ecuaci\u00f3n 2.27, que es resoluble por el m\u00e9todo de m\u00ednimos cuadrados Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 111 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda y = a + b 1 x1 + b 2 x 2  + b 3 x 3 Ec. 2.27 \n. No obstante, algunos modelos son especialmente no lineales como, por \nejemplo, la suma de t\u00e9rminos exponenciales y no pueden convertirse a un modelo \nlineal. Para estos casos, puede ser posible obtener las estimaciones del m\u00ednimo cuadrado a trav\u00e9s de c\u00e1lculos extensos en formulas m\u00e1s complejas. \n \n \nLos modelos lineales generalizados representan el fundamento te\u00f3rico en que \nla regresi\u00f3n lineal puede aplicarse para modelar las categor\u00edas de las variables dependientes. En los modelos lineales generalizados, la variaci\u00f3n de la variable y es \nuna funci\u00f3n del valor medio de y, distinto a la regresi\u00f3n lineal d\u00f3nde la variaci\u00f3n de y \nes constante. Los tipos comunes de modelos lineales generalizados incluyen regresi\u00f3n log\u00edstica y regresi\u00f3n del Poisson. La regresi\u00f3n log\u00edstica modela la probabilidad de alg\u00fan evento que ocurre como una funci\u00f3n lineal de un conjunto de variables independientes. Frecuentemente los datos exhiben una distribuci\u00f3n de Poisson y se modelan normalmente usando la regresi\u00f3n del Poisson. \n \nLos modelos lineales logar\u00edtmicos [PEA 88] aproximan las distribuciones de \nprobabilidad multidimensionales discretas, y pueden usarse para estimar el valor de probabilidad asociado con los datos de las c\u00e9lulas c\u00fabicas. Por ejemplo, suponiendo que se tienen los datos para los atributos ciudad, art\u00edculo, a\u00f1o, y ventas. En el m\u00e9todo \nlogar\u00edtmico lineal, todos los atributos deben ser categor\u00edas; por lo que los atributos \nestimados continuos (como las ventas) deben ser previamente discretizados.  \n \n3.4.2. \u00c1rboles de Predicci\u00f3n \nLos \u00e1rboles de predicci\u00f3n num\u00e9rica son similares a los \u00e1rboles de decisi\u00f3n, que \nse estudiar\u00e1n m\u00e1s adelante, excepto en que la clase a predecir es continua. En este \ncaso, cada nodo hoja almacena un valor de clase consistente en la media de las instancias que se clasifican con esa hoja, en cuyo caso estamos hablando de un \u00e1rbol \nde regresi\u00f3n , o bien un\n modelo lineal que predice el valor de la clase, en cuyo caso se \nhabla de \u00e1rbol de modelos . En el caso del algoritmo  M5 [WF00], se trata de obtener un \n\u00e1rbol de modelos, si bien se puede utilizar para obtener un \u00e1rbol de regresi\u00f3n, por ser \n\u00e9ste un caso espec\u00edfico de \u00e1rbol de modelos. \n \nMientras que en el caso de los \u00e1rboles de decisi\u00f3n se emplea la entrop\u00eda de \nclases para definir el atributo con el que dividir, en el caso de la predicci\u00f3n num\u00e9rica se emplea la varianza del error  en cada hoja. Una vez construido el \u00e1rbol que clasifica \nlas instancias se realiza la poda del mismo, tras lo cual, se obtiene para cada nodo \nhoja una constante en el caso de los \u00e1rboles de regresi\u00f3n o un plano de regresi\u00f3n en el caso de \u00e1rboles de modelos. En \u00e9ste \u00faltimo caso, los atributos que formar\u00e1n parte de la regresi\u00f3n ser\u00e1n aquellos que participaban en el sub\u00e1rbol que ha sido podado. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 112 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Al construir un \u00e1rbol de modelos y definir, para cada hoja, un modelo lineal con \nlos atributos del sub\u00e1rbol podado suele ser beneficioso, sobre todo cuando se tiene un \npeque\u00f1o conjunto de entrenamiento, realizar un proceso de suavizado  [smoothing] que \ncompense las discontinuidades que ocurren entre modelos lineales adyacentes. Este proceso consiste en: cuando se predice el valor de una instancia de test con el modelo lineal del nodo hoja correspondiente, este valor obtenido se filtra hacia atr\u00e1s hasta el nodo hoja, suavizando  dicho valor al combinarlo con el modelo lineal de cada nodo \ninterior por el que pasa. Un modelo que se suele utilizar es el que se muestra en la \necuaci\u00f3n 2.28. \nknkq npp'++=  Ec. 2.28\n \nEn esta ecuaci\u00f3n, p es la predicci\u00f3n que llega al nodo (desde abajo), p\u2019 es la \npredicci\u00f3n filtrada hacia el nivel superior, q el valor obtenido por el modelo lineal de \neste nodo, n es el n\u00famero de ejemplos que alcanzan el nodo inferior y k el factor de \nsuavizado. \n \nPara construir el \u00e1rbol se emplea como  heur\u00edstica el minimizar la variaci\u00f3n \ninterna de los valores de la clase dentro de cada subconjunto. Se trata de seleccionar \naquel atributo que maximice la reducci\u00f3n de la desviaci\u00f3n est\u00e1ndar de error (SDR, \n[standard deviation reduction]) con la f\u00f3rmula que se muestra en la ecuaci\u00f3n 2.29. \n\u2211\u2212 =\niii) SD(EEESD(E) SDR  Ec. 2.29\nEn esta ecuaci\u00f3n E es el conjunto de ejemplos en el nodo a dividir, Ej es cada \nuno de los conjuntos de ejemplos que resultan en la divisi\u00f3n en el nodo seg\u00fan el \natributo considerado, |E| es el n\u00famero de ejemplos del conjunto E y SD(E)  la \ndesviaci\u00f3n t\u00edpica de los valores de la clase en E. El proceso de divisi\u00f3n puede finalizar \nporque la desviaci\u00f3n t\u00edpica es una peque\u00f1a fracci\u00f3n (por ejemplo, el 5%) de la desviaci\u00f3n t\u00edpica del conjunto original de instancias o porque hay pocas instancias (por ejemplo, 2). \n \nEn la figura 2.12 se muestra un ejemplo de generaci\u00f3n del \u00e1rbol de predicci\u00f3n \ncon el algoritmo M5. Para ello se muestra en primer lugar los ejemplos de \nentrenamiento, en los que se trata de predecir los puntos que un jugador de baloncesto anotar\u00eda en un partido. \n \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 113 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 3.10: Ejemplo de generaci\u00f3n del \u00e1rbol de predicci\u00f3n con M5.  \n \nEn cada nodo del \u00e1rbol se muestra la desviaci\u00f3n t\u00edpica de los ejemplos de \nentrenamiento que inciden en el nodo ( SD(E) ) y la desviaci\u00f3n est\u00e1ndar del error para \nel atributo y el punto de corte que lo maximiza, por lo que es el seleccionado. Para \nobtener el atributo y el punto de corte se debe calcular la desviaci\u00f3n est\u00e1ndar del error \npara cada posible punto de corte. En este caso, la finalizaci\u00f3n de la construcci\u00f3n del \u00e1rbol ocurre porque no se puede seguir subdividiendo, ya que en cada hoja hay dos ejemplos (n\u00famero m\u00ednimo permitido). Por \u00faltimo, tras generar el \u00e1rbol, en cada hoja se a\u00f1ade la media de los valores de la clase de los ejemplos que se clasifican a trav\u00e9s de \ndicha hoja. Una vez se ha construido el \u00e1rbol se va definiendo, para cada nodo interior \n(no para las hojas para emplear el proceso de suavizado ) un modelo lineal, \nconcretamente una regresi\u00f3n lineal m\u00faltiple, tal y como se mostr\u00f3 anteriormente. \u00danicamente se emplean para realizar esta regresi\u00f3n aquellos atributos que se utilizan en el sub\u00e1rbol del nodo en cuesti\u00f3n. \n \nA continuaci\u00f3n se pasa al proceso de poda, en el que se estima, para cada \nnodo, el error esperado en el conjunto de test. Para ello, lo primero que se hace es calcular la desviaci\u00f3n de las predicciones del nodo con los valores reales de la clase para los ejemplos de entrenamiento que se clasifican por el mismo nodo. Sin embargo, dado que el \u00e1rbol se ha construido con estos ejemplos, el error puede infravalorarse, \ncon lo que se compensa con el factor \nv)(nv)(n \u2212 + , donde n es el n\u00famero de ejemplos \nde entrenamiento que se clasifican por el nodo actual y v es el n\u00famero de par\u00e1metros \ndel modelo lineal. De esta forma, la estimaci\u00f3n del error en un conjunto I de ejemplos \nse realizar\u00eda con la ecuaci\u00f3n 2.30. \nny-y\nv-nvnMAEv-nvne(I)Iii i\u2211\n\u2208\u00d7+= \u00d7+=\u02c6\n Ec. 2.30 \nEn la ecuaci\u00f3n 2.30, MAE  es el error medio absoluto [mean absolute error] del \nmodelo, donde yi es el valor de la clase para el ejemplo i y iy\u02c6 la predicci\u00f3n del modelo \npara el mismo ejemplo. Para podar el \u00e1rbol, se comienza por las hojas del mismo y se \nva comparando el error estimado para el nodo con el error estimado para los hijos del mismo, para lo cu\u00e1l se emplea la ecuaci\u00f3n 2.31. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 114 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda nde(d)|i|e(i)) e(sub\u00e1rbol+=  Ec. 2.31 \nEn la ecuaci\u00f3n 2.31, e(i) y e(d) son los errores estimados para los nodos hijo \nizquierdo y derecho, |x| el n\u00famero de ejemplos que se clasifica por el nodo x y n el \nn\u00famero de ejemplos que se clasifica por el nodo padre. Comparando el error estimado \npara el nodo con el error estimado para el sub\u00e1rbol, se decide podar si no es menor el error para el sub\u00e1rbol.  \n El proceso explicado hasta el momento sirve para el caso de que los atributos \nsean num\u00e9ricos pero, si los atributos son nomi nales ser\u00e1 preciso modificar el proceso: \nen primer lugar, se calcula el promedio de la clase en los ejemplos de entrenamiento \npara cada posible valor del atributo nominal, y se ordenan dichos valores de acuerdo a este promedio. Entonces, un atributo nominal con k posibles valores se transforma en \nk-1 atributos binarios. El i-\u00e9simo atributo binario tendr\u00e1, para un ejemplo dado, un 0 si \nel valor del atributo nominal es uno de los primeros i valores del orden establecido y un \n1 en caso contrario. Con este proceso se logra tratar los atributos nominales como \nnum\u00e9ricos. Tambi\u00e9n es necesario determinar c\u00f3mo se actuar\u00e1 frente a los atributos para los que faltan valores. En este caso, se modifica ligeramente la ecuaci\u00f3n 2.29 para llegar hasta la ecuaci\u00f3n 2.32. \n\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\n\uf8f0\uf8ee\u2212 = \u2211\niii) SD(EEESD(E)|E|cSDR  Ec. 2.32 \nEn esta ecuaci\u00f3n c es el n\u00famero de ejemplos con el atributo conocido. Una vez \nexplicadas las caracter\u00edsticas de los \u00e1rboles de predicci\u00f3n num\u00e9rica, se pasa a mostrar el algoritmo M5, cuyo pseudoc\u00f3digo se recoge en la figura 2.13. \n \nM5 (ejemplos) { \n SD = sd(ejemplos) \n Para cada atributo nominal con k-valores \n  convertir en k-1 atributos binarios \n ra\u00edz = nuevo nodo \n ra\u00edz.ejemplos = ejemplos \n Dividir(ra\u00edz) \n Podar(ra\u00edz) \n Dibujar(ra\u00edz) \n} \n \nDividir(nodo) { \n Si tama\u00f1o(nodo.ejemplos)<4 O sd(nodo.ejemplos)<=0.05*SD Entonces \n  nodo.tipo = HOJA \n Si no \n  nodo.tipo = INTERIOR \n  Para cada atributo \n   Para cada posible punto de divisi\u00f3n del atributo \n    calcular el SDR del atributo \n  nodo.atributo = atributo con mayor SDR \n  Dividir(nodo.izquierda) \n  Dividir(nodo.derecha) \n} \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 115 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Podar(nodo) { \n Si nodo = INTERIOR \n Podar(nodo.hijoizquierdo) \n Podar(nodo.hijoderecho) \n nodo.modelo = RegresionLinear(nodo) \n Si ErrorSubarbol(nodo) > Error(nodo) Entonces \n  nodo.tipo = HOJA \n} \n \nErrorSubarbol(nodo) { \n l = nodo.izquierda \n r = nodo.derecha \n Si nodo = INTERIOR Entonces \n  ErrorSubarbol = (tama\u00f1o(l.ejemplos)*ErrorSubarbol(l) + \ntama\u00f1o(r.ejemplos)*ErrorSubarbol(r))tama\u00f1o(nodo.ejemplos) \n Si no \n  ErrorSubarbol = error(nodo) \n} \nFigura 3.11: Pseudoc\u00f3digo del algoritmo M5.  \n \nLa funci\u00f3n RegresionLinear  generar\u00e1 la regresi\u00f3n correspondiente al nodo en \nel que nos encontramos. La funci\u00f3n error  evaluar\u00e1 el error del nodo mediante la \necuaci\u00f3n 2.31. \n \n3.4.3. Estimador de N\u00facleos \nLos estimadores de densidad de n\u00facleo [kernel density] son estimadores no \nparam\u00e9tricos. De entre los que destaca el conocido histograma, por ser uno de los m\u00e1s antiguos y m\u00e1s utilizado, que tiene ciertas deficiencias relacionadas con la \ncontinuidad que llevaron a desarrollar otras t\u00e9cnicas. El estimador de n\u00facleos  fue \npropuesto por Rosenblatt en 1956 y Parzen en 1962 [DFL96]. La idea en la que se basan los estimadores de densidad de n\u00facleo es la siguiente. Si X es una variable \naleatoria con funci\u00f3n de distribuci\u00f3n F y densidad f, entonces en cada punto de \ncontinuidad x de f se confirma la ecuaci\u00f3n 2.33. \n() ()() hxFhxF2h1lim f(x)0h \u2212 \u2212+ =\u2192  Ec. 2.33 \nDada una muestra X1,...,X n proveniente de la distribuci\u00f3n F, para cada h fijo, \nF(x+h)-F(x-h)  se puede estimar por la proporci\u00f3n de observaciones que est\u00e1n dentro \ndel intervalo (x-h, x+h) . Por lo tanto, tomando h peque\u00f1o, un estimador natural de la \ndensidad es el que se muestra en la ecuaci\u00f3n 2.34, donde #A es el n\u00famero de \nelementos del conjunto A. \n() {} hx h,-x XX2hn1(x)fi i hn, + \u2208 = : # \u02c6  Ec. 2.34 \nOtra manera de expresar este estimador es considerando la funci\u00f3n de peso w \ndefinida como se muestra en la ecuaci\u00f3n 2.35, de manera que el estimador de la \ndensidad f en el punto x se puede expresar como se expresa en la ecuaci\u00f3n 2.36. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 116 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n Ec. 2.35 \n\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb=\u2211\n= hX-xwh1\nn1(x)fin\n1ihn,\u02c6 Ec. 2.36 \nPero este estimador no es una funci\u00f3n continua, ya que tiene saltos en los \npuntos Xi\u00b1h y su derivada es 0 en todos los otros puntos. Por ello se ha sugerido \nreemplazar a la funci\u00f3n w por funciones m\u00e1s suaves K, llamadas n\u00facleos, lo que da \norigen a los estimadores de n\u00facleos. El estimador de n\u00facleos de una funci\u00f3n de \ndensidad f calculado a partir de una muestra aleatoria X1,...,X n de dicha densidad se \ndefine seg\u00fan la ecuaci\u00f3n 2.37. \n\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb=\u2211\n= hX-xKnh1(x)fin\n1ihn,\u02c6 Ec. 2.37 \nEn la ecuaci\u00f3n 2.37, la funci\u00f3n K se elige generalmente entre las funciones de \ndensidad conocidas, por ejemplo gaussiana, que se muestra en la ecuaci\u00f3n 2.38, \ndonde \u03c3 es la desviaci\u00f3n t\u00edpica de la distribuci\u00f3n y \u00b5 la media.  \n()\n22\n2\u03c3\u00b5x\ne\n\u03c32\u03c01f(x)\u2212\u2212\n=  Ec. 2.38 \nEl otro par\u00e1metro de la ecuaci\u00f3n 2.37 es h, llamado ventana, par\u00e1metro de \nsuavizado o ancho de banda, el cual determina las propiedades estad\u00edsticas del estimador: el sesgo crece y la varianza decrece con h [HALI94]. Es decir que si h es \ngrande, los estimadores est\u00e1n sobresuavizados y son sesgados, y si h es peque\u00f1o, los \nestimadores resultantes est\u00e1n subsuavizados, lo que equivale a decir que su varianza \nes grande.  \n \n \n \nFigura 3.12: Importancia del par\u00e1metro \u201ctama\u00f1o de ventana\u201d en el estimador de n\u00facleos.  \n \nA pesar de que la elecci\u00f3n del n\u00facleo K determina la forma de la densidad \nestimada, la literatura sugiere que esta elecci\u00f3n no es cr\u00edtica, al menos entre las alternativas usuales [DEA97]. M\u00e1s importante es la elecci\u00f3n del tama\u00f1o de ventana. En la figura 2.14 se muestra c\u00f3mo un valor peque\u00f1o para este factor hace que la Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 117 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda funci\u00f3n de distribuci\u00f3n generada est\u00e9 subsuavizada. Mientras, al emplear un h \ndemasiado grande provoca el sobresuavizado de la funci\u00f3n de distribuci\u00f3n. Por \u00faltimo, \nempleando el h \u00f3ptimo se obtiene la funci\u00f3n de distribuci\u00f3n adecuada. \n Para determinar un ancho de banda con el cual comenzar, una alternativa es \ncalcular el ancho de banda \u00f3ptimo si se supone que la densidad tiene una forma espec\u00edfica. La ventana \u00f3ptima en el sentido de minimizar el error medio cuadr\u00e1tico integrado, definido como la esperanza de la integral del error cuadr\u00e1tico sobre toda la \ndensidad, fue calculada por Bowman [BOW85], y Silverman [SIL86] y depende de la \nverdadera densidad f y del n\u00facleo K. Al suponer que ambos, la densidad y el n\u00facleo \nson normales, la ventana \u00f3ptima resulta ser la que se muestra en la ecuaci\u00f3n 2.39. \n-1/5n  1.06 h* \u03c3 =  Ec. 2.39 \nEn la ecuaci\u00f3n 2.39 \u03c3 es la desviaci\u00f3n t\u00edpica de la densidad. La utilizaci\u00f3n de \nesta h ser\u00e1 adecuada si la poblaci\u00f3n se asemeja en su distribuci\u00f3n a la de la normal; \nsin embargo si trabajamos con poblaciones multimodales se producir\u00e1 una sobresuavizaci\u00f3n de la estimaci\u00f3n. Por ello el mismo autor sugiere utilizar medidas \nrobustas de dispersi\u00f3n en lugar de \u03c3, con lo cual el ancho de banda \u00f3ptimo se obtiene \ncomo se muestra en la ecuaci\u00f3n 2.40. \n( )-1/5n IQR 0.75 \u03c3, min 1.06 h*=  Ec. 2.40 \nEn la ecuaci\u00f3n 2.40 IQR es el rango intercuart\u00edlico, esto es, la diferencia entre \nlos percentiles 75 y 25 [DEA97]. \n Una vez definidos todos los par\u00e1metros a tener en cuenta para emplear un \nestimador de n\u00facleos, hay que definir c\u00f3mo se obtiene, a partir del mismo, el valor de la variable a predecir, y, en funci\u00f3n del valor de la variable dependiente, x. Esto se \nrealiza mediante el estimador de Nadaray a-Watson, que se muestra en la ecuaci\u00f3n \n2.41. \n()\n\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\n== =\n\u2211\u2211\n==\nhX-xKYhX-xK\nxX|YE (x)m\nrn\n1riin\n1i\u02c6  Ec. 2.41 \nEn la ecuaci\u00f3n 2.41 x es el valor del atributo dependiente a partir del cual se \ndebe obtener el valor de la variable independiente y; Yi es el valor del atributo \nindependiente para el ejemplo de entrenamiento i. \n \nUna vez completada la explicaci\u00f3n de c\u00f3mo aplicar los estimadores de n\u00facleos \npara predecir el valor de una clase num\u00e9rica, se muestra, en la figura 2.15, un ejemplo \nde su utilizaci\u00f3n basado en los ejemplos de la tabla 2.1 (apartado 2.5), tomando la variable temperatura  como predictora y la variable humedad  como dependiente o a \npredecir. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 118 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 3.13: Ejemplo de predicci\u00f3n con un estimador de n\u00facleos.  \n \nEn primer lugar se definen los par\u00e1metros que se van a emplear para el \nestimador de n\u00facleos: la funci\u00f3n n\u00facleo y el par\u00e1metro de suavizado. Posteriormente se puede realizar la predicci\u00f3n, que en este caso consiste en predecir el valor del atributo humedad sabiendo que la temperatura  es igual a 77. Despu\u00e9s de completar el \nproceso se determina que el valor de la humedad es igual a 82.97 . \n \nAplicaci\u00f3n a problemas multivariantes \nHasta el momento se han explicado las bases sobre las que se sustentan los \nestimadores de n\u00facleos, pero en los problemas reales no es una \u00fanica variable la que debe tratarse, sino que han de tenerse en cuenta un n\u00famero indeterminado de variables. Por ello, es necesario ampliar el modelo explicado para permitir la introducci\u00f3n de d variables. As\u00ed, supongamos n ejemplos X\ni, siendo Xi un vector d-\ndimensional.  El estimador de n\u00facleos de la funci\u00f3n de densidad f calculado a partir de \nla muestra aleatoria X1,...,X n de dicha densidad se define como se muestra en la \necuaci\u00f3n 2.42. \n()()i1n\n1iHn, X-x HKHn1(x)f\u2212\n=\u2211 = \u02c6  Ec. 2.42 \nTal y como puede verse, la ecuaci\u00f3n 2.42 es una mera ampliaci\u00f3n de la \necuaci\u00f3n 2.37: en este caso H no es ya un \u00fanico valor num\u00e9rico, sino una matriz \nsim\u00e9trica  y definida positiva de orden dd\u00d7, denominada matriz de anchos de \nventana. Por su parte K es generalmente una funci\u00f3n de densidad multivariante. Por \nejemplo, la funci\u00f3n gaussiana normalizada en este caso pasar\u00eda a ser la que se muestra en la ecuaci\u00f3n 2.43. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 119 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ()2xx\ndT\ne\n2\u03c01f(x)\u2212=\n2 Ec. 2.43 \nDe nuevo, es m\u00e1s importante definir una correcta matriz H que la funci\u00f3n \nn\u00facleo elegida. Tambi\u00e9n el estimador de Nadaraya-Watson, que se muestra en la \necuaci\u00f3n 2.44, es una ampliaci\u00f3n del visto en la ecuaci\u00f3n 2.41. \n()()()\n()()i1n\n1ri i1n\n1i\nX-x HKYX-x HK\nx X|YE (x)m\n\u2212\n=\u2212\n=\n\u2211\u2211\n== =\u02c6 Ec. 2.44 \nTal y como se ve en la ecuaci\u00f3n 2.44, el cambio radica en que se tiene una \nmatriz de anchos de ventana en lugar de un \u00fanico valor de ancho de ventana. \n \nAplicaci\u00f3n a problemas de clasificaci\u00f3n \nSi bien los estimadores de n\u00facleo son dise\u00f1ados para la predicci\u00f3n num\u00e9rica, \ntambi\u00e9n pueden utilizarse para la clasificaci\u00f3n. En este caso, se dispone de un conjunto de c clases a las que puede pertenecer un ejemplo determinado. Y estos \nejemplos se componen de d variables o atributos. Se puede estimar la densidad de la \nclase j mediante la ecuaci\u00f3n 2.45, en la que n\nj es el n\u00famero de ejemplos de \nentrenamiento que pertenecen a la clase j, Yij ser\u00e1 1 en caso de que el ejemplo i \npertenezca a la clase j y 0 en otro caso, K vuelve a ser la funci\u00f3n n\u00facleo y h el ancho \nde ventana. En este caso se ha realizado la simplificaci\u00f3n del modelo multivariante, empleando en lugar de una matriz de anchos de ventana un \u00fanico valor escalar \nporque es el modelo que se utiliza en la  implementaci\u00f3n que realiza WEKA de los \nestimadores de n\u00facleo. \n\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb=\u2211\n= hX-xKhYn1(x)fin\n1id-j\ni\njj\u02c6  Ec. 2.45 \nLa probabilidad a priori  de que un ejemplo pertenezca a la clase j es igual a \nnn Pj j= . Se puede estimar la probabilidad a posteriori , definida mediante qj(x), de \nque el ejemplo pertenezca a j, tal y como se muestra en la ecuaci\u00f3n 2.46. \n(x)q\nhXxKhhXxKhY\n(x)fP(x)fP\nf(x)(x)fP(x)qj n\n1rr di dn\n 1ij\ni\nc\n1kkkjj jj\nj \u02c6\n\u02c6\u02c6 \u02c6\n=\n\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\u2212\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\u2212\n= \u2248 =\n\u2211\u2211\n\u2211\n=\u2212\u2212\n=\n= Ec. 2.46 \nDe esta forma, el estimador en este caso es id\u00e9ntico al estimador de \nNadayara-Watson representado en las ecuaciones 2.41 y 2.44. \n Por \u00faltimo, se muestra un ejemplo de la aplicaci\u00f3n de un estimador de \nn\u00facleos a un problema de clasificaci\u00f3n: se trata del problema planteado en la tabla 2.1 \n(apartado 2.5), y m\u00e1s concretamente se trata de predecir el valor de la clase jugar  a \npartir \u00fanicamente del atributo num\u00e9rico temperatura . Este ejemplo se muestra en la \nfigura 2.16. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 120 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 3.14: Ejemplo de clasificaci\u00f3n mediante un estimador de n\u00facleos.  \n \nAl igual que para el problema de predi cci\u00f3n, en primer lugar se definen los \npar\u00e1metros del estimador de n\u00facleos para, posteriormente, estimar la clase a la que pertenece el ejemplo de test. En este caso se trata de predecir si se puede jugar o no al tenis teniendo en cuenta que la temperatura  es igual a 77. Y la conclusi\u00f3n a la que \nse llega utilizando el estimador de n\u00facleos es que s\u00ed se puede jugar. \n \n3.5.  La clasificaci\u00f3n \nLa clasificaci\u00f3n es el proceso de dividir un conjunto de datos en grupos \nmutuamente excluyentes [WK91, LAN96, MIT97], de tal forma que cada miembro de un grupo est\u00e9 lo mas cerca posible de otros y grupos diferentes est\u00e9n lo m\u00e1s lejos \nposible de otros, donde la distancia se mide con respecto a las variables \nespecificadas, que se quieren predecir.  \n \nTabla2.1. Ejemplo de problema de clasificaci\u00f3n. \nEjemplo Vista Temperatura Humedad Viento Jugar \n1 Soleado Alta (85) Alta (85) No No \n2 Soleado Alta (80) Alta (90) S\u00ed No \n3 Nublado Alta (83) Alta (86) No S\u00ed \n4 Lluvioso Media (70) Alta (96) No S\u00ed Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 121 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5 Lluvioso Baja (68) Normal (80) No S\u00ed \n6 Lluvioso Baja (65) Normal (70) S\u00ed No \n7 Nublado Baja (64) Normal (65) S\u00ed S\u00ed \n8 Soleado Media (72) Alta (95) No No \n9 Soleado Baja (69) Normal (70) No S\u00ed \n10 Lluvioso Media (75) Normal (80) No S\u00ed \n11 Soleado Media (75) Normal (70) S\u00ed S\u00ed \n12 Nublado Media (72) Alta (90) S\u00ed S\u00ed \n13 Nublado Alta (81) Normal (75) No S\u00ed \n14 Lluvioso Media (71) Alta (91) S\u00ed No \n \nEl ejemplo empleado tiene dos atributos, temperatura y humedad , que pueden \nemplearse como simb\u00f3licos o num\u00e9ricos. Entre par\u00e9ntesis se presentan sus valores num\u00e9ricos. \n \nEn los siguientes apartados se presentan y explican las principales t\u00e9cnicas de \nclasificaci\u00f3n. Adem\u00e1s, se mostrar\u00e1n ejemplos que permiten observar el funcionamiento del algoritmo, para lo que se utilizar\u00e1 la tabla 2.1, que presenta un sencillo problema de clasificaci\u00f3n consistente en, a partir de los atributos que modelan el tiempo (vista, temperatura, humedad y viento), determinar si se puede o no jugar al \ntenis. \n \n3.5.1. Tabla de Decisi\u00f3n \nLa tabla de decisi\u00f3n constituye la forma m\u00e1s simple y rudimentaria de \nrepresentar la salida de un algoritmo de aprendizaje, que es justamente representarlo \ncomo la entrada.  \n \nEstos algoritmos consisten en seleccionar subconjuntos de atributos y calcular \nsu precisi\u00f3n [accuracy] para predecir o clasificar los ejemplos. Una vez seleccionado el mejor de los subconjuntos, la tabla de decisi\u00f3n estar\u00e1 formada por los atributos \nseleccionados (m\u00e1s la clase), en la que se insertar\u00e1n todos los ejemplos de \nentrenamiento \u00fanicamente con el subconjunto de atributos elegido. Si hay dos ejemplos con exactamente los mismos pares atributo-valor  para todos los atributos del \nsubconjunto, la clase que se elija ser\u00e1 la media de los ejemplos (en el caso de una clase num\u00e9rica) o la que mayor probabilidad de aparici\u00f3n tenga (en el caso de una clase simb\u00f3lica). \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 122 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda La precisi\u00f3n de un subconjunto S de atributos para todos los ejemplos de \nentrenamientos se calcular\u00e1 mediante la ecuaci\u00f3n 2.47 para el caso de que la clase \nsea simb\u00f3lica o mediante la ecuaci\u00f3n 2.48 en el caso de que la clase sea num\u00e9rica: \ntotales ejemplosos clasificad bien ejemplosS) precisi\u00f3n( =  Ec. 2.47 \nn)y-(y\nRMSE S) precisi\u00f3n(Ii2\ni i\u2211\n\u2208\u2212= \u2212=\u02c6\n Ec. 2.48 \nDonde, en la ecuaci\u00f3n 2.48, RMSE es la ra\u00edz cuadrada del error cuadr\u00e1tico \nmedio [root mean squared error], n es el n\u00famero de ejemplos totales, yi el valor de la \nclase para el ejemplo i y iy\u02c6 el valor predicho por el modelo para el ejemplo i.  \n \nComo ejemplo de tabla de decisi\u00f3n, simplemente se puede utilizar la propia \ntabla 2.1, dado que si se comenzase a combinar atributos y a probar la precisi\u00f3n de dicha combinaci\u00f3n, se obtendr\u00eda como resultado que los cuatro atributos deben emplearse, con lo que la tabla de salida ser\u00eda la misma. Esto no tiene por qu\u00e9 ser as\u00ed, \nya que en otros problemas no ser\u00e1n necesarios todos los atributos para generar la \ntabla de decisi\u00f3n, como ocurre en el ejemplo de la tabla 2.2 donde se dispone de un conjunto de entrenamiento en el que aparecen los atributos sexo, y tipo (tipo de profesor) y la clase a determinar es si el tipo de contrato es o no fijo. \n \n \n    \nTabla2.2. Determinaci\u00f3n del tipo de contrato. \n Atributos Clase \n    \nEjemplo N\u00ba Sexo Tipo Fijo \n1 Hombre Asociado No \n2 Mujer Catedr\u00e1tico Si \n3 Hombre Titular Si \n4 Mujer Asociado No \n5 Hombre Catedr\u00e1tico Si \n6 Mujer Asociado No \n7 Hombre Ayudante No \n8 Mujer Titular Si Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 123 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 9 Hombre Asociado No \n10 Mujer Ayudante No \n11 Hombre Asociado No \n \nSi se toma como primer subconjunto el formado por el atributo sexo, y se \neliminan las repeticiones resulta la tabla 2.3 \nTabla2.3. Subconjunto 1. \nEjemplo N\u00ba Sexo Fijo \n1 Hombre No \n2 Mujer Si \n3 Hombre Si \n4 Mujer No \n \nCon lo que se pone de manifiesto que la probabilidad de clasificar bien es del \n50%. Si por el contrario se elimina  el atributo Sexo,  quedar\u00e1 la tabla 2.4.   \n \nTabla2.4. Subconjunto 2. \nEjemplo N\u00ba Tipo Fijo \n1 Asociado No  \n2 Catedr\u00e1tico Si \n3 Titular Si \n7 Ayudante No \n \nQue tiene una  precisi\u00f3n de aciertos del 100%, por lo que se deduce que \u00e9sta \n\u00faltima tabla es la que se debe tomar como tabla de decisi\u00f3n. El resultado es l\u00f3gico ya \nque el atributo sexo es irrelevante a la hora de determinar si el contrato es o no fijo. \n \n \n3.5.2. \u00c1rboles de Decisi\u00f3n \nEl aprendizaje de \u00e1rboles de decisi\u00f3n est\u00e1 englobado como una metodolog\u00eda \ndel aprendizaje supervisado. La representaci\u00f3n que se utiliza para las descripciones \ndel concepto adquirido es el \u00e1rbol de decisi\u00f3n, que consiste en una representaci\u00f3n del conocimiento relativamente simple y que es una de las causas por la que los procedimientos utilizados en su aprendizaje son m\u00e1s sencillos que los de sistemas que Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 124 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda utilizan lenguajes de representaci\u00f3n m\u00e1s potentes, como redes sem\u00e1nticas, \nrepresentaciones en l\u00f3gica de primer orden etc.  No obstante, la potencia expresiva de \nlos \u00e1rboles de decisi\u00f3n es tambi\u00e9n menor que la de esos otros sistemas. El \naprendizaje de \u00e1rboles de decisi\u00f3n suele ser m\u00e1s robusto frente al ruido y conceptualmente sencillo, aunque los sistemas que han resultado del perfeccionamiento y de la evoluci\u00f3n de los m\u00e1s antiguos se complican con los procesos que incorporan para ganar fiabilidad. La mayor\u00eda de los sistemas de aprendizaje de \u00e1rboles suelen ser no incrementales, pero existe alguna excepci\u00f3n \n[UTG88]. \n \nEl primer sistema que constru\u00eda \u00e1rboles de decisi\u00f3n fue CLS de Hunt, \ndesarrollado en 1959 y depurado a lo largo de los a\u00f1os sesenta. CLS es un sistema desarrollado por psic\u00f3logos como un modelo del proceso cognitivo de formaci\u00f3n de \nconceptos sencillos. Su contribuci\u00f3n fundamental fue la propia metodolog\u00eda pero no \nresultaba computacionalmente eficiente debido al m\u00e9todo que empleaba en la extensi\u00f3n de los nodos. Se guiaba por una estrategia similar al minimax con una \nfunci\u00f3n que integraba diferentes costes. \n \nEn 1979 Quinlan desarrolla el sistema ID3 [QUIN79], que \u00e9l denominar\u00eda \nsimplemente herramienta porque la consideraba experimental. Conceptualmente es \nfiel a la metodolog\u00eda de CLS pero le aventaja en el m\u00e9todo de expansi\u00f3n de los nodos, basado en una funci\u00f3n que utiliza la medida de la informaci\u00f3n de Shannon. La versi\u00f3n definitiva, presentada por su autor Quinlan como un sistema de aprendizaje, es el sistema C4.5 que expone con cierto detalle en la obra C4.5: Programs for Machine \nLearning [QUIN93]. La evoluci\u00f3n -comercial- de ese sistema es otro denominado C5 \ndel mismo autor, del que se puede obtener una versi\u00f3n de demostraci\u00f3n restringida en cuanto a capacidades; por ejemplo, el n\u00famero m\u00e1ximo de ejemplos de entrenamiento. \n \nRepresentaci\u00f3n de un \u00e1rbol de decisi\u00f3n \n \nUn \u00e1rbol de decisi\u00f3n [MUR98] puede interpretarse esencialmente como una \nserie de reglas compactadas para su representaci\u00f3n en forma de \u00e1rbol. Dado un \nconjunto de ejemplos, estructurados como vectores de pares ordenados atributo-valor, de acuerdo con el formato general en el aprendizaje inductivo a partir de ejemplos, el concepto que estos sistemas adquieren durante el proceso de aprendizaje consiste en \nun \u00e1rbol. Cada eje est\u00e1 etiquetado con un par atributo-valor y las hojas con una clase, \nde forma que la trayectoria que determinan desde la ra\u00edz los pares de un ejemplo de entrenamiento alcanzan una hoja etiquetada -nor malmente- con la clase del ejemplo. \nLa clasificaci\u00f3n de un ejemplo nuevo del que se desconoce su clase se hace con la misma t\u00e9cnica, solamente que en ese caso al atributo clase, cuyo valor se desconoce, \nse le asigna de acuerdo con la etiqueta de la hoja a la que se accede con ese ejemplo. \n \uf020Problemas apropiados para este tipo de aprendizaje \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 125 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Las caracter\u00edsticas de los problemas apropiados para resolver mediante este \naprendizaje dependen del sistema de aprendizaje  espec\u00edfico utilizado, pero hay una \nserie de ellas generales y comunes a la mayor\u00eda y que se describen a continuaci\u00f3n: \n- Que la representaci\u00f3n de los ejempl os sea mediante vectores de pares \natributo-valor, especialmente cuando los valores son disjuntos y en un n\u00famero peque\u00f1o. Los sistemas actuales est\u00e1n preparados para tratar atributos con valores continuos, valores desconocidos e incluso valores con una distribuci\u00f3n de probabilidad. \n- Que el atributo que hace el papel de la clase sea de tipo discreto  y con un \nn\u00famero peque\u00f1o de valores, sin embargo existen sistemas que adquieren como concepto aprendido funciones con valores continuos. \n- Que las descripciones del concepto adquirido deban ser expresadas en forma \nnormal disyuntiva. \n- Que posiblemente existan errores de cl asificaci\u00f3n en el conjunto de ejemplos \nde entrenamiento, as\u00ed como valores desconocidos en algunos de los atributos en algunos ejemplos Estos sistemas, por lo general, son robustos frente a los errores del tipo mencionado. \n \nA continuaci\u00f3n se presentan tres algoritmos de \u00e1rboles de decisi\u00f3n, los dos \nprimeros dise\u00f1ados por Quinlan [QUIN86, QUIN93], los sistemas ID3 y C4.5; y el \ntercero un \u00e1rbol de decisi\u00f3n muy sencillo, con un \u00fanico nivel de decisi\u00f3n. \n \n\u2022 El sistema ID3 \n \nEl sistema ID3 [QUIN86] es un algoritmo simple y, sin embargo, potente, cuya \nmisi\u00f3n es la elaboraci\u00f3n de un \u00e1rbol de decisi\u00f3n. El procedimiento para generar un \u00e1rbol de decisi\u00f3n consiste, como se com ent\u00f3 anteriormente en seleccionar un atributo \ncomo ra\u00edz del \u00e1rbol y crear una rama con cada uno de los posibles valores de dicho atributo. Con cada rama resultante (nuevo nodo del \u00e1rbol), se realiza el mismo \nproceso, esto es, se selecciona otro atributo y se genera una nueva rama para cada \nposible valor del atributo. Este procedimiento contin\u00faa hasta que los ejemplos se clasifiquen a trav\u00e9s de uno de los caminos del \u00e1rbol. El nodo final de cada camino ser\u00e1 un nodo hoja, al que se le asignar\u00e1 la clase correspondiente. As\u00ed, el objetivo de los \u00e1rboles de decisi\u00f3n es obtener reglas o relaciones que permitan clasificar a partir de los atributos. \nEn cada nodo del \u00e1rbol de decisi\u00f3n se debe seleccionar un atributo para seguir \ndividiendo, y el criterio que se toma para elegirlo es: se selecciona el atributo que mejor separe (ordene) los ejemplos de acuerdo a las clases. Para ello se emplea la entrop\u00eda, que es una medida de c\u00f3mo est\u00e1 ordenado el universo. La teor\u00eda de la informaci\u00f3n (basada en la entrop\u00eda) calcula el n\u00famero de bits (informaci\u00f3n, preguntas \nsobre atributos) que hace falta suministrar para conocer la clase a la que pertenece un \nejemplo. Cuanto menor sea el valor de la ent rop\u00eda, menor ser\u00e1 la incertidumbre y m\u00e1s \n\u00fatil ser\u00e1 el atributo para la clasificaci\u00f3n. La definici\u00f3n de entrop\u00eda que da Shannon en su Teor\u00eda de la Informaci\u00f3n  (1948) es: Dado un conjunto de eventos A={A\n1, A2,..., A n}, \ncon probabilidades {p1, p2,..., p n}, la informaci\u00f3n en el conocimiento de un suceso Ai Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 126 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda (bits) se define en la ecuaci\u00f3n 2.49, mientras que la informaci\u00f3n media de A (bits) se \nmuestra en la ecuaci\u00f3n 2.50. \n()i 2\ni2 i p logp1log)I(A \u2212=\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb=  Ec. 2.49 \n\u2211 \u2211\n= =\u2212= =n\n1ii 2 in\n1ii i )(plogp )I(Ap I(A)  Ec. 2.50 \nSi aplicamos la entrop\u00eda a los problemas de clasificaci\u00f3n se puede medir lo que \nse discrimina (se gana por usar) un atributo Ai empleando para ello la ecuaci\u00f3n 2.51, \nen la que se define la ganancia de informaci\u00f3n. \n)I(AI) G(Ai i \u2212=  Ec. 2.51 \nSiendo I la informaci\u00f3n antes de utilizar el atributo e I(Ai) la informaci\u00f3n \ndespu\u00e9s de utilizarlo. Se definen ambas en las ecuaciones 2.52 y 2.53. \n\u2211\n=\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\u2212=nc\n1cc\n2c\nnnlognnI  Ec. 2.52 \n\u2211\n==)inv(A\n1jijij\ni Inn)(AI ; \u2211\n=\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n\u2212=nc\n1k ijijk\n2\nijijk\nijnnlognnI  Ec. 2.53 \nEn estas ecuaciones nc ser\u00e1 el n\u00famero de clases y nc el n\u00famero de ejemplares \nde la clase c, siendo n el n\u00famero total de ejemplos. Ser\u00e1 nv(A i) el n\u00famero de valores \ndel atributo Ai, nij el n\u00famero de ejemplos con el valor j en Ai y nijk el n\u00famero de \nejemplos con valor j en Ai y que pertenecen a la clase k. Una vez explicada la \nheur\u00edstica empleada para seleccionar el mejor atributo en un nodo del \u00e1rbol de decisi\u00f3n, se muestra el algoritmo ID3: \n \n \n1. Seleccionar el atributo Ai que maximice la ganancia G(Ai). \n2. Crear un nodo para ese atributo con tantos sucesores como \nvalores tenga. \n3. Introducir los ejemplos en los sucesores seg\u00fan el valor que \ntenga el atributo Ai. \n4. Por cada sucesor: \na. Si s\u00f3lo hay ejemplos de una clase, Ck, entonces etiquetarlo \ncon Ck. \nb. Si no, llamar a ID3 con una tabla formada por los ejemplos \nde ese nodo, eliminando la columna del atributo Ai. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 127 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.15: Pseudoc\u00f3digo del algoritmo ID3.  \n \nPor \u00faltimo, en la figura 2.18 se representa el proceso de generaci\u00f3n del \u00e1rbol \nde decisi\u00f3n para el problema planteado en la tabla 2.1. \n \n \n \nFigura 3.16: Ejemplo de clasificaci\u00f3n con ID3.  \n \nEn la figura 2.18 se muestra el \u00e1rbol de decisi\u00f3n que se generar\u00eda con el \nalgoritmo ID3. Adem\u00e1s, para el primer nodo del \u00e1rbol se muestra c\u00f3mo se llega a decidir que el mejor atributo para dicho nodo es vista. Se generan nodos para cada \nvalor del atributo y, en el caso de vista = Nublado  se llega a un nodo hoja ya que todos \nlos ejemplos de entrenamiento que llegan a dicho nodo son de clase S\u00ed. Sin embargo, \npara los otros dos casos se repite el proceso de elecci\u00f3n con el resto de atributos y con los ejemplos de entrenamiento que se clasifican a trav\u00e9s de ese nodo. \n \n \n\u2022 El sistema C4.5 \nEl ID3 es capaz de tratar con atributos cuyos valores sean discretos o \ncontinuos. En el primer caso, el \u00e1rbol de decisi\u00f3n generado tendr\u00e1 tantas  ramas  como \nvalores posibles tome el atributo. Si los valores del atributo son continuos, el ID3 no clasifica correctamente los ejemplos dados. Por ello, Quinlan [QUIN93] propuso el C4.5, como extensi\u00f3n del ID3, que permite:  \n \n1. Empleo del concepto raz\u00f3n de ganancia (GR, [Gain Ratio]) \n2. Construir \u00e1rboles de decisi\u00f3n cuando algunos de los ejemplos presentan \nvalores desconocidos para algunos de los atributos.  \n3. Trabajar con atributos que presenten valores continuos.  4. La  poda  de los \u00e1rboles de decisi\u00f3n [QUIN87, QR89].  Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 128 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5. Obtenci\u00f3n de Reglas de Clasificaci\u00f3n. \n \nRaz\u00f3n de Ganancia \n \nEl test basado en el criterio de maximizar la ganancia tiene como sesgo la \nelecci\u00f3n de atributos con muchos valores. Esto es debido a que cuanto m\u00e1s fina sea la \nparticipaci\u00f3n producida por los valores del atributo, normalmente, la incertidumbre o entrop\u00eda en cada nuevo nodo ser\u00e1 menor, y por lo tanto tambi\u00e9n ser\u00e1 menor la media de la entrop\u00eda a ese nivel. C4.5 modifica el criterio de selecci\u00f3n del atributo empleando en lugar de la ganancia  la raz\u00f3n de ganancia , cuya definici\u00f3n se muestra en la \necuaci\u00f3n 2.54. \n\u2211\n=\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\u2212= =) nv(A\n1jij\n2iji\nii\nii\nnnlognn)(AG\n)A I(Divisi\u00f3n)(AG)(AGR  \nEc. 2.54 \nAl t\u00e9rmino I(Divisi\u00f3n A i) se le denomina informaci\u00f3n de ruptura. En esta medida \ncuando nij tiende a n, el denominador se hace 0. Esto es un problema aunque seg\u00fan \nQuinlan, la raz\u00f3n de ganancia elimina el sesgo. \n \nValores Desconocidos \n \nEl sistema C4.5 admite ejemplos c on atributos desconocidos tanto en el \nproceso de aprendizaje como en el de validaci\u00f3n. Para calcular, durante el proceso de aprendizaje, la raz\u00f3n de ganancia de un atributo con valores desconocidos, se \nredefinen sus dos t\u00e9rminos, la ganancia, ecuaci\u00f3n 2.55, y la informaci\u00f3n de ruptura, \necuaci\u00f3n 2.56.   \n))I(A-(InnG(Aiic\ni=)  Ec. 2.55 \n\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\u2212\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\u2212=\u2211\n= nnlognn\nnnlognnA I(Divisi\u00f3nid\n2id) nv(A\n1jij\n2ij\nii\n)  Ec. 2.56 \nEn estas ecuaciones, nic es el n\u00famero de ejemplos con el atributo i conocido, y \nnid el n\u00famero de ejemplos con valor desconocido en el mismo atributo. Adem\u00e1s, para \nel c\u00e1lculo de las entrop\u00eda I(Ai) se tendr\u00e1n en cuenta \u00fanicamente los ejemplos en los \nque el atributo Ai tenga un valor definido. \n \nNo se toma el valor desconocido como significativo, sino que se supone una \ndistribuci\u00f3n probabil\u00edstica del atributo de acuerdo con los valores de los ejemplos en la \nmuestra de entrenamiento. Cuando se entrena, los casos con valores desconocidos se distribuyen con pesos de acuerdo a la frecuencia de aparici\u00f3n de cada posible valor del atributo en el resto de ejemplos de entrenamiento. El peso \u03c9\nij con que un ejemplo i \nse distribuir\u00eda desde un nodo etiquetado con el atributo A hacia el hijo con valor j en Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 129 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda dicho atributo se calcula mediante la ecuaci\u00f3n 2.57, en la que \u03c9i es el peso del \nejemplo i al llegar al nodo, esto es, antes de distribuirse, y p(A=j)  la suma de pesos de \ntodos los ejemplos del nodo con valor j en el atributo A entre la suma total de pesos de \ntodos los ejemplos del nodo ( \u03c9).  \n \n\u03c9\u03c9\u03c9j) p(A\u03c9\u03c9jA\ni i ij=== =  Ec. 2.57 \nEn cuanto a la clasificaci\u00f3n de un ejemplo de test, si se alcanza un nodo con un \natributo que el ejemplo no tiene (desconocido), se distribuye el ejemplo (divide) en \ntantos casos como valores tenga el atributo, y se da un peso a cada resultado con el \nmismo criterio que en el caso del entrenamiento: la frecuencia de aparici\u00f3n de cada posible valor del atributo en los ejemplos de entrenamiento. El resultado de esta t\u00e9cnica es una clasificaci\u00f3n con probabilidades, correspondientes a la distribuci\u00f3n de ejemplos en cada nodo hoja.  \nAtributos Continuos \nEl tratamiento que realiza C4.5 de los atributos continuos est\u00e1 basado en la ganancia \nde informaci\u00f3n, al igual que ocurre con los atributos discretos. Si un atributo continuo A\ni presenta los valores ordenados v1, v2,..., v n, se comprueba cu\u00e1l de los valores z i =(v i \n+ v i+1)/2 ; 1 \u2264 j < n , supone una ruptura del intervalo [v1, vn] en dos subintervalos [v1, zj] \ny (zj, vn] con mayor ganancia de informaci\u00f3n. El atributo continuo, ahora con dos \n\u00fanicos valores posibles, entrar\u00e1 en competencia con el resto de los atributos disponibles para expandir el nodo.  \n \n \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 130 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Figura 3.17: Ejemplo de tratamiento de atri butos continuos con C4.5.  \n \nPara mejorar la eficiencia del algoritmo no se consideran todos los posibles \npuntos de corte, sino que se tienen en cuenta las siguientes reglas: \n \n1. Cada subintervalo debe tener un n\u00famero m\u00ednimo de ejemplos (por ejemplo, 2). \n2. No se divide el intervalo si el siguiente ejemplo pertenece a la misma clase que \nel actual. \n3. No se divide el intervalo si el siguiente ejemplo tiene el mismo valor que el \nactual. \n4. Se unen subintervalos adyacentes si tienen la misma clase mayoritaria. \n \nComo se ve en el ejemplo de la figura 2.19, aplicando las reglas anteriores s\u00f3lo \nes preciso probar dos puntos de corte ( 66,5 y 77,5), mientras que si no se empleara \nninguna de las mejoras que se comentaron anteriormente se deber\u00edan haber probado un total de trece puntos. Como se ve en la figura 2.19, finalmente se tomar\u00eda como \npunto de ruptura el 77,5, dado que obtiene una mejor ganancia. Una vez seleccionado \nel punto de corte, este atributo num\u00e9rico competir\u00eda con el resto de atributos. Si bien aqu\u00ed se ha empleado la ganancia, realmente se emplear\u00eda la raz\u00f3n de ganancia, pero no afecta a la elecci\u00f3n del punto de corte. Cabe mencionar que ese atributo no deja de estar disponible en niveles inferiores como en el caso de los discretos, aunque con sus valores restringidos al intervalo que domina el camino. \n Poda del \u00e1rbol de decisi\u00f3n \n \nEl \u00e1rbol de decisi\u00f3n ha sido construido a partir de un conjunto de ejemplos, por \ntanto, reflejar\u00e1 correctamente todo el grupo de casos. Sin embargo, como esos ejemplos pueden ser muy diferentes entre s\u00ed, el \u00e1rbol resultante puede llegar a ser \nbastante complejo, con trayectorias largas y muy desiguales. Para facilitar la \ncomprensi\u00f3n del \u00e1rbol puede realizarse una poda  del mismo. C4.5 efect\u00faa la poda \ndespu\u00e9s de haber desarrollado el \u00e1rbol completo ( post-poda ), a diferencia de otros \nsistemas que realizan la construcci\u00f3n del \u00e1rbol y la poda a la vez (pre-poda); es decir, estiman la necesidad de seguir desarrollando un nodo aunque no posea el car\u00e1cter de hoja. En C4.5 el proceso de podado comienza en los nodos hoja y recursivamente \ncontin\u00faa hasta llegar al nodo ra\u00edz. Se consideran dos operaciones de poda en C4.5: \nreemplazo de sub-\u00e1rbol por hoja ( subtree replacement ) y elevaci\u00f3n de sub-\u00e1rbol \n(subtree raising ). En la figura 2.20 se muestra en lo que consiste cada tipo de poda. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 131 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 3.18: Tipos de operaciones de poda en C4.5.  \n \nEn esta figura tenemos el \u00e1rbol original antes del podado (a), y las dos posibles \nacciones de podado a realizar sobre el nodo interno C. En (b) se realiza subtree \nreplacement , en cuyo caso el nodo C es reemplazado por uno de sus sub\u00e1rboles. Por \n\u00faltimo, en (c) se realiza subtree raising : El nodo B es sustituido por el sub\u00e1rbol con ra\u00edz \nC. En este \u00faltimo caso hay que tener en cuenta que habr\u00e1 que reclasificar de nuevo \nlos ejemplos a partir del nodo C. Adem\u00e1s, subtree raising  es muy costoso \ncomputacionalmente hablando, por lo que se suele restringir su uso al camino m\u00e1s \nlargo a partir del nodo (hasta la hoja) que estamos podando. Como se coment\u00f3 anteriormente, el proceso de podado comienza en las hojas y contin\u00faa hacia la ra\u00edz pero, la cuesti\u00f3n es c\u00f3mo decidir reemplazar un nodo interno por una hoja (replacement ) o reemplazar un nodo interno por uno de sus nodos hijo (raising ). Lo \nque se hace es comparar el error estimado de clasificaci\u00f3n en el nodo en el que nos \nencontramos y compararlo con el error en cada uno de sus hijos y en su padre para realizar alguna de las operaciones o ninguna. En la figura 2.21 se muestra el pseudoc\u00f3digo del proceso de podado que se emplea en C4.5. \n \nPodar (ra\u00edz) { \n Si ra\u00edz No es HOJA Entonces \n  Para cada hijo H de ra\u00edz Hacer \n   Podar (H) \n   \n  Obtener Brazo m\u00e1s largo (B) de ra\u00edz // raising \n  ErrorBrazo = EstimarErrorArbol (B, ra\u00edz.ejemplos) \n \n  ErrorHoja = EstimarError (ra\u00edz, ra\u00edz.ejemplos) // replacement \n \n  Error\u00c1rbol = EstimarErrorArbol (ra\u00edz, ra\u00edz.ejemplos) \n \n  Si ErrorHoja <= Error\u00c1rbol Entonces // replacement \n   ra\u00edz es Hoja \n   Fin Poda \n \n  Si ErrorBrazo <= Error\u00c1rbol Entonces // raising \n   ra\u00edz = B \n   Podar (ra\u00edz) \n} \n \nEstimarErrorArbol (ra\u00edz, ejemplos) { \n Si ra\u00edz es HOJA Entonces Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 132 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda   EstimarError (ra\u00edz, ejemplos) \n Si no \n  Distribuir los ejemplos (ej[]) en los brazos \n  Para cada brazo (B) \n   error = error + EstimarErrorArbol (B, ej[B]) \n} \nFigura 3.19: Pseudoc\u00f3digo del algoritmo de podado en C4.5. \n \nDe esta forma, el subtree raising  se emplea \u00fanicamente para el sub\u00e1rbol m\u00e1s \nlargo. Adem\u00e1s, para estimar su error se emplean los ejemplos de entrenamiento, pero \nlos del nodo origen, ya que si se eleva  deber\u00e1 clasificarlos \u00e9l. En cuanto a la funci\u00f3n \nEstimarError , es la funci\u00f3n que estima el error de clasificaci\u00f3n de una hoja del \u00e1rbol. \nAs\u00ed, para tomar la decisi\u00f3n debemos estimar el error de clasificaci\u00f3n en un nodo \ndeterminado para un conjunto de test independiente. Habr\u00e1 que estimarlo tanto para \nlos nodos hoja como para los internos (suma de errores de clasificaci\u00f3n de sus hijos). No se puede tomar como dato el error de clasificaci\u00f3n en el conjunto de entrenamiento dado que, l\u00f3gicamente, el error se subestimar\u00eda. \n \nUna t\u00e9cnica para estimar el error de clasificaci\u00f3n es la denominada reduced-\nerror pruning , que consiste en dividir el conjunto de entrenamiento en n subconjuntos \nn-1 de los cu\u00e1les servir\u00e1n realmente para el entrenamiento del sistema y 1 para la \nestimaci\u00f3n del error. Sin embargo, el problema es que la construcci\u00f3n del clasificador se lleva a cabo con menos ejemplos. Esta no es la t\u00e9cnica empleada en C4.5. La t\u00e9cnica empleada en C4.5 consiste en estimar el error de clasificaci\u00f3n bas\u00e1ndose en los propios ejemplos de entrenamiento. Para ello, en el nodo donde queramos estimar \nel error de clasificaci\u00f3n, se toma la clase mayoritaria de sus ejemplos como clase \nrepresentante. Esto implica que habr\u00e1 E errores de clasificaci\u00f3n de un total de N \nejemplos que se clasifican a trav\u00e9s de dicho nodo. El error observado ser\u00e1 f=E/N , \nsiendo q la probabilidad de error de clasificaci\u00f3n del nodo y p=1-q  la probabilidad de \n\u00e9xito. Se supone que la funci\u00f3n f sigue una distribuci\u00f3n binomial de par\u00e1metro q. Y lo \nque se desea obtener es el error e, que ser\u00e1 la probabilidad del extremo superior con \nun intervalo [f-z, f+z]  de confianza c. Dado que se trata de una distribuci\u00f3n binomial, se \nobtendr\u00e1 e mediante las ecuaciones 2.58 y 2.59. \n \nczq)/N-q(1q-fP =\uf8fa\uf8fb\uf8f9\n\uf8ef\uf8f0\uf8ee\u2264  Ec. 2.58 \n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\n\uf8ed\uf8eb\n++ \u2212 + +\n=\nNz14Nz\nNf\nNfz2Nzf\ne222 2 2\n Ec. 2.59 \nComo factor c (factor de confianza) se suele emplear en C4.5 el 25%, dado que \nes el que mejores resultados suele dar y que corresponde a un z=0.69 . \n \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 133 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Obtenci\u00f3n de Reglas de Clasificaci\u00f3n \n \nCualquier \u00e1rbol de decisi\u00f3n se puede convertir en reglas de clasificaci\u00f3n, \nentendiendo como tal una estructura del tipo Si <Condici\u00f3n> Entonces <Clase> . El \nalgoritmo de generaci\u00f3n de reglas consiste b\u00e1sicamente en, por cada rama del \u00e1rbol \nde decisi\u00f3n, las preguntas y sus valores estar\u00e1n en la parte izquierda de las reglas y la \netiqueta del nodo hoja correspondiente en la parte derecha (clasificaci\u00f3n). Sin embargo, este procedimiento generar\u00eda un sistema de reglas con mayor complejidad de la necesaria. Por ello, el sistema C4.5 [QUIN93] realiza un podado de las reglas obtenidas. En la figura 2.22 se muestra el algoritmo completo de obtenci\u00f3n de reglas. \n \nObtenerReglas (\u00e1rbol) { \n Convertir el \u00e1rbol de decisi\u00f3n (\u00e1rbol) a un conjunto de reglas, R \n error = error de clasificaci\u00f3n con R \n Para cada regla Ri de R Hacer \n  Para cada precondici\u00f3n pj de Ri Hacer \n   nuevoError = error al eliminar pj de Ri \n   Si nuevoError <= error Entonces \n    Eliminar pj de Ri \n    error = nuevoError \n  Si Ri no tiene precondiciones Entonces \n   Eliminar Ri \n}   \nFigura 3.20: Pseudoc\u00f3digo del algoritmo de obtenci\u00f3n de reglas de C4.5.  \n \nEn cuanto a la estimaci\u00f3n del error, se realiza del mismo modo que para \nrealizar el podado del \u00e1rbol de decisi\u00f3n. \n \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 134 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n\u2022 Decision Stump  (\u00c1rbol de un solo nivel ) \n \nTodav\u00eda existe un algoritmo m\u00e1s sencillo que genera un \u00e1rbol de decisi\u00f3n de un \n\u00fanico nivel. Se trata de un algoritmo, [decision stump], que utiliza un \u00fanico atributo para construir el \u00e1rbol de decisi\u00f3n. La elecci\u00f3n del \u00fanico atributo que formar\u00e1 parte del \n\u00e1rbol se realizar\u00e1 bas\u00e1ndose en la ganancia de informaci\u00f3n, y a pesar de su \nsimplicidad, en algunos problemas puede llegar a conseguir resultados interesantes. No tiene opciones de configuraci\u00f3n, pero la implementaci\u00f3n es muy completa, dado que admite tanto atributos num\u00e9ricos como simb\u00f3licos y clases de ambos tipos \ntambi\u00e9n. El \u00e1rbol de decisi\u00f3n tendr\u00e1 tres ramas: una de ellas ser\u00e1 para el caso de que \nel atributo sea desconocido, y las otras dos ser\u00e1n para el caso de que el valor del \natributo del ejemplo de test sea igual a un valor concreto del atributo o distinto a dicho valor, en caso de los atributos simb\u00f3licos, o que el valor del ejemplo de test sea mayor o menor a un determinado valor en el caso de atributos num\u00e9ricos. En el caso de los atributos simb\u00f3licos se considera cada valor posible del mismo y se calcula la ganancia de informaci\u00f3n con el atributo igual al valor, distinto al valor y valores \ndesconocidos del atributo. En el caso de atributos simb\u00f3licos se busca el mejor punto \nde ruptura, tal y como se vio en el sistema C4.5. Deben tenerse en cuenta cuatro posibles casos al calcular la ganancia de informaci\u00f3n: que sea un atributo simb\u00f3lico y la clase sea simb\u00f3lica o que la clase sea num\u00e9rica, o que sea un atributo num\u00e9rico y la clase sea simb\u00f3lica o que la clase sea num\u00e9rica. A continuaci\u00f3n se comenta cada \ncaso por separado. \n \nAtributo Simb\u00f3lico y Clase Simb\u00f3lica \nSe toma cada vez un valor v\nx del atributo simb\u00f3lico Ai como base y se \nconsideran \u00fanicamente tres posibles ramas en la construcci\u00f3n del \u00e1rbol: que el atributo \nAi sea igual a vx, que el atributo Ai sea distinto a vx o que el valor del atributo Ai sea \ndesconocido. Con ello, se calcula la entrop\u00eda del atributo tomando como base el valor escogido tal y como se muestra en la ecuaci\u00f3n 2.60, en la que el valor de j en el \nsumatorio va desde 1 a 3 porque los valores del atributo se restringen a tres: igual a v\nx \n, distinto de vx o valor desconocido. En cuanto a los par\u00e1metros, nij es el n\u00famero de \nejemplos con valor j en el atributo i, n el n\u00famero total de ejemplos y nijk el n\u00famero de \nejemplos con valor j en el atributo i y que pertenece a la clase k. \n()\nnI nlogn\n)(AI3\n1jij ij ij\nivx\u2211\n=\u2212\n= ; () \u2211\n=\u2212=nc\n1kijk ijk ij nlogn I  Ec. 2.60 \n \nAtributo Num\u00e9rico y Clase Simb\u00f3lica \nSe ordenan los ejemplos seg\u00fan el atributo Ai y se considera cada valor vx del \natributo como posible punto de corte. Se consideran entonces como posibles valores del atributo el rango menor o igual a v\nx, mayor a vx y valor desconocido. Se calcula la \nentrop\u00eda del rango tomando como base esos tres posibles valores restringidos del \natributo. \n \nAtributo Simb\u00f3lico y Clase Num\u00e9rica Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 135 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Se vuelve a tomar como base cada vez cada valor del atributo, tal y como se \nhac\u00eda en el caso Atributo Simb\u00f3lico y Clase Simb\u00f3lica , pero en este caso se calcula la \nvarianza de la clase para los valores del atributo mediante la ecuaci\u00f3n 2.61, donde Sj \nes la suma de los valores de la clase de los ejemplos con valor j en el atributo i, SSj es \nla suma de los valores de la clase al cuadrado y Wj es la suma de los pesos de los \nejemplos (n\u00famero de ejemplos si no se incluyen pesos) con valor j en el atributo. \n\u2211\n=\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n=3\n1j jj\nj ivWS-SS )(A Varianza\nx Ec. 2.61 \nAtributo Num\u00e9rico y Clase Num\u00e9rica \nSe considera cada valor del atributo como punto de corte tal y como se hac\u00eda \nen el caso Atributo Num\u00e9rico y Clase Simb\u00f3lica . Posteriormente, se calcula la varianza \ntal y como se muestra en la ecuaci\u00f3n 2.61. \n \nEn cualquiera de los cuatro casos que se han comentado, lo que se busca es el \nvalor m\u00ednimo de la ecuaci\u00f3n calculada, ya sea la entrop\u00eda o la varianza. De esta forma se obtiene el atributo que ser\u00e1 ra\u00edz del \u00e1rbol de decisi\u00f3n y sus tres ramas. Lo \u00fanico que se har\u00e1 por \u00faltimo es construir dicho \u00e1rbol: cada rama finaliza en un nodo hoja con \nel valor de la clase, que ser\u00e1 la media o la moda de los ejemplos que se clasifican por ese camino, seg\u00fan se trate de una clase num\u00e9rica o simb\u00f3lica.  \n \n3.5.3. Reglas de Clasificaci\u00f3n \nLas t\u00e9cnicas de Inducci\u00f3n de Reglas [QUIN87, QUIN93] surgieron hace m\u00e1s de \ndos d\u00e9cadas y permiten la generaci\u00f3n y contraste de \u00e1rboles de decisi\u00f3n, o reglas y \npatrones a partir de los datos de entrada. La informaci\u00f3n de entrada ser\u00e1 un conjunto de casos donde se ha asociado una clasificaci\u00f3n o evaluaci\u00f3n a un conjunto de variables o atributos. Con esa informaci\u00f3n estas t\u00e9cnicas obtienen el \u00e1rbol de decisi\u00f3n o conjunto de reglas que soportan la evaluaci\u00f3n o clasificaci\u00f3n [CN89, HMM86]. En los casos en que la informaci\u00f3n de entrada posee alg\u00fan tipo de \u201cruido\" o defecto \n(insuficientes atributos o datos, atributos irrelevantes o errores u omisiones en los \ndatos) estas t\u00e9cnicas pueden habilitar m\u00e9todos estad\u00edsticos de tipo probabil\u00edstico para generar \u00e1rboles de decisi\u00f3n recortados o podados. Tambi\u00e9n en estos casos pueden identificar los atributos irrelevantes, la falta de atributos discriminantes o detectar \"gaps\" o huecos de conocimiento. Esta t\u00e9cnica suele llevar asociada una alta \ninteracci\u00f3n con el analista de forma que \u00e9ste pueda intervenir en cada paso de la \nconstrucci\u00f3n de las reglas, bien para aceptarlas, bien para modificarlas [MM95]. \n La inducci\u00f3n de reglas se puede lograr fundamentalmente mediante dos \ncaminos: Generando un \u00e1rbol de decisi\u00f3n y extrayendo de \u00e9l las reglas [QUIN93], \ncomo puede hacer el sistema C4.5 o bien mediante una estrategia de covering , \nconsistente en tener en cuenta cada vez una clase y buscar las reglas necesarias para cubrir [cover] todos los ejemplos de esa clase; cuando se obtiene una regla se eliminan todos los ejemplos que cubre y se contin\u00faa buscando m\u00e1s reglas hasta que no haya m\u00e1s ejemplos de la clase. A continuaci\u00f3n se muestran una t\u00e9cnica de inducci\u00f3n de reglas basada en \u00e1rboles de decisi\u00f3n, otra basada en covering  y una m\u00e1s \nque mezcla las dos estrategias. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 136 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n \n\u2022 Algoritmo 1R \n \nEl m\u00e1s simple algoritmo de reglas de clasificaci\u00f3n para un conjunto de \nejemplos es el 1R [HOL93]. Este algoritmo genera un \u00e1rbol de decisi\u00f3n de un nivel \nexpresado mediante reglas. Consiste en seleccionar un atributo (nodo ra\u00edz) del cual \nnace una rama por cada valor, que va a parar a un nodo hoja con la clase m\u00e1s probable de los ejemplos de entrenamiento que se clasifican a trav\u00e9s suyo. Este algoritmo se muestra en la figura 2.23.  \n1R (ejemplos) { \n Para cada atributo (A) \n  Para cada valor del atributo (Ai) \n   Contar el n\u00famero de apariciones de cada clase con Ai \n   Obtener la clase m\u00e1s frecuente (Cj) \n   Crear una regla del tipo Ai -> Cj \n  Calcular el error de las reglas del atributo A \n Escoger las reglas con menor error \n} \nFigura 3.21: Pseudoc\u00f3digo del algoritmo 1R.  \n \nLa clase debe ser simb\u00f3lica, mientras los atributos pueden ser simb\u00f3licos o \nnum\u00e9ricos. Tambi\u00e9n admite valores desconocidos, que se toman como otro valor m\u00e1s del atributo. En cuanto al error de las reglas de un atributo, consiste en la proporci\u00f3n entre los ejemplos que cumplen la regla y los ejemplos que cumplen la premisa de la regla. En el caso de los atributos num\u00e9ricos, se generan una serie de puntos de \nruptura  [breakpoint], que discretizar\u00e1n dicho atributo formando conjuntos. Para ello, se \nordenan los ejemplos por el atributo num\u00e9rico y se recorren. Se van contando las apariciones de cada clase hasta un n\u00famero m que indica el m\u00ednimo n\u00famero de \nejemplos que pueden pertenecer a un conjunto, para evitar conjuntos demasiado peque\u00f1os. Por \u00faltimo, se unen a este conjunto ejemplos con la clase m\u00e1s frecuente y \nejemplos con el mismo valor en el atributo. \n \nLa sencillez de este algoritmo es un poco insultante. Su autor llega a decir \n[HOL93; pag 64] :   \u201cProgram 1R is ordinary in most respects.\u201d   Tanto es as\u00ed que 1R no \ntiene ning\u00fan elemento de sofistificaci\u00f3n y genera para cada atributo un \u00e1rbol de profundidad 1, donde una rama est\u00e1 etiquetada por missing  si es que aparecen \nvalores desconocidos ( missing  values)  en ese atributo en el conjunto de \nentrenamiento; el resto de las ramas tienen como etiqueta un intervalo construido de \nuna manera muy simple, como se ha explicado antes, o un valor nominal, seg\u00fan el tipo \nde atributo del que se trate. Lo sorprendente de este sistema es su rendimiento. En [HOL93] se describen rendimientos que en media est\u00e1n por debajo de los de C4.5 en 5,7 puntos porcentuales de aciertos de clasificaci\u00f3n. Para la realizaci\u00f3n de las pruebas, Holte, elige un conjunto de 16 problemas del almac\u00e9n de la U.C.I. [Blake, Keog, Merz, 98] que desde entonces han gozado de cierto reconocimiento como \nconjunto de pruebas; en alguno de estos problemas introduce algunas modificaciones \nque tambi\u00e9n se han hecho est\u00e1ndar. El mecanismo de estimaci\u00f3n consiste en separar el subconjunto de entrenamiento original en subconjuntos de entrenamiento y test en Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 137 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda proporci\u00f3n 2/3 y 1/3 respectivamente y repetir el experimento 25 veces. Aunque la \ndiferencia de 5,7 es algo elevada, en realidad en 14 de los 16 problemas la diferencia \nes solo de 3,1 puntos. En la tabla 2.5 se presenta un ejemplo de 1R, basado en los \nejemplos de la tabla 2.1. \n \nTabla2.5. Resultados del algoritmo 1R. \natributo reglas errores error total \nvista Soleado \u00c6 no \nNublado \u00c6 si \nLluvioso \u00c6 si 2/5 \n0/4 \n2/5  \n4/14 \ntemperatura Alta \u00c6 no \nMedia \u00c6 si \nBaja \u00c6 si 2/4 \n2/6 \n1/4  \n5/14 \nhumedad Alta \u00c6 no \nNormal \u00c6 si 3/7 \n1/7 4/14 \nviento Falso \u00c6 si \nCierto \u00c6 no 2/8 \n3/6 5/14 \n  \n Para clasificar seg\u00fan la clase jugar, 1R considera cuatro conjuntos de reglas, \nuno por cada atributo, que son las mostradas en la tabla anterior, en las que adem\u00e1s aparecen los errores  que se cometen. De esta forma se concluye que como los errores m\u00ednimos corresponden a las reglas generadas por los atributos vista y \nhumedad, cualquiera de ellas es valida, de manera que arbitrariamente se puede \nelegir cualquiera de estos dos conjuntos de reglas como generador de 1R. \n \n\u2022 Algoritmo PRISM \n \nPRISM [CEN87] es un algoritmo b\u00e1sico de aprendizaje de reglas que asume \nque no hay ruido en los datos. Sea t el n\u00famero de ejemplos cubiertos por la regla y p \nel n\u00famero de ejemplos positivos cubiertos por la regla. Lo que hace PRISM es a\u00f1adir condiciones a reglas que maximicen la relaci\u00f3n p/t (relaci\u00f3n entre los ejemplos \npositivos cubiertos y ejemplos cubiertos en total). En la figura 2.24 se muestra el \nalgoritmo de PRISM. \n \nPRISM (ejemplos) { \n Para cada clase (C) \n  E = ejemplos \n  Mientras E tenga ejemplos de C \n   Crea una regla R con parte izquierda vac\u00eda y clase C \n   Hasta R perfecta Hacer \n    Para cada atributo A no incluido en R y cada valor v de A \n     Considera a\u00f1adir la condici\u00f3n A=v a la parte izquierda de R \n     Selecciona el par A=v que maximice p/t \n      (en caso de empates, escoge la que tenga p mayor) \n    A\u00f1adir A=v a R \n  Elimina de E los ejemplos cubiertos por R \nFigura 3.22: Pseudoc\u00f3digo del algoritmo PRISM.  \n \nEste algoritmo va eliminando los ejemplos que va cubriendo cada regla, por lo \nque las reglas tienen que interpretarse en orden. Se habla entonces de listas de reglas [decision list]. En la figura 2.25 se muestra un ejemplo de c\u00f3mo act\u00faa el algoritmo. Concretamente se trata de la aplicaci\u00f3n del mismo sobre el ejemplo de la tabla 2.1. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 138 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nFigura 3.23: Ejemplo de PRISM.  \n \n \nEn la figura 2.25 se muestra c\u00f3mo el algoritmo toma en primer lugar la clase S\u00ed. \nPartiendo de todos los ejemplos de entrenamiento (un total de catorce) calcula el cociente p/t para cada par atributo-valor y escoge el mayor. En este caso, dado que la \ncondici\u00f3n escogida hace la regla perfecta ( p/t = 1 ), se eliminan los cuatro ejemplos que \ncubre dicha regla y se busca una nueva regla. En la segunda regla se obtiene en un \nprimer momento una condici\u00f3n que no hace perfecta la regla, por lo que se contin\u00faa \nbuscando con otra condici\u00f3n. Finalmente, se muestra la lista de decisi\u00f3n completa que genera el algoritmo. \n \n\u2022 Algoritmo PART \n \nUno de los sistemas m\u00e1s importantes  de aprendizaje de reglas es el \nproporcionado por C4.5 [QUI93], explicado anteriormente. Este sistema, al igual que otros sistemas de inducci\u00f3n de reglas, realiza dos fases: primero, genera un conjunto \nde reglas de clasificaci\u00f3n y despu\u00e9s refina estas reglas para mejorarlas, realizando as\u00ed \nuna proceso de optimizaci\u00f3n global de dichas  reglas. Este proceso de optimizaci\u00f3n \nglobal es siempre muy complejo y costoso computacionalmente hablando. Por otro lado, el algoritmo PART [FRWI98] es un sistema que obtiene reglas sin dicha optimizaci\u00f3n global. Recibe el nombre PART por su modo de actuaci\u00f3n: obtaining \nrules from PARTial decision trees , y fue desarrollado por el grupo neozeland\u00e9s que \nconstruy\u00f3 el entorno WEKA [WF98]. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 139 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El sistema se basa en las dos estrategias b\u00e1sicas para la inducci\u00f3n de reglas: \nel covering  y la generaci\u00f3n de reglas a partir de \u00e1rboles de decisi\u00f3n. Adopta la \nestrategia del covering  (con lo que se obtiene una lista de decisi\u00f3n) dado que genera \nuna regla, elimina los ejemplares que dicha regla cubre y contin\u00faa generando reglas hasta que no queden ejemplos por clasificar. Sin embargo, el proceso de generaci\u00f3n de cada regla no es el usual. En este caso, para crear una regla, se genera un \u00e1rbol de decisi\u00f3n podado, se obtiene la hoja que clasifique el mayor n\u00famero de ejemplos, \nque se transforma en la regla, y posteriormente se elimina el \u00e1rbol. Uniendo estas dos \nestrategias se consigue mayor flexibilidad y velocidad. Adem\u00e1s, no se genera un \u00e1rbol \ncompleto, sino un \u00e1rbol parcial [partial decisi\u00f3n tree]. Un \u00e1rbol parcial es un \u00e1rbol de decisi\u00f3n que contiene brazos  con sub\u00e1rboles no definidos. Para generar este \u00e1rbol se \nintegran los procesos de construcci\u00f3n y podado hasta que se encuentra un sub\u00e1rbol estable  que no puede simplificarse m\u00e1s, en cuyo caso se para el proceso y se genera \nla regla a partir de dicho sub\u00e1rbol. Este proceso se muestra en la figura 2.26. \n \nExpandir (ejemplos) { \n elegir el mejor atributo para dividir en subconjuntos \n Mientras (subconjuntos No expandidos) \n      Y (todos los subconjuntos expandidos son HOJA) \n  Expandir (subconjunto) \n Si (todos los subconjuntos expandidos son HOJA)  \n      Y (errorSub\u00e1rbol >= errorNodo) \n  deshacer la expansi\u00f3n del nodo y nodo es HOJA \nFigura 3.24: Pseudoc\u00f3digo de expansi\u00f3n de PART.  \n \nEl proceso de elecci\u00f3n del mejor atributo se hace como en el sistema C4.5, \nesto es, bas\u00e1ndose en la raz\u00f3n de ganancia. La expansi\u00f3n de los subconjuntos generados se realiza en orden, comenzando por el que tiene menor entrop\u00eda y \nfinalizando por el que tiene mayor. La raz\u00f3n de realizarlo as\u00ed es porque si un \nsubconjunto tiene menor entrop\u00eda hay m\u00e1s probabilidades de que se genere un sub\u00e1rbol menor y consecuentemente se cree una regla m\u00e1s general. El proceso contin\u00faa recursivamente expandiendo los subconjuntos hasta que se obtienen hojas , \nmomento en el que se realizar\u00e1 una vuelta atr\u00e1s [backtracking]. Cuando se realiza \ndicha vuelta atr\u00e1s y los hijos del nodo en cuesti\u00f3n son hojas , comienza el podado tal y \ncomo se realiza en C4.5 (comparando el error esperado del sub\u00e1rbol con el del nodo), pero \u00fanicamente se realiza la funci\u00f3n de reemplazamiento del nodo por hoja [subtree replacement]. Si se realiza el podado se realiza otra vuelta atr\u00e1s hacia el nodo padre , \nque sigue explorando el resto de sus hijos, pero si no se puede realizar el podado el \npadre  no continuar\u00e1 con la exploraci\u00f3n del resto de nodos hijos (ver segunda \ncondici\u00f3n del bucle \u201c mientras\u201d  en la figura 2.26). En este momento finalizar\u00e1 el proceso \nde expansi\u00f3n y generaci\u00f3n del \u00e1rbol de decisi\u00f3n.  Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 140 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nFigura 3.25: Ejemplo de generaci\u00f3n de \u00e1rbol parcial con PART.  \n \nEn la figura 2.27 se presenta un ejemplo de generaci\u00f3n de un \u00e1rbol parcial \ndonde, junto a cada brazo de un nodo, se muestra el orden de exploraci\u00f3n (orden ascendente seg\u00fan el valor de la entrop\u00eda). Los nodos con relleno gris claro son los que a\u00fan no se han explorado y los nodos con relleno gris oscuro los nodos hoja. Las \nflechas ascendentes representan el proceso de backtracking . Por \u00faltimo, en el paso 5, \ncuando el nodo 4 es explorado y los nodos 9 y 10 pasan a ser hoja, el nodo padre  \nintenta realizar el proceso de podado, pero no se realiza el reemplazo (representado con el 4 en negrita), con lo que el proceso, al volver al nodo 1, finaliza sin explorar el nodo 2. \n \nUna vez generado el \u00e1rbol parcial se extrae una regla del mismo. Cada hoja se \ncorresponde con una posible regla, y lo que se busca es la mejor hoja. Si bien se \npueden considerar otras heur\u00edsticas, en el algoritmo PART se considera mejor hoja aquella que cubre un mayor n\u00famero de ejemplos. Se podr\u00eda haber optado, por ejemplo, por considerar mejor aquella que tiene un menor error esperado, pero tener una regla muy precisa no significa lograr un conjunto de reglas muy preciso. Por \n\u00faltimo, PART permite que haya atributos con valores desconocidos tanto en el proceso \nde aprendizaje como en el de validaci\u00f3n y atributos num\u00e9ricos, trat\u00e1ndolos exactamente como el sistema C4.5. \n \n3.5.4. Clasificaci\u00f3n Bayesiana \nLos clasificadores Bayesianos [DH73] son clasificadores estad\u00edsticos, que \npueden predecir tanto las probabilidades del n\u00famero de miembros de clase, como la probabilidad de que una muestra dada pertenezca a una clase particular. La clasificaci\u00f3n Bayesiana se basa en el teorema de Bayes, y los clasificadores Bayesianos han demostrado una alta exactitud y velocidad cuando se han aplicado a \ngrandes bases de datos Diferentes estudios comparando los algoritmos de \nclasificaci\u00f3n han determinado que un clasificador Bayesiano sencillo conocido como el clasificador \u201c naive  Bayesiano\u201d [JOH97] es comparable en rendimiento a un \u00e1rbol de \ndecisi\u00f3n y  a clasificadores de redes de neuronas. A continuaci\u00f3n se explica los fundamentos de los clasificadores bayesianos y, m\u00e1s concretamente, del clasificador naive  Bayesiano. Tras esta explicaci\u00f3n se comentar\u00e1 otro clasificador que, si bien no \nes un clasificador bayesiano, esta relacionado con \u00e9l, dado que se trata tambi\u00e9n de un \nclasificador basado en la estad\u00edstica. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 141 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n\u2022 Clasificador Naive Bayesiano  \n \nLo que normalmente se quiere saber en aprendizaje es cu\u00e1l es la mejor \nhip\u00f3tesis (m\u00e1s probable) dados los datos. Si denotamos P(D) como la probabilidad a \npriori de los datos (i.e., cuales datos son m\u00e1s probables que otros), P(D|h)  la \nprobabilidad de los datos dada una hip\u00f3tesis, lo que queremos estimar es: P(h|D) , la \nprobabilidad posterior de h dados los datos. Esto se puede estimar con el teorema de \nBayes, ecuaci\u00f3n 2.62. \n()( )()\n()DPhPh|DPD|hP =  Ec. 2.62 \nPara estimar la hip\u00f3tesis m\u00e1s probable (MAP, [maximum a posteriori hip\u00f3tesis]) \nse busca el mayor P(h|D)  como se muestra en la ecuaci\u00f3n 2.63. \n( ) ( )\n() ( )\n()\n() ( )() hPh|DP argmaxDPhPh|DPargmaxD|hP argmax h\nHhHhHh MAP\n\u2208\u2208\u2208\n=\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb==\n Ec. 2.63 \nYa que P(D) es una constante independiente de h. Si se asume que todas las \nhip\u00f3tesis son igualmente probables, entonces resulta la hip\u00f3tesis de m\u00e1xima \nverosimilitud (ML, [maximum likelihood]) de la ecuaci\u00f3n 2.64.  \n( ) ( )h|DP argmax hHh ML \u2208 =  Ec. 2.64 \nEl clasificador naive  [ingenuo] Bayesiano  se utiliza cuando se quiere clasificar \nun ejemplo descrito por un conjunto de atributos ( ai's) en un conjunto finito de clases \n(V). Clasificar un nuevo ejemplo de acuerdo con el valor m\u00e1s probable dados los \nvalores de sus atributos. Si se aplica 2.64 al problema de la clasificaci\u00f3n se obtendr\u00e1 \nla ecuaci\u00f3n 2.65. \n( ) ( )\n() ( )\n()\n() ( )()j j n 1 Vvn 1j j n 1\nVvn 1 j Vv MAP\nvPv|a,...,aP argmaxa,...,aPvPv|a,...,aPargmaxa,...,a|vP argmax v\njjj\n\u2208\u2208\u2208\n=\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb==\n Ec. 2.65 \nAdem\u00e1s, el clasificador naive  Bayesiano asume que los valores de los atributos \nson condicionalmente independientes dado el valor de la clase, por lo que se hace \ncierta la ecuaci\u00f3n 2.66 y con ella la 2.67. \n \n( ) ( ) \u220f=ij i j n 1 v|aP v|a,...,aP  Ec. 2.66 Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 142 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ( )() ( ) \u220f\u00d7 =ij i j n 1 j v|aP vP a,...,a|vP  Ec. 2.67 \nLos clasificadores naive  Bayesianos asumen que el efecto de un valor del \natributo en una clase dada es independiente de los valores de los otros atributos. Esta \nsuposici\u00f3n se llama \u201cindependencia condicional de clase\u201d. \u00c9sta simplifica los c\u00e1lculos \ninvolucrados y, en este sentido, es considerado \"ingenuo\u201d [naive]. Esta asunci\u00f3n es una simplificaci\u00f3n de la realidad. A pesar del nombre del clasificador y de la simplificaci\u00f3n realizada, el naive  Bayesiano funciona muy bien, sobre todo cuando se \nfiltra el conjunto de atributos seleccionado para eliminar redundancia, con lo que se elimina tambi\u00e9n dependencia entre datos. En la figura 2.28 se muestra un ejemplo de \naprendizaje con el clasificador naive Bayesiano , as\u00ed como una muestra de c\u00f3mo se \nclasificar\u00eda un ejemplo de test. Como ejemplo se emplear\u00e1 el de la tabla 2.1. \n \n \n \nFigura 3.26: Ejemplo de aprendizaje y clasificaci\u00f3n con naive Bayesiano.  \n \nEn este ejemplo se observa que en la fase de aprendizaje se obtienen todas \nlas probabilidades condicionadas P(a i|vj) y las probabilidades P(v j). En la clasificaci\u00f3n \nse realiza el productorio y se escoge como clase del ejemplo de entrenamiento la que obtenga un mayor valor. Algo que puede ocurrir durante el entrenamiento con este clasificador es que para cada valor de cada atributo no se encuentren ejemplos para todas las clases. Sup\u00f3ngase que para el atributo a\ni y el valor j de dicho atributo no hay \nning\u00fan ejemplo de entrenamiento con clase k. En este caso, P(a ij|k)=0 . Esto hace que \nsi se intenta clasificar cualquier ejemplo con el par atributo-valor aij, la probabilidad \nasociada para la clase k ser\u00e1 siempre 0, ya que hay que realizar el productorio de las \nprobabilidades condicionadas para todos los atributos de la instancia. Para resolver este problema se parte de que las probabilidades se contabilizan a partir de las \nfrecuencias de aparici\u00f3n de cada evento o, en nuestro caso, las frecuencias de \naparici\u00f3n de cada terna atributo-valor-clase. El estimador de Laplace , consiste en Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 143 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda comenzar a contabilizar la frecuencia de aparici\u00f3n de cada terna a partir del 1 y no del \n0, con lo que ninguna probabilidad condicionada ser\u00e1 igual a 0.  \nUna ventaja de este clasificador es la cuesti\u00f3n de los valores perdidos o \ndesconocidos: en el clasificador naive  Bayesiano si se intenta clasificar un ejemplo con \nun atributo sin valor simplemente el atributo en cuesti\u00f3n no entra en el productorio que sirve para calcular las probabilidades. Respecto a los atributos num\u00e9ricos, se suele suponer que siguen una distribuci\u00f3n Normal o Gaussiana. Para estos atributos se calcula la media \u00b5 y la desviaci\u00f3n t\u00edpica \u03c3 obteniendo los dos par\u00e1metros de la \ndistribuci\u00f3n N(\u00b5, \u03c3), que sigue la expresi\u00f3n de la ecuaci\u00f3n 2.68, donde el par\u00e1metro x \nser\u00e1 el valor del atributo num\u00e9rico en el ejemplo que se quiere clasificar. \n()()\n22\n2\u03c3\u00b5x\ne\n\u03c32\u03c01xf\u2212\u2212\n=  Ec. 2.68 \n \n\u2022 Votaci\u00f3n por intervalos de caracter\u00edsticas \n \nEste algoritmo es una t\u00e9cnica basada en la proyecci\u00f3n de caracter\u00edsticas. Se le \ndenomina \u201cvotaci\u00f3n por intervalos de caracter\u00eds ticas\u201d (VFI, [Voting Feature Interval]) \nporque se construyen intervalos para cada caracter\u00edstica [feature] o atributo en la fase de aprendizaje y el intervalo correspondiente en cada caracter\u00edstica \u201cvota\u201d para cada clase en la fase de clasificaci\u00f3n. Al igual que en el clasificador naive  Bayesiano, cada \ncaracter\u00edstica es tratada de forma individual e independiente del resto. Se dise\u00f1a un \nsistema de votaci\u00f3n para combinar las clasificaciones individuales de cada atributo por separado. \n \nMientras que en el clasificador naive  Bayesiano cada caracter\u00edstica participa en \nla clasificaci\u00f3n asignando una probabilidad para cada clase y la probabilidad final para \ncada clase consiste en el producto de cada probabilidad dada por cada caracter\u00edstica, \nen el algoritmo VFI cada caracter\u00edstica distribuye sus votos para cada clase y el voto final de cada clase es la suma de los votos obtenidos por cada caracter\u00edstica. Una ventaja de estos clasificadores, al igual que ocurr\u00eda con el clasificador naive  \nBayesiano, es el tratamiento de los valores desconocidos tanto en el proceso de \naprendizaje como en el de clasificaci\u00f3n: simplemente se ignoran, dado que se \nconsidera cada atributo como independiente del resto.  \n \nEn la fase de aprendizaje del algoritmo VFI se construyen intervalos para cada \natributo contabilizando, para cada clase, el n\u00famero de ejemplos de entrenamiento que aparecen en dicho intervalo. En la fase de clasificaci\u00f3n, cada atributo del ejemplo de \ntest a\u00f1ade votos para cada clase dependiendo del intervalo en el que se encuentre y \nel conteo de la fase de aprendizaje para dicho intervalo en cada clase. En la figura 2.29 se muestra este algoritmo. \n \nAprendizaje (ejemplos) { \n Para cada atributo (A) Hacer Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 144 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda   Si A es NUM\u00c9RICO Entonces \n   Obtener m\u00ednimo y m\u00e1ximo de A para cada clase en ejemplos \n   Ordenar los valores obtenidos (I intervalos) \n  Si no /* es SIMB\u00d3LICO */ \n   Obtener los valores que recibe A para cada clase en ejemplos \n   Los valores obtenidos son puntos (I intervalos) \n \n  Para cada intervalo I Hacer \n   Para cada clase C Hacer \n    contadores [A, I, C] = 0 \n \n  Para cada ejemplo E Hacer \n   Si A es conocido Entonces \n    Si A es SIMB\u00d3LICO Entonces \n     contadores [A, E.A, E.C] += 1 \n    Si no /* es NUM\u00c9RICO */ \n     Obtener intervalo I de E.A \n     Si E.A = extremo inferior de intervalo I Entonces \n      contadores [A, I, E.C] += 0.5 \n      contadores [A, I-1, E.C] += 0.5 \n     Si no \n      contadores [A, I, E.C] += 1 \n \n  Normalizar contadores[] /* \u03a3c contadores[A, I, C] = 1 */ \n} \n \nclasificar (ejemplo E) { \n Para cada atributo (A) Hacer \n  Si E.A es conocido Entonces \n   Si A es SIMB\u00d3LICO \n    Para cada clase C Hacer \n     voto[A, C] = contadores[A, E.A, C] \n   Si no /* es NUM\u00c9RICO */ \n    Obtener intervalo I de E.A \n    Si E.A = l\u00edmite inferior de I Entonces \n     Para cada clase C Hacer \n      voto[A, C] = 0.5*contadores[A,I,C] + \n          0.5*contadores[A,I-1,C] \n    Si no \n     Para cada clase C Hacer \n      voto[A, C] = contadores [A, I, C]    \n   \n  voto[C] = voto[C] +  voto[A, C] \n \n Normalizar voto[]/* \u03a3c voto[C] = 1 */ \nFigura 3.27: Pseudoc\u00f3digo del algoritmo VFI.  \n \nEn la figura 2.30 se presenta un ejemplo de entrenamiento y clasificaci\u00f3n con el \nalgoritmo VFI, en el que se muestra una tabla con los ejemplos de entrenamiento y \nc\u00f3mo el proceso de aprendizaje consiste en el establecimiento de intervalos para cada \natributo con el conteo de ejemplos que se encuentran en cada intervalo. Se muestra entre par\u00e9ntesis el n\u00famero de ejemplos que se encuentran en la clase e intervalo concreto, mientras que fuera de los par\u00e9ntesis se encuentra el valor normalizado. Para el atributo simb\u00f3lico simplemente se toma como intervalo (punto) cada valor de dicho atributo y se cuenta el n\u00famero de ejemplos que tienen un valor determinado en el \natributo para la clase del ejemplo en cuesti\u00f3n. En el caso del atributo num\u00e9rico, se \nobtiene el m\u00e1ximo y el m\u00ednimo valor del atributo para cada clase que en este caso son 4 y 7 para la clase A, y 1 y 5 para la clase B. Se ordenan los valores form\u00e1ndose un Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 145 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda total de cinco intervalos y se cuenta el n\u00famero de ejemplos que se encuentran en un \nintervalo determinado para su clase, teniendo en cuenta que si se encuentra en el \npunto compartido por dos intervalos se contabiliza la mitad para cada uno de ellos. \nTambi\u00e9n se muestra un ejemplo de clasificaci\u00f3 n: en primer lugar, se obtienen los votos \nque cada atributo por separado concede a cada clase, que ser\u00e1 el valor normalizado del intervalo (o punto si se trata de atributos simb\u00f3licos) en el que se encuentre el valor del atributo, y posteriormente se suman los votos (que se muestra entre par\u00e9ntesis) y se normaliza. La clase con mayor porcentaje de votos (en el ejemplo la \nclase A) gana . \n \n \nFigura 3.28: Ejemplo de aprendizaje y clasificaci\u00f3n con VFI.  \n \n3.5.5. Aprendizaje Basado en Ejemplares \nEl aprendizaje basado en ejemplares o instancias [BRIS96] tiene como \nprincipio de funcionamiento, en sus m\u00faltiples variantes, el almacenamiento de ejemplos: en unos casos todos los ejemplos de entrenamiento, en otros solo los m\u00e1s representativos, en otros los incorrectamente clasificados cuando se clasifican por \nprimera vez, etc. La clasificaci\u00f3n posterior se realiza por medio de una funci\u00f3n que \nmide la proximidad o parecido. Dado un ejemplo para clasificar se le clasifica de acuerdo al ejemplo o ejemplos m\u00e1s pr\u00f3ximos. El bias (sesgo) que rige este m\u00e9todo es \nla proximidad; es decir, la generalizaci\u00f3n se  gu\u00eda por la proximidad de un ejemplo a \notros. Algunos autores consideran este bias m\u00e1s apropiado para el aprendizaje de \nconceptos naturales que el correspondiente al proceso inductivo (Bareiss et al. en \n[KODR90]), por otra parte tambi\u00e9n se ha estudiado la relaci\u00f3n entre este m\u00e9todo y los que generan reglas (Clark, 1990). \n \nSe han enumerado ventajas e inconvenientes del aprendizaje basado en \nejemplares [BRIS96], pero se suele considerar no adecuado para el tratamiento de Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 146 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda atributos no num\u00e9ricos y valores desconoc idos. Las mismas medidas de proximidad \nsobre atributos simb\u00f3licos suelen proporci onar resultados muy dispares en problemas \ndiferentes. A continuaci\u00f3n se muestran dos t\u00e9cnicas de aprendizaje basado en \nejemplares: el m\u00e9todo de los k-vecinos m\u00e1s pr\u00f3ximos y el k estrella. \n \n\u2022 Algoritmo de los k-vecinos m\u00e1s pr\u00f3ximos \n \nEl m\u00e9todo de los k-vecinos m\u00e1s pr\u00f3ximos [MITC97] (KNN, [k-Nearest \nNeighbor]) est\u00e1 considerado como un buen representante de este tipo de aprendizaje, y es de gran sencillez conceptual. Se suele denominar m\u00e9todo porque es el esqueleto de un algoritmo que admite el intercambio de la funci\u00f3n de proximidad dando lugar a \nm\u00faltiples variantes. La funci\u00f3n de proximidad puede decidir la clasificaci\u00f3n de un \nnuevo ejemplo atendiendo a la clasificaci\u00f3n del ejemplo o de la mayor\u00eda de los k \nejemplos m\u00e1s cercanos. Admite tambi\u00e9n funciones de proximidad que consideren el peso o coste de los atributos que intervienen, lo que permite, entre otras cosas, eliminar los atributos irrelevantes. U na funci\u00f3n de proximidad cl\u00e1sica entre dos \ninstancias x\ni y xj , si suponemos que un ejemplo viene representado por una n-tupla de \nla forma ( a1(x), a 2(x), ... , a n(x)) en la que ar(x) es el valor de la instancia para el atributo \nar, es la distancia eucl\u00eddea, que se muestra en la ecuaci\u00f3n 2.69. \n()\u2211\n=\u2212 =n\n1l2\njl il j i xx )x,d(x  Ec. 2.69 \nEn la figura 2.31 se muestra un ejemplo del algoritmo KNN para un sistema de \ndos atributos, represent\u00e1ndose por ello en un plano. En este ejemplo se ve c\u00f3mo el proceso de aprendizaje consiste en el almacenamiento de todos los ejemplos de entrenamiento. Se han representado los ejemplos de acuerdo a los valores de sus dos atributos y la clase a la que pertenecen (las clases son + y -). La clasificaci\u00f3n consiste \nen la b\u00fasqueda de los k ejemplos (en este caso 3)  m\u00e1s cercanos al ejemplo a \nclasificar. Concretamente, el ejemplo a se clasificar\u00eda como -, y el ejemplo b como +. \n \n \n \nFigura 3.29: Ejemplo de Aprendizaje y Clasificaci\u00f3n con KNN.  \n \nDado que el algoritmo k-NN permite que los atributos de los ejemplares sean \nsimb\u00f3licos y num\u00e9ricos, as\u00ed como que haya atributos sin valor [missing values] el Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 147 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda algoritmo para el c\u00e1lculo de la distancia entre ejemplares se complica ligeramente. En \nla figura 2.32 se muestra el algoritmo que calcula la distancia entre dos ejemplares \ncualesquiera. \n \nDistancia (E1, E2) { \n dst = 0 \n n = 0 \n Para cada atributo A Hacer { \n  dif = Diferencia(E1.A, E2.A) \n  dst = dst + dif * dif \n  n = n + 1 \n } \n dst = dst / n \n Devolver dst \n} \n \nDiferencia (A1, A2) { \n Si A1.nominal Entonces { \n  Si SinValor(A1) O SinValor(A2) O A1 <> A2 Entonces \n   Devolver 1 \n  Si no \n   Devolver 0 \n } Si no { \n  Si SinValor(A1) O SinValor(A2) Entonces { \n   Si SinValor(A1) Y SinValor(A2) Entonces \n    Devolver 1 \n   Si SinValor(A1) Entonces \n    dif = A2 \n   Si no Entonces \n    dif = A1 \n   Si dif < 0.5 Entonces \n    Devolver 1 \u2013 dif \n   Si no \n    Devolver dif \n  } Si no \n   Devolver abs(A1 \u2013 A2) \n } \n} \nFigura 3.30: Pseudoc\u00f3digo del algoritmo empleado para definir la distancia entre dos ejemplos.  \n \nAdem\u00e1s de los distintos tipos de atributos hay que tener en cuenta tambi\u00e9n, en \nel caso de los atributos num\u00e9ricos, los rangos en los que se mueven sus valores. Para evitar que atributos con valores muy altos tengan mucho mayor peso que atributos con valores bajos, se normalizar\u00e1n dichos valores con la ecuaci\u00f3n 2.70. \nl ll il\nmin Maxminx\n\u2212\u2212 Ec. 2.70 \nEn esta ecuaci\u00f3n xif ser\u00e1 el valor i del atributo f, siendo min f el m\u00ednimo valor del \natributo f y Max f el m\u00e1ximo. Por otro lado, el algoritmo permite dar mayor preferencia a \naquellos ejemplares m\u00e1s cercanos al que deseamos clasificar. En ese caso, en lugar \nde emplear directamente la distancia entre ejemplares, se utilizar\u00e1 la ecuaci\u00f3n 2.71. Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 148 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda )x,d(x11\nj i + Ec. 2.71 \n \n \n\u2022 Algoritmo k-estrella \n \nEl algoritmo K* [CLTR95] es una t\u00e9cnica de data mining basada en ejemplares \nen la que la medida de la distancia entre ejemplares se basa en la teor\u00eda de la informaci\u00f3n. Una forma intuitiva de verlo es que la distancia entre dos ejemplares se \ndefine como la complejidad de transformar un ejemplar en el otro. El c\u00e1lculo de la \ncomplejidad se basa en primer lugar en definir un conjunto de transformaciones T={t\n1, \nt2, ..., t n , \u03c3} para pasar de un ejemplo (valor de atributo) a a uno b. La transformaci\u00f3n \u03c3 \nes la de parada y es la transformaci\u00f3n identidad ( \u03c3(a)=a ). El conjunto P es el conjunto \nde todas las posibles secuencias de transformaciones descritos en T* que terminan en \n\u03c3, y (a)t es una de estas secuencias concretas sobre el ejemplo a. Esta secuencia de \ntransformaciones tendr\u00e1 una probabilidad determinada )tp(, defini\u00e9ndose la funci\u00f3n de \nprobabilidad P*(b|a)  como la probabilidad de pasar del ejemplo a al ejemplo b a trav\u00e9s \nde cualquier secuencia de transformaciones, tal y como se muestra en la ecuaci\u00f3n \n2.72. \n\u2211\n= \u2208=\nb(a)t:Pt)tp( a)|(b*P  Ec. 2.72 \nEsta funci\u00f3n de probabilidad cumplir\u00e1 las propiedades que se muestran en  \n2.73. \n1a)|(b*P\nb= \u2211 ;  1a)|(b*P0 \u2264 \u2264   Ec. 2.73 \nLa funci\u00f3n de distancia K* se define entonces tomando logaritmos, tal y como \nse muestra en la ecuaci\u00f3n 2.74. \na)|(b*Plog a)|(b*K2 \u2212=  Ec. 2.74 \nRealmente K* no es una funci\u00f3n de distancia dado que, por ejemplo K*(a|a)  \ngeneralmente no ser\u00e1 exactamente 0, adem\u00e1s de que el operador | no es sim\u00e9trico, \nesto es, K*(a|b)  no es igual que K*(b|a) . Sin embargo, esto no interfiere en el algoritmo \nK*. Adem\u00e1s, la funci\u00f3n K* cumple las propiedades que se muestran en la ecuaci\u00f3n \n2.75. \n0a)|(b*K \u2265; a)|(c*Ka)|(b*Kb)|(c*K \u2265 +  Ec. 2.75 \nUna vez explicado c\u00f3mo se obtiene la funci\u00f3n K* y cuales son sus propiedades, \nse presenta a continuaci\u00f3n la expresi\u00f3n concreta de la funci\u00f3n P*, de la que se obtiene \nK*, para los tipos de atributos admitidos por el algoritmo: num\u00e9ricos y simb\u00f3licos.  \n \nProbabilidad de transformaci\u00f3n para los atributos permitidos Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 149 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \nEn cuanto a los atributos num\u00e9ricos, la s transformaciones consideradas ser\u00e1n \nrestar del valor a un n\u00famero n o sumar al valor a un n\u00famero n, siendo n un n\u00famero \nm\u00ednimo. La probabilidad de pasar de un ejemplo con valor a a uno con valor b vendr\u00e1 \ndeterminada \u00fanicamente por el valor absoluto de la diferencia entre a y b, que se \ndenominar\u00e1 x. Se escribir\u00e1 la funci\u00f3n de probabilidad como una funci\u00f3n de densidad, \ntal y como se muestra en la ecuaci\u00f3n 2.76, donde x0 ser\u00e1 una medida de longitud de la \nescala, por ejemplo, la media esperada para x sobre la distribuci\u00f3n P*. Es necesario \nelegir un x0 razonable. Posteriormente se mostrar\u00e1 un m\u00e9todo para elegir este factor. \nPara los simb\u00f3licos, se considerar\u00e1n las probabilidades de aparici\u00f3n de cada uno de \nlos valores de dicho atributo. \ndx e2x1(x)*P0xx\n0\u2212\n=  Ec. 2.76 \nSi el atributo tiene un total de n posibles valores, y la probabilidad de aparici\u00f3n \ndel valor i del atributo es pi (obtenido a partir de las apariciones en los ejemplos de \nentrenamiento), se define la probabilidad de transformaci\u00f3n de un ejemplo con valor i a \nuno con valor j como se muestra en la ecuaci\u00f3n 2.77. \n()\n() \uf8f4\uf8f3\uf8f4\uf8f2\uf8f1\n= +\u2260\n=\nji     sips-1sji i          sps-1\ni)|(j*P\nij Ec. 2.77 \nEn esta ecuaci\u00f3n s es la probabilidad del s\u00edmbolo de parada ( \u03c3). De esta forma, \nse define la probabilidad de cambiar de valor como la probabilidad de que no se pare la transformaci\u00f3n multiplicado por la probabilidad del valor de destino, mientras la probabilidad de continuar con el mismo valor es la probabilidad del s\u00edmbolo de parada m\u00e1s la probabilidad de que se contin\u00fae transformando multiplicado por la probabilidad \ndel valor de destino. Tambi\u00e9n es importante, al igual que con el factor x\n0, definir \ncorrectamente la probabilidad s. Y como ya se coment\u00f3 con x0, posteriormente se \ncomentar\u00e1 un m\u00e9todo para obtenerlo. Tambi\u00e9n deben tenerse en cuenta la posibilidad de los atributos con valores desconocidos. Cuando los valores desconocidos aparecen en los ejemplos de entrenamiento se propone como soluci\u00f3n el considerar que el \natributo desconocido se determina a trav\u00e9s del  resto de ejemplares de entrenamiento. \nEsto se muestra en la ecuaci\u00f3n 2.78, donde n es el n\u00famero de ejemplos de \nentrenamiento. \n\u2211\n==n\n1b na)|(b*Pa)|(?*P  Ec. 2.78 \n \nCombinaci\u00f3n de atributos \n \nYa se han definido las funciones de probabilidad para los tipos de atributos \npermitidos. Pero los ejemplos reales tienen m\u00e1s de un atributo, por lo que es necesario combinar los resultados obtenidos para cada atributo. Y para combinarlos, y definir as\u00ed la distancia entre dos ejemplos, se entiende la probabilidad de transformaci\u00f3n de un ejemplar en otro como la probabilidad de trans formar el primer atributo del primer \nejemplo en el del segundo, seguido de la transformaci\u00f3n del segundo atributo del \nprimer ejemplo en el del segundo, etc. De esta forma, la probabilidad de transformar Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 150 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda un ejemplo en otro viene determinado por la multiplicaci\u00f3n de las probabilidades de \ntransformaci\u00f3n de cada atributo de forma individual, tal y como se muestra en la \necuaci\u00f3n 2.79. En esta ecuaci\u00f3n m ser\u00e1 el n\u00famero de atributo de los ejemplos. Y con \nesta definici\u00f3n la distancia entre dos ejemplos se define como la suma de distancias entre cada atributo de los ejemplos. \n\u220f\n==m\n1i1i 2i 1 2 )v|(v*P )E|(E*P  Ec. 2.79 \n \nSelecci\u00f3n de los par\u00e1metros aleatorios \n \nPara cada atributo debe determinarse el valor para los par\u00e1metros s o x0 seg\u00fan \nse trate de un atributo simb\u00f3lico o num\u00e9rico respectivamente. Y el valor de este atributo es muy importante. Por ejemplo, si a s se le asigna un valor muy bajo las \nprobabilidades de transformaci\u00f3n ser\u00e1n muy altas, mientras que si s se acerca a 0 las \nprobabilidades de transformaci\u00f3n ser\u00e1n muy bajas. Y lo mismo ocurrir\u00eda con el \npar\u00e1metro x\n0. En ambos casos se puede observar c\u00f3mo var\u00eda la funci\u00f3n de \nprobabilidad P* seg\u00fan se var\u00eda el n\u00famero de ejemplos incluidos partiendo desde 1 \n(vecino m\u00e1s cercano) hasta n (todos los ejemplares con el mismo peso). Se puede \ncalcular para cualquier funci\u00f3n de probabilidad el n\u00famero efectivo de ejemplos como se muestra en la ecuaci\u00f3n 2.80, en la que n es el n\u00famero de ejemplos de \nentrenamiento y n\n0 es el n\u00famero de ejemplos con la distancia m\u00ednima al ejemplo a \n(para el atributo considerado). El algoritmo K* escoger\u00e1 para x0 (o s) un n\u00famero entre \nn0 y n. \n() ()\n()n\na|b*Pa|b*P\nn2n\n1b2n\n1b\n0 \u2264 \u2264\n\u2211\u2211\n== Ec. 2.80 \nPor conveniencia se expresa el valor escogido como un par\u00e1metro de \nmezclado  [blending] b, que var\u00eda entre b=0%  (n0) y b=100%  (n). La configuraci\u00f3n de \neste par\u00e1metro se puede ver como una esfera de influencia  que determina cuantos \nvecinos de a deben considerarse importantes. Para obtener el valor correcto para el \npar\u00e1metro x0 (o s) se realiza un proceso iterativo en el que se obtienen las esferas de \ninfluencia m\u00e1xima ( x0 o s igual a 0) y m\u00ednima ( x0 o s igual a 1), y se aproximan los \nvalores para que dicha esfera se acerque a la necesaria para cumplir con el par\u00e1metro \nde mezclado. \n \nEn la figura 2.33 se presenta un ejemplo pr\u00e1ctico de c\u00f3mo obtener los valores \npara los par\u00e1metros x0 o s. Se va a utilizar para ello el problema que se present\u00f3 en la \ntabla 2.1, y m\u00e1s concretamente el atributo Vista  con el valor igual a Lluvioso , de dicho \nproblema. \n Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 151 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nFigura 3.31: Ejemplo de obtenci\u00f3n del par\u00e1metros de un atributo simb\u00f3lico con el \nalgoritmo K*.  \n \nEn la figura 2.33 se muestra c\u00f3mo el objetivo es conseguir un valor para s tal \nque se obtenga una esfera de influencia de 6,8 ejemplos. Los par\u00e1metros de \nconfiguraci\u00f3n necesarios para el funcionamiento del sistema son: el par\u00e1metro de \nmezclado b, en este caso igual a 20%; una constante denominada EPSILON , en este \ncaso igual a 0,01, que determina entre otras cosas cu\u00e1ndo se considera alcanzada la \nesfera de influencia deseada. En cuanto a la nomenclatura empleada, n ser\u00e1 el \nn\u00famero total de ejemplos de entrenamiento, nv el n\u00famero de valores que puede \nadquirir el atributo, y se han empleado abreviaturas para denominar los valores del \natributo: lluv por lluvioso, nub por nublado y sol por soleado.  \n \nTal y como puede observarse en la figura 2.33, las ecuaciones empleadas para \nel c\u00e1lculo de la esfera y de P* no son exactamente las definidas en las ecuaciones \ndefinidas anteriormente. Sin embargo, en el ejemplo se han empleado las implementadas en la herramienta WEKA por los creadores del algoritmo. En cuanto al Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 152 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ejemplo en s\u00ed, se muestra c\u00f3mo son necesarias 8 iteraciones para llegar a conseguir el \nobjetivo planteado, siendo el resultado de dicho proceso, el valor de s, igual a 0,75341 . \n \nClasificaci\u00f3n de un ejemplo \nSe calcula la probabilidad de que un ejemplo a pertenezca a la clase c \nsumando la probabilidad de a a cada ejemplo que es miembro de c, tal y como se \nmuestra en 2.81. \n() () \u2211\n\u2208=\ncba|b*P a|c*P  Ec. 2.81 \nSe calcula la probabilidad de pertenencia a cada clase y se escoge la que \nmayor resultado haya obtenido como predicci\u00f3n para el ejemplo. \n \n \n \nFigura 3.32: Ejemplo de clasificaci\u00f3n con K*. \n \nUna vez definido el modo en que se clasifica un determinado ejemplo de test \nmediante el algoritmo K*, en la figura 2.34 se muestra un ejemplo concreto en el que Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 153 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda se emplea dicho algoritmo. En el ejemplo se clasifica un ejemplo de test tomando \ncomo ejemplos de entrenamiento los que se mostraron en la tabla 2.1, tomando los \natributos Temperatura  y Humedad  como num\u00e9ricos. El proceso que se sigue para \ndeterminar a qu\u00e9 clase pertenece un ejemplo de test determinado es el siguiente: en \nprimer lugar, habr\u00eda que calcular los par\u00e1metros x0 y s que a\u00fan no se conocen para los \npares atributo-valor  del ejemplo de test. Posteriormente se aplican las ecuaciones, que \nde nuevo no son exactamente las definidas anteriormente: se han empleado las que los autores del algoritmo implementan en la herramienta WEKA. Una vez obtenidas las \nprobabilidades, se normalizan y se escoge la mayor de las obtenidas. En este caso \nhay m\u00e1s de un 99% de probabilidad a favor de la clase no. Esto se debe a que el \nejemplo 14 (el \u00faltimo) es casi id\u00e9ntico al ejemplo de test por clasificar. En este ejemplo \nno se detallan todas las operaciones realizadas, sino un ejemplo de cada tipo: un ejemplo de la obtenci\u00f3n de P* para un atributo simb\u00f3lico, otro de la obtenci\u00f3n de P* \npara un atributo num\u00e9rico y otro para la obtenci\u00f3n de la probabilidad de transformaci\u00f3n \ndel ejemplo de test en un ejemplo de entrenamiento. \n \n3.5.6. Redes de Neuronas \nLas redes de neuronas constituyen una t\u00e9cnica inspirada en los trabajos de \ninvestigaci\u00f3n, iniciados en 1930, que pretend\u00edan modelar computacionalmente el \naprendizaje humano llevado a cabo a trav\u00e9s de las neuronas en el cerebro [RM86, CR95]. Posteriormente se comprob\u00f3 que tales modelos no eran del todo adecuados para describir el aprendizaje humano. Las redes de neuronas constituyen una nueva forma de analizar la informaci\u00f3n con una diferencia fundamental con respecto a las \nt\u00e9cnicas tradicionales: son capaces de detectar y aprender complejos patrones y \ncaracter\u00edsticas dentro de los datos [SN88, FU94]. Se comportan de forma parecida a nuestro cerebro aprendiendo de la experiencia y del pasado, y aplicando tal conocimiento a la resoluci\u00f3n de problemas nuevos. Este aprendizaje se obtiene como resultado del adiestramiento (\" training \") y \u00e9ste permite la sencillez y la potencia de \nadaptaci\u00f3n y evoluci\u00f3n ante una realidad cambiante y muy din\u00e1mica. Una vez \nadiestradas las redes de neuronas pueden hacer previsiones, clasificaciones y segmentaci\u00f3n. Presentan adem\u00e1s, una eficienc ia y fiabilidad similar a los m\u00e9todos \nestad\u00edsticos y sistemas expertos, si no mejor, en la mayor\u00eda de los casos. En aquellos casos de muy alta complejidad las redes neuronales se muestran como especialmente \u00fatiles dada la dificultad de modelado que supone para otras t\u00e9cnicas. Sin embargo las \nredes de neuronas tienen el inconveniente de la dificultad de acceder y comprender \nlos modelos que generan y presentan dificultades para extraer reglas de tales modelos. Otra caracter\u00edstica es que son capaces de trabajar con datos incompletos e, incluso, contradictorios lo que, dependiendo del problema, puede resultar una ventaja o un inconveniente. Las redes de neuronas poseen las dos formas de aprendizaje: \nsupervisado y no supervisado; ya comentadas [WI98], derivadas del tipo de paradigma \nque usan: el no supervisado (usa paradigmas como los ART \u201c Adaptive Resonance \nTheory \"), y el supervisado que suele usar el paradigma del \u201c Backpropagation \" \n[RHW86].  \n Las redes de neuronas est\u00e1n siendo utilizadas en distintos y variados sectores \ncomo la industria, el gobierno, el ej\u00e9rcito, las comunicaciones, la investigaci\u00f3n \naerospacial, la banca y las finanzas, los seguros, la medicina, la distribuci\u00f3n, la rob\u00f3tica, el marketing, etc. En la actualidad se est\u00e1 estudiando la posibilidad de utilizar t\u00e9cnicas avanzadas y novedosas como los Algoritmos Gen\u00e9ticos para crear nuevos \nparadigmas que mejoren el adiestramiento y la propia selecci\u00f3n y dise\u00f1o de la Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 154 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda arquitectura de la red (n\u00famero de capas y neuronas), dise\u00f1o que ahora debe \nrealizarse en base a la experiencia del analista y para cada problema concreto. \n \n\u2022 Estructura de las Redes de Neuronas \n \nLas redes neuronales se construyen estructurando en una serie de niveles o \ncapas (al menos tres: entrada, procesamient o u oculta y salida) compuestas por nodos \no \"neuronas\", que tienen la estructura que se muestra en la figura 2.35. \n \nFigura 3.33: Estructura de una neurona. \n \nTanto el umbral como los pesos son constantes que se inicializar\u00e1n \naleatoriamente y durante el proceso de aprendizaje ser\u00e1n modificados. La salida de la neurona se define tal y como se muestra en las ecuaciones 2.82 y 2.83. \n \nUwX NETN\n1iii+ =\u2211\n= Ec. 2.82 \nf(NET)S=  Ec. 2.83 \nComo funci\u00f3n f se suele emplear una funci\u00f3n sigmoidal, bien definida entre 0 y \n1 (ecuaci\u00f3n 2.84) o entre \u20131 y 1 (ecuaci\u00f3n 2.85). \n \nx-e11f(x)+=  Ec. 2.84 \nx xx x\neeeef(x)\u2212\u2212\n+\u2212=  Ec. 2.85 \nCada neurona est\u00e1 conectada a todas las neuronas de las capas anterior y \nposterior a trav\u00e9s de los pesos o \"dendritas\", tal y como se muestra en la figura 2.36.  \n \nCap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 155 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nFigura 3.34: Estructura de la red de neuronas. \n \nCuando un nodo recibe las entradas o \"est\u00edmulos\" de otras los procesa para \nproducir una salida que transmite a la siguiente capa de neuronas. La se\u00f1al de salida tendr\u00e1 una intensidad fruto de la combinaci\u00f3n de la intensidad de las se\u00f1ales de entrada y de los pesos que las transmiten. Los pesos o dendritas tienen un valor \ndistinto para cada par de neuronas que conectan pudiendo as\u00ed fortalecer o debilitar la \nconexi\u00f3n o comunicaci\u00f3n entre neuronas parti culares. Los pesos  son modificados \ndurante el proceso de adiestramiento.   \nEl dise\u00f1o de la red de neuronas consistir\u00e1, entre otras cosas, en la definici\u00f3n \ndel n\u00famero de neuronas de las tres capas de la red. Las neuronas de la capa de \nentrada y las de la capa de salida vienen dadas por el problema a resolver, dependiendo de la codificaci\u00f3n de la informaci\u00f3n. En cuanto al n\u00famero de neuronas ocultas (y/o n\u00famero de capas ocultas) se determinar\u00e1 por prueba y error. Por \u00faltimo, debe tenerse en cuenta que la estructura de las neuronas de la capa de entrada se simplifica, dado que su salida es igual a su entrada: no hay umbral ni funci\u00f3n de salida. \n \n\u2022 Proceso de adiestramiento (retropropagaci\u00f3n) \n \nExisten distintos m\u00e9todos o paradigmas mediante los cuales estos pesos \npueden ser variados durante el adiestramiento de los cuales el m\u00e1s utilizado es el de \nretropropagaci\u00f3n [Backpropagation] [RHW86]. Este paradigma var\u00eda los pesos de acuerdo a las diferencias encontradas entre la salida obtenida y la que deber\u00eda obtenerse. De esta forma, si las diferencias son grandes se modifica el modelo de forma importante y seg\u00fan van siendo menores , se va convergiendo a un modelo final \nestable.  El error en una red de neuronas para un patr\u00f3n  \n[x= (x 1, x2, \u2026, x n), t(x)] , siendo \nx el patr\u00f3n de entrada, t(x) la salida deseada e y(x) la proporcionada por la red, se \ndefine como se muestra en la ecuaci\u00f3n 2.86 para m neuronas de salida y como se \nmuestra en la ecuaci\u00f3n 2.87 para 1 neurona de salida. \n \n\u2211\n=\u2212 = \u2212 =m\n1i2\ni i2(x))y (x)(t21y(x) t(x) e(x)  Ec. 2.86 \n2y(x)) (t(x)21e(x) \u2212 =  Ec. 2.87 Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 156 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El m\u00e9todo de descenso de gradiente consiste en modificar los par\u00e1metros de la \nred siguiendo la direcci\u00f3n negativa del gradiente del error. Lo que se realizar\u00eda \nmediante 2.88. \nwe\u03b1 wwe\u03b1 w wanterior anterior nuevo\n\u2202\u2202\u2212 =\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\n\u2202\u2202\u2212+ =  Ec. 2.88 \nEn la ecuaci\u00f3n 2.88, w es el peso a modificar en la red de neuronas (pasando \nde wanterior a wnuevo) y \u03b1 es la raz\u00f3n de aprendizaje, que se encarga de controlar cu\u00e1nto \nse desplazan los pesos en la direcci\u00f3n negativa del gradiente. Influye en la velocidad \nde convergencia del algoritmo, puesto que determina la magnitud del desplazamiento. \nEl algoritmo de retropropagaci\u00f3n es el resultado de aplicar el m\u00e9todo de descenso del gradiente a las redes de neuronas. El algoritmo completo de retropropagaci\u00f3n se muestra en la figura 2.37. \n \n \nPaso 1:  Inicializaci\u00f3n aleatoria de los pesos y umbrales. \nPaso 2:  Dado un patr\u00f3n del conjunto de entrenamiento (x, t(x)) , se \npresenta el vector xa la red y se calcula la salida de la \nred para dicho patr\u00f3n, y(x). \nPaso 3:  Se eval\u00faa el error e(x) cometido por la red. \nPaso 4:  Se modifican todos los par\u00e1metros de la red utilizando la \nec.2.88. \nPaso 5:  Se repiten los pasos 2, 3 y 4 para todos los patrones de \nentrenamiento, completando as\u00ed un ciclo de aprendizaje. \nPaso 6:  Se realizan n ciclos de aprendizaje (pasos 2, 3, 4 y 5) \nhasta que se verifique el criterio de parada establecido. \n \nFigura 3.35: Pseudoc\u00f3digo del algoritmo de retropropagaci\u00f3n. \n \nEn cuanto al criterio de parada, se debe calcular la suma de los errores en los \npatrones de entrenamiento. Si el error es constante de un ciclo a otro, los par\u00e1metros dejan de sufrir modificaciones y se obtiene as\u00ed el error m\u00ednimo. Por otro lado, tambi\u00e9n se debe tener en cuenta el error en los patrones de validaci\u00f3n, que se presentar\u00e1n a la red tras n ciclos de aprendizaje. Si el error en los patrones de validaci\u00f3n evoluciona \nfavorablemente se contin\u00faa con el proceso de aprendizaje. Si el error no desciende, se \ndetiene el aprendizaje. \n \n \n3.5.7. L\u00f3gica borrosa (\u201cFuzzy logic\u201d)  \nLa l\u00f3gica borrosa surge de la necesidad de modelar la realidad de una forma \nm\u00e1s exacta evitando precisamente el determini smo o la exactitud [ZAD65, CPS98]. En Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 157 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda palabras menos pretenciosas lo que la l\u00f3gica borrosa permite es el tratamiento proba-\nbil\u00edstico de la categorizaci\u00f3n de un colectivo [ZAD65]. \n \nAs\u00ed, para establecer una serie de grupos, segmentos o clases en los cuales se \npuedan clasificar a las personas por la edad, lo inmediato ser\u00eda proponer unas edades l\u00edmite para establecer tal clasificaci\u00f3n de forma disjunta. As\u00ed los ni\u00f1os ser\u00edan aquellos \ncuya edad fuera menor a los 12 a\u00f1os, los adolescentes aquellos entre 12 y 17 a\u00f1os, los j\u00f3venes aquellos entre 18 y 35, las personas maduras entre 36 y 45 a\u00f1os y as\u00ed \nsucesivamente. Se habr\u00edan creado unos grupos disjuntos cuyo tratamiento, a efectos \nde clasificaci\u00f3n y procesamiento, es muy sencillo: basta comparar la edad de cada persona con los l\u00edmites establecidos. Sin embargo enseguida se observa que esto supone una simplificaci\u00f3n enorme dado que una persona de 16 a\u00f1os 11 meses y veinte d\u00edas pertenecer\u00eda al grupo de los adolescentes y, seguramente, es m\u00e1s pareci-\ndo a una persona de 18 (miembro de otro grupo) que a uno de 12 (miembro de su \ngrupo). L\u00f3gicamente no se puede establecer un grupo para cada a\u00f1o, dado que s\u00ed se reconocen grupos, y no muchos, con comportamientos y actitudes similares en funci\u00f3n de la edad. Lo que impl\u00edcitamente se esta des cubriendo es que las clases existen pero \nque la frontera entre ellas no es clara ni disjunta sino \u201cdifusa\u201d  y que una persona puede \ntener aspectos de su mentalidad asociados a un grupo y otros asociados a otro grupo, \nes decir que impl\u00edcitamente se est\u00e1 distribuyendo la pertenencia entre varios grupos. \nCuando esto se lleva a una formalizaci\u00f3n matem\u00e1tica surge el concepto de distribuci\u00f3n de posibilidad, de forma que lo que entender\u00eda como funci\u00f3n de pertenencia a un grupo de edad ser\u00edan unas curvas de posibilidad. Por tanto, la l\u00f3gica borrosa es aquella t\u00e9cnica que permite y trata la existencia de barreras difusas o suaves entre los \ndistintos grupos en los que se categoriza un colectivo o entre los distintos elementos, \nfactores o proporciones que concurren en una situaci\u00f3n o soluci\u00f3n [BS97].  \n \nPara identificar las \u00e1reas de utilizaci\u00f3n de la l\u00f3gica difusa basta con determinar \ncuantos problemas hacen uso de la categoriz aci\u00f3n disjunta en el tratamiento de los \ndatos para observar la cantidad de posibles aplicaciones que esta t\u00e9cnica puede tener \n[ZAD65].. Sin embargo, el tratamiento ortodoxo y purista no siempre est\u00e1 justificado dada la complejidad que induce en el procesamiento (pasamos de valores a funciones de posibilidad) y un modelado sencillo puede ser m\u00e1s que suficiente. A\u00fan as\u00ed, existen problem\u00e1ticas donde este modelado s\u00ed resulta justificado, como en el control de procesos y la rob\u00f3tica, entre otros. Tal es as\u00ed que un pa\u00eds como Jap\u00f3n, l\u00edder en la \nindustria y la automatizaci\u00f3n, dispone del \"Laboratory for International Fuzzy \nEngineering Research\" (LIFE) y empresas como Yamaichi Securities y Canon hacen un extenso uso de esta t\u00e9cnica. \n \n3.5.8. T\u00e9cnicas Gen\u00e9ticas: Algoritmos Gen\u00e9ticos \n(\u201cGenetic Algorithms\u201d)  \nLos Algoritmos Gen\u00e9ticos son otra t\u00e9cnica que tiene su inspiraci\u00f3n, en la \nBiolog\u00eda como las Redes de Neuronas [GOLD89, MIC92, MITC96]. Estos algoritmos representan el modelado matem\u00e1tico de como los cromosomas en un marco \nevolucionista alcanzan la estructura y composici\u00f3n m\u00e1s \u00f3ptima en aras de la \nsupervivencia. Entendiendo la evoluci\u00f3n como un proceso de b\u00fasqueda y optimizaci\u00f3n de la adaptaci\u00f3n de las especies que se plasma en mutaciones y cambios de los Cap\u00edtulo 3  T\u00e9cnicas de Miner\u00eda de Datos basadas en  \n  Aprendizaje Autom\u00e1tico \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 158 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda genes o cromosomas, los Algoritmos Gen\u00e9ticos hacen uso de las t\u00e9cnicas biol\u00f3gicas \nde reproducci\u00f3n (mutaci\u00f3n y cruce) para se r utilizadas en todo tipo de problemas de \nb\u00fasqueda y optimizaci\u00f3n. Se da la mutaci\u00f3n cuando alguno o algunos de los genes \ncambian bien de forma aleatoria o de forma controlada v\u00eda funciones y se obtiene el cruce cuando se construye una nueva soluci\u00f3n a partir de dos contribuciones procedentes de otras soluciones \"padre\". En cualquier caso, tales transformaciones se realizan sobre aquellos especimenes o soluciones m\u00e1s aptas o mejor adaptadas. Dado que los mecanismos biol\u00f3gicos de ev oluci\u00f3n han dado lugar a soluciones, los \nseres vivos, realmente id\u00f3neas cabe esperar que la aplicaci\u00f3n de tales mecanismos a \nla b\u00fasqueda y optimizaci\u00f3n de otro tipo de problemas tenga el mismo resultado. De esta forma los Algoritmos Gen\u00e9ticos transforman los problemas de b\u00fasqueda y optimizaci\u00f3n de soluciones un proceso de evoluci\u00f3n de unas soluciones de partida. Las soluciones se convierten en cromosomas, transformaci\u00f3n que se realiza pasando \nlos datos a formato binario, y a los mejores se les van aplicando las reglas de \nevoluci\u00f3n (funciones probabil\u00edsticas de transici\u00f3n) hasta encontrar la soluci\u00f3n \u00f3ptima. En muchos casos, estos mecanismos brindan posibilidades de convergencia m\u00e1s r\u00e1pidos que otras t\u00e9cnicas. \n El uso de estos algoritmos no est\u00e1 tan extendido como otras t\u00e9cnicas, pero van \nsiendo cada vez m\u00e1s utilizados directamente en la soluci\u00f3n de problemas, as\u00ed como en \nla mejora de ciertos procesos presentes en otras herramientas. As\u00ed, por ejemplo, se usan para mejorar los procesos de adiestramiento y selecci\u00f3n de arquitectura de las redes de neuronas, para la generaci\u00f3n e inducci\u00f3n de \u00e1rboles de decisi\u00f3n y para la s\u00edntesis de programas a partir de ejemplos (\"Genetic Programming\"). \n \n \n \n   Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 159 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n \nCap\u00edtulo 4. T\u00e9cnicas de \nAn\u00e1lisis de Datos en Weka  \n \nIntroducci\u00f3n \nEn este cap\u00edtulo se presenta de forma concisa y pr\u00e1ctica la herramienta de \nminer\u00eda de datos WEKA. WEKA, acr\u00f3nimo de Waikato Environment for \nKnowledge Analysis , es un entorno para experimentaci\u00f3n de an\u00e1lisis de datos \nque permite aplicar, analizar y evaluar las t\u00e9cnicas m\u00e1s relevantes de an\u00e1lisis de datos, principalmente las provenient es del aprendizaje autom\u00e1tico, sobre \ncualquier conjunto de datos del usuario. Para ello \u00fan icamente se requiere que \nlos datos a analizar se almacenen con un cierto formato, conocido como ARFF  \n(Attribute-Relation File Format ).  \nWEKA se distribuye como software de lib re distribuci\u00f3n desarrollado en Java. \nEst\u00e1 constituido por una serie de paquet es de c\u00f3digo abierto con diferentes \nt\u00e9cnicas de preprocesado, clasificac i\u00f3n, agrupamiento, asociaci\u00f3n, y \nvisualizaci\u00f3n, as\u00ed como facilidades para su  aplicaci\u00f3n y an\u00e1lisis de prestaciones \ncuando son aplicadas a los datos de ent rada seleccionados. Estos paquetes \npueden ser integrados en cualquier proyec to de an\u00e1lisis de datos, e incluso \npueden extenderse con contribuciones de los usuarios que desarrollen nuevos \nalgoritmos. Con objeto de facilitar su  uso por un mayor n\u00famero de usuarios, \nWEKA adem\u00e1s incluye una interfaz gr \u00e1fica de usuario para acceder y \nconfigurar las diferentes herramientas integradas.  \nEste cap\u00edtulo tiene un enfoque pr\u00e1ctico y f uncional, pretendiendo servir de gu\u00eda \nde utilizaci\u00f3n de esta herramienta desde su  interfaz gr\u00e1fica, como material \ncomplementario a la escasa  documentaci\u00f3n disponible. Para ello se obviar\u00e1n \nlos detalles t\u00e9cnicos y espec\u00edficos de los diferentes algoritmos, que se presentan en un cap\u00edtulo aparte, y se centrar\u00e1 en su aplicaci\u00f3n, configuraci\u00f3n y an\u00e1lisis dentro de la herramient a. Por tanto, se remite al  lector al cap\u00edtulo con \nlos detalles de los algoritmos para conocer  sus caracter\u00edsticas, par\u00e1metros de \nconfiguraci\u00f3n, etc. A qu\u00ed se han seleccionado algunas de las t\u00e9cnicas \ndisponibles para aplicarlas a ejemplos c oncretos, siguiendo el acceso desde la \nherramienta al resto de t\u00e9cnicas implementadas, una mec\u00e1nica totalmente an\u00e1loga a la presentada a modo ilustrativo. \nPara reforzar el car\u00e1cter pr\u00e1ctico de este cap\u00edtulo, adem\u00e1s se adoptar\u00e1 un \nformato de tipo tutorial, con un conjunto de  datos disponibles sobre el que se \nir\u00e1n aplicando las diferentes  facilidades de WEKA. Se sugiere que el lector Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 160 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda aplique los pasos indicados y realice lo s an\u00e1lisis sugeridos para cada t\u00e9cnica \ncon objeto de familiarizarse y mejorar su comprensi\u00f3n.  Los ejemplos \nseleccionados son contienen datos proven ientes del campo de la ense\u00f1anza, \ncorrespondientes a alumnos que realizar on las pruebas de se lectividad en los \na\u00f1os 1993-2003 procedentes de diferentes  centros de ense\u00f1anza secundaria \nde la comunidad de Madrid. Por tanto, es ta gu\u00eda ilustra la apl icaci\u00f3n y an\u00e1lisis \nde t\u00e9cnicas de extracci\u00f3n de conocimi ento sobre datos del campo de la \nense\u00f1anza, aunque ser\u00eda directa su traslaci\u00f3n a cualquier otra disciplina. \n \nPreparaci\u00f3n de los datos \nLos datos de entrada a la herramienta, sobre los que operar\u00e1n las t\u00e9cnicas \nimplementadas, deben estar codificados en un formato espec\u00edfico, denominado Attribute-Relation File Format  (extensi\u00f3n \"arff\"). La he rramienta permite cargar \nlos datos en tres soportes: fichero de texto, acceso a una base de datos y \nacceso a trav\u00e9s de internet sobre una direcci\u00f3n URL de un servidor web. En \nnuestro caso trabajaremos con ficheros  de texto. Los datos deben estar \ndispuestos en el fichero de la forma si guiente: cada instancia en una fila, y con \nlos atributos separados por comas. El  formato de un fichero arff sigue la \nestructura siguiente: \n% comentarios \n@relation NOMBRE_RELACION @attribute r1 real \n@attribute r2 real ... \n... \n@attribute i1 integer  \n@attribute i2 integer  \n\u2026 \n@attribute s1 {v1_s1, v2_s1,\u2026vn_s1} @attribute s2 {v1_s1, v2_s1,\u2026vn_s1} \n\u2026 \n@data  \nDATOS \npor tanto, los atributos pueden ser principalmente de dos tipos: num\u00e9ricos de \ntipo real o entero (indicado con las palabra real o integer tras el nombre del \natributo), y simb\u00f3licos, en cuyo caso se  especifican los valores posibles que \npuede tomar entre llaves.  \n \nMuestra de datos \nEl fichero de datos objeto de an\u00e1lisis  en esta gu\u00eda contiene muestras \ncorrespondientes a 18802 alum nos presentados a las prue bas de selectividad y \nlos resultados obtenidos en las pruebas. Los datos que describen cada alumno \ncontienen la siguiente informaci\u00f3n: a\u00f1o, convocatoria, localidad del centro, \nopci\u00f3n cursada (de 5 posibles), calificac iones parciales obtenidas en lengua, Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 161 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda historia, idioma y las tres asignatura s opcionales, as\u00ed como la designaci\u00f3n de \nlas asignaturas de idioma y las 3 opc ionales cursadas, calificaci\u00f3n en el \nbachillerato, calificaci\u00f3n final  y si el alumno se pres ent\u00f3 o no a la prueba. Por \ntanto, puede comprobarse que la c abecera del fichero de datos, \n\"selectividad.arff\", sigue el fo rmato mencionado anteriormente: \n@relation selectividad \n \n@attribute A\u00f1o_acad\u00e9mico real \n@attribute convocatoria {J, S} \n@attribute localidad {ALPEDRETE, ARANJUEZ, ... } @attribute opcion1\u00aa {1,2,3,4,5} \n@attribute nota_Lengua real \n@attribute nota_Historia real \n@attribute nota_Idioma real \n@attribute des_Idioma {INGLES, FRANCES, ALEMAN} \n@attribute des_asig1 {BIOLOGIA, DIB.ARTISTICO_II,... } \n@attribute calif_asig1 real \n@attribute des_asig2 {BIOLOGIA, C.TIERRA, ...} \n@attribute calif_asig2 real @attribute des_asig3 {BIOLOGIA, C.TIERRA, ...} \n@attribute calif_asig3 real \n@attribute cal_prueba real \n@attribute nota_bachi real \n@attribute cal_final real \n@attribute Presentado {SI, NO} \n@data  \n... \n \nObjetivos del an\u00e1lisis \nAntes de comenzar con la aplicaci\u00f3n de las t\u00e9cnicas de WEKA a los datos de \neste dominio, es muy conveniente hacer  una consideraci\u00f3n acerca de los \nobjetivos perseguidos en el an\u00e1lisis. Co mo se mencion\u00f3 en la  introducci\u00f3n, un \npaso previo a la b\u00fasqueda de relaciones y modelos subyacentes en los datos \nha de ser la comprensi\u00f3n del dominio de aplicaci\u00f3n y establecer una idea clara \nacerca de los objetivos del usuario final. De esta manera, el proceso de an\u00e1lisis \nde datos (proceso KDD ), permitir\u00e1 dirigir la b\u00fas queda y hacer refinamientos, \ncon una interpretaci\u00f3n adecuada de los resultados generados. Los objetivos, \nutilidad, aplicaciones, etc., del an\u00e1lisis  efectuado no \"emergen\" de los datos, \nsino que deben ser considerados con de tenimiento como primer paso del \nestudio. \nEn nuestro caso, uno de los objetivos perseguidos podr\u00eda ser el intentar \nrelacionar los resultados obtenidos en las pruebas con caracter\u00edsticas o perfiles \nde los alumnos, si bien la descripci\u00f3n disponible no es muy rica y habr\u00e1 que \natenerse a lo que est\u00e1 disponible . Algunas de las pregunt as que podemos \nplantearnos a responder como objetivos del  an\u00e1lisis podr\u00edan ser las siguientes: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 162 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 \u00bfQu\u00e9 caracter\u00edsitcas comunes tienen lo s alumnos que superan la prueba? \n\u00bfy los alumnos mejor preparados que  la superan sin perjudicar su \nexpediente? \n\u2022 \u00bfexisten grupos de alumnos, no conocidos de antemano, con \ncaracter\u00edsticas similares? \n\u2022 \u00bfhay diferencias significativas  en los resultados obtenidos seg\u00fan las \nopciones, localidades, a\u00f1os, etc.?, \n\u2022 \u00bfla opci\u00f3n seleccionada y el resu ltado est\u00e1 influida depende del entorno? \n\u2022 \u00bfse puede predecir la calificaci\u00f3n del alumno con alguna variable conocida? \n\u2022 \u00bfqu\u00e9 relaciones entre variables son las m\u00e1s significativas?  \n \nComo veremos, muchas veces el re sultado alcanzado puede ser encontrar \nrelaciones triviales o conocidas previa mente, o puede ocurrir que el hecho de \nno encontrar relaciones significativas , lo  puede ser muy relevante. Por \nejemplo, saber despu\u00e9s de un an\u00e1lisis ex haustivo que la opci\u00f3n o localidad no \ncondiciona significativamente la calific aci\u00f3n, o que la prueba es homog\u00e9nea a \nlo largo de los a\u00f1os, puede ser una conclusi\u00f3n valiosa, y en este caso \n\"tranquilizadora\".  \nPor otra parte, este an\u00e1lisis tiene un  enfoque introductorio e ilustrativo para \nacercarse a las t\u00e9cnicas disponibles y su manipulaci\u00f3n desde la herramienta, \ndejando abierto para el investigador lle var el estudio de este dominio a \nresultados y conclusiones m\u00e1s elaboradas. \n \nEjecuci\u00f3n de WEKA \nWEKA se distribuye como un fichero ej ecutable comprimido de java (fichero \n\"jar\"), que se invoca directamente s obre la m\u00e1quina virtual JVM. En las \nprimeras versiones de WEKA se requer\u00eda  la m\u00e1quina virtur al Java 1.2 para \ninvocar a la interfaz gr\u00e1fica, desa rrollada con el paquete gr\u00e1fico de Java Swing . \nEn el caso de la \u00faltimo versi\u00f3n, W EKA 3-4, que es la que se ha utilizado para \nconfeccionar estas notas, se requiere Java 1.3 o superior. La herramienta se \ninvoca desde el int\u00e9rprete de Java, en el caso de utilizar  un entorno windows, \nbastar\u00eda una ventana de comandos para invocar al int\u00e9prete Java:  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 163 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nUna vez invocada, aparece la ventana de entrada a la interfaz gr\u00e1fica ( GUI-\nChooser ), que nos ofrece cuatro opciones posibles de trabajo: \n \n  \n\u2022 Simple CLI : la interfaz \"Command-Line \nInterfaz\" es simplemente una ventana de \ncomandos java para ejecutar las clases de WEKA. La primera di stribuci\u00f3n de WEKA no \ndispon\u00eda de interfaz gr\u00e1fica y las clases de sus paquetes se pod\u00edan ejecutar  desde la l\u00ednea de \ncomandos pasando los argumentos adecuados. \n\u2022 Explorer : es la opci\u00f3n que permite llevar \na cabo la ejecuci\u00f3n de los algoritmos de an\u00e1lisis implementados s obre los ficheros de \nentrada, una ejecuci\u00f3n independiente por cada \nprueba. Esta es la opci\u00f3n sobre la que se centra la totalidad de esta gu\u00eda. \n\u2022 Experimenter : esta opci\u00f3n permite \ndefinir experimentos m\u00e1s complejos, con \nobjeto de ejecutar uno o varios algoritmos \nsobre uno o varios conjuntos de datos de entrada, y comparar estad\u00edsticamente los \nresultados \n\u2022 KnowledgeFlow : esta opci\u00f3n es una \nnovedad de WEKA 3-4 que permite llevar a \ncabo las mismas acciones del \"Explorer\", con una configuraci\u00f3n totalm ente gr\u00e1fica, inspirada \nen herramientas de tipo \"data-flow\" para seleccionar componentes y conectarlos en un proyecto de miner\u00eda de datos, desde que se cargan los datos, se aplican algoritmos de \ntratmiento y an\u00e1lisis, hasta el tipo de evaluaci\u00f3n deseada. \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 164 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En esta gu\u00eda nos centraremos \u00fani camente en la segunda opci\u00f3n, Explorer . Una \nvez seleccionada, se crea una ventana con 6 pesta\u00f1as en la parte superior que \nse corresponden con diferentes tipos de operaciones, en etapas independientes, que se pueden realizar sobre los datos: \n\u2022 Preprocess : seleccion de la fuente de dat os y preparaci\u00f3n (filtrado). \n\u2022 Clasify : Facilidades para aplicar esquemas  de clasificaci\u00f3n, entrenar \nmodelos y evaluar su precisi\u00f3n \n\u2022 Cluster : Algoritmos de agrupamiento \n\u2022 Associate : Algoritmos de b\u00fasqueda de reglas de asociaci\u00f3n \n\u2022 Select Attributes : B\u00fasqueda supervisada de s ubconjuntos de atributos \nrepresentativos \n\u2022 Visualize : Herramienta interactiva de presentaci\u00f3n gr\u00e1fica en 2D. \nAdem\u00e1s de estas pesta\u00f1as de selecci\u00f3n,  en la parte inferi or de la ventana \naparecen dos elementos comunes. Uno es el bot\u00f3n de \u201c Log\u201d, que al activarlo \npresenta una ventana text ual donde se indica la secuencia de todas las \noperaciones que se han llevado a cabo den tro del \u201cExplorer\u201d, sus tiempos de \ninicio y fin, as\u00ed como los mensajes de error m\u00e1s frecuentes. Junto al bot\u00f3n de \nlog aparece un icono de actividad (el p\u00e1jaro WEKA, que se mueve cuando se est\u00e1 realizando alguna tarea) y un indica dor de status, que indica qu\u00e9 tarea se \nest\u00e1 realizando en este momento dentro del  Explorer. \n \nPreprocesado de los datos \nEsta es la parte primera por la que se  debe pasar antes de realizar ninguna \notra operaci\u00f3n, ya que se precisan da tos para poder llevar a cabo cualquier \nan\u00e1lisis. La disposici\u00f3n de la parte de preprocesado del Explorer, Preprocess , \nes la que se indica en la figura siguiente. Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 165 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n   \n Cargar datos, guar dar datos filtrados \nSelecci\u00f3n  y \naplicaci\u00f3n de \nfiltros \nAtributos en \nla relaci\u00f3n \nactual Propiedades \ndel atributo \nseleccionado \n \n \nComo se indic\u00f3 anteriormente, hay tres  posibilidades para obtener los datos: un \nfichero de texto, una direcci\u00f3n URL  o una base de datos, dadas por las \nopciones: Open file , Open URL y Open DB . En nuestro caso utilizaremos \nsiempre los datos almacenados en un fic hero, que es lo m\u00e1s r\u00e1pido y c\u00f3modo \nde utilizar. La preparaci\u00f3n del fichero de datos en formato ARFF ya se describi\u00f3 \nen la secci\u00f3n 1.2. \nEn el ejemplo que nos ocupa, abra el fich ero \u201cselectividad.arff\u201d con la opci\u00f3n \nOpen File . \nCaracter\u00edsticas de los atributos \nUna vez cargados los datos, aparece un cuadro resumen, Current relation , con \nel nombre de la relaci\u00f3n que se indica en el fichero (en la l\u00ednea @relation del \nfichero arff), el n\u00famero de instancias y el n\u00famero de atributos. M\u00e1s abajo, \naparecen listados todos los atributos  disponibles, con los nombres \nespecificados en el fichero, de modo q ue se pueden seleccionar para ver sus \ndetalles y propiedades. Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 166 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nEn la parte derecha aparec en las propiedades del atribut o seleccionado. Si es \nun atributo simb\u00f3lico, se presenta la di stribuci\u00f3n de valore s de ese atributo \n(n\u00famero de instancias que tienen cada u no de los valores). Si es num\u00e9rico \naparece los valores m\u00e1ximo, m\u00ednimo, valo r medio y desviaci\u00f3n est\u00e1ndar.  Otras \ncaracter\u00edsticas que se destacan del atributo seleccionado son el tipo ( Type ), \nn\u00famero de valores distintos ( Distinct ),  n\u00famero y porcent aje de instancias con \nvalor desconocido para el atributo ( Missing , codificado en el fichero arff con \n\u201c?\u201d), y valores de atributo que sola mente se dan en una instancia ( Unique ). \nAdem\u00e1s, en la parte inferior se pres enta gr\u00e1ficamente el histograma con los \nvalores que toma el atributo. Si es si mb\u00f3lico, la distribuci\u00f3n de frecuencia de \nlos valores, si es num\u00e9rico, un histogr ama con intervalos uniformes. En el \nhistograma se puede presentar  adem\u00e1s con colores distin tos la distribuci\u00f3n de \nun segundo atributo para cada valor del at ributo visualizado. Por \u00faltimo, hay un \nbot\u00f3n que permite visualizar los hi stogramas de todos los atributos \nsimult\u00e1neamente.  \nA modo de ejemplo, a cont inuaci\u00f3n mostramos el hi stograma por localidades, \nindicando con colores la distribuciones por opciones elegidas.  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 167 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nSe ha seleccionado la columna de la localidad de Legan\u00e9s, la que tiene m\u00e1s \ninstancias, y donde puede verse que la pr oporci\u00f3n de las opciones cient\u00edficas \n(1 y 2) es superior a otras localidades , como Getafe, la segunda localidad en \nn\u00famero de alumnos presentados. \nVisualice a continuaci\u00f3n los histogramas de las calificaciones de bachillerato y \ncalificaci\u00f3n final de la prueba, indicando como segundo atributo la convocatoria \nen la que se presentan los alumnos. \n \nTrabajo con Filtros. Preparaci\u00f3n de ficheros de muestra \nWEKA tiene integrados filtros que permit en realizar manipulaciones sobre los \ndatos en dos niveles: atributos e instanc ias. Las operaciones de filtrado pueden \naplicarse \u201cen cascada\u201d, de manera que c ada filtro toma como entrada el  \nconjunto de datos resultant e de haber aplicado un filt ro anterior. Una vez que \nse ha aplicado un filtro, la relaci\u00f3n ca mbia ya para el resto de operaciones \nllevadas a cabo en el Experimenter , existiendo siempre la opci\u00f3n de deshacer  \nla \u00faltima operaci\u00f3n de f iltrado aplicada con el bot\u00f3n Undo . Adem\u00e1s, pueden \nguardarse los resultados de aplicar f iltros en nuevos ficheros, que tambi\u00e9n \nser\u00e1n de tipo ARFF, para mani pulaciones posteriores. \nPara aplicar un filtro a los dato s, se selecciona con el bot\u00f3n Choose de Filter , \ndespleg\u00e1ndose el \u00e1rbol con todos  los que est\u00e1n integrados.  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 168 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nPuede verse que los filtros de esta opc i\u00f3n son de tipo no supervisado \n(unsupervised ): son operaciones independientes del algoritmo an\u00e1lisis \nposterior, a diferencia de los filtros s upervisados que se ver\u00e1n en la secci\u00f3n 1.9 \nde \u201cselecci\u00f3n de atributos\u201d, que operan en conjunci\u00f3n con algoritmos de \nclasificaci\u00f3n para analizar su efecto . Est\u00e1n agrupados seg\u00fan modifiquen los \natributos resultantes o seleccionen un s ubconjunto de instancias (los filtros de \natributos pueden verse como filtros \"vertical es\" sobre la tabla de datos, y los \nfiltros de instancias como filtros \"hor izontales\"). Como puede verse, hay m\u00e1s de \n30 posibilidades, de las que destacare mos \u00fanicamente algunas de las m\u00e1s \nfrecuentes. \nFiltros de atributos \nVamos a indicar, de entre todas las po sibilidades implementadas , la utilizaci\u00f3n \nde filtros para eliminar at ributos, para discretizar at ributos num\u00e9ricos, y para \na\u00f1adir nuevos atributos con expresiones, por la frecuencia con la que se \nrealizan estas operaciones. \nFiltros de selecci\u00f3n \nVamos a utilizar el filtro de atributos \u201c Remove \u201d, que permite eliminar una serie \nde atributos del conjunt o de entrada. En primer  lugar procedemos a \nseleccionarlo desde el \u00e1rbol  desplegado con el bot\u00f3n Choose de los filtros. A \ncontinuaci\u00f3n lo configuraremos para det erminar qu\u00e9 atributos queremos filtrar.  \nLa configuraci\u00f3n de un filtro sigue el esquema general de configuraci\u00f3n de \ncualquier algoritmo integrado en WEKA . Una vez seleccionado el filtro \nespec\u00edfico con el bot\u00f3n Choose , aparece su nombre dentro del \u00e1rea de filtro (el \nlugar donde antes aparec\u00eda la palabra None ). Se puede configurar sus \npar\u00e1metros haciendo clic sobre esta \u00e1rea, momento en el que aparece la \nventana de configuraci\u00f3n corre spondiente a ese filtro part icular. Si no se realiza \nesta operaci\u00f3n se utilizar\u00edan los valore s por defecto del filtro seleccionado. \nComo primer filtro de sele cci\u00f3n, vamos a eliminar de los atributos de entrada \ntodas las calificaciones parci ales de la prueba y la calif icaci\u00f3n final, quedando \ncomo \u00fanicas calificaciones la nota de bach illerato y la calific aci\u00f3n de la prueba. \nPor tanto tenemos que seleccionar los \u00edndices 5,6,7,10,12,14 y 17, indic\u00e1ndolo \nen el cuadro de configuraci\u00f3n del filtro Remove : Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 169 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nComo puede verse, en el conjunto de at ributos a eliminar  se pueden poner \nseries de valores contiguos delimitados por gui\u00f3n (5-7) o valores sueltos entre \ncomas (10,12,14,17). Adem\u00e1s , puede usarse \u201cfirst\u201d y \u201cl ast\u201d para indicar el \nprimer y \u00faltimo atributo, respectivamente. La opci\u00f3n invertSelection  es \u00fatil \ncuando realmente queremos seleccionar  un peque\u00f1o subconjunto de todos los \natributos y eliminar el resto. Open y Save nos permiten guardar \nconfiguraciones de inter\u00e9s en archivos. El boton More , que aparece \nopcionalmente en algunos elementos de WEKA, muestra informaci\u00f3n de \nutilidad acerca de la configuraci\u00f3n de los mismos. Estas convenciones para \ndesignar y seleccionar atributos, ayuda, y para guardar y cargar configuraciones espec\u00edficas es co m\u00fan a otros elementos de WEKA. \nUna vez configurado, al accionar el bot\u00f3n Apply del \u00e1rea de filtros se modifica \nel conjunto de datos (se filtra) y se genera una relaci\u00f3n transformada. Esto se \nhace indicar en la descripci\u00f3n \u201cCurrent Relation\u201d, que pasa a ser la resultante de aplicar la operaci\u00f3n co rrespondiente (esta informaci\u00f3n se puede ver con \nm\u00e1s nitidez en la ventana de log, que adem\u00e1s  nos indicar\u00e1 la cascada de filtros \naplicados a la relaci\u00f3n operativa). La rela ci\u00f3n transformada tras aplicar el filtro \npodr\u00eda almacenarse en un nuevo fichero ARFF con el bot\u00f3n Save , dentro de la \nventana Preprocess. \n \nFiltros de discretizaci\u00f3n \nEstos filtros son muy \u00fatiles cuando se trabaja con atributos  num\u00e9ricos, puesto \nque muchas herramientas de an\u00e1lisis requi eren datos simb\u00f3licos , y por tanto se \nnecesita aplicar esta transformaci\u00f3n antes. Tambi\u00e9n son necesarios cuando queremos hacer una clasif icaci\u00f3n sobre un atributo num\u00e9rico, por ejemplo \nclasificar los alumnos apr obados y suspensos. Este filtrado transforma los \natributos num\u00e9ricos seleccionados en at ributos simb\u00f3licos, con una serie de \netiquetas resultantes de divi dir la amplitud total del atributo en intervalos, con \ndiferentes opciones para seleccionar lo s l\u00edmites. Por defecto, se divide la \namplitud del intervalo en tantas  \"cajas\" como se indique en bins (por defecto \n10), todas ellas de la misma amplitud.  \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 170 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por ejemplo, para discretiz ar las calificaciones num\u00e9ricas en 4 categor\u00edas, \ntodas de la misma amplitud, se configurar\u00eda as\u00ed: \n \nobserve el resultado despu\u00e9s de aplicar el  filtro y los l\u00edmites elegidos para cada \natributo. En este caso se ha aplicado a todos los atributos num\u00e9ricos con la \nmisma configuraci\u00f3n (los atributos selecc ionados son first-last, no considerando \nlos atributos que antes del filtrado no eran num\u00e9ricos).  Observe que la relaci\u00f3n \nde trabajo ahora (\u201ccurrent relation\u201d) ahora es el resultado de aplicar en \nsecuencia el filtro anterior y el actual. \nA veces es m\u00e1s \u00fatil no fijar todas las cajas de la misma anchura sino forzar a \nuna distribuci\u00f3n uniforme de instancia s por categor\u00eda, con la opci\u00f3n \nuseEqualFrequency . La opci\u00f3n findNumBins  permite opimizar el n\u00famero de \ncajas (de la misma amplitud), con un criter io de clasificaci\u00f3n de m\u00ednimo error en \nfunci\u00f3n de las etiquetas. \nHaga una nueva discretizaci\u00f3n de la rela ci\u00f3n (eliminando el ef ecto del filtro \nanterior y dejando la relaci\u00f3n original con el bot\u00f3n Undo ) que divida las \ncalificaciones en 4 intervalos de la mism a frecuencia, lo que permite determinar \nlos cuatro cuartiles (intervalos al 25% ) de la calificaci\u00f3n en la prueba: los \nintervalos delimitados por lo s valores {4, 4.8, 5.76} Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 171 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \npodemos ver que el 75% alcanza la nota de compensaci\u00f3n (4). El 50% est\u00e1 \nentre 4 y 5.755, y el 25% restante a partir de 5.755. \n \nFiltros de a\u00f1adir expresiones \nMuchas veces es interesante incluir n uevos atributos resultantes de aplicar \nexpresiones a los existentes, lo que puede traer informaci\u00f3n de inter\u00e9s o \nformular cuestiones interesantes sobre lo s datos. Por ejemplo, vamos a a\u00f1adir \ncomo atributo de inter\u00e9s la \"mejora\" sobr e la nota de bachillerato, lo que puede \nservir para calificar el \"\u00e9xito\" en la pr ueba. Seleccionamos el filtro de atributos \nAddExpression , configurado para obtener la di ferencia entre los atributos \ncalificaci\u00f3n en la prueba, y nota de bac hillerato, en las posiciones15 y 16: \n \ndespu\u00e9s de aplicarlo aparece este atributo en la relaci \u00f3n, ser\u00eda el n\u00famero 19, \ncon el histograma indicado en la figura: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 172 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nFiltros de instancias \nDe entre todas las posibi lidades implementadas para filtros de selecci\u00f3n de \ninstancias (selecci\u00f3n de r angos, muestreos, etc.),  nos centraremos en la \nutilizaci\u00f3n de filtros para seleccionar instancias cuyos atributos cumplen \ndeterminadas condiciones. \nSelecci\u00f3n de instancias con condiciones sobre atributos \nVamos a utilizar el filtro RemoveWithValues , que elimina las instancias de \nacuerdo a condiciones definid as sobre uno de los atributos. Las opciones que \naparecen en la ventana de configuraci\u00f3n son las indicadas a continuaci\u00f3n. \n \nel atributo utilizado para filtrar se indi ca en \"attributeIndex\". Si es un atributo \nnominal, se indican los valores a filtrar en el \u00faltimo par\u00e1metro, \"nominalIndices\". \nSi es num\u00e9rico, se filtran las instancias con un valor inferior al punto de corte, \n\"splitPoint\". Se puede invertir el cr iterio de filtrado mediante el campo \n\"invertSelection\". \nEste filtro permite verificar una cond ici\u00f3n simple sobre un atributo. Sin \nembargo, es posible hacer un filtrado m\u00e1s complejo sobre varias condiciones \naplicadas a uno o varios atributos sin m\u00e1s que aplicar en cascada varios filtros Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 173 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda A modo de ejemplo, utilice tres filtros de este tipo para seleccionar los alumnos \nde Getafe y Legan\u00e9s con una calificaci\u00f3 n de la prueba entre 6.0 y 8.0. \nCompruebe el efecto de filtrado visua lizando los histogramas de los atributos \ncorrespondientes (localidad y calificaci\u00f3n en la prueba), tal y como se indica en \nla figura siguiente: \n \nVisualizaci\u00f3n \nUna de las primeras etapas del an\u00e1lisis de datos puede ser el mero an\u00e1lisis \nvisual de \u00e9stos, en ocas iones de gran utilidad para desvelar relaciones de \ninter\u00e9s utilizando nuestra capacid ad para comprender im\u00e1genes. La \nherramienta de visualizaci\u00f3n de WEKA pe rmite presentar gr\u00e1ficas 2D que \nrelacionen pares de atributos, con la opc i\u00f3n de utilizar adem\u00e1s los colores para \na\u00f1adir informaci\u00f3n de un tercer atributo. Adem\u00e1s, tiene incorporada una facilidad interactiva para seleccionar instancias con el rat\u00f3n. \nRepresentaci\u00f3n 2D de los datos \nLas instancias se pueden vi sualizar en gr\u00e1ficas 2D  que relacionen pares de \natributos. Al sele ccionar la opci\u00f3n Visualize  del Explorer aparecen todas los \npares posibles de atributos  en las coordenadas horizonta l y vertical. La idea es \nque se selecciona la gr\u00e1fica deseada par a verla en detalle en una ventana \nnueva. En nuestro caso, aparecer\u00e1n todas  las combinaciones posibles de \natributos. Como primer ejemplo vamos a visualizar el rango de calificaciones \nfinales de los alumnos a lo largo de lo s a\u00f1os, poniendo la convocatoria (junio o \nseptiembre) como color de la gr\u00e1fica.  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 174 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nVamos a visualizar ahora dos variables cuya relaci\u00f3n es de gran inter\u00e9s, la \ncalificaci\u00f3n de la prueba en funci\u00f3n de la nota de bachillerato, y tomando como \ncolor la convocatoria (junio o septiembre).  \n \nen esta gr\u00e1fica podemos apreciar la re laci\u00f3n entre ambas magnitudes, que si \nbien no es directa al menos  define una cierta tendenci a creciente, y como la \nconvocatoria est\u00e1 bastante rela cionada con ambas calificaciones.  \nCuando lo que se relacionan son variab les simb\u00f3licas, se presentan sus \nposibles valores a lo largo del eje. Sin embargo, en esto s casos todas las \ninstancias que comparten cada valor de un atributo simb\u00f3lico pueden ocultarse Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 175 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda (ser\u00edan un \u00fanico punto en el plano), raz\u00f3n por la que se  utiliza la facilidad de \nJitter .  Esta opci\u00f3n permite introducir un de splazamiento aleatorio (ruido) en las \ninstancias, con objeto de poder visualizar todas aque llas que comparten un par \nde valores de atributos simb\u00f3licos, de manera que puede visualizarse la \nproporci\u00f3n de instancias que aparece en c ada regi\u00f3n. A modo de ejemplo se \nmuestra a continuaci\u00f3n la relaci\u00f3n entre las tres asignaturas optativas, y con la \nopci\u00f3n cursada como color \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 176 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \npuede verse una marcada relaci\u00f3n entre las asignaturas opcionales, de manera \nque este gr\u00e1fico ilustra qu\u00e9 tipo de asignaturas engloba cada una de las cinco posibles opciones cursadas.  \nSe sugiere preparar el sigu iente gr\u00e1fico, que relaciona  la calificaci\u00f3n obtenida \nen la prueba con la localidad de origen y la nota de bachillerato, estando las \ncalificaciones discretizadas en intervalos de amplitud 2 \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 177 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Aqu\u00ed el color trae m\u00e1s informaci\u00f3n, pues indica en cada intervalo de \ncalificaciones de la prueba, la  calificaci\u00f3n en bachillera to, lo que permite ilustrar \nla \"satisfacci\u00f3n\" con la calificaci\u00f3n en la prueba o resultados no esperados, \nadem\u00e1s distribuido por localidades.  \n \nFiltrado \u201cgr\u00e1fico\u201d de los datos \nWEKA permite tambi\u00e9n realizar filtro s de selecci\u00f3n de instancias sobre los \npropios gr\u00e1ficos, con una interacci\u00f3n a trav \u00e9s del rat\u00f3n para aislar los grupos \nde instancias cuyos atributos cumplen determinadas condiciones. Esta facilidad permite realizar filtrados de instancia s de modo interactivo y m\u00e1s intuitivo que \nlos filtros indicados en la secci\u00f3n 1.4. 2.2. Las opciones que existen son:  \n\u2022 Selecci\u00f3n de instancias con un valo r determinado (hacer clic sobre la \nposici\u00f3n en el gr\u00e1fico) \n\u2022 Selecci\u00f3n con un rect\u00e1ngulo de un subconjunto de combinaciones \n(comenzando por el v\u00e9rtice superior izquierdo) ( Rectangle ) \n\u2022 Selecci\u00f3n con un pol\u00edgono cerrado de un subconjunto ( Polygon ) \n\u2022 Selecci\u00f3n con una l\u00edne a abierta de frontera ( Polyline ) \nPor ejemplo, a continuaci \u00f3n se indica la selecci\u00f3n  de alumnos que obtuvieron \nuna calificaci\u00f3n por debajo de sus expecta tivas (calificaci\u00f3n en la prueba \ninferior a su nota en el bachillerato), con la opci\u00f3n Polygon . \n \nUna vez realizada la selecci\u00f3n, la opci\u00f3n Submit permite eliminar el resto de \ninstancias, y Save almacenarlas en un fichero. Reset devuelve la relaci\u00f3n a su \nestado original. \nUtilice estas facilidades gr\u00e1ficas para hac er subconjuntos de los datos con los \nalumnos aprobados de las opciones 1 y 2 frente a los de las opciones 3, 4 y 5. Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 178 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Salve las relaciones filt radas para a continuaci\u00f3n cargarlas y mostrar los \nhistogramas, que aparecer\u00e1n como se indica en la figura siguiente. \n \n \n \n \n \nAsociaci\u00f3n \nLos algoritmos de asociaci\u00f3n permiten la  b\u00fasqueda autom\u00e1tica de reglas que \nrelacionan conjuntos de at ributos entre s\u00ed. Son algoritmos no supervisados, en \nel sentido de que no existen relacione s conocidas a priori con las que \ncontrastar la validez de los resultados, sino que se eval\u00faa si esas reglas son estad\u00edsticamente significativas.  La ventana de Asociaci\u00f3n ( Associate en el \nExplorer), tiene los siguiente elementos: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 179 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n   \n Selecci\u00f3n  y configuraci\u00f3n del algoritmo de asociaci\u00f3n \nVisualizaci\u00f3n \nde resultados y almacenamient\noResultados \n(en texto) \n \nEl principal algoritmo de asociaci\u00f3n implementado en WEKA es el algoritmo \n\"Apriori\". Este algoritmo \u00fanicament e puede buscar reglas entre atributos \nsimb\u00f3licos, raz\u00f3n por la que se requier e haber discretizado todos los atributos \nnum\u00e9ricos.  \nPor simplicidad, vamos a aplicar un filtro  de discretizaci\u00f3n de todos los atributos \nnum\u00e9ricos en cuatro intervalos de la misma frecuencia para explorar las \nrelaciones m\u00e1s significativas. El algor itmo lo ejecutamos con sus par\u00e1metros \npor defecto. \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 180 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda las reglas que aparecen aportan poca info rmaci\u00f3n. Aparecen en primer lugar \nlas relaciones triviales entre asignaturas y opciones, as\u00ed como las que relacionan suspensos en la prueba y en la calificaci\u00f3n final. En cuanto a las que \nrelacionan alumnos pres entados con idioma seleccionado son debidas a la \nfuerte descompensaci\u00f3n en el idioma se leccionado. Lla abrumadora mayor\u00eda \nde los presentados a la prueba de idioma seleccionaron el ingl\u00e9s, como indica \nla figura siguiente: \n \n \nCon objeto de buscar relaciones no cono cidas, se filtrar\u00e1n ahora todos los \natributos relacionados con descriptore s de asignaturas y calificaciones \nparciales, quedando \u00fanicamente los atributos: \n              A\u00f1o_acad\u00e9mico \n        convocatoria \n        localidad         opcion1\u00aa         cal_prueba         nota_bachi\n \n \nEn este caso, las reglas  m\u00e1s significativas son: \n1. nota_bachi='(8-inf)' 2129 ==> convocatoria=J 2105    conf:(0.99) \n2. cal_prueba='(5.772-7.696]' nota_bachi='(6-8]' 2521 ==> \nconvocatoria=J 2402    conf:(0.95) \n3. cal_prueba='(5.772-7.696]' 4216 ==>  \nconvocatoria=J 3997    conf:(0.95) \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 181 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda estas reglas aportan informaci\u00f3n no tan tr ivial: el 99% de alumnos con nota \nsuperior a 8 se presentan a la convocatoria de Junio, as\u00ed el 95% de los alumnos con calificaci\u00f3n en la prueba entre 5.772 y 7. \n  \n \n \nes significativo ver que no aparece ninguna relaci\u00f3n importante entre las \ncalificaciones, localidad y a\u00f1o de la conv ocatoria. Tambi\u00e9n es destacado ver la \nausencia de efecto de la opci\u00f3n cursada. \nSi preparamos los datos para dejar s\u00f3lo ci nco atributos,              \n   A\u00f1o_acad\u00e9mico \nconvocatoria \n   localidad    opcion1\u00aa \n   cal_fina\nl,  \n \ncon el \u00faltimo discretizado en dos grupos iguales (hasta 5.85 y 5.85 hasta 10), \ntenemos que de nuevo las reglas m\u00e1s significativas relacionan convocatoria \ncon calificaci\u00f3n, pero ahora entran en juego opciones y loca lidades, si bien \nbajando la precisi\u00f3n de las reglas:  \n1. opcion1\u00aa=1 cal_final='(5.685-inf)' 2810 ==>  \nconvocatoria=J 2615    conf:(0.93) \n2. localidad=LEGANES cal_final='(5.685-inf)' 2514 ==>  \nconvocatoria=J 2315    conf:(0.92) \n3. A\u00f1o_acad\u00e9mico='(1998.4-2000.2]' cal_final='(5.685-inf)' 3175 ==>  \nconvocatoria=J 2890    conf:(0.91) \n4. cal_final='(5.685-inf)' 9397 ==>  \nconvocatoria=J 8549    conf:(0.91) \n5. opcion1\u00aa=4 cal_final='(5.685-inf)' 2594 ==>  \nconvocatoria=J 2358    conf:(0.91) \n6. A\u00f1o_acad\u00e9mico='(2000.2-inf)' cal_final='(5.685-inf)' 3726 ==> Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 182 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda convocatoria=J 3376    conf:(0.91) \n7. localidad=GETAFE cal_final='(5.685-inf)' 2156 ==>  \nconvocatoria=J 1951    conf:(0.9) \n \nAl filtrar la convocatoria, que nos origina relaciones bastante evidentes, \ntendremos las reglas m\u00e1s significativa s entre localidad, a\u00f1o, calificaci\u00f3n y \nopci\u00f3n. Como podemos ver, al lanzar el algoritmo con los par\u00e1metros por \ndefecto no aparece ninguna regla. Esto es debido a que se forz\u00f3 como umbral \nm\u00ednimo aceptable para una regla el 90%. Vamos a bajar ahora este par\u00e1metro \nhasta el 50%:  \n \n \nBest rules found:   1. opcion1\u00aa=4 5984 ==> cal_final='(-inf-5.685]' 3390    conf:(0.57)  2. opcion1\u00aa=1 5131 ==> cal_final='(5.685-inf)' 2810    conf:(0.55) \n 3. A\u00f1o_acad\u00e9mico='(2000.2-inf)' 7049 ==>  \n cal_final='(5.685-inf)' 3726    conf:(0.53) \n 4. opcion1\u00aa=2 4877 ==> cal_final='(5.685-inf)' 2575    conf:(0.53)  5. localidad=GETAFE 4464 ==>  \n cal_final='(-inf-5.685]' 2308    conf:(0.52) \n 6. localidad=LEGANES 4926 ==>  \ncal_final='(5.685-inf)' 2514    conf:(0.51) \n 7. A\u00f1o_acad\u00e9mico='(1998.4-2000.2]' 6376 ==>  \ncal_final='(-inf-5.685]' 3201    conf:(0.5) \n \nPor tanto, forzando los t\u00e9rminos, tenemos que los estudiantes de las 2 primeras \nopciones tienen mayor probabilidad de aprobar la prueba, as\u00ed como los estudiantes de la localidad de Legan\u00e9s. Los estudiantes de Getafe tienen una \nprobabilidad superior de obten er una calificaci\u00f3n inferi or. Hay que destacar que \nestas reglas rozan el umbral del 50% , pero han sido seleccionadas como las \nm\u00e1s significativas de t odas las posibles. Tambi\u00e9n hay que considerar que si \naparecen estas dos localidades  en primer lugar es si mplemente por su mayor \nvolumen de datos, lo que ot orga una significatividad superior en las relaciones \nencontradas. Si se consulta la bibliograf\u00eda, el primer criterio de selecci\u00f3n de \nreglas del algoritmo \"A priori\" es la  precisi\u00f3n o confianza, dada por el \nporcentaje de veces que instancias que cumplen el antecedente cumplen el \nconsecuente, pero el segundo es el sopor te, dado por el n\u00famero de instancias Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 183 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda sobre las que es aplicable la regla. En  todo caso, son reglas de muy baja \nprecisi\u00f3n y que habr\u00eda  que considerar simplemente como ciertas tendencias. \n \nAgrupamiento \nLa opci\u00f3n Cluster del Experimenter  nos permite aplicar algoritmos de \nagrupamiento de instancias a nuestros dat os. Estos algoritmos buscan grupos \nde instancias con caracter\u00edsticas \"similare s\", seg\u00fan un criterio de comparaci\u00f3n \nentre valores de atributos de las inst ancias definidos en los algoritmos. \nEl mecanismo de selecci\u00f3n, configurac i\u00f3n y ejecuci\u00f3n es similar a otros \nelementos: primero se sele cciona el algoritmo con Choose , se ajustan sus \npar\u00e1metros seleccionando sobre el \u00e1r ea donde aparece, y se despu\u00e9s se \nejecuta. El \u00e1rea de agr upamiento del Explorer pr esenta los siguientes \nelementos de configuraci\u00f3n: \n \n  \n \n Selecci\u00f3n  y configuraci\u00f3n del algoritmo \nEvaluaci\u00f3n \ndel resultado \nde cluster \nVisualizaci\u00f3n de resultados Clusters en \ntexto \n \nUna vez que se ha realizado la selecci\u00f3n  y configuraci\u00f3n del algoritmo,  se \npuede pulsar el bot\u00f3n Start , que har\u00e1 que se aplique sobre la relaci\u00f3n de \ntrabajo. Los resultados se presentar\u00e1n en la ventana de texto de la parte \nderecha. Adem\u00e1s, la vent ana izquierda permite listar todos los algoritmos y \nresultados que se hayan ej ecutado en la sesi\u00f3n act ual. Al seleccionarlos en \nesta lista de visualizaci\u00f3n se present an en la ventana de texto a la derecha, y \nadem\u00e1s se permite abrir ventanas gr \u00e1ficas de visualizaci\u00f3n con un men\u00fa \ncontextual que aparece al pulsar el  bot\u00f3n derecho sobre el resultado \nseleccionado. Por \u00faltimo, en esta opci\u00f3n de Agrupamiento aparecen las \nsiguientes opciones adicio nales en la pantalla.  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 184 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Ignorar atributos \nLa opci\u00f3n Ignoring Attributes permite sacar fuera atri butos que no interesa \nconsiderar para el agrupamiento, de m anera que el an\u00e1lisis de parecido entre \ninstancias no considera los atributos se leccionados. Al accionar esta opci\u00f3n \naparecen todos los atributos disponibles. Se pueden seleccionar con el bot\u00f3n izquierdo sobre un atributo espec\u00edfico, o seleccionar grupos usando SHIFT para \nun grupo de atributos conti guos y CONTROL para grupos de atributos sueltos. \nEvaluaci\u00f3n \nLa opci\u00f3n Cluster Mode  permite elegir como evaluar los resultados del \nagrupamiento. Lo m\u00e1s simple es utilizar  el propio conjunto de entrenamiento, \nUse tranining set , que indica que porcentaje de instancias se van a cada \ngrupo. El resto de opciones realizan un  entrenamiento con un conjunto, sobre \nel que construyen los clusters y a continuaci\u00f3n aplican estos clusters para clasificar un conjunto  independiente que puede  proporcionarse aparte \n(Supplied test ), o ser un porcentaje del  conjunto de entrada ( Percentage \nsplit). Existe tambi\u00e9n la opci\u00f3n de com parar los clusters con un atributo de \nclasificaci\u00f3n ( Classes to clusters evaluation ) que no se considera en la \nconstruicci\u00f3n de los clusters. Nosotros  nos centraremos \u00fanicamente en la \nprimera opci\u00f3n, dejando el  resto de opciones de evaluaci\u00f3n para m\u00e1s adelante, \ncuando lleguemos a los algoritmos de clasificaci\u00f3n. \nFinalmente, el cuadro opcional de almacenamiento de instancias, Store \nclusters for visualization ,  es muy \u00fatil para despu\u00e9s analizar los resultados \ngr\u00e1ficamente. \n \nAgrupamiento num\u00e9rico  \nEn primer lugar utilizar emos el algoritmo de agr upamiento K-medias, por ser \nuno de los m\u00e1s veloces y eficientes, si bien uno de los m\u00e1s limitados. Este \nalgoritmo precisa \u00fanicam ente del n\u00famero de categor\u00edas similares en las que \nqueremos dividir el conjunto de datos. Suel e ser de inter\u00e9s repetir la ejecuci\u00f3n \ndel algoritmo K-medias con diferentes se millas de inicializa ci\u00f3n, dada la notable \ndependencia del arranque cuando no est\u00e1 cl ara la soluci\u00f3n que mejor divide el \nconjunto de instancias. \nEn nuestro ejemplo, vamos a comprobar  si el atributo \u201copci\u00f3n\u201d divide \nnaturalmente a los alumnos en grupos simi lares, para lo que seleccionamos el \nalgoritmo SimpleKMeans  con par\u00e1metro numClusters con valor 5. Los \nresultados aparecen en la ventana de texto derecha: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 185 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nNos aparecen los 5 grupos de ejemplos m\u00e1s similares, y sus centroides \n(promedios para atributos num\u00e9ricos, y valores m\u00e1s repetidos en cada grupo para atributos simb\u00f3licos).  \nEn este caso es de inter\u00e9s analizar  gr\u00e1ficamente como se distribuyen \ndiferentes valores de los atributos en los grupos generados. Para ello basta \npulsar con bot\u00f3n derecho del rat\u00f3n sobre el cuadro de resultados, y seleccionar \nla opci\u00f3n visualizeClusterAssignments \n \n \nSi seleccionamos combinaciones del atributo opci\u00f3n con localidad, nota o \nconvocatoria podemos ver la distribuci\u00f3n de grupos: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 186 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \n \nA la vista de estos gr\u00e1ficos podemos c oncluir que el \u201cparecido\u201d entre casos \nviene dado fundamentalm ente por las opciones seleccionadas. Los clusters 0, \n1 y 4 se corresponden con las opciones 3, 4 y 1, mientras que los clusters 2 y 3 \nrepresentan la opci\u00f3n 3 en las conv ocatorias de junio y septiembre. \nAprovechando esta posibilidad de bu scar grupos de semejanzas, podr\u00edamos \nhacer un an\u00e1lisis m\u00e1s particularizado a las dos localidades mayores, Legan\u00e9s y \nGetafe, buscando qu\u00e9 opciones y calificaciones aparecen con m\u00e1s frecuencia. Vamos a preparar los datos con filtros de modo que tengamos  \u00fanicamente tres \natributos: localidad, opci\u00f3n, y calificac i\u00f3n final. Adem\u00e1s, discretizamos las \ncalificaciones en dos grupos de la mism a frecuencia (estudiantes con mayor y \nmenor \u00e9xito), y \u00fanicamente nos quedam os con los alumnos de Legan\u00e9s y \nGetafe. Utilizaremos para ello los f iltros adecuados. A continuaci\u00f3n aplicamos \nel algoritmo K-medias con 4 grupos. Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 187 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nvemos que los grupos nos muestran la  presencia de buenos alumnos en \nGetafe en la opci\u00f3n 4, y buenos alumnos  en Legan\u00e9s en la opci\u00f3n 1, siempre \nconsiderando estas conclusiones como tendencias en promed io. Gr\u00e1ficamente \nvemos la distribuci\u00f3n de clusters  por combinaciones de atributos: \n \n Si consideramos que en Legan\u00e9s hay es cuelas de ingenier\u00eda, y en Getafe \nfacultades de Humanidades, podr\u00edamos c oncluir que podr\u00eda ser achacable al \nimpacto de la universidad en la zona.  \n El algoritmo EM proviene de  la estad\u00edstica y es bast ante m\u00e1s elaborado que el \nK-medias, con el coste de que requiere muchas m\u00e1s operaciones, y es apropiado cuando sabemos que los datos tienen una variabi lidad estad\u00edstica de \nmodelo conocido. Dada esta mayor co mplejidad, y el notable volumen del Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 188 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda fichero de datos, primero aplicaremos un fi ltro de instancias al 3% para dejar un \nn\u00famero de 500 instancias aproximadament e. Para esto \u00faltimo iremos al \npreprocesado y aplicamos un filt ro de instancias, el filtro Resample , con factor \nde reducci\u00f3n al 3%: \n \nUna ventaja adicional del  algoritmo de clustering EM es que permite adem\u00e1s \nbuscar el n\u00famero de grupos m\u00e1s apropiado,  para lo cual basta indicar a \u20131 el \nn\u00famero de clusters a formar, que es la opci\u00f3n que viene por defecto. Esto se \ninterpreta como dejar el par\u00e1metro del  n\u00famero de clusters como un valor a \noptimizar por el propio algoritmo. \nTras su aplicaci\u00f3n, este algoritmo determina que hay cinco clusters \nsignificativos en la muestra de 500 al umnos, y a continuaci\u00f3n indica los \ncentroides de cada grupo: \n \n \nAl igual que antes, es interesante analiz ar el resultado del agrupamiento sobre \ndiferentes combinaciones de atributos, haciendo uso de la facilidad \nvisualizeClusterAssignments  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 189 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nPor tanto podr\u00eda concluirse que para  este segundo algoritmo de agrupamiento \npor criterios estad\u00edsticos  y no de distancias entre vectores de atributos, \npredomina el agrupamiento de los alumnos b\u00e1sicamente por tramos de \ncalificaciones, independientemente de la  opci\u00f3n, mientras que en el anterior \npesaba m\u00e1s el perfil de asignaturas cursado que las calificaciones.  \nEsta disparidad sirve para ilustrar la pr oblem\u00e1tica de la decis i\u00f3n del criterio de \n\u201cparecido\u201d entre instancias par a realizar el  agrupamiento. \n \nAgrupamiento simb\u00f3lico \nFinalmente, como alternativa a los al goritmos de agrupamiento anteriores, el \nagrupamiento simb\u00f3lico tiene la  ventaja de efectuar un an\u00e1lisis cualitativo que \nconstruye categor\u00edas jer\u00e1rquicas para or ganizar los datos. Estas categor\u00edas se \nforman con un criterio pr obabil\u00edstico de \"utilidad\", llegando a las que permiten \nhomogeneidad de los valores de los atri butos dentro de cada una y al mismo \ntiempo una separaci\u00f3n entre categor\u00edas dadas por los atributos, propag\u00e1ndose \nestas caracter\u00edsticas en un \u00e1rbol de conceptos. \nSi aplicamos el algoritmo cobweb con los par\u00e1metros por defecto sobre la \nmuestra reducida de instancias (dada la complejidad del algoritmo), el \u00e1rbol \ngenerado llega hasta 800 nodos. Vamos a modificar el par\u00e1metro cut-off , que \npermite poner condiciones m\u00e1s restrictiv as a la creaci\u00f3n de  nuevas categor\u00edas \nen un nivel y subcategor\u00edas. Con los par \u00e1metros siguientes se llega a un \u00e1rbol \nmuy manejable: \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 190 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda la opci\u00f3n saveInstanceData  es de gran utilidad para despu\u00e9s analizar la \ndistribuci\u00f3n de valores de atributos entre las instancias de cada parte del \u00e1rbol \nde agrupamiento. Una vez ejecutado Cobw eb, genera un resultado como el \nsiguiente: \n \nhay 3 grupos en un primer nivel, y el  segundo se subdivi de en otros dos. De \nnuevo activando el bot\u00f3n derecho sobr e la ventana de resultados, ahora \npodemos visualizar el \u00e1rbol gr\u00e1ficamente: \n \nlas opciones de visualizaci\u00f3n aparecen al pulsar el bot\u00f3n derecho en el fondo \nde la figura. Se pueden visualizar las instancias que van a cada nodo sin m\u00e1s \nque pulsar el bot\u00f3n derecho sobre \u00e9l. Si nos fijamos en como quedan \ndistribuidas las instancias por clusters, con la opci\u00f3n \nvisualizeClusterAssignments,  llegamos a la figura: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 191 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \npor tanto, vemos que de nuevo vuelve a pesar la opci\u00f3n como criterio de \nagrupamiento. Los nodos hoja 1, 3, 4 y 5 se corresponden con las opciones \ncursadas 2, 3, 1 y 4 respectivamente.  En  un primer nivel hay tres grupos, uno \npara la opci\u00f3n 2, otro para la opci\u00f3n 4 y otro que une las opciones 1 y 3. Este \n\u00faltimo se subdivide en dos grupos que se corresponden con ambas opciones.  \n \nClasificaci\u00f3n \nFinalmente, en esta secci\u00f3n abordamos el problema de la clasificaci\u00f3n, que es \nel m\u00e1s frecuente en la pr\u00e1ctica. En ocas iones, el problema de clasificaci\u00f3n se \nformula como un refinamiento en el an\u00e1lisis, una vez que se han aplicado \nalgoritmos no supervisados de agrupami ento y asociaci\u00f3n para describir \nrelaciones de inter\u00e9s en los datos.  \nSe pretende construir un modelo que permi ta predecir la categor\u00eda de las \ninstancias en funci\u00f3n de una serie de atri butos de entrada. En  el caso de \nWEKA, la clase es simplemente uno de lo s atributos simb\u00f3licos disponibles, \nque se convierte en la va riable objetivo a predecir. Por defecto, es el \u00faltimo \natributo (\u00faltima columna) a no ser que se  indique otro expl\u00edcitamente. La \nconfiguraci\u00f3n de la clasificaci\u00f3n se  efect\u00faa con la ventana siguiente: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 192 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n  \n \n \n Selecci\u00f3n  y configuraci\u00f3n del algoritmo de clasificaci\u00f3n \nModo de \nevaluaci\u00f3n del \nclasificador \nVisualizaci\u00f3n de resultados Modelo y \nevaluaci\u00f3n  \n(en texto) Atributo \nseleccionado \ncomo clase \n \nla parte superior, como es habitual sirve para seleccionar el algoritmo de \nclasificaci\u00f3n y configurarlo. El resto de elementos a definir en esta ventana se \ndescriben a continuaci\u00f3n. \nModos de evaluaci\u00f3n del clasificador \nEl resultado de aplicar el algoritmo de clasificaci\u00f3n se  efect\u00faa comparando la \nclase predicha con la clase real de las instancias. Esta evaluaci\u00f3n puede realizarse de diferentes modos, seg\u00fan la selecci\u00f3n en el cuadro Test options :  \n\u2022 Use training set: esta opci\u00f3n eval\u00faa el clasific ador sobre el mismo conjunto \nsobre el que se construye el modelo pr edictivo para determinar el error, que \nen este caso se denomina \"error de resu stituci\u00f3n\". Por tanto, esta opci\u00f3n \npuede proporcionar una estimaci\u00f3n  demasiado optimista del \ncomportamiento del clasificador, al eval uarlo sobre el mismo conjunto sobre \nel que se hizo el modelo. \n\u2022 Supplied test set : evaluaci\u00f3n sobre conjunto independiente. Esta opci\u00f3n \npermite cargar un conjunto nuevo de da tos. Sobre cada dato se realizar\u00e1 \nuna predicci\u00f3n de clase para contar los errores. \n\u2022 Cross-validation : evaluaci\u00f3n con validaci\u00f3n cr uzada. Esta opci\u00f3n es la \nm\u00e1s elaborada y costosa. Se realizan tantas evaluaciones como se indica \nen el par\u00e1metro Folds . Se dividen las instancias en tantas carpetas como \nindica este par\u00e1metro y en cada evaluac i\u00f3n se toman las instancias de cada \ncarpeta como datos de test, y el re sto como datos de entrenamiento para Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 193 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda construir el modelo. Los errores calc ulados son el promedio de todas las \nejecuciones. \n\u2022 Percentage split : esta opci\u00f3n divide los datos en dos grupos, de acuerdo \ncon el porcentaje indicado ( %). El valor indicado es el porcentaje de \ninstancias para construir el modelo, que a continuaci\u00f3n es evaluado sobre \nlas que se han dejado aparte. Cuando el n\u00famero de instancias es suficientemente elevado, esta opci\u00f3n es suficiente para estimar con \nprecisi\u00f3n las prestaciones del clasificador en el dominio.  \nAdem\u00e1s de estas opciones para seleccio nar el modo de evaluaci\u00f3n, el bot\u00f3n \nMore Options  abre un cuadro con otras opciones adicionales: \n \nOutput model : permite visualizar (en modo text o y, con algunos algoritmos, en \nmodo gr\u00e1fico) el modelo construido por el clasificador (\u00e1rbol, reglas, etc.) \nOutput per-class stats : obtiene estad\u00edsticas de los errores de clasificaci\u00f3n por \ncada uno de los valores que toma el atributo de clase \nOutput entropy evaluation measures : generar\u00eda tambi\u00e9n medidas de \nevaluaci\u00f3n de entrop\u00eda \nStore predictions for visualization:  permite analizar los errores de \nclasificaci\u00f3n en una v entana de visualizaci\u00f3n \nCost-sensitive evaluation : con esta opci\u00f3n se puede especificar una funci\u00f3n \ncon costes relativos de los diferentes errores, que se rellena con el bot\u00f3n Set \nen nuestro ejemplo utilizaremos los va lores por defecto de estas \u00faltimas \nopciones. \n \nEvaluaci\u00f3n del clasificador en ventana de texto \nUna vez se ejecuta el clasificador sele ccionado sobre los datos de la relaci\u00f3n, \nen la ventana de texto de la derecha a parece informaci\u00f3n de ejecuci\u00f3n, el \nmodelo generado con todos los datos de ent renamiento y los resultados de la Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 194 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda evaluaci\u00f3n. Por ejemplo, al predecir el  atributo \"presentado\" , con un \u00e1rbol de \ndecisi\u00f3n de tipo J48, aparece el modelo textual siguiente: \nJ48 pruned tree \n------------------ \n \ncal_prueba <= 0: NO (153.0) cal_prueba > 0: SI (18649.0/2.0)  Number of Leaves  :  2 \n \nSize of the tree :  3\n \n \nSe obtiene a partir de los dat os esta relaci\u00f3n trivial, salvo dos \u00fanicos casos de \nerror: los presentados son los que tien en una calificaci\u00f3n superior a 0. Con \nreferencia al informe de evaluaci\u00f3n del clasificador, podemos destacar tres elementos: \n\u2022 Resumen  (Summary ): es el porcentaje global de errores cometidos en la \nevaluaci\u00f3n \n\u2022 Precisi\u00f3n detallada por clase : para cada uno de los valores que puede \ntomar el atributo de clase: el porc entaje de instancias con ese valor que son \ncorrectamente predichas (TP: true posit ives), y el porcentaje de instancias \ncon otros valores que son incorrectam ente predichas a ese valor aunque \nten\u00edan otro (FP: false positives). Las otras columnas, precision, recall, F-\nmeasure , se relacionan con estas dos anteriores. \n\u2022 Matriz de confusi\u00f3n : aqu\u00ed aparece la informaci\u00f3n detallada de cuantas \ninstancias de cada clase son predichas a cada uno de los valores posibles. \nPor tanto, es una matriz con N2 posiciones, con N el n\u00famero de valores que \npuede tomar la clase. En cada fila i, i=1...N, aparecen las instancias que \nrealmente son de la clase i, mientras que las columnas j, j=1...N, son las \nque se han predicho al valor j de la clas e. En el ejemplo anterior, la matriz \nde confusi\u00f3n que aparece es la siguiente: \n=== Confusion Matrix === \n      a     b   <-- classified as \n 18647     0 |     a = SI \n     2   153 |     b = NO\n \n \npor tanto, los valores en la diagonal s on los aciertos, y el resto de valores \nson los errores. De los 18647 alumnos presentados, todos son correctamente clasificados, mientras  que de los 155 no presentados, hay \n153 correctamente clasificados y 2 con error.  \nLista de resultados \nAl igual que con otras opciones de an\u00e1lisis , la ventana izquier da de la lista de \nresultados contiene el resumen de toda s las aplicaciones de clasificadores \nsobre conjuntos de datos en la sesi\u00f3n del Explorer . Puede accederse a esta Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 195 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda lista para presentar los resultados, y al activar el bot\u00f3n derecho aparecen \ndiferentes opciones de visualizaci\u00f3n,  entre las que podemos destacar las \nsiguientes: \n\u2022 Salvar y cargar modelos: Load model, Save model . Estos modelos pueden \nrecuperarse de fichero para posteriorm ente aplicarlos a nu evos conjuntos \nde datos \n\u2022 Visualizar \u00e1rbol y errores de predicci\u00f3n: Visualize tree, Visualize classifier \nerrors ,... \nel \u00e1rbol (permite almacenar Una vez se  ejecuta el clasif icador seleccionado \nsobre los datos de la relaci\u00f3n, \nSelecci\u00f3n y configuraci\u00f3n de clasificadores \nVamos a ilustrar la aplicaci\u00f3n de algor itmos de clasificaci\u00f3n a diferentes \nproblemas de predicci\u00f3n de at ributos definidos sobre los datos de entrada en \neste ejemplo. El problema de clasificaci\u00f3n siempre se realiza sobre un atributo \nsimb\u00f3lico, en el caso de utilizar un at ributo num\u00e9rico se precisa por tanto \ndiscretizarlo antes en intervalos q ue representar\u00e1n los valores de clase. \nEn primer lugar efectuarem os an\u00e1lisis de predicci\u00f3n de la calificaci\u00f3n en la \nprueba de selectividad a partir de los sigu ientes atributos: a\u00f1o, convocatoria, \nlocalidad, opci\u00f3n, presentado y nota de bachill erato. Se van a realizar dos tipos \nde predicciones: aprobados, e intervalos de  clasificaci\u00f3n. Por tanto tenemos \nque aplicar en primer lugar una combinaci\u00f3n de filtros que elimine los atributos \nno deseados relativos a calificaciones parci ales y asignaturas opcionales, y un \nfiltro que discretice la calific aci\u00f3n en la prueba en dos partes: \n \nobs\u00e9rvese que se prefiere realizar las pr edicciones sobre la  calificaci\u00f3n en la \nprueba, puesto que la calificaci\u00f3n fina l depende expl\u00edcitament e de la nota del \nbachillerato. Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 196 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Clasificador \u201cOneR\u201d \nEste es uno de los clasificadores m\u00e1s sencillos y r\u00e1pidos, aunque en ocasiones \nsus resultados son sorprendentemente buenos en comparaci\u00f3n con algoritmos \nmucho m\u00e1s complejos. Simplemente sele cciona el atributo que mejor \u201cexplica\u201d \nla clase de salida. Si hay atributos num\u00e9ricos, busca los umbrales para hacer \nreglas con mejor tasa de aciertos. Lo ap licaremos al problema de predicci\u00f3n de \naprobados en la prueba a partir de los at ributos de entrada,  para llegar al \nresultado siguiente: \n \npor tanto, el algoritmo ll ega a la conclusi\u00f3n que la mejor predicci\u00f3n posible con \nun solo atributo es la  nota del bachillerato, fijando el umbral que determina el \n\u00e9xito en la prueba en 6.55. La tasa de acie rtos sobre el propio conjunto de \nentrenamiento es del 72.5% . Comp\u00e1rese este resu ltado con el obtenido \nmediante ejecuci\u00f3n sobre instancias independientes. \n \nClasificador como \u00e1rbol de decisi\u00f3n: J48 \nEl algoritmo J48 de WEKA es una implementaci\u00f3n del algoritmo C4.5, uno de \nlos algoritmos de miner\u00eda de datos que m\u00e1s se ha utilizado en multitud de \naplicaciones. No vamos a entrar en los detalles de todos lo s par\u00e1metros de \nconfiguraci\u00f3n, dej\u00e1ndolo par a el lector interesado en los detalles de este \nalgoritmo, y \u00fanicamente resaltaremos uno de los m\u00e1s importantes, el factor de \nconfianza para la poda, confidence level , puesto que influye notoriamente en \nel tama\u00f1o y capacidad de predicci\u00f3n del \u00e1rbol construido.  \nUna explicaci\u00f3n simplificada de este par \u00e1metro de construcci \u00f3n del \u00e1rbol es la \nsiguiente: para cada operaci \u00f3n de poda, define la probabilidad de error que se \npermite a la hip\u00f3tesis de que el empeor amiento debido a esta operaci\u00f3n es \nsignificativo. Cuanto m\u00e1s baja se haga esa probabilidad, se  exigir\u00e1 que la \ndiferencia en los errores de predicci\u00f3n antes y despu\u00e9s de podar sea m\u00e1s Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 197 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda significativa para no podar. El valor por def ecto de este factor es del 25%, y \nconforme va bajando se permiten m\u00e1s oper aciones de poda y por tanto llegar a \n\u00e1rboles cada vez m\u00e1s peque\u00f1os. Otra forma de variar el tama\u00f1o del \u00e1rbol es a \ntrav\u00e9s de un par\u00e1metro que especifica el  m\u00ednimo n\u00famero de instancias por \nnodo, si bien es menos elegante puesto  que depende del n\u00famero absoluto de \ninstancias en el conjunto de partida. \nConstruiremos el \u00e1rbol de decisi\u00f3n con los par\u00e1metros por defecto del \nalgoritmo J48: se llega a un clasificador con m\u00e1 s de 250 nodos, con una \nprobabilidad de acierto ligeramente superior al del clasificador OneR . Modifique \nahora la configuraci\u00f3n de l algoritmo para llegar a un \u00e1rbol m\u00e1s manejable, \ncomo el que se presenta a continuaci\u00f3n \n \nObs\u00e9rvese que este modelo es un refi namiento del generado  con OneR, que \nsupone una mejorar moderada en las presta ciones. De nuevo los atributos m\u00e1s \nimportantes son la calific aci\u00f3n de bachillerato, la c onvocatoria, y despu\u00e9s el \na\u00f1o, antes que la localidad o las opciones. Analice las diferencias con evaluaci\u00f3n independiente y validaci\u00f3n cruz ada, y comp\u00e1relas con las del \u00e1rbol \nm\u00e1s complejo con menos poda. \nPodr\u00eda ser de inter\u00e9s analizar el efecto de las opciones y asignaturas \nseleccionadas sobre el \u00e9xito en la prueba, para lo cual  quitaremos el atributo \nm\u00e1s importante, nota de bachi llerato. Llegamos a un \u00e1r bol como el siguiente, \nen el que lo m\u00e1s importante es la prim era asignatura optativa, en diferentes \ncombinaciones con el a\u00f1o y segunda asignatura optativa: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 198 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \n \nEste resultado generado por el clasificador puede comp robarse si se analizan \nlos histogramas de cada variable y visualizando el porcentaje de aprobados \ncon el color, que esta variable es la que mejor separa las clases, no obstante, \nla precisi\u00f3n apenas supera el 55%. \n Otros problemas de clasificaci\u00f3n pueden fo rmularse sobre cualquier atributo de \ninter\u00e9s, a continuaci\u00f3n mostramos al gunos ejemplos a t\u00edtulo ilustrativo. \nClasifiaci\u00f3n multinivel  de las calificaciones  \nel problema anterior puede inte ntar refinarse y dividir el atributo de inter\u00e9s, la \ncalificaci\u00f3n final, en m\u00e1s niveles, en este caso 5. Los resultados se muestran a continuaci\u00f3n \noneR \n J48 \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 199 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda La precisi\u00f3n alcanzada es tan s\u00f3lo del 60%, indicando que hay bastante \nincertidumbre una vez generada la predi cci\u00f3n con los modelos anteriores. \nPredicci\u00f3n de la opci\u00f3n \nSi dejamos todos los atributos en la m uestra y aplicamos el clasificador a la \nopci\u00f3n cursado, se desvela una relaci\u00f3n tr ivial entre opci\u00f3n y asignaturas en las \nopciones que predice con pr\u00e1ctica mente el 100% de los casos. \n \nA continuaci\u00f3n elimin amos estos designadores con un  filtro de atributos. Si \naplicamos el algoritmo J48 sobre los datos filtrados, llega mos a un \u00e1rbol de \nm\u00e1s de 400 nodos, y con much\u00edsimo sobre- ajuste (observe la diferencia de \nerror de predicci\u00f3n sobre el conjunto de entrenamiento y s obre un conjunto \nindependiente). Forzando la poda del \u00e1rbol, llegamos al modelo siguiente: \n \nlos atributos m\u00e1s significativos para s eparar las opciones s on precisamente las \ncalificaciones en las asignaturas optativ as, pero apenas predi ce correctamente \nun 40% de los casos. Por tanto, vemos que no hay una relaci\u00f3n directa entre \nopciones y calificaciones en la prueba, al menos relaciones que se puedan \nmodelar con los algoritmos de clasific aci\u00f3n disponibles. Si nos fijamos en \ndetalle en las calificaciones en func i\u00f3n de las opciones, podr\u00edamos determinar \nque apenas aparecen diferencia s aparecen en los \u00faltimos percentiles, a la vista \nde las gr\u00e1ficas siguientes: \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 200 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nnota historia \n nota idioma \nnota lengua \n nota final \n \n \n \nnota asig 1 \n nota asig 2 Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 201 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \nnota asig3  \n \n Vemos que las diferencias no son significa tivas, salvo quiz\u00e1 en los \u00faltimos \npercentiles. \nPredicci\u00f3n de localidad y opci\u00f3n \nLa clasificaci\u00f3n se puede rea lizar sobre cualquier atributo disponible. Con el \nn\u00famero de atributos reducido a tres, loca lidad, opci\u00f3n y calif icaci\u00f3n (aprobados \ny suspensos), vamos a buscar modelos de  clasificaci\u00f3n, para cada uno de los \natributos: \npredicci\u00f3n de localidad \n predicci\u00f3n de opci\u00f3n \n \nEs decir, la opci\u00f3n 1 y 2 aparec en mayoritariamente en Legan\u00e9s, y las \nopciones 3 y 4 m\u00e1s en los alumnos que  aprobaron la prueba en Legan\u00e9s. No \nobstante, obs\u00e9rvese que los errores son tan abrumadores (menos del 30% de \naciertos) que cuestionan fuertement e la validez de estos modelos.  \n \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 202 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Mejora en la prueba \nUn problema de clasificaci\u00f3n intere sante puede ser determinar qu\u00e9 alumnos \ntienen m\u00e1s \"\u00e9xito\" en la prueba, en el se ntido de mejorar su calificaci\u00f3n de \nbachillerato con la calificaci\u00f3n en la pr ueba. Para ello util izaremos el atributo \n\"mejora\", introducido en la secci\u00f3n 1.4.2. 3, y lo discretizamos en dos valores de \nla misma frecuencia (obt enemos una mediana de -1.75, de manera que \ndividimos los alumnos en dos grupos: los que obtienen una diferencia menor a \neste valor y superior a este valor, para diferenciar los alumnos seg\u00fan el \nresultado se atenga m\u00e1s o menos a sus expectativas. Evidentemente, para \nevitar construir modelos triviales,  tenemos que eliminar los atributos \nrelacionados con las calificaciones en  la prueba, para no llegar a la relaci\u00f3n \nque acabamos de construir entre la variable  calculada y las originales. Vamos a \npreparar el problema de clasificac i\u00f3n con los siguientes atributros: \nAttributes:   7 \n              A\u00f1o_acad\u00e9mico \n              convocatoria               localidad \n              opcion1\u00aa \n              nota_bachi \n              Presentado \n              mejora\n \nLlegamos al siguiente \u00e1r bol de clasificaci\u00f3n.  \n \nEs decir, los atributos que m\u00e1s determinan el \"\u00e9xito\" en la prueba son: a\u00f1o \nacad\u00e9mico, opci\u00f3n y localidad. Para esto s resultados tenemos una precisi\u00f3n, \ncon evaluaci\u00f3n sobre un conj unto independiente, en torno al 60%, por lo que s\u00ed \npodr\u00edamos tomarlo en consideraci\u00f3n. Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 203 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Predicci\u00f3n num\u00e9rica \nLa predicci\u00f3n num\u00e9rica se define en WE KA como un caso particular de \nclasificaci\u00f3n, en el que la clase es un valor num\u00e9rico. No obstante, los \nalgoritmos integrados para cl asificar s\u00f3lo admiten cl ases simb\u00f3licas y los \nalgoritmos de predicci\u00f3n num\u00e9ricas, que  aparecen mayoritariamente en el \napartado classifiers->functions , aunque tambi\u00e9n en classifiers->trees . \nVamos a ilustrar algoritmos de predicci \u00f3n num\u00e9rica en WEKA con dos tipos de \nproblemas. Por un lado, \"descubrir\" relaciones deterministas que aparecen entre variables conocidas, como calific aci\u00f3n en la prueba co n respecto a las \nparciales y la calificaci\u00f3n final con res pecto a la prueba y bachillerato, y buscar \notros modelos de mayor posible inter\u00e9s. \n \nRelaci\u00f3n entre calificaci\u00f3n final y parciales \nSeleccionamos los atributos con las 6 ca lificaciones parciales y la calificaci\u00f3n \nen la prueba: \n \nVamos a aplicar el modelo de predicci \u00f3n m\u00e1s popular: regresi\u00f3n simple, que \nconstruye un modelo lineal del atributo clase a partir de los atributos de \nentrada: functions->LinearRegresion  \nComo resultado, aparece la relaci\u00f3n con los pesos relativos de las pruebas \nparciales sobre la calificaci\u00f3n de la prueba: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 204 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nHay que observar que en los problemas de predicci\u00f3n la evaluaci\u00f3n cambia, \napareciendo ahora el coeficiente de corre laci\u00f3n y los errores medio y medio \ncuadr\u00e1tico, en t\u00e9rminos absolutos y relati vos. En este caso el coeficiente de \ncorrelaci\u00f3n es de 0.998, lo que indica que la rela ci\u00f3n es de una precisi\u00f3n muy \nnotable. \nSi aplicamos ahora esta funci\u00f3n a la relaci\u00f3n entre calificaci\u00f3n final con \ncalificaci\u00f3n en la prueba y nota de bachille rato (filtro que selecciona \u00fanicamente \nlos atributos 15-17), podemos determinar la  relaci\u00f3n entre es tas variables: qu\u00e9 \npeso se lleva la calificac i\u00f3n de bachillerato y de la  prueba en la nota final. \nVamos a hacerlo primero con lo s alumnos de una poblaci\u00f3n peque\u00f1a, de \nGuadarrama (posici\u00f3n 12 del atributo localidad). Aplicamos los filtros \ncorrespondientes para tener  \u00fanicamente estos alumno s, y los atributos de \ncalificaciones de la prueba,  bachillerato y final: \n \n \nllegamos a 40 instancias:  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 205 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nsi aplic\u00e1ramos regresi\u00f3n lineal como en el ejemplo anterio r, obtenemos el \nsiguiente resultado: \n \nel resultado deja bastante que desear por que la relaci\u00f3n no es lineal. Para \nsolventarlo podemos aplicar el algoritmo M5P, sele ccionado en WEKA como \ntrees->m5->M5P, que lleva a cabo una regresi\u00f3n por tramos, con cada tramo \ndeterminado a partir de un \u00e1rbol de regres i\u00f3n. Llegamos al siguiente resultado: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 206 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nque es pr\u00e1cticamente la relaci\u00f3n exacta utilizada en la actua lidad: 60% nota de \nbachillerato y 40% de la  prueba, siempre que se supere en \u00e9sta un valor \nm\u00ednimo de 4 puntos. \nSi aplicamos este algoritmo a otro s centros no siempr e obtenemos este \nresultado, por una raz\u00f3n: hasta 1998 se  ponderaba al 50%, y a partir de 1999 \nse comenz\u00f3 con la ponderaci\u00f3n anterio r. Verif\u00edquese aplicando este algoritmo \nsobre datos filtrados que contengan alumnos de an tes de 1998 y de 1999 en \nadelante. En este caso, el algoritmo M5 P no tiene capacidad para construir el \nmodelo correcto, debido a la ligera difer encia en los resultados al cambiar la \nforma de ponderaci\u00f3n. Los \u00e1rboles obteni dos en ambos casos se incluyen a \ncontinuaci\u00f3n: \nhasta 1998 \n de 1999 en adelante Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 207 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Predicci\u00f3n de la calificaci\u00f3n \nVamos a aplicar ahora este modelo para intentar construir un modelo aplicaci\u00f3n \nm\u00e1s interesante, o, al menos, analizar  tendencias de inter\u00e9s. Se trata de \nintentar predecir la calif icaci\u00f3n final a partir de lo s atributos de entrada, los \nmismos que utilizamos para el problem a de clasificar los alumnos que \naprueban la prueba. Si aplicamos el algo ritmo sobre el conjunto completo \nllegamos al siguiente modelo: \n \nobs\u00e9rvese c\u00f3mo trata el al goritmo los atributos nominal es para incluirlos en la \nregresi\u00f3n: ordena los valores seg\u00fan el va lor de la magnitud a predecir (en el \ncaso de localidad, desde Collado hasta Los Pe\u00f1ascales y en el de opci\u00f3n, \nordenadas como 4\u00ba, 5\u00ba, 3\u00ba, 2\u00ba, 1\u00ba), y va  tomando variables binarias resultado de \ndividir en diferentes puntos, determinando su peso en la funci\u00f3n. En esta funci\u00f3n lo que m\u00e1s pesa es la convocator ia, despu\u00e9s la nota de bachillerato, y \ndespu\u00e9s entran en juego la localidad, asignat uras optativas, y opci\u00f3n, con un \nmodelo muy complejo.  \nSi simplificamos el conjunto de at ributos de entrada, y nos quedamos \n\u00fanicamente con el a\u00f1o, opci\u00f3n, nota de bachillerato, y convocatoria, llegamos \na: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 208 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \neste modelo es mucho m\u00e1s manejable. Compare los errores de predicci\u00f3n con \nambos casos: \nmodelo extenso \n modelo simplificado \n \nCorrelaci\u00f3n entre nota de bachillerato y calificaci\u00f3n en prueba  \nFinalmente, es interesante a veces hacer un modelo \u00fanicamente entre dos \nvariables para ver el grado de corre laci\u00f3n entre ambas. Continuando con \nnuestro inter\u00e9s por las relaciones entre  calificaci\u00f3n en prueba y calificaci\u00f3n en \nbachillerato, vamos a ver las diferencias por  opci\u00f3n. Para ello filtraremos por un \nlado los alumnos de opci\u00f3n 1 y los de opci\u00f3n 4. A continuaci\u00f3n dejamos Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 209 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u00fanicamente los atributos calificaci\u00f3n en prueba y nota de bachillerato, para \nanalizar la correlaci\u00f3n de los modelos para cada caso. \nalumnos opci\u00f3n 1\u00ba \n alumnos opci\u00f3n 4\u00ba \n \npodemos concluir que para estas dos opc iones el grado de relaci\u00f3n entre las \nvariables s\u00ed es significativamente difer ente, los alumnos que cursan la opci\u00f3n 1\u00ba \ntienen una relaci\u00f3n m\u00e1s \"lineal\" entre am bas calificaciones que los procedentes \nde la opci\u00f3n 4\u00ba \n \nAprendizaje del modelo y aplicaci\u00f3n a nuevos datos. \nPara finalizar esta secci\u00f3n de clasificaci\u00f3 n, ilustramos aqu\u00ed las posibilidades de \nconstruir y evaluar un clasificador de forma cruzada con dos ficheros de datos. \nSeleccionaremos el conjunto atributos si guiente: A\u00f1o_acad\u00e9mic o, convocatoria, \nlocalidad, opcion1\u00aa, des_Idioma, des_asi g1, des_asig2, des_asig3, cal_prueba, \nnota_bachi, Presentado. El atributo con la calificaci\u00f3n, \u201ccal_prueba\u201d, lo \ndiscretizamos en dos intervalos. \nVamos a generar, con el filtro de instancias dos conjuntos de datos \ncorrespondientes a los alumnos de Getafe  y Torrelodones. Para ello primero \nseleccionamos las instancias con el atribut o localidad con valor 10, lo salvamos \n(\u201cdatosGetafe\u201d) y a continuaci\u00f3n las instan cias con dicho atri buto con valor 21 \n(\u201cdatosTorrelodones\u201d). Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 210 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nAhora vamos a generar los modelos de cl asificaci\u00f3n de alumnos con buen y \nmal resultado en la prueba con el fichero de alumnos de la localidad de Torrelodones, para evaluarlo con los alumnos de Getafe.  \nPara ello en primer lugar  cargamos el fichero con los alumnos de Torrelodones \nque acabamos de generar, \u201cdatosTorrel odones\u201d, y lo evaluamos sobre el \nconjunto con alumnos de Getafe. Para ello, seleccionaremos la opci\u00f3n de \nevaluaci\u00f3n con un fichero de datos independiente, Supplied test set , y fijamos \ncon el bot\u00f3n Set, que el fichero de test es \u201cdat osGetafe\u201d. Obs\u00e9rvese el modelo \ngenerado y los resultados: \n \nSi ahora hacemos la operaci\u00f3n inversa,  entrenar con los datos de Getafe y \nevaluar con los de Torrelodones, llegamos a: Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 211 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \n \nHay ligeras diferencias en los model os generados para am bos conjuntos de \ndatos (para los alumnos de Torrelodones , lo m\u00e1s importante es tener una \ncalificaci\u00f3n de bachillerato superior a 6. 8, mientras que a los de Getafe les \nbasta con un 6.5), y los resultados de evaluaci\u00f3n con los datos cruzados \nmuestran una variaci\u00f3n mu y peque\u00f1a. El modelo cons truido a partir de los \ndatos de Torrelodones predice ligeramente peor los resultados de Getafe que a \nla inversa. \n \nSelecci\u00f3n de atributos \nEsta \u00faltima secci\u00f3n permite autom atizar la b\u00fasqueda de subconjuntos de \natributos m\u00e1s apropiados par a \"explicar\" un atributo objetivo, en un sentido de \nclasificaci\u00f3n supervisada: permite expl orar qu\u00e9 subconjuntos de atributos son \nlos que mejor pueden clasificar la clas e de la instancia. Esta selecci\u00f3n \n\"supervisada\" aparece en contraposici\u00f3 n a los filtros de preprocesado \ncomentados en la secci\u00f3n 1. 4.2, que se realizan de forma independiente al \nproceso posterior, raz\u00f3n por la que se etiquetaron como \"no supervisados\".  \nLa selecci\u00f3n supervisada de atributos tiene dos componentes: \n\u2022 M\u00e9todo de Evaluaci\u00f3n ( Attribute Evaluator ): es la funci \u00f3n que determina la \ncalidad del conjunto de atributos para discriminar la clase. \n\u2022 M\u00e9todo de B\u00fasqueda ( Search Method ): es la forma de realizar la b\u00fasqueda \nde conjuntos. Como la evaluaci\u00f3n exhaus tiva de todos los subconjuntos es \nun problema combinatorio inabordable en cuanto crece el n\u00famero de \natributos, aparecen estrategias que permi ten realizar la b\u00fasqueda de forma \neficiente \nDe los m\u00e9todos de evaluaci\u00f3n, podemos distinguir dos tipos: los m\u00e9todos que \ndirectamente utilizan un clasificador es pec\u00edfico para medir la calidad del \nsubconjunto de atributos a trav\u00e9s de la ta sa de error del clasificador, y los que \nno. Los primeros, denominados m\u00e9todos  \"wrapper\", porque \"envuelven\" al \nclasificador para explorar la mejor selecci\u00f3n de atributos  que optimiza sus \nprestaciones, son muy costosos por que necesitan un proceso completo de \nentrenamiento y evaluaci\u00f3n en cada pa so de b\u00fasqueda. Entre los segundos \npodemos destacar el m\u00e9todo \"CfsSubsetEval\" , que calcula la correlaci\u00f3n de la Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 212 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda clase con cada atributo, y eliminan atributos que ti enen una correlaci\u00f3n muy \nalta como atributos redundantes.  \nEn cuanto el m\u00e9todo de b\u00fasqueda, vamo s a mencionar por su rapidez el \n\"ForwardSelection \", que es un m\u00e9todo de b\u00fasqueda  sub\u00f3ptima en escalada, \ndonde elije primero el mejor atributo, despu\u00e9s a\u00f1ade el siguiente atributo que \nm\u00e1s aporta y continua as\u00ed hasta llegar a la  situaci\u00f3n en la que a\u00f1adir un nuevo \natributo empeora la situaci\u00f3n. Otr o m\u00e9todo a destacar ser\u00eda el \" BestSearch\" , \nque permite buscar interacciones entre atri butos m\u00e1s  complejas que el an\u00e1lisis \nincremental anterior. Este  m\u00e9todo va analizando lo que mejora y empeora un \ngrupo de atributos al a\u00f1adir elementos, c on la posibilidad de hacer retrocesos \npara explorar con m\u00e1s detalle. El m\u00e9todo \"ExhaustiveSearch\"  simplemente \nenumera todas las posibilidades y la s eval\u00faa para seleccionar la mejor \nPor otro lado, en la configuraci\u00f3n del problema debemos  seleccionar qu\u00e9 \natributo objetivo se utiliza para la se lecci\u00f3n supervisada, en la ventana de \nselecci\u00f3n, y determinar si la evaluaci\u00f3n se realizar\u00e1 con todas las instancias \ndisponibles, o mediante validaci\u00f3n cruzada. \nLos elementos por tanto a configurar en esta secci\u00f3n se resumen en la figura \nsiguiente: \n \n  \n \n Evaluaci\u00f3n de \nla selecci\u00f3n \nsupervisada \nVisualizaci\u00f3n de resultados Resultados  \n ( en texto) atributo de \nclase Algoritmo evaluador \nAlgoritmo de \nb\u00fasqueda \n \nSiguiendo con nuestro ejemplo, vamo s a aplicar b\u00fasqueda de atributos para \n\"explicar\" algunos atributos objetivo. Para obtener resultados sin necesidad de \nmucho tiempo, vamos a seleccionar lo s algoritmos m\u00e1s eficientes de \nevaluaci\u00f3n y b\u00fasqueda, CsfSubsetEval y ForwardSelection  \n Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 213 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por ejemplo, para la calificaci\u00f3n final tenemos 8 atributos seleccionados: \nSelected attributes: 5,6,7,10,12,14,17,18 : 8 \n                     nota_Lengua \n                     nota_Historia \n                     nota_Idioma                      calif_asig1                      calif_asig2                      calif_asig3 \n                     cal_final \n                     Presentado\n \ny para la opci\u00f3n 1 atributo: \nSelected attributes: 9 : 1 \n                     des_asig1 \n \nPor tanto, hemos llegado a los atribut os que mejor explican ambos (la \ncalificaci\u00f3n en la prueba depende directam ente de las parciales, y la opci\u00f3n se \nexplica con la 1\u00aa asignatura), si bien  son relaciones bastante triviales. A \ncontinauci\u00f3n preparamos los datos para buscar relaciones no conocidas, \nquitando los atributos refe rentes a cada prueba parcial . Dejando como atributos \nde la relaci\u00f3n: \nAttributes:   7 \n              A\u00f1o_acad\u00e9mico               convocatoria               localidad               opcion1\u00aa \n              cal_prueba \n              nota_bachi               Presentado\n \n \npara la calificaci\u00f3n final llegamos a 2 atributos: \nSelected attributes: 6,7 : 2 \n                     nota_bachi \n                     Presentado  \ny para la opci\u00f3n 2: \nSelected attributes: 3,5,6 : 3 \n                     localidad                      cal_prueba                      nota_bachi\n \n \nNo obstante, si observamos la figur a de m\u00e9rito con ambos problemas, que \naparece en la ventana textual de resultados, vemos que este segundo es mucho menos fiable, como ya hemos comprobado en secciones anteriores. \n  Cap\u00edtulo 4  T\u00e9cnicas de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 214 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 215 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n  \nCap\u00edtulo 5. Implementaci\u00f3n \nde las t\u00e9cnicas de An\u00e1lisis \nde Datos en Weka  \n \n5.1.  Utilizaci\u00f3n de las clases de WEKA en \nprogramas independientes \n \n \n5.2.  Tabla de Decisi\u00f3n en WEKA \nEl algoritmo de tabla de decisi\u00f3n impl ementado en la herramienta WEKA se \nencuentra en la clase weka.classifiers.DecisionTable.java . Las opciones de \nconfiguraci\u00f3n de que disponen son las que vemos en la tabla 5.1. \nTabla 5.1: Opciones de configuraci\u00f3n para el algo ritmo de tabla de decisi\u00f3n en WEKA. \nOpci\u00f3n Descripci\u00f3n \nuseIBk (False) Utilizar NN (ver punt o 2.2.5.1) en lugar de la tabla de \ndecisi\u00f3n si no la instancia a clasificar no se corresponde \ncon ninguna regla de la tabla. \ndisplayRules (False) Por defecto no se muestran la s reglas del clasificador, \nconcretamente la tabla de decisi\u00f3n construida. \nmaxStale (5) Indica el n\u00famero m\u00e1 ximo de conjuntos que intenta mejorar \nel algoritmo para encontrar una tabla mejor sin haberla encontrado en los \u00faltimos n-1 subconjuntos. \ncrossVal (1) Por defecto se eval\u00faa el sistema mediante el proceso \nleave-one-out . Si se aumenta el valor 1 se realiza \nvalidaci\u00f3n cruzada con n carpetas. \n \nEn primer lugar, en cuanto a los atributos que permite  el sistema, \u00e9stos pueden \nser tanto num\u00e9ricos (que se discretizar \u00e1n) como simb\u00f3licos. La clase tambi\u00e9n \npuede ser num\u00e9rica o simb\u00f3lica. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 216 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda El algoritmo consiste en ir sele ccionando uno a uno lo s subconjuntos, \na\u00f1adiendo a cada uno de los ya probados c ada uno de los atributos que a\u00fan no \npertenecen a \u00e9l. Se prueba la precis i\u00f3n del subconjunto,  bien mediante \nvalidaci\u00f3n cruzada o leave-one-out  y, si es mejor, se contin\u00faa con \u00e9l. Se \ncontin\u00faa as\u00ed hasta que se alcanza maxStale . Para ello, una variable comienza \nsiendo 0 y aumenta su valor en una unidad cuando a un subconjunto no se le \npuede a\u00f1adir ning\u00fan atributo para  mejorarlo, volviendo a 0 si se a\u00f1ade un \nnuevo atributo a un subconjunto. \nEn cuanto al proceso leave-one-out , es un m\u00e9todo de estimaci\u00f3n del error. Es \nuna validaci\u00f3n cruzada en la que el n\u00famer o de conjuntos es igual al n\u00famero de \nejemplos de entrenamiento.  Cada vez se elim ina un ejemplo del conjunto de  \nentrenamiento y se entrena con el resto. Se juzgar\u00e1 el acierto del sistema con \nel resto de instancias seg\u00fan se acierte o se falle en la predicci\u00f3n del ejemplo que se elimin\u00f3. El resultado de las n pruebas (siendo n el n\u00famero inicial de \nejemplos de entrenamiento) se promedia y dicha media ser\u00e1 el error estimado. \nPor \u00faltimo, para clasificar un ejemplo pueden ocurrir dos cosas. En primer \nlugar, que el ejemplo corresponda exacta mente con una de las reglas de la \ntabla de decisi\u00f3n, en cuyo caso se devolve r\u00e1 la clase de dicha regla. Si no se \ncorresponde con ninguna regla, se puede utilizar Ibk (si se seleccion\u00f3 dicha \nopci\u00f3n) para predecir la clase, o la medi a o moda de la clase seg\u00fan el tipo de \nclase del que se trate (num\u00e9rica o simb\u00f3lica). \n \n5.3.  ID3 en WEKA \nLa clase en la que est\u00e1 codificado el algoritmo ID3 es weka.classifiers.ID3.java .  \nEn primer lugar, en cuanto a la im plementaci\u00f3n, no permite ning\u00fan tipo de \nconfiguraci\u00f3n. Esta implementaci\u00f3n se  ajusta exactamente a lo descrito \nanteriormente. Lo \u00fanico rese\u00f1able es que para determinar si un nodo es hoja o \nno, se calcula la ganancia de informa ci\u00f3n y, si la m\u00e1xima ganancia es 0 se \nconsidera nodo hoja, indepe ndientemente de que haya ej emplos de distintas \nclases en dicho nodo. \nLos atributos introducidos al sistema deben ser simb\u00f3licos, al igual que la \nclase. \n \n5.4.  C4.5 en WEKA (J48) \nLa clase en la que se implementa el algoritmo C4.5 en la herramienta WEKA es \nweka.classifers.j48.J48.java . Las opciones que permite este algoritmo son las \nque se muestran en la tabla 2.3. \nTabla 5.2: Opciones de configuraci\u00f3n para el algoritmo C4.5 en WEKA. \nOpci\u00f3n Descripci\u00f3n \nminNumObj (2) N\u00famero m\u00edni mo de instancias por hoja. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 217 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda saveInstanceData \n(False) Una vez finalizada la creaci\u00f3n del \u00e1rbol de decisi\u00f3n se \neliminan todas las instancias que se clasifican en cada \nnodo, que hasta el momento se  manten\u00edan almacenadas.\nbinarySplits (False) Con los atributos nominales tambi\u00e9n no se divide (por \ndefecto) cada nodo en dos ramas. \nunpruned (False) En caso de no activar la opci\u00f3n, se realiza la poda del \n\u00e1rbol. \nsubtreeRaising (True) Se permite realizar el  podado con el proceso subtree \nraising . \nconfidenceFactor \n(0.25) Factor de confianza para el podado del \u00e1rbol. \nreducedErrorPruning \n(False) Si se activa esta opci\u00f3n, el proceso de podado no es el \npropio de C4.5, sino que el conjunto de ejemplos se \ndivide en un subconjunto de entrenamiento y otro de test, \nde los cuales el \u00faltimo servir \u00e1 para estimar el error para \nla poda. \nnumFolds (3) Define el n\u00famero de subconjuntos en que hay que dividir \nel conjunto de ejemplos para, el \u00faltimo de ellos, \nemplearlo como conjunto de test  si se activa la opci\u00f3n \nreducedErrorPruning . \nuseLaplace (False) Si se activa esta opci\u00f3n, cuando se intenta predecir la \nprobabilidad de que una instancia pertenezca a una clase, se emplea el suavizado de Laplace . \n \nEl algoritmo J48 se ajusta al al goritmo C4.5 al que se le ampl\u00edan \nfuncionalidades tales como permitir la  realizaci\u00f3n del proceso de podado \nmediante reducedErrorPruning  o que las divisiones sean siempre binarias \nbinarySplits . Algunas propiedades concretas de la implementaci\u00f3n son las \nsiguientes: \n\u2022 En primer lugar, en cuanto a los tipos de atributos admitidos, estos \npueden ser simb\u00f3licos y num\u00e9ricos. Se permiten ejemplos con faltas en \ndichos atributos, tanto en el mom ento de entrenami ento como en la \npredicci\u00f3n de dicho ejemplo. En c uanto a la clase, \u00e9sta debe ser \nsimb\u00f3lica. \n\u2022 Se permiten ejemplos con peso. \n\u2022 El algoritmo no posibilita la generaci \u00f3n de reglas de clasificaci\u00f3n a partir \ndel \u00e1rbol de decisi\u00f3n. \n\u2022 Para el tratamiento de los atribu tos num\u00e9ricos el algoritmo prueba los \npuntos secuencialmente, con lo que emplea tres de las cuatro opciones \nque se comentaron anteriormente (ver figura 2.3). La cuarta opci\u00f3n, que \nconsist\u00eda en unir intervalos adyacent es con la misma clase mayoritaria \nno se realiza. \n\u2022 Tambi\u00e9n respecto a los atributos num \u00e9ricos, cuando se intenta dividir el \nrango actual en dos subrangos se ejecuta la ecuaci\u00f3n 2.14. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 218 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda ncn0.1 nima Divisi\u00f3nM\u00edic\u00d7 =  (2.14) \nEn esta ecuaci\u00f3n nic es el n\u00famero de ejemplos  de entrenamiento con el \natributo i conocido, y nc el n\u00famero de clases. A dem\u00e1s, si el resultado de \nla ecuaci\u00f3n es menor que el n\u00famer o m\u00ednimo de ejemplos que debe \nclasificarse por cada nodo hijo, se i guala a \u00e9ste n\u00famero y si es mayor \nque 25, se iguala a dicho n\u00famero. Lo que indica este n\u00famero es el \nn\u00famero m\u00ednimo de ejemplos que debe haber por cada uno de los dos \nnodos hijos que resultar\u00edan de la divisi \u00f3n por el atributo num\u00e9rico, con lo \nque no se considerar\u00edan divisiones que no cumplieran este dato. \n\u2022 El c\u00e1lculo de la entrop\u00eda y de la ganancia de informaci\u00f3n se realiza con \nlas ecuaciones 2.15, 2.16 y 2.17. \n))I(A-(InnG(Ai 2ic\ni=)  (2.15) \n() ( )\u2211\n=\u2212 =nc\n1cc 2 c ic 2 ic n logn n log nI  (2.16) \n() \u2211\n=\u2212 =) nv(A\n1jij ij 2 ij ii\nI n logn )(AI ; () \u2211\n=\u2212=nc\n1kijk 2 ijk ij n logn I  (2.17) \nEn estas ecuaciones, nic es el n\u00famero de ejemplos con el atributo i \nconocido, n el n\u00famero total de ejemplos, nc el n\u00famero de ejemplos \nconocidos (el atributo i) con clase c, nij el n\u00famero de ejemplos con valor j \nen el atributo i y nijk el n\u00famero de atributos con valor j en el atributo i y \ncon clase k. \n\u2022 Adem\u00e1s, la informaci\u00f3n de ruptura se expresa como se muestra en la \necuaci\u00f3n 2.18. \n() () ( )\nnn log n n logn n logn\n)A I(Divisi\u00f3n2 ic 2 ic) nv(A\n1jij 2 ij\nii\n+ \u2212\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\u2212\n=\u2211\n= (2.18) \nEn la ecuaci\u00f3n 2.18, nij es el n\u00famero de ejemplos con valor j en el \natributo i, nic es el n\u00famero de ejemplos con valor conocido en el atributo i \ny n es el n\u00famero total de ejemplos. \n\u2022 El suavizado de Laplace  se emplea en el proces o de clasificaci\u00f3n de un \nejemplar. Para calcular la pr obabilidad de que un ejem plo pertenezca a \nuna clase determinada en un nodo hoja se emplea la ecuaci\u00f3n 2.19. \n()Cn1nE|kPk\n++=  (2.19) Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 219 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En la ecuaci\u00f3n 2.19, nk es el n\u00famero de ejemplos de la clase \nclasificados en el nodo hoja, n el n\u00famero total de ej emplos clasificados \nen el nodo y C el n\u00famero de clases para los que hay alg\u00fan ejemplo \nclasificado en el nodo. \n \n5.5.  \u00c1rbol de Decisi\u00f3n de un solo nivel en WEKA \nLa clase en la que se implementa el algoritmo toc\u00f3n de decisi\u00f3n en la \nherramienta WEKA es weka.classifers.DecisionStump.java . As\u00ed, en WEKA se \nllama a este algoritmo toc\u00f3n de decisi\u00f3n  [decisi\u00f3n stump]. No tiene opciones de \nconfiguraci\u00f3n, pero la im plementaci\u00f3n es muy completa, dado que admite tanto \natributos num\u00e9ricos como simb\u00f3licos y cl ases de ambos tipos tambi\u00e9n. El \u00e1rbol \nde decisi\u00f3n tendr\u00e1 tres ramas: una de ellas ser\u00e1 para el caso de que el atributo \nsea desconocido, y las otras dos ser\u00e1n par a el caso de que el valor del atributo \ndel ejemplo de test sea i gual a un valor concreto del atributo o distinto a dicho \nvalor, en caso de los atributos simb\u00f3licos , o que el valor del ejemplo de test sea \nmayor o menor a un determinado valor en el caso de atributos num\u00e9ricos. \nEn el caso de los atributos simb\u00f3licos  se considera cada valor posible del \nmismo y se calcula la ganancia de informa ci\u00f3n con el atributo igual al valor, \ndistinto al valor y valores perdidos de l atributo. En el caso de atributos \nsimb\u00f3licos se busca el mejor punto de rupt ura, tal y como se vio en el sistema \nC4.5 (ver punto 2.2.2.2). \nDeben tenerse en cuenta cuatro posibles casos al calcular la ganancia de \ninformaci\u00f3n: que sea un atributo simb\u00f3lico y la clase sea simb\u00f3lica o que la clase sea num\u00e9rica, o que s ea un atributo num\u00e9rico y la  clase sea simb\u00f3lica o \nque la clase sea num\u00e9rica. A contin uaci\u00f3n se comenta cada caso por \nseparado. \nAtributo Simb\u00f3lico y Clase Simb\u00f3lica \nSe toma cada vez un valor \nvx del atributo simb\u00f3lico Ai como base y se \nconsideran \u00fanicamente tres posibles rama s en la construcci\u00f3n del \u00e1rbol: que el \natributo Ai sea igual a vx, que el atributo Ai sea distinto a vx o que el valor del \natributo Ai sea desconocido. Con ello, se calcula la entrop\u00eda del atributo \ntomando como base el valor escogido ta l y como se muestr a en la ecuaci\u00f3n \n2.20. \n()\n()2log nI nlogn\n)(AI3\n1jij ij ij\nivx\u2211\n=\u2212\n= ; () \u2211\n==nc\n1kijk ijk ij nlogn I  (2.20) \nEn la ecuaci\u00f3n 2.20 el valor de j en el sumatorio va desde 1 hasta 3 porque los \nvalores del atributo se restringen a tres: igual a vx, distinto a vx o valor \ndesconocido. En cuanto a los par\u00e1metros, nij es el n\u00famero de ejemplos con \nvalor j en el atributo i, n el n\u00famero total de ejemplos y nijk el n\u00famero de \nejemplos con valor j en el atributo i y que pertenece a la clase k. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 220 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Atributo Num\u00e9rico y Clase Simb\u00f3lica \nSe ordenan los ejemplos seg\u00fan el atributo Ai y se considera cada zx, definido \ncomo el punto medio entre los valores vx y v x+1, del atributo como posible punto \nde corte. Se consideran entonces como po sibles valores del atributo el rango \nmenor o igual a zx, mayor a zx y valor desconocido. Se  calcula la entrop\u00eda \n(ecuaci\u00f3n 2.20) del rango tomando como  base esos tres posibles valores \nrestringidos del atributo. \nAtributo Simb\u00f3lico y Clase Num\u00e9rica \nSe vuelve a tomar como base cada vez un valor del atributo, tal y como se \nhac\u00eda en el caso Atributo Simb\u00f3lico y Clase Simb\u00f3lica , pero en este caso se \ncalcula la varianza de la clase para los valores del atributo mediante la \necuaci\u00f3n 2.21. \n\u2211\n=\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n=3\n1j jj\nj ivWS-SS )(A Varianza\nx (2.21) \nEn la ecuaci\u00f3n 2.21, Sj es la suma de los valores de  la clase de los ejemplos \ncon valor j en el atributo i, SSj es la suma de los valore s de la clase al cuadrado \ny Wj es la suma de los pesos de los ejem plos (n\u00famero de ejemplos si no se \nincluyen pesos) con valor j en el atributo. \nAtributo Num\u00e9rico y Clase Num\u00e9rica \nSe considera cada valor del atributo como  punto de corte tal y como se hac\u00eda \nen el caso Atributo Num\u00e9rico y Clase Simb\u00f3lica . Posteriormente, se calcula la \nvarianza tal y como se muestra en la ecuaci\u00f3n 2.21. \nEn cualquiera de los cuatro casos que se han comentado, lo que se busca es el \nvalor m\u00ednimo de la ecuaci\u00f3n calculada, ya sea la entrop\u00eda o la varianza. De esta \nforma se obtiene el atributo que ser\u00e1 ra\u00edz del \u00e1rbol de decisi\u00f3n y sus tres \nramas. Lo \u00fanico que se har\u00e1 por \u00faltimo es construir dicho \u00e1rbol: cada rama \nfinaliza en un nodo hoja con el valor de la clase, que  ser\u00e1 la media o la moda \nde los ejemplos que se clasifican por ese camino, seg\u00fan se trate de una clase \nnum\u00e9rica o simb\u00f3lica. \n \n5.6.  1R en WEKA \nLa clase weka.classifers.OneR.java  implementa el algoritmo 1R. La \u00fanica \nopci\u00f3n configurable es la que se  muestra en la tabla 2.4. \n \nTabla 5.3: Opciones de configuraci\u00f3n para el algoritmo 1R en WEKA. \nOpci\u00f3n Descripci\u00f3n \nminBucketSize N\u00famero m\u00ednimo de ej emplos que deben pertenecer a un Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 221 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda (6) conjunto en caso de atributo num\u00e9rico. \n \nLa implementaci\u00f3n que se lleva a cabo en WEKA de este algoritmo cumple \nexactamente con lo descrito anteriormente. \nComo vemos, 1R es un clasificador mu y sencillo, que \u00fanic amente utiliza un \natributo para la clasificaci\u00f3n. Sin embargo, a\u00fan hay otro clasificador m\u00e1s \nsencillo, el 0R, implementado en weka.classifers.ZeroR.java , que simplemente \ncalcula la media en el caso de tener una clase num\u00e9rica o la moda, en caso de \nuna clase simb\u00f3lica. No tiene ning\u00fan tipo de opci\u00f3n de configuraci\u00f3n. \n \n5.7.  PRISM en WEKA \nLa clase weka.classifers.Prism.java  implementa el algoritmo PRISM. No tiene \nning\u00fan tipo de configuraci\u00f3n posible. \u00dan icamente permite atributos nominales, \nla clase debe ser tambi\u00e9n nominal y no puede haber atributos con valores \ndesconocidos. La implementaci\u00f3n de es ta clase sigue completamente el \nalgoritmo expuesto en la figura 2.10. \n \n5.8.  PART en WEKA \nLa clase weka.classifers.j48.PART.java  implementa el algor itmo PART. En la \ntabla 2.5 se muestran las opciones de configuraci\u00f3n de dicho algoritmo. \n \nTabla 5.4: Opciones de configuraci\u00f3n para el algoritmo  PART en WEKA. \nOpci\u00f3n Descripci\u00f3n \nminNumObj (2) N\u00famero m\u00edni mo de instancias por hoja. \nbinarySplits (False) Con los atributos nominales tambi\u00e9n no se divide (por \ndefecto) cada nodo en dos ramas. \nconfidenceFactor (0.25) Factor de confianza para el podado del \u00e1rbol. \nreducedErrorPruning \n(False) Si se activa esta opci\u00f3n, el proceso de podado no es el \npropio de C4.5, sino que el conjunto de ejemplos se \ndivide en un subconjunto de entrenamiento y otro de test, \nde los cuales el \u00faltimo servir \u00e1 para estimar el error para \nla poda. \nnumFolds (3) Define el n\u00famero de subconjuntos en que hay que dividir \nel conjunto de ejemplos para, el \u00faltimo de ellos, emplearlo como conjunto de test  si se activa la opci\u00f3n \nreducedErrorPruning . \n \nComo se ve en la t abla 2.5, las opciones de l algoritmo PART son un \nsubconjunto de las ofrecidas por J48, que implementa el si stema C4.5, y es Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 222 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda que PART emplea muchas de las clases que implementan C4.5, con lo que los \nc\u00e1lculos de la entrop\u00eda, del error esperado,... son los mismos. \nLa implementaci\u00f3n que se realiza en W EKA del sistema PART se corresponde \nexactamente con lo comentado anteriorm ente, y m\u00e1s teniend o en cuenta que \nlos implementadores de la versi\u00f3n son los propios creadores del algoritmo. \nPor \u00faltimo, en cuanto a los tipos de datos admitidos por el algoritmo, estos son \nnum\u00e9ricos y simb\u00f3licos para los atri butos y simb\u00f3lico para la clase. \n \n5.9.  Naive  Bayesiano en WEKA \nEl algoritmo naive  Bayesiano se encuentra implementado en la clase \nweka.classifiers.NaiveBayesSimple.java . No dispone de ninguna opci\u00f3n de \nconfiguraci\u00f3n. El algoritmo que im plementa esta clase se corresponde \ncompletamente con el expuesto anteriorm ente. En este caso no se usa el \nestimador de Laplace , sino que la aplicaci\u00f3n muestra un error si hay menos de \ndos ejemplos de entrenam iento para una terna atributo-valor-clase  o si la \ndesviaci\u00f3n t\u00edpica de un atributo num\u00e9rico es igual a 0. \nUna alternativa a esta clase que tambi\u00e9n implementa un clasificador naive \nBayesiano es la clase weka.classifiers.NaiveBayes.java . Las opciones de \nconfiguraci\u00f3n de que disponen son la s mostradas en la tabla 2.6. \n \nTabla 5.5: Opciones de configuraci\u00f3n para el algoritmo Bayes naive en WEKA. \nOpci\u00f3n Descripci\u00f3n \nuseKernelEstimator \n(False) Emplear un estimador de densidad de n\u00facleo (ver punto \n2.3.3) para modelar los atri butos num\u00e9ricos en lugar de \nuna distribuci\u00f3n normal. \n \nEn este caso, sin embargo, en lugar de emplear la frecuencia de aparici\u00f3n \ncomo base para obtener las probabilida des se emplean distribuciones de \nprobabilidad. Para los atributos discretos o simb\u00f3licos se emplean estimadores \ndiscretos, mientras que para los atribut os num\u00e9ricos se emplean bien un \nestimador basado en la distribuci\u00f3n no rmal o bien un estimador de densidad de \nn\u00facleo. \nSe crear\u00e1 una distribuci\u00f3n para cada cl ase, y una distribuci\u00f3n para cada \natributo-clase , que ser\u00e1 discreta en el caso de que el atributo sea discreto. El \nestimador se basar\u00e1 en una distribuci\u00f3n normal o kernel  en el caso de los \natributo-clase  con atributo num\u00e9rico seg\u00fan se active o no la opci\u00f3n mostrada \nen la tabla 2.6. \nEn el caso de los atributos num\u00e9ricos, en primer lugar se obtiene la precisi\u00f3n \nde los rangos, que por defecto en la implementaci\u00f3n ser\u00e1 de 0,01 pero que se Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 223 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda calcular\u00e1 siguiendo el algor itmo descrito, mediante ps eudoc\u00f3digo, en la figura \n2.15. \nPrecisi\u00f3n (ejemplos, atributo) { \n p = 0.01 // valor por defecto \n // se ordenan los ejemplos de acuerdo al atributo num\u00e9rico \n Ordenar_ejemplos (ejemplos, atributo) \n vUltimo = Valor(ejemplos(0), atributo) \n delta = 0; \n distintos = 0; \n Para cada ejemplo (ej) de ejemplos \n  vActual = Valor (ej, atributo) \n  Si vActual <> vUltimo Entonces \n   delta = delta + (vActual \u2013 vUltimo) \n   vActual = vUltimo \n   distintos = distintos + 1 \n Si distintos > 0 Entonces \n  p = delta / distintos \n Devolver p \n}   \nFigura 5.1: Algoritmo empleado para definir la prec isi\u00f3n de los rangos para un atributo. \nUna vez obtenida la precisi\u00f3n de los ra ngos, se crea el estimador basado en la \ndistribuci\u00f3n correspondiente y con la pr ecisi\u00f3n calculada. Se recorrer\u00e1n los \nejemplos de entrenamiento y de esta forma  se generar\u00e1 la distribuci\u00f3n de cada \natributo-clase  y de cada clase. \nCuando se desee clasificar un ejemplo el proceso ser\u00e1 el mismo que se \ncoment\u00f3 anteriormente, y que se basaba en la ecuaci\u00f3n 2.27, pero obteniendo \nlas probabilidades a partir de estas distri buciones generadas. En el caso de los \natributos num\u00e9ricos, se calcul ar\u00e1 la probabilidad del rango [x-precisi\u00f3n, \nx+precisi\u00f3n] , siendo x el valor del atributo. \n \n5.10.  VFI en WEKA \nEl clasificador VFI se implementa en la clase weka.classifiers.VFI.java . Las \nopciones de configuraci\u00f3n de que disp one son las que se muestran en la tabla \n2.7. \nTabla 5.6: Opciones de configuraci\u00f3n para el algoritmo Bayes naive en WEKA. \nOpci\u00f3n Descripci\u00f3n \nweightByConfidence  \n(True) Si se mantiene activa esta opc i\u00f3n cada atributo se pesar\u00e1 \nconforme a la ecuaci\u00f3n 2.29. \nbias (0.6) Par\u00e1metro de configuraci\u00f3n para el pesado por \nconfianza. \n \nEl algoritmo que se implem enta en la clase VFI es similar al mostrado en la \nfigura 2.16. Sin embargo, sufre ca mbios sobretodo en el proceso de \nclasificaci\u00f3n de un nuevo ejemplar: Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 224 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 La normalizaci\u00f3n de los intervalos por clase se realiza durante la \nclasificaci\u00f3n y no durant e el entrenamiento. \n\u2022 Si se activa la opci\u00f3n de pesado por confianza , cada voto de cada \natributo a cada clase se pesa mediante la ecuaci\u00f3n 2.29. \n() ()() ( ) ()\n()biasnC\n0i i i bias\ni i2lg nnlg n nlg nAI Aw\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb + \u2212= =\u2211= (2.29) \nEn la ecuaci\u00f3n 2.29 I(Ai) es la entrop\u00eda del atributo Ai, siendo n el \nn\u00famero total de ejemplares, nC el n\u00famero de clases y ni el n\u00famero de \nejemplares de la clase i. El par\u00e1metro bias es el que se configur\u00f3 como \nentrada al sistema, tal y como se mostraba en la tabla 2.7. \n\u2022 En cuanto a los atributos, pueden ser num\u00e9ricos y simb\u00f3licos, mientras \nque la clase debe ser simb\u00f3lica. \nRelacionado con este clasificador se enc uentra otro que se implementa en la \nherramienta WEKA. Se trata de la clase weka.classifiers.HyperPipes.java . Este \nclasificador no tiene ning\u00fan par\u00e1metro de configuraci\u00f3n y es una simplificaci\u00f3n \ndel algoritmo VFI: En este caso se almacena para cada atributo num\u00e9rico el \nm\u00ednimo y el m\u00e1ximo valor que dicho atri buto obtiene para cada clase, mientras \nque en el caso de los atri butos simb\u00f3licos marca los valores que el atributo \ntiene para cada clase. A la hora de clas ificar un nuevo ejemplo, simplemente \ncuenta, para cada clase, el n\u00famero de atributos que se encuentran en el \nintervalo almacenado en el caso de at ributos num\u00e9ricos y el n\u00famero de \natributos simb\u00f3licos con valor activado en dicha clase. La clase con mayor \nn\u00famero de coincidencias gana . \n \n5.11.  KNN en WEKA (IBk) \nEn WEKA se implementa el clasif icador KNN con el nombre IBk, \nconcretamente en la clase weka.classifiers.IBk.java . Adem\u00e1s, en la clase \nweka.classifiers.IB1.java  hay una versi\u00f3n simplificada del mismo, \nconcretamente un clasificador NN [N earest Neighbor], sin ning\u00fan tipo de \nopci\u00f3n, en el que, como su propio nom bre indica, tiene en cuenta \u00fanicamente \nel voto del vecino m\u00e1s cercano. Por e llo, en la tabla 2.8 se muestran las \nopciones que se permiten con el clasificador IBk. \nTabla 5.7: Opciones de configuraci\u00f3n para el algoritmo IBk en WEKA. \nOpci\u00f3n Descripci\u00f3n \nKNN (1) N\u00famero de vecinos m\u00e1s cercanos. \ndistanceWeighting \n(No distance weighting) Los valores posibles son: \nNo distance weighting , Weight by \n1-distance  y Weight by 1/distance . Permite definir si se \ndeben \u201cpesar\u201d los vecinos a la hora de votar bien seg\u00fan su semejanza o con la inversa de su distancia con respecto al ejemplo a clasificar. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 225 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda crossValidate \n(False) Si se activa esta opci\u00f3n, cuando se vaya a clasificar una \ninstancia se selecciona el n\u00famero de vecinos (hasta el \nn\u00famero especificado en la opci\u00f3n KNN) mediante el proceso \nhold-one-out . \nmeanSquared (False) Minimiza el error cuadr\u00e1tico en lugar del error absoluto para \nel caso de clases num\u00e9ricas  cuando se activa la opci\u00f3n \ncrossValidate . \nwindowSize (0) Si es 0 el n\u00famero de ejemplos de entrenamiento es \nilimitado. Si es mayor que 0, \u00fanicamente se almacenan los \nn \u00faltimos ejemplos de  entrenamiento, siendo n el n\u00famero \nque se ha especificado. \ndebug (False) Muestra el proceso de  construcci\u00f3n del clasificador. \nnoNormalization \n(False) No normaliza los atributos. \n \nEl algoritmo implementado en la herra mienta WEKA consiste en crear el \nclasificador a partir de los ejempl os de entrenamiento, simplemente \nalmacenando todas las instanc ias disponibles (a menos que se restrinja con la \nopci\u00f3n windowSize ). Posteriormente, se clasif icar\u00e1n los ejemplos de test a \npartir del clasificador gener ado, bien con el n\u00famero de vecinos especificados o \ncomprobando el mejor k si se activa la opci\u00f3n crossValidate . En cuanto a los \ntipos de datos permitidos y las propiedades  de la implementaci\u00f3n, estos son: \n\u2022 Admite atributos num\u00e9ricos y simb\u00f3licos. \n\u2022 Admite clase num\u00e9rica y simb\u00f3lica. Si  la clase es num\u00e9rica se calcular\u00e1 \nla media de los valores de la clase para los k vecinos m\u00e1s cercanos. \n\u2022 Permite dar peso a cada ejemplo. \n\u2022 El proceso de hold-one-out  consiste en, para cada k entre 1 y el valor \nconfigurado en KNN (ver t abla 2.8), calcular el error en la clasificaci\u00f3n de \nlos ejemplos de entrenamiento. Se escoge el k con un menor error \nobtenido. El error cometido para cada k se calcula como el error medio \nabsoluto o el cuadr\u00e1tico (ver tabla 2. 8) si se trata de una clase num\u00e9rica. \nEl c\u00e1lculo de estos dos errores se  puede ver en las ecuaciones 2.33 y \n2.34 respectivamente. Si la clase es simb\u00f3lica se tomar\u00e1  como error el \nn\u00famero de ejemplos fallados entre  el n\u00famero total de ejemplos. \nmyy\nMAEm\n1i i i\u2211=\u2212\n=\u02c6\n (2.33) \n( )\nmyy\nMSEm\n1i2\ni i \u2211=\u2212\n=\u02c6\n (2.34) \nEn las ecuaciones 2.33 y 2.34 yi es el valor de la clase para el ejemplo i \ne iy\u02c6 es el valor predicho por el modelo para el ejemplo i. El n\u00famero m \nser\u00e1 el n\u00famero de ejemplos. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 226 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n5.12.  K* en WEKA \nLa clase en la que se implementa el algoritmo K* en la herramienta WEKA es \nweka.classifers.kstar.KStar.java . Las opciones que permite este algoritmo son \nlas que se muestran en la tabla 2.9. \nTabla 5.8: Opciones de configuraci\u00f3n para el algoritmo K* en WEKA. \nOpci\u00f3n Descripci\u00f3n \nentropicAutoBlend \n(False) Si se activa esta opci\u00f3n se calcula el valor de los \npar\u00e1metros x0 (o s) bas\u00e1ndose en la entrop\u00eda en lugar del \npar\u00e1metro de mezclado. \nglobalBlend (20) Par\u00e1metro de mezclad o, expresado en tanto por ciento. \nmissingMode \n(Average column entropy curves) Define c\u00f3mo se tratan los valores desconocidos en los \nejemplos de entrenamiento: las opciones posibles son \nIgnore the Instance with missing value  (no se tienen en \ncuenta los ejemplos con atributos desconocidos), Treat \nmissing value as maximally different (diferencia igual al del \nvecino m\u00e1s lejano considerado), Normilize over the \nattributes  (se ignora el atributo desconocido) y Average \ncolumn entropy curves  (ver ecuaci\u00f3n 2.41). \n \nDado que los autores de la implementaci \u00f3n de este algoritmo en WEKA son los \nautores del propio algoritmo, dich a implementaci\u00f3n se corresponde \nperfectamente con lo visto anteriorment e. Simplemente son destacables los \nsiguientes puntos: \n\u2022 Admite atributos num\u00e9ricos y simb\u00f3licos, as\u00ed como pesos por cada \ninstancia. \n\u2022 Permite que la clase sea simb\u00f3lica o num\u00e9rica. En el caso de que se \ntrate de una clase num\u00e9rica se empl ear\u00e1 la ecuaci\u00f3n 2.45 para predecir \nel valor de un ejemplo de test. \n()()\n() \u2211\u2211\n===n\n1in\n1i\na|b*Pv(b)*a|b*Pav  (2.45) \nEn la ecuaci\u00f3n 2.45 v(i) es el valor (num\u00e9rico)  de la clase para el \nejemplo i, n el n\u00famero de ejemplos de entrenamiento, y P*(i|j)  la \nprobabilidad de transformaci\u00f3n del ejemplo j en el ejemplo i. \n\u2022 Proporciona cuatro modos de ac tuaci\u00f3n frente a p\u00e9rdidas en los \natributos en ejemplos  de entrenamiento.  \n\u2022 Para el c\u00e1lculo de los par\u00e1metros x0 y s permite basarse en el par\u00e1metro \nb o en el c\u00e1lculo de la entrop\u00eda. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 227 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Las ecuaciones para el c\u00e1lculos de P* y de la esfera de influencia  no son \nlas comentadas en la explicaci\u00f3n del  algoritmo, sino las empleadas en \nlos ejemplos de las figuras 2.20 y 2.21. \n \n5.13.  Redes de Neuronas en WEKA \nLa clase en la que se implementan las redes de neuronas en weka es \nweka.classifiers.neural.NeuralNetwork.java . Las opciones que permite \nconfigurar son las que se muestran en la tabla 2.10. \n \nTabla 5.9: Opciones de configuraci\u00f3n para la s redes de neuronas en WEKA. \nOpci\u00f3n Descripci\u00f3n \nmomentum (0.2) Factor que se utiliza en el proceso de actualizaci\u00f3n de \nlos pesos. Se multiplica este  par\u00e1metro por el peso en \nel momento actual (el que se va a actualizar) y se suma al peso actualizado. \nvalidationSetSize (0) Determina el  porcentaje de patrones que se \nemplear\u00e1n como test del sist ema. De esta forma, tras \ncada entrenamiento se va lidar\u00e1 el sistema, y \nterminar\u00e1 el proceso de entrenamiento si la validaci\u00f3n da un valor menor o igual a \n0, o si se super\u00f3 el \nn\u00famero de entrenamient os configurado. \nnominalToBinaryFilter \n(False) Transforma los atributos nominales en binarios. \nlearningRate (0.3) Raz\u00f3n de aprendizaje. Tiene valores entre 0 y 1. \nhiddenLayers (a) Determina el n\u00fam ero de neuronas ocultas. Sus \nposibles valores son: \u2018a\u2019=(atribs+clases)/2 , \u2018i\u2019=atribs , \n\u2018o\u2019=clases , \u2018t\u2019=atribs+clases . \nvalidationThreshold (20) Si el proceso de validaci\u00f3n arroja unos resultados en \ncuanto al error que empeoran durante el \nn veces \nconsecutivas (siendo nel valor de esta variable), se \ndetiene el aprendizaje. \nreset (True) Permite al sistema modificar la raz\u00f3n de aprendizaje \nautom\u00e1ticamente (la divide entre 2) y comenzar de \nnuevo el proceso de aprendiz aje si el proceso de \nentrenamiento no converge. \nGUI (False) Visualizaci\u00f3n de la re d de neuronas. Si se activa esta \nopci\u00f3n se puede modificar la red de neuronas, parar el proceso de entrenamiento en cualquier momento, \nmodificar par\u00e1metros como el de la raz\u00f3n de aprendizaje,... \nautoBuild (True) El sistema c onstruye autom\u00e1ticamente la red \nbas\u00e1ndose en las entradas, salidas y el par\u00e1metro \nhiddenLayers . Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 228 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda normalizeNumericClass \n(True) Normaliza los posibles valores de la clase si \u00e9sta es \nnum\u00e9rica, de forma que est\u00e9n entre \u20131 y 1. \ndecay (False) La raz\u00f3n de ganancia se modifica con el ciclo de \naprendizaje: \u03b1= \u03b1/n, donde nes el n\u00famero de ciclo de \naprendizaje actual. \ntrainingTime (500) N\u00famero total de ciclos de aprendizaje. \nnormalizeAttributes (True) Normaliza los atributos num\u00e9ricos para que est\u00e9n \nentre \n\u20131 y 1. \nrandomSeed (0) Semilla para generar los n\u00fameros aleatorios que \ninicializar\u00e1n los par\u00e1metros de la red. \n \nLa implementaci\u00f3n de redes de neuronas que se realiza en la  herramienta se \nci\u00f1e al algoritmo de retropropagaci\u00f3n.  \n Algunas caracter\u00edsticas que se pueden destacar de esta implementaci\u00f3n son: \n\u2022 Se admiten atributos num\u00e9ricos y simb\u00f3licos. \n\u2022 Se admiten clases num\u00e9ricas (predi cci\u00f3n) y simb\u00f3licas (clasificaci\u00f3n). \n\u2022 Permite la generaci\u00f3n manual de redes que no se ci\u00f1an a la arquitectura \nmostrada anteriormente, por ejem plo, eliminando conexiones de \nneuronas de una capa con la siguiente. \n\u2022 Como funci\u00f3n sigmoidal se utiliza la restringida entre 0 y 1 (ver ecuaci\u00f3n \n2.48). \n\u2022 Los ejemplos admiten pesos: Cuando se aprende con dicho ejemplo se \nmultiplica la raz\u00f3n de apr endizaje por el peso del ejemplo. Todo esto \nantes de dividir la raz\u00f3n de apr endizaje por el n\u00famero de ciclo de \naprendizaje si se activa decay . \n \n5.14.  Regresi\u00f3n Lineal en WEKA \nEs en la clase weka.classifers.LinearRegression.java  en la que se implementa \nla regresi\u00f3n lineal m\u00faltiple. Las opcio nes que permite este algoritmo son las \nque se muestran en la tabla 2.11. \nTabla 5.10: Opciones de configuraci\u00f3n para el al goritmo de regresi\u00f3n lineal en WEKA. \nOpci\u00f3n Descripci\u00f3n \nAttributeSeleccionMethod \n(M5 method) M\u00e9todo de selecci\u00f3n del at ributo a elim inar de la \nregresi\u00f3n. Las opciones son M5 Method , Greedy y \nNone . \ndebug (False) Muestra el proceso de  construcci\u00f3n del clasificador. \n Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 229 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda La regresi\u00f3n lineal se cons truye tal y como se com ent\u00f3 anteriormente. Algunas \npropiedades de la im plementaci\u00f3n son: \n\u2022 Admite atributos num\u00e9ricos y nominales. Los nominales con k valores se \nconvierten en k-1 atributos binarios. \n\u2022 La clase debe ser num\u00e9rica. \n\u2022 Se permite pesar cada ejemplo. \nEn cuanto al proceso en s\u00ed, si bien se c onstruye la regresi\u00f3n como se coment\u00f3 \nanteriormente, se sigue un proceso m\u00e1s comp licado para eliminar los atributos.  \nEl algoritmo completo ser\u00eda el siguiente:  \n1. Construir regresi\u00f3n para los atribut os seleccionados (en principio todos). \n2. Comprobar la ecuaci\u00f3n 2.64 sobre todos los atributos.  \ncii\niSSbc=  (2.64) \nEn la ecuaci\u00f3n 2.64, Sc es la desviaci\u00f3n t\u00edpica de la clase. Se elimina de \nla regresi\u00f3n el atributo con mayo r valor si cumple la condici\u00f3n ci>1.5. Si \nse elimin\u00f3 alguno, volver a 1. \n3. Calcular el error cuadr\u00e1tico m edio (ecuaci\u00f3n 2.63) y el factor Akaike  tal y \ncomo se define en la ecuaci\u00f3n 2.65. \n( )2ppm AIC +\u2212 =  (2.65) \nEn la ecuaci\u00f3n 2.65 m es el n\u00famero de ejem plos de entrenamiento, p el \nn\u00famero de atributos que fo rman parte de la regresi\u00f3n al llegar a este \npunto.  \n4. Escoger un atributo: \na. Si el m\u00e9todo es Greedy , se generan regresiones lineales en las \nque se elimina un atributo distinto en cada una de ellas, y se escoge la regresi\u00f3n con menor  error medio absoluto.  \nb. Si el m\u00e9todo es \nM5, se calcula el valor de ci (ecuaci\u00f3n 2.64) para \ntodos los atributos y se escoge el menor. Se genera la regresi\u00f3n \nsin el atributo i y se calcula la regresi\u00f3 n lineal sin dicho atributo. \nSe calcula el error medio absolut o de la nueva regresi\u00f3n lineal. \nc. Si el m\u00e9todo es None , se finaliza el proceso. \n5. Mejorar regresi\u00f3n. Se  calcula el nuevo factor Akaike  con la nueva \nregresi\u00f3n como es mues tra en la ecuaci\u00f3n 2.66. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 230 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda () 2p pmMSEMSEAICcc+ \u2212 =  (2.66) \nEn la ecuaci\u00f3n 2.66 MSE c es el error cuadr\u00e1tico medio absoluto de la \nnueva regresi\u00f3n lineal y pc el n\u00famero de atributos de la misma. Mientras, \nMSE  es el valor obtenido en el punto 3 y p el n\u00famero de par\u00e1metros al \nllegar al mismo. Si el valor nuevo de AIC es menor que el anterior, se \nactualiza \u00e9ste como nuevo y se m antiene la nueva regresi\u00f3n lineal, \nvolviendo a intentar mejorarla (volver a 4). Si no es as\u00ed, se finaliza el \nproceso. \n \n5.15.  Regresi\u00f3n Lineal Ponderada Localmente en \nWEKA \nEl algoritmo se implementa en la clase weka.classifers.LWR.java . Las opciones \nque permite configurar son las que se muestran en la tabla 2.12. \nTabla 5.11: Opciones de configuraci\u00f3n para el algoritmo LWR en WEKA. \nOpci\u00f3n Descripci\u00f3n \nweightingKernel \n(0) Indica cu\u00e1l va a ser el m\u00e9todo para ponderar a los ejemplos \nde entrenamiento: 0, lineal; 1,  inverso; 2, gaussiano. \ndebug (False) Muestra el proceso de construcci\u00f3n del clasificador y \nvalidaci\u00f3n de los ejemplos de test. \nKNN (5) N\u00famero de vecinos que se tendr\u00e1n en cuenta para ser \nponderados y calcular la regresi\u00f3n lineal. Si bien el valor por \ndefecto es 5, si no se modifica  o confirma se utilizan todos los \nvecinos. \n \nEn primer lugar, las ecuac iones que se emplean en los m\u00e9todos para ponderar \na los ejemplos de entrenamiento son: para el m\u00e9todo inverso, la ecuaci\u00f3n 2.67; \npara el m\u00e9todo lineal, la ecuaci\u00f3n 2. 68; y para el m\u00e9t odo gaussiano, la \necuaci\u00f3n 2.69. \n( )0 ,d 1.0001max\u03c9ij i \u2212 =  (2.68) \nij ijd*d\nie\u03c9\u2212=  (2.69) \nEl proceso que sigue el algoritmo es el que se coment\u00f3 anter iormente. Algunas \npropiedades que hay que mencionar s obre la implementaci\u00f3n son: \n\u2022 Se admiten atributos simb\u00f3licos y num\u00e9ricos. \n\u2022 Se admiten ejemplos ya pesados, en cuyo caso, el peso obtenido del \nproceso explicado anterio rmente se multiplica por el peso del ejemplo. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 231 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 Se toma como par\u00e1metro de suavizad o la siguiente distancia mayor al \ndel k-\u00e9simo ejemplo m\u00e1s pr\u00f3ximo. \n\u2022 Para la generaci\u00f3n de la regresi\u00f3n lineal se emplea la clase explicada en \nel punto anterior (ver punto 2.3.1. 1), con los par\u00e1metros por defecto y \ncon los ejemplos pesados. \n \n5.16.  M5 en WEKA \nLa clase en la que se implementa el al goritmo M5 en la herramienta WEKA es \nweka.classifers.m5.M5Prime.java . Las opciones que permite este algoritmo son \nlas que se muestran en la tabla 2.13. \nTabla 5.12: Opciones de configuraci\u00f3n para el algoritmo M5 en WEKA. \nOpci\u00f3n Descripci\u00f3n \nModelType \n(ModelTree) Permite seleccionar como model o a construir entre un \u00e1rbol \nde modelos, un \u00e1rbol de regres i\u00f3n o una regres i\u00f3n lineal. \nuseUnsmoothed \n(False) Indica si se realizar\u00e1 el proceso de suavizado ( False ) o si no \nse realizar\u00e1 ( True). \npruningFactor (2.0) Permite definir el factor de poda. \nverbosity (0) Sus posibles valores son 0, 1 y 2, y permite definir las \nestad\u00edsticas que se most rar\u00e1n con el modelo. \n \n En cuanto a la implementaci\u00f3n conc reta que se lleva a cabo en esta \nherramienta del algoritmo M5, cabe destacar lo siguiente: \n\u2022 Admite atributos simb\u00f3licos y num\u00e9ricos; la clase debe ser, por \nsupuesto, num\u00e9rica. \n\u2022 Para la generaci\u00f3n de las regresi ones lineales se emplea la clase que \nimplementa la regresi\u00f3n lineal m\u00fa ltiple en WEKA (punto 2.3.1.1). \n\u2022 El n\u00famero m\u00ednimo de ejemplos que deben clasificarse a trav\u00e9s de un \nnodo para seguir dividiendo dicho n odo, definido en la constante \nSPLIT_NUM  es 3.5, mientras la otra condici\u00f3n de parada, que es la \ndesviaci\u00f3n t\u00edpica de las clases en el nodo respecto a la desviaci\u00f3n t\u00edpica de todas las clases del conjunto de entrenamiento, est\u00e1 fijada en \n0.05. \n\u2022 En realidad no se intenta minimizar el SDR  tal y como se defini\u00f3 en la \necuaci\u00f3n 2.71, sino que se  intenta minimizar la ecuaci\u00f3n 2.75, que se \nmuestra a continuaci\u00f3n. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 232 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda 5 2\nDD 5 2\nII 5 2SnnSnnS SDR \u2212 \u2212 =  (2.75) \nEn la ecuaci\u00f3n 2.75 n es el n\u00famero total de ejemplos, nI y nD el n\u00famero \nde ejemplos del grupo izquierdo y derecho respectivamente; y S, S2\nI y \nS2\nD la varianza del conjunto completo , del grupo izquierdo y del grupo \nderecho respectivamente, defini\u00e9ndose la varianza como se muestra en \nla ecuaci\u00f3n 2.76. \n( )\nnnx\nx\nS2n\n1i i n\n1i2\ni2\u2211\u2211=\n=\u2212\n=  (2.76) \nEn la ecuaci\u00f3n 2.76 n es el n\u00famero de ejemplos y xi el valor de la clase \npara el ejemplo i. \n\u2022 El c\u00e1lculo del error de estimaci \u00f3n para un nodo dete rminado, mostrado \nen la ecuaci\u00f3n 2.73, se modifica ligeramente hasta llegar al que se \nmuestra en la ecuaci\u00f3n 2.77. \n()\nn)yy yy\nv-npvne(I)2\nIii i\nIi2\ni i \uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\u2212 \u2212 \u2212\n\u00d7+=\u2211 \u2211\n\u2208 \u2208\u02c6 \u02c6\n (2.77) \nEn la ecuaci\u00f3n 2.77 p es el factor de podado que  es configurable y, \ncomo se ve\u00eda en la tabla 2.13, por defecto es 2. \n\u2022 Por \u00faltimo, la constante k empleada en el modelo de suavizado \n(ecuaci\u00f3n 2.70) se co nfigura con el valor 15. \nPor lo dem\u00e1s la implement aci\u00f3n que se lleva a cabo respeta en todo momento \nel algoritmo mostrado en la figura 2.26. \n  \n5.17.  Kernel Density en WEKA \nEs en la clase weka.classifiers.KernelDensity  en la que se implementa eel \nalgoritmo de densidad de n\u00facleo. No se  puede configurar di cho algoritmo con \nninguna propiedad. Adem\u00e1s, s\u00f3lo se admit en clases simb\u00f3licas, a pesar de que \nlos algoritmos de densidad de n\u00facleo, como se com ent\u00f3 anteriormente nacen \ncomo un m\u00e9todo de estimaci\u00f3n no para m\u00e9trica (clases num\u00e9ricas). A \ncontinuaci\u00f3n se muestran las principales propiedades de la implementaci\u00f3n as\u00ed \ncomo los atributos y clases permitidas: Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 233 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda \u2022 En cuanto a los atributos, pueden ser num\u00e9ricos y simb\u00f3licos. \n\u2022 La clase debe ser simb\u00f3lica. \n\u2022 Como funci\u00f3n n\u00facleo [kernel] se  emplea la distribuci\u00f3n normal o \ngaussiana (ecuaci\u00f3n 2.83) normalizada, esto es, con media 0 y \ndesviaci\u00f3n t\u00edpica 1. \n\u2022 Como tama\u00f1o de ventana se emplea n1h= , siendo n el n\u00famero de \nejemplos de entrenamiento. \n\u2022 Para clasificar el ejemplo Ai, para cada ejemplo de entrenamiento Aj se \ncalcula la ecuaci\u00f3n 2.92. \nn)n)A, K(dist(A)A ,V(Aj i j i \u00d7 \u00d7 =  (2.92) \nEn la ecuaci\u00f3n 2.92, dist es la distancia entre el ejemplo de test y uno de \nlos ejemplos de entrenam iento, definida tal y co mo se describe en la \nfigura 2.19. El resultado de esta ecuaci\u00f3n para el par Ai-Aj se sumar\u00e1 al \nresto de resultados obtenidos para la  clase a la que pertenezca el \nejemplo Aj. \nEl pseudoc\u00f3digo del algorit mo implementado por WEKA es el que se muestra \nen la figura 2.30. \nKernel Density (ejemplo) { \n Para cada ejemplo de entrenamiento (E) Hacer \n  prob = 1 \n  c = clase de E \n  Para cada atributo de E (A) Hacer \n   temp = V(ejemplo, A) \n   Si temp < LB Entonces \n    prob = prob * LB \n   Si no \n    prob = prob * temp \n  probs[c] = probs[c] + prob \n Normalizar(probs) \n} \nFigura 5.2: Pseudoc\u00f3digo del algoritmo Kernel Density. \nLa clase que obtenga una mayor probabilid ad ser\u00e1 la que resulte ganadora, y la \nque se asignar\u00e1 al ejemplo de test. En cuanto a la constante LB, se define en la \necuaci\u00f3n 2.93. \n1-t1min LB=  (2.93) \nEn la ecuaci\u00f3n 2.93 min es el n\u00famero m\u00edni mo almacenable por un double  en \nJava y t el n\u00famero de atributos de los ej emplos (incluida la clase). \n  Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 234 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n5.18.  k-means en WEKA \nEl algoritmo de k-medias se encuentra implementado en la clase \nweka.clusterers.SimpleKMeans.java . Las opciones de configuraci\u00f3n de que \ndisponen son las que vemos en la tabla 2.14. \nTabla 5.13: Opciones de configuraci\u00f3n para el  algoritmo k-medias en WEKA. \nOpci\u00f3n Descripci\u00f3n \nnumClusters (2) N\u00famero de clusters. \nseed (10) Semilla a partir de la cu\u00e1l  se genera el n\u00famero aleatorio \npara inicializar los centros de los clusters. \n \nEl algoritmo es exactamente el mi smo que el descrito anteriormente. A \ncontinuaci\u00f3n se enumeran los tipos de datos que admite y las propiedades de \nla implementaci\u00f3n: \n\u2022 Admite atributos simb\u00f3licos y num\u00e9ricos. \n\u2022 Para obtener los centroides inicia les se emplea un n\u00famero aleatorio \nobtenido a partir de la semilla empleada. Los k ejemplos \ncorrespondientes a los k n\u00fameros enteros siguient es al n\u00famero aleatorio \nobtenido ser\u00e1n los que conformen dichos centroides. \n\u2022 En cuanto a la medida de similaridad,  se emplea el mismo algoritmo que \nel que ve\u00edamos en el algoritmo KNN (figura 2.19). \n\u2022 No se estandarizan lo s argumentos, sino que se normalizan (ecuaci\u00f3n \n2.96). \nl ll il\nmin Maxminx\n\u2212\u2212 (2.96) \nEn la ecuaci\u00f3n 2.96, xif ser\u00e1 el valor i del atributo f, siendo min f el m\u00ednimo \nvalor del atributo f y Max f el m\u00e1ximo. \n  \n5.19.  COBWEB en WEKA \nEl algoritmo de COBWEB se enc uentra implementado en la clase \nweka.clusterers.Cobweb.java . Las opciones de configur aci\u00f3n de que disponen \nson las que vemos en la tabla 2.15. \nTabla 5.14: Opciones de configuraci\u00f3n para el algoritmo COBWEB en WEKA. \nOpci\u00f3n Descripci\u00f3n Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 235 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda acuity (100) Indica la m\u00ednima  varianza permitida en un cluster \ncutoff (0) Factor de poda. Indica la mejora en utilidad m\u00ednima por una \nsubdivisi\u00f3n para que se permita llevar a cabo. \n \nLa implementaci\u00f3n de COBWEB en WEKA es similar al algoritmo explicado \nanteriormente. Algunas caracter\u00edsti cas de esta implementaci\u00f3n son: \n\u2022 Se permiten atributos num\u00e9ricos y simb\u00f3licos. \n\u2022 La semilla para obtener n\u00fameros aleatorios es fija e igual a 42. \n\u2022 Permite pesos asociados a cada ejemplo. \n\u2022 Realmente el valor de cutoff es ()\u03c021 0.01 \u00d7. \n\u2022 En el caso de que el ejemplo que se desea clas ificar genere, en un nodo \ndeterminado, un CU menor al cutoff , se eliminan los hijos del nodo \n(poda). \n \n5.20.  EM en WEKA \nEl algoritmo EM se encuentra  implementado en la clase \nweka.clusterers.EM.java . Las opciones de configur aci\u00f3n de que disponen son \nlas que vemos en la tabla 2.16. \nTabla 5.15: Opciones de configuraci\u00f3n para el algoritmo EM en WEKA. \nOpci\u00f3n Descripci\u00f3n \nnumClusters (-1) N\u00famero de clusters. Si es n\u00famero es \u20131 el algoritmo \ndeterminar\u00e1 autom\u00e1ticamente el n\u00famero de clusters. \nmaxIteration (100) N\u00famero m\u00e1ximo de it eraciones del algoritmo si esto no \nconvergi\u00f3 antes. \ndebug (False) Muestra informaci\u00f3n sobre el proceso de clustering. \nseed (100) Semilla a partir de la c u\u00e1l se generan los n\u00famero aleatorios \ndel algoritmo. \nminStdDev (1e-6) Desviaci\u00f3n t\u00edpica m\u00ednima admis ible en las distribuciones de \ndensidad. \n \nEn primer lugar, si no se especifica el  n\u00famero de clusters, el algoritmo realiza \nun primer proceso consist ente en obtener el n\u00famero \u00f3ptimo de clusters. Se \nrealiza mediante validaci\u00f3n cruzada con 10 conjuntos [folders]. Se va \naumentando el n\u00famero de clusters hasta que se aumenta y empeora el \nresultado. Se ejecuta el algoritmo en diez ocasiones, c ada una de ellas con \nnueve conjuntos de entrenam iento, sobre los que se  ejecuta EM con los \npar\u00e1metros escogidos y posteriormente se valida el sistema sobre el conjunto \nde test, obteniendo como medida la verosimilitud  sobre dicho conjunto. Se \ncalcula la media de las diez medidas obtenidas y se toma como base para determinar si se contin\u00faa o no aumentando el n\u00famero de clusters. Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 236 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Una vez seleccionado el n\u00famer o \u00f3ptimo de clusters, se procede a ejecutar el \nalgoritmo EM sobre el co njunto total de entrenamient o hasta un m\u00e1ximo de \niteraciones que se configur\u00f3 previamente (ver tabla 2.16)  si es que el algoritmo \nno converge previamente.  \nEn cuanto a los tipos de atributos con admite el algoritmo y algunas \npropiedades interesantes, \u00e9stas son: \n\u2022 En cuanto a los atributos, \u00e9stos pueden ser num\u00e9ricos o simb\u00f3licos. \n\u2022 Se entiende que se converge si en la siguiente iteraci\u00f3n la verosimilitud \naumenta en menos de 1e-6. \n\u2022 No tiene en cuenta posibles co rrelaciones entre atributos. \n \n5.21.  Asociaci\u00f3n A Priori en WEKA \nLa clase en la que se impl ementa el algoritmo de as ociaci\u00f3n A Priori es \nweka.associations.Apriori.java . Las opciones que permite configurar son las \nque se muestran en la tabla 2.17. \nTabla 5.16: Opciones de configuraci\u00f3n para el algori tmo de asociaci\u00f3n A Priori en WEKA. \nOpci\u00f3n Descripci\u00f3n \nnumRules (10) N\u00famero de reglas requerido. \nmetricType \n(Confidence) Tipo de m\u00e9trica por la que ordenar las reglas. Las \nopciones son Confidence (confianza, ecuaci\u00f3n 2.106), Lift (ecuaci\u00f3n 2.107), Leverage (ecuaci\u00f3n 2.108) y Conviction (ecuaci\u00f3n 2.109). \nminMetric M\u00ednimo valor de la m\u00e9trica empleada. Su valor por \ndefecto depende del tipo de m\u00e9trica empleada: 0.9\npara Confidence, 1.1 para Lift y Conviction y 0.1 para \nLeverage. \ndelta (0.05) Constante por la que va decreciendo el soporte en \ncada iteraci\u00f3n del algoritmo. \nupperBoundMinSupport (1.0) M\u00e1ximo valor del soporte de los \nitem-sets . Si los item-\nsets tienen un soporte mayor, no se les toma en consideraci\u00f3n. \nlowerBoundMinSupport (0.1) M\u00ednimo valor del soporte de los \nitem-sets . \nsignificanceLevel (-1.0) Si se emplea, las reglas se validan para comprobar si \nsu correlaci\u00f3n es estad\u00edstic amente significativa (del \nnivel requerido) mediante el test 2\u03c7. En este caso, la \nm\u00e9trica a utilizar es Confidence. \nremoveAllMissingsCols \n(False) Si se activa, no se toman en consideraci\u00f3n los \natributos con todos los valores perdidos. \n-I (s\u00f3lo modo texto)  Si se activa, se muestran los itemsets encontrados. \n Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 237 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda En primer lugar, el algoritmo que implementa la herramienta WEKA es \nligeramente distinto al explicado anteriorm ente. En la figura 2.36 se muestra el \nalgoritmo concreto. \nApriori (ejemplos, MS, mS) { /* MS: M\u00e1x. soporte;  mS: M\u00edn. soporte */ \n S = MS-delta \n Mientras No Fin \n  Generar ItemSets en rango (MS, S) \n  GenerarReglas (ItemSets) \n  MS = MS-delta \n  S = S-delta \n  Si suficientes reglas O S menor que mS Entonces \n   Fin \n} \n \nGenerarReglas (ItemSets) { \n Para cada ItemSet \n  Generar posibles reglas del ItemSet \n  Eliminar reglas seg\u00fan la m\u00e9trica \n} \nFigura 5.3: Algoritmo A Priori en WEKA. \nAs\u00ed, el algoritmo no obtiene de una vez todos los item-sets  entre los valores \nm\u00e1ximo y m\u00ednimo permitido, sino que se  va iterando y cada ve z se obtienen los \nde un rango determinado, que ser\u00e1 de tama\u00f1o delta  (ver tabla 2.17). \nAdem\u00e1s, el algoritmo permite seleccionar las reglas atendien do a diferentes \nm\u00e9tricas.  Adem\u00e1s de la confianza (e cuaci\u00f3n 2.106), se puede optar por una de \nlas siguientes tres m\u00e9tricas. \n\u2022 Lift: Indica cu\u00e1ndo una regla es me jor prediciendo el  resultado que \nasumiendo el resultado de forma aleat oria. Si el resultado es mayor que \nuno, la regla es buena, pero si es menor que uno, es peor que elegir un \nresultado aleatorio. Se mues tra en la ecuaci\u00f3n 2.107. \n()( )\n()BPBA confianzaBAlift\u21d2= \u21d2  (2.107) \n\u2022 Leverage:  Esta medida de una regla de as ociaci\u00f3n indica la proporci\u00f3n \nde ejemplos adicionales cubiertos por  dicha regla (tanto por la parte \nizquierda como por la derecha) sobre los cubiertos por cada parte si \nfueran independientes. Se mues tra en la ecuaci\u00f3n 2.108. \n() ( )()()BP*APBAPBA leverage \u2212 \u2229 = \u21d2  (2.108) \n\u2022 Convicci\u00f3n:  Es una medida de implicaci\u00f3n.  Es direccional y obtiene su \nm\u00e1ximo valor (infinito) si la implicac i\u00f3n es perfecta, esto es, si siempre \nque A ocurre sucede tambi\u00e9n B. Se muestra en la ecuaci\u00f3n 2.109. \n()()()\n() B!APB!P*APB A convicci\u00f3n\u2229= \u21d2  (2.109) Cap\u00edtulo 5  Implementaci\u00f3n  de las T\u00e9cnicas  \n  de An\u00e1lisis de Datos en Weka \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 238 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda Por \u00faltimo, cabe destacar que esta implementaci\u00f3n permite \u00fanicamente \natributos simb\u00f3licos. Adem\u00e1s, para mejora r la eficiencia del algoritmo en la \nb\u00fasqueda de item-sets , elimina todos los atributos que tengan sus valores \ndesconocidos en todos los ejemplos. \n Cap\u00edtulo 6  Ejemplos sobre casos de estudio \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 239 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n  \nCap\u00edtulo 6. Ejemplos sobre \ncasos de estudio \n \n        Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 240 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n  \nBibliograf\u00eda \n   \n[ACO02] Acosta Franco, Javier. \"\nAplicaci\u00f3n de los Sistemas Clasificadores \ntradicionales al an\u00e1lisis de dat os. Adquisici\u00f3n autom\u00e1tica de \nreglas \". Proyecto Fin de Carrera, Universidad Carlos III de Madrid, \n2002. \n[AGR96] A. Agresti. An Introduction to Categorical Data Analysis. New \nYork: John Wiley & Sons, 1996. \n[AIS93b] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules \nbetween sets of item s in large databases . In Proc. 1993 \nACM-SIGMOD Int. Conf. Mana gement of Data (SIGMOD'93) , \npages 207-216, Washington, DC, May 1993. \n[AKA73] Akaike, A. (1973). \" Information theory and an extension of the \nmaximum likelihood principle \" In Petrov, B. N., and Saki, \nF. L.(eds.), Second International Sy mposium of Information \nTheory . Budapest. \n[AS94a] R. Agrawal and R. Srikant. \u201c Fast algorithms for mining association \nrules in large databases \u201d. In Research Report RJ 9839, IBM \nAlmaden Research Center, San Jose, CA, June 1994. \n[AS94b] R. Agrawal and R. Srikant. \u201c Fast algorithms for mining association \nrules \u201d. In Proc. 19941nt. Conf Very LargeData Bases (VLDB'94), \npages 487-499, Santiago, Chile, Sept.1994. \n[BERR97] M.Berry, and G.Linoff, , \" Data Mining  Techniques for Marketing, \nSales, and Customer Support \" John Wiley, NY, 1997. \n[BMS97] S. Brin, R. Motwani, and C. Silverstein. \u201c Beyond market basket: \nGeneralizing association rules to correlations \u201d. In Proc. \n1997ACM-SIGMOD I of Data (SIGMOD'97), pages 265-276, \nTucson, AZ, May 1997.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 241 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [BRIS96] G. Briscoe, and T. Caelli, \u201c A Compendiu m of Machine Learning . \nVol. 1: Symbolic Machine Learning.\u201d Ablex Publishing Corporation, \nNew Jersey, 1996. \n[CEN87] J. Cendrowska (1987). \u201c PRISM: An algorithm for inducing modular \nrules \u201d. International Journal of Man-Machine Studies. Vol.27, \nNo.4, pp.349-370. \n[CHSVZ] P. Cabena, P. Hadjin ian, R. Stadler, J. Verhees, A.Zanasi, \nDiscovering Data Mining From  concept to implementation . \nPrentice Hall 1997. \n[CHY96] M.S. Chen, J. Han, and P. S. Yu. \u201c Data mining: An overview from \na database perspective \u201d. IEEE Trans. Knowledge and Data \nEngineering, 8:866-883, 1996. \n[CLTR95] John, G. Cleary and Leonard,  E. Trigg (1995) \" K*: An Instance- \nbased Learner Using an En tropic Distance Measure \", Proceedings \nof the 12th International C onference on Machine learning , pp. 108-\n114. \n[CN89] P. Clark and T. Niblett. \u201c The CN2 induction algorithm. Machine \nLearning\u201d. 3:261-283, 1989. \n[CODD70] E. F. Codd, \" A Relational Model of Data for Large Shared Data \nBanks ,\" Communications of the ACM, Vol. 13, 1970. \n[CR95] Y.Chauvin and D. Rumelhart. \u201c Backpropagation: Theory, \nArchitectures, and Applications \u201d. Hillsdale, NJ: Lawrence Erlbaurn \nAssoc., 1995. \n[DARP98] Workshop on Knowledge Discovery in Databases , Defense \nAdvanced Research Projects Agency, Pittsburgh, PA, June 1998. \n[DEA97] Deaton, A. (1997): \u201c The Analysis of Household Surveys. A \nMicroeconometric Approach to De velopment Policy. The World \nBank \u201d. The Johns Hopkins University Press. \n[DECI] Decision Support Journal , Elsevier/North Holland Publications. \n[DEGR86] T. DeGroot, \" Probability and Statistics ,\" Addison Wesley, MA, \n1986. \n[DEV95] J. L. Devore. \u201c Probability and Statistics for Engineering and the \nSciences \u201d. 4th ed. New York: Duxbury Press, 1995. \n[DFL96] DiNardo, J., Fortin, N. and Lemieux, T. (1996): \u201c Labor Market \nInstitutions and the Distri bution of Wages, 1973-1992: a \nSemiparametric Appr oach. Econometrica \u201d, Vol. 64, No.5. \nSeptember.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 242 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [DH73] R. Duda and P. Hart. \u201c Pattern Classification and Scene Analysis \u201d. \nNew York: John Wiley & Sons, 1973. \n[DOB90] A. J. Dobson. \u201c An Introduction to Generalized Linear Models \u201d. \nNew York: Chapman and Hall, 1990. \n[FAYY96] U. Fayyad, et al. \" Advanced in Knowledge Discovery and Data \nMining ,\" MIT Press, MA, 1996. \n[FIS87] D. Fisher, \u201c Improving inference through conceptual clustering \u201d. In \nProc. 1987 AAAI Conf., pages 461-465, Seattle, WA, July 1987. \n[FRWI98] Eibe Frank and Ian H. Witten (1998). \u201c Generating Ach\u00farate Rule \nSets Without Global Optimization. \u201d In Shavlik, J., ed., Machine \nLearning: Proceedings of  the Fifteenth Inter national Conference , \nMorgan Kaufmann Publishers, San Francisco, CA. \n[FU94] L. Fu, \u201c Neural Networks in Computer Intelligence \u201d, New York, \nMcGraw Hill, 1994 \n[FUR87] Furnas, G. W. et al. \u201c The vocabulary proble m in human system \ncommunication \u201d. Communications of the ACM, 30, n\u00ba 11, Nov. \n1987. \n[HALI94] H\u00e4rdle, W. and Linton, O. (1994): \u201c Applied Nonparametric \nMethods. Handbook of Econometrics \u201d, Volume IV, Chapter 38. \nElsevier Science. \n[HH96] Hearst, M.; Hirsh, H. (eds.)  Papers from the AAAI Spring \nSymposium on Machine Learning in information Access, Stanford, \nMarch 25-27, 1996. http://www.parc.xerox.com/istl/projects/mlia  \n[HMM86] J. Hong, I. Mozetic, and R. S. Michalski. \u201c AQ15: Incremental \nlearning of attribute-based de scriptions from examples, the \nmethod and user's guide \u201d. In Report ISG 85-5, \nUIUCDCS-F-86-949, Departm ent of Computer Science, University \nof Illinois at Ur bana-Champaign, 1986. \n[HOL93] R.C. Holte (1993). \u201c Very simple classificati on rules perform well on \nmost commonly used datasets \u201d. Machine Learning,  Vol. 11, pp. \n63-91. \n[IEEE89] \"Parallel Architectures for Databases ,\" IEEE Tutorial, 1989 (Ed: A. \nHurson et al.). \n[JAM85]  M. James. \u201c Classification Algorithms \u201d. New York: John Wiley & \nSons, 1985. \n[JOH97] G. H. John. \u201c Enhancements to the Data Mining Process \u201d. Ph.D. \nThesis, Computer Science D ept., Stanford University, 1997.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 243 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [KB00] Kosala, R.; Blockeel, H. \u201c Web Mining Research: A Survey \u201d ACM \nSIGKDD Explorations, Newsletter of the ACM Special Interest \nGroup on Knowledge Discovery and Data Mining, June 2000, Vol. \n2, n\u00ba 1, pp. 1-15 \n[KODR90] Kodratoff, Y. and Michalski, R.S., \u201c Machine Learning and Artificial \nInteligence Approach, Vol. 3 \u201d, San Mateo, CA: Morgan Kaufmann, \n1990 \n[LAN96] P. Langley. \u201c Elements of Machine Learning \u201d. San Francisco: \nMorgan Kaufmann, 1996. \n[LOP01] A. L\u00f3pez Cilleros, \u201c Modelizaci\u00f3n del Comportamiento Humano \npara un Agente de la Robocup m ediante Aprendizaje Autom\u00e1tico \u201d. \nProyecto Fin de Carrera, Universidad Carlos III de Madrid, 2001. \n[MAC67] MacQueen. \u201c Some methods for classification and analysis of \nmultivariate observations \u201d. Proc. 5th Berkeley Symp. Math. Statisi. \nProb., 1:281-297, 1967. \n[MBK98] R. S. Michalski, I. Brakto, and M. Kubat. Machine Learning and \nData Mining. Methods and Applications. New York: John Wiley & \nSons, 1998. \n[MIT97] T. Mitchell, \u201c Machine Learning ,\u201d McGraw Hill, NY, 1997. \n[MM95] J. Major and J. Mangano. \u201c Selecting among rules induced from a \nhurricane database . Journal of Intelligent Information Systems\u201d , \n4:39-52, 1995. \n[MORE98a] D. Morey, \" Knowledge Management Architecture \" Handbook of \nData Management, Auerbach Publ ications, NY, 1998 (Ed: B. \nThuraisingham). \n[MS83] R.S. Michalski, and R.E. Stepp, \u201c Learning from observation: \nConceptual clustering \u201d. In R.S. Michalski, J.G. Carbonell, and \nMitchell, T.M, editors, Machine Lear ning: An Artificial Intelligence \nApproach, Vol 1. San Mateo, CA: Morgan Kaufmann, 1983.  \n[PSF91] G. Piatesky-Shapiro and W.J. Frawley. Knowledge Discovery in \nDatabases . Cambridge, MA:AAA/MIT Press, 1991 \n[PTVF96] W. H. Press, S. A. Teukolos ky, W. T. Vetterling, and B. P \nFlannery. \u201c Numerical Recipes in C: The Art of Scientific \nComputing \u201d. Cambridge, UK: Cambridge University Press, 1996. \n[QUIN79] J.R.Quinlan, \u201d Discovering Rules from Large Collections of \nExamples \u201d, In Expert Systems in the Microelectronic Age, Michie, \nD. (Ed.), Edimburgo Univer sity Press, Edimburgo. 1979   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 244 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [QUIN86] J.R. Quinlan, \u201c Induction of Decision Trees (ID3 algorithm )\u201d, \nMachine Learning J., vol. 1, n\u00fam. 1, pp. 81-106. 1986 \n[QUIN87] J.R. Quinlan, \u201cSimplifying decision trees\u201d, International Journal of \nMan-Machine Studies, n\u00fam. 27, pp. 221-234. 1987 \n[QUIN93] J.R. Quinlan, \" C4.5: Programs for Machine Learnirig ,\" Morgan \nKaufmann, CA, 1993. \n[RHW86] D. E. Rumelhart, G. E. Hin ton, and R. J. Williams. \u201c Learning \ninternal representations by error propagation \u201d. In D. E. Rumelhart \nand J. L. McClelland, editors, Parallel Distributed Processing. \nCambridge, MA: MIT Press, 1986. \n[RIO02] R\u00edos Monta\u00f1o, Pablo Miguel. \" Sistemas Clasificadores Extendidos \n(XCS). Desarrollo de una librer\u00eda y comparaci\u00f3n CS vs. XCS \". \nProyecto Fin de Carrera, Universidad Carlos III de Madrid, 2002. \n[RM86] D. E. Rumelhart and J. L. McClelland. \u201c Parallel Distributed \nProcessing \u201d. Cambridge, MA: MIT Press, 1986. \n[RMS98] S. Ramaswamy, S. Mahajan,  and A. Silberschatz. \u201c On the \ndiscovery of interesting patterns in association rules \u201d. In Proc. \n1998 Int. Conf Very Large Data Ba ses (VLDB'98), pages 368-379, \nNew York, Aug. 1998. \n[SIL86] Silverman, B. W. (1986): \u201c Density Estimation for Statistics and \nData Analysis \", London and New York, Chapman and Hall. \n[SLK96] E. Simoudis, B. Livezey and  R. Kerber. Integrating Inductive and \nDeductive Reasoning for Data Mining . In U.M. Fayyad, G. \nPiatetsky-Shapiro, P. Smiyth, and R. Uthurusamy, editors, \nAdvances in knowledge Discove ry and Data Mining, pages 353-\n373. Cambridge, MA:AAAI/MIT Press, 1996  \n[SN88] K. Saito and R. Nakano. \u201c Medical diagnostic expert system based \non PDP model \u201d. In Proc. IEEE International Conf on Neural \nNetworks, volume 1, pages 225262. San Mateo, CA: 1988. \n[THUR99] B. Thuraisingham, \u201c Data Mining: Technologies, Techniques, Tools \nand Trends .\u201d CRC Press, 1999 \n[UTG88] P.E. Utgoff, \u201c An Incremental ID3 .\u201d In Proc. Fifth Int. Conf. Machine \nLearning, pages 107-120,  San Mateo, CA, 1988 \n[VIS95] Proceedings of the 1995 Workshop on Visualization and \nDatabases , Atlanta, GA, October 1997 (Ed: G. Grinstein) \n[VIS97] Proceedings of the 1997 Workshop on Visualization and Data \nMining  , Phoenix, AZ, October 19 97 (Ed: G. Grinstein).   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 245 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [WF00] H. Witten, and  E Frank (2000). Data Mining: Practical Machine \nLearning Tools and Techniques with Java Implementations . San \nFrancisco, CA: Morgan Kaufmann. \n[WI98] S.M Weiss, and Indurkhya. \u201c Predictive Data Mining \u201d. San \nFrancisco: Morgan Kaufmann 1998 \n[WK91] S.M. Weiss and C. A. Kulikowski. \u201c Computer Systems That Learn: \nClassification and Prediction Met hodsfrom Statistics, Neural Nets, \nMachine Learning, and Expert Systems \u201d. San Mateo, CA: Morgan \nKaufmann, 199 1. \n  [ACM90] Special Issue on Heterogeneous  Database Systems, ACM Computing \nSurveys, September 1990. \n [ACM91] Special Issue on Next Generation Da tabase Systems, Com-\nmunications of the ACM, October 1991. \n [ACM95] Special Issue on Digital Librar ies, Communications of the ACM, May \n1995. \n [ACM96a] Special Issue on Data Mini ng, Communications of the ACM, \nNovember 1996. \n [ACM96b] Special Issue on Electronics Commerce, Communications of the \nACM, June 1996. \n [ADRI96] Adriaans, P., and Zantinge, D., \"Data Mining,\" Addison Wesley, MA, \n1996. \n [AFCE97] Proceedings of the First Federal Data Mining Symposium, \nWashington D.C., December 1997. \n [AFSB83] Air Force Summer Study Board Report on Multilevel Secure \nDatabase Systems, Department of Defense Document, 1983.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 246 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n[AGRA93] Agrawal, A. et al.., \"Databas e Mining a Performance Perspective,\" \nIEEE Transactions on Knowledge ani Da ta Engineering, Vol. 5, December \n1993. \n [BANE87] Banerjee, J. et al., \"A Data Model for Object-Oriented Applications,\" \nACM Transactions on Office Infonnat ion Systems, Vol. 5, 1987. \n [BELL92] Bell D. an d Grimson, J., \"Distributed Database Systems,\" Addison \nWesley, MA, 1992. \n [BENS95] Bensley, E. et al., \"Evolv able Systems Initiative for Realtime \nCommand and Control Systems,\" Proceedings of the Ist IEEE Complex Systems Conference, Orl ando, FL, November 1995. \n [BERN87] Bernstein, P. et al., \"Concurrency Contro l and Recovery in Database \nSystems,\" Addison Wesley, MA, 1987. \n \n[BERR97] Berry, M. and Linoff, G., \"Data Mining Techniques for Marketing, \nSales, and Customer Suppor t,\" John Wiley, NY, 1997. \n [BRIS96] Briscoe, G., Caelli, T. \u201cA Co mpendium of Machine Learning. Vol. 1: \nSymbolic Machine Learning\n.\u201d Ablex Publishing Corporat ion, New Jersey, 1996. \n [BROD84] Brodie, M. et al., \"On C onceptual Modeling: Perspectives from \nArtificial Intelligence, Databases, and Programming Languages,\" Springer Verlag, NY, 1984. \n [BROD86] Brodie, M. and Mylopoulos, J., \"On Knowledge Base Management \nSystems,\" Springer Verlag, NY, 1986. \n [BROD88] Brodie, M. et al., \"Readings in Artificial Intelligence and Databases,\" \nMorgan Kaufmann, CA, 1988.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 247 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n[BROD95] Brodie M. and Stonebraker, M., \"Migrating Legacy Databases,\" \nMorgan Kaufmann, CA, 1995. \n [BUNE82] Buneman, P., \"Functional Da ta Model,\" ACM Transactions on \nDatabase Systems, 1983. \n [CARB98] Carbone, P., \"Data Mi ning,\" Handbook of Data Management, \nAuerbach Publications, NY, 1998 (Ed: B. Thuraisingham). \n [CERI84] Ceri, S. and Pelagatti, G., \"Distributed Databases, Principles and \nSystems,\" McGraw Hill, NY, 1984. \n [CHAN73] Chang C., and Lee R., \"Symbolic Logic and Mechanical Theorem \nProving,\" Academic Press, NY, 1973. \n [CHEN76] Chen, P., \"The Entity Relations hip Model - Toward a Unified View of \nData,\" ACM Transactions on Database Systems, Vol. 1, 1976. \n \n[CHOR94] Chorafas, D., \"Intelligent Mult imedia Databases,\" Prentice Hall, NJ, \n1994. \n [CLIF96a] Clifton, C, and Morey, D., \"D ata Mining Technology Survey,\" Private \nCommunication, Bedford, MA, December 1996. \n [CLIF96b] Clifton, C. and Marks, D., \"Security and Privacy Issues for Data \nMining,\" Proceedings of the ACM S IGMOD Conference Workshop on Data \nMining, Montreal, Canada, June 1996. \n [CLIF98a] Clifton, C., \"Image Mining,\" Private Communication, Bedford, MA, \nJuly 1998. \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 248 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [CLIF98b] Clifton C., \"Privacy Issues for Data Mining,\" Private Communication, \nBedford, MA, April 1998. \n [CODD70] Codd, E. F., \"A Relational M odel of Data for Large Shared Data \nBanks,\" Communications of the ACM, Vol. 13, 1970. \n [COOL98] Cooley, R., \"Tax onomy for Web Mining,\" Private Communication, \nBedford, MA, August 1998. \n [DARPA98] Workshop on Knowledge Di scovery in Databases, Defense \nAdvanced Research Projects Agency, Pittsburgh, PA, June 1998. \n \n[DAS92] Das, S., \"Deductive Databases  and Logic Programming,\" Addison \nWesley, MA, 1992. \n [DATE90] Date, C. J., \"An Introducti on to Database Management Systems,\" \nAddison Wesley, MA, 1990 (6th edition publi shed in 1995 by Addison Wesley). \n \n[DCI96] Proceedings of t he DCI Conference on Databases and Client Server \nComputing, Boston, MA, March 1996. \n [DE98] Proceedings of the 1998 Data Engineering Conferenc e, Orlando, FL, \nFebruary 1998. \n [DECI] Decision Support Journal, Else vier/North Holland Publications. \n [DEGR86] DeGroot, T., \"Probability and Statistics,\" Addison Wesley, MA, 1986. \n [DEVL88] Devlin, B. and Murphy, P.T., \u201cAn Architecture for a Bussiness and \nInformation System\u201d.  \nIBM Sys, J 27 , No 1, 1988  \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 249 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [DIGI95] Proceedings of the Advances in  Digital Libraries Conference, McLean, \nVA, May 1995, (Ed: N. Adam et al.). \n [DIST98] Workshop on Distributed and Parallel Data Mining, Melbourne, \nAustralia, April 1998. \n [DMH94] Data Management Handbook, Auer bach Publications, NY, 1994 (Ed: \nB. von Halle and D. Kull).  \n[DMH95] Data Management Handbook S upplement, Auerbach Publications, \nNY, 1995 (Ed: B. von Halle and D. Kull). \n [DMH96] Data Management Handbook S upplement, Auerbach Publications, \nNY, 1996 (Ed: B. Thuraisingham). \n [DMH98] Data Management Handbook S upplement, Auerbach Publications, \nNY, 1998 (Ed: B. Thuraisingharn). \n [DOD94] Proceedings of the 1994 DoD Database Colloquium, San Diego, CA, \nAugust 1994. \n [DOD95] Proceedings of the 1994 DoD Database Colloquium, San Diego, CA, \nAugust 1995. \n [DSV98] DSV Laboratory, \"Inducti ve Logic Programming,\" Private \nCommunication, Stockholm, Sweden, June 1998. \n [FAYY96] Fayyad, U. et al., \"Advanc ed in Knowledge Discovery and Data \nMining,\" MIT Press, MA, 1996. \n [FELD95] Feldman, R.  and Dagan, I., \"Knowledge Discovery in Textual \nDatabases (KDT),\" Proceedings of the 1995 Knowledge Discovery in \nDatabases Conference, Mont real, Canada, August 1995. \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 250 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [FOWL97] Fowler, M. et al., \"UML Dis tilled: Applying the Standard Object \nModeling Language,\" Addiso n Wesley, MA, 1997. \n [FROS86] Frost, R., \"On Knowledge Ba se Management Systems,\" Collins \nPublishers, U.K., 1986. \n [GALL78] Gallaire, H. and Minker, J., \"L ogic and Databases,\" Plenum Press, \nNY, 1978. \n [GOLD89] Goldberg, D., \u201cGenetic Algorit hms in Search, Optimization, and \nMachine Learning. Reading, MA: Addison-Weslwy, 1989 \n [GRIN95] Grinstein, G. and Th uraisingham, B., \"Data Mi ning and Visualization: \nA Position Paper,\" Proceedings  of the Workshop on Databas es in Visualization, \nAtlanta GA, October 1995. \n [GRUP98] Grupe F. and Owrang, M., \"Database Mining Tools\", in the \nHandbook of Data Management  Supplement, Auerbach P ublications, NY, 1998 \n(Ed: B.Thuraisingham). \n [HAN98] Han, J., \"Data Mining,\" Keyn ote Address, Second Pacific Asia \nConference on Data Mining, Mel bourne, Australia, April 1998. \n [HAN98] Han, J. and Kamber, M., \"Dat a Mining: Concepts and Techniques,\" \nACADEMIC Press, 2001. \n [HINK88] Hinke T., \"Inference and A ggregation Detection in Database \nManagement Systems,\" Proceedings of the 1988 Conference on Security and \nPrivacy, Oakland, CA, April 1988. \n [ICTA97] Panel on Web Mining, International  Conference on Tools for Artificial \nIntelligence, Newport Beach, CA, November 1997. \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 251 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [EEE89] \"Parallel Architectures for Da tabases,\" IEEE Tutorial, 1989 (Ed: A. \nHurson et al.). \n [IEEE91] Special Issue in Multidatabas e Systems, IEEE Computer, December \n1991. \n [IEEE98] IEEE Data Engineer ing Bulletin, June 1998. \n [IFIP] Proceedings of the IDFIP Conferenc e Series in Database Security, North \nHolland. \n [IFIP97] \"Web Mining,\" Proceedings of the 1997 IFIP Conference in Database \nSecurity, Lake Tahoe, CA, August 1997.. \n [ELP97] Summer School on Inductive Logic Programming, Prague, Czech \nRepublic, September 1998. \n [INMO88] Inmon, W., \"Data Architecture : The Information Paradigm,\" Wellesley, \nMas: QED Information Sciences, 1988. \n [INMO93] Inmon, W., \"Bui lding the Data Warehouse,\" John Wiley and Sons, \nNY, 1993. \n [JUNG98] Junglee Corporation, \"Virtual  Database Technology, XML, and the \nEvolution of the Web,\" IEEE Data E ngineering Bulletin, June 1998 (authors: \nPrasad and Rajaraman). \n [KDD95] Proceedings of the First Knowledge Discovery in Databases \nConference, Montreal , Canada, August 1995. \n [KDD96] Proceedings of the Second Knowledge Discovery in Databases \nConference, Portland, OR, August 1996.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 252 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n[KDD97] Proceedings of the Third Knowledge Di scovery in Databases \nConference, Newport Beach, CA, August 1997. \n [KDD98] Proceedings of the Fourth Knowledge Di scovery in Databases \nConference, New York, NY, August 1998. \n [KDP98] Panel on Privacy Issues for Data Mining, Knowledge Discovery in \nDatabases Conference, New York, NY, August 1998. \n [KDT98] Tutorial on Commercial Data Mi ning Tools, Knowledge Discovery in \nDatabases Conference, August 1998 (Pre senters: J. Elder  and D. Abbott) \n [KIM85] Kim, W.  et al., \"Query Processing in Database Systems,\" Springer \nVerlag, NY, 1985. \n [KODR90] Kodratoff, Y. and Michalski, R.S., \u201cMachine Learning and Artificial \nInteligence Approach, Vol. 3, San Mateo, CA: Morgan Kaufmann, 1990 \n \n[KORT86] Korth, H. and Silberschatz,  A., \"Database System Concepts,\" \nMcGraw Hill, NY, 1986. \n [KOWA74] Kowalski, R. A., \"Predica te Logic as a Programming Language,\" \nInformation Processing 74, Stockhol m, North Holland Publications, 1974. \n [LIN97] Lin, T.Y., (Editor) \"Rough Sets and Data Mining,\" Kluwer Publishers, \nMA, 1997. \n [LLOY87] Lloyd, J., \"F oundations of Logic Progra mming,\" Springer Verlag, \nGermany, 1987. \n [LOOM95] Loomis, M., \"Object Databas es,\" Addison Wesley, MA, 1995.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 253 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n[MAIE83] Maier, D., \"Theor y of Relational Databases,\" Computer Science \nPress, MD, 1983. \n [MATTO98] Mattox, D. et al., \"So ftware Agents for Data Management,\" \nHandbook of Data Management, Auerbach  Publications, NY, 1998 (Ed: B. \nThuraisingham). \n [MDDS94] Proceedings of the Massive Digital Data Systems Workshop, \npublished by the Community Management Staff, Washington D.C., 1994. \n [MERL97] Merlino, A. et al., \"Br oadcast News Navigation using Story \nSegments,\" Proceedings of the 1997 ACM Multimedia Conf erence, Seattle, WA, \nNovember 1998. \n [META96] Proceedings of the Ist IEEE Metadata Conference, Silver Spring, \nMD, April 1996 (Originally pub lished on the web, Editor : R. Musick, Lawrence \nLivermore National Laboratory). \n \n[MICH92] Michalewicz, Z., \u201cGenetic Algori thms + Data Structures = Evolutions \nPrograms.\u201d, NY: Springer-Verlag, 1992. \n [MINK88] Minker, J., (Editor) \"Foundatio ns of Deductive Databases and Logic \nProgramming,\" Morgan Kaufmann, CA, 1988 (Editor). \n [MIT] Technical Reports on Data Quality,  Sloan School, Massachusetts Institute \nof Technology, Cambridge, MA. \n [MIT96] Mitchell, M., \u201cAn Introduction to Genetic Algorithms.\u201d Cambridge, \nMA:MIT Press, 1996 \n \n[MITC97] Mitchell, T., \"Machine Learning,\" McGraw Hill, NY, 1997. \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 254 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [MORE98a] Morey, D., \"Knowledge Mana gement Architecture,\" Handbook of \nData Management, Auerbach Publications , NY, 1998 (Ed: B. Thuraisingham). \n [MORE98b] Morey, D., \"Web Mining,\" Pr ivate Communication, Bedford, MA, \nJune 1998. \n [MORG88] Morgenstern, M., \"Security and Inference in Mult ilevel Database and \nKnowledge Base Systems,\" Proc eedings of the 1987 ACM SIGMOD \nConference, San Francisco, CA, June 1987. \n [NG97] Ng, R., \"Image Mining,\" Privat e Communication, Vancouver, British \nColumbia, December 1997. \n [NISS96] Panel on Data Warehousing, Da ta Mining, and Security, Proceedings \nof the 1996 National Information Systems Security Conference, Baltimore, MD, \nOctober 1996. \n [NISS97] Papers on Internet Securi ty, Proceedings of the 1997 National \nInformation Systems Conference,  Baltimore, MD, October 1997. \n [NSF90] Proceedings of t he Database Systems Worksh op, Report published by \nthe National Science Foundation, 1990 (also in ACM SIGMOD Record, \nDecember 1990). \n [NSF95] Proceedings of t he Database Systems Workshop, Report published by \nthe National Science Foundation, 1995 (a lso in ACM SIGMOD Record, March \n1996). \n [NWOS96] Nwosu, K. et al., (Editors) \"Multimedia Database Systems, Design \nand Implementation Strategies.\" Kluwer Publications, MA, 1996. \n [ODMG93] \"Object Database Standar d: ODMB 93,\" Object Database \nManagement Group, Morgan Kaufmann, CA, 1993. \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 255 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [OMG95] \"Common Object Request Broker  Architecture and Specification,\" \nOMG Publications, John Wiley, NY, 1995. \n [ORFA94] Orfali, R. et al., \"E ssential, Client Server Su rvival Guide,\" John Wiley, \nNY, 1994. \n [ORFA96] Orfali, R. et al., \"The Essential,  Distributed Objects Survival Guide,\" \nJohn Wiley, NY, 1994. \n [PAKDD97] Proceedings of the Kn owledge Discovery in Databases \nConference, Singapore, February 1997. \n [PAKDD98] Proceedings of  the Second Knowledge Di scovery in Databases \nConference, Melbourne, Australia, April 1998. \n [PAW91] Pawlak, Z. \u201cRough Sets, Theoret ical  Aspects of Reasoning about \nData.\u201d Boston: Kluwer Academic Publishers, 1991 \n \n[PRAB97] Prabhakaran, B., \"Multim edia Database Systems,\" Kluwer \nPublications, MA, 1997. \n [QUIN79] Quinlan, J.R.):\u201dDiscovering  Rules from Large Collections of \nExamples\u201d, In \nExpert Systems in the Microelectronic Age , Michie, D. (Ed.), \nEdimburgo University Press, Edimburgo. 1979 \n [QUIN86] Quinlan, J.R.: \u201cInduction of  Decision Trees (ID3 algorithm)\u201d,\n Machine \nLearning J ., vol. 1, n\u00fam. 1, pp. 81-106. 1986 \n \n[QUIN87] Quinlan, J.R.: \u201cS implifying decision trees\u201d, International Journal of \nMan-Machine Studies , n\u00fam. 27, pp. 221-234. 1987 \n [QUIN88] Quinlan, J.R.: \u201cDecision trees and multivalued attributes\u201d, \nMachine \nIntelligence , n\u00fam. 11, pp. 305-318. 1988 \n \n[QUIN89] Quinlan, J.R.: \u201cUnknown attr ibute values in induction\u201d. In Proc. 6th Int. \nWorkshop o n  Machine Intelligence ,  pp. 164-168, Ithaca, NY, June. 1989 \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 256 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [QUIN90] Quinlan, J.R., \"Learning l ogic definitions from relations.\u201d Machine \nLearning , 5:139-166, 1990 \n [QUIN93] Quinlan, J.R., \"C4.5: Pr ograms for Machine Learnirig,\" Morgan \nKaufmann, CA, 1993. \n [QUIN96] Quinlan, J.R., \"B agging boosting, and C4.5\u201d In Proc. 13 th Natl. Conf \nArtificial Intelligence (AAAI\u201996) pages 725-730, Portland, OR, Aug. 1996 \n [RAMA94] Ramakrishnan, R., (Editor) Ap plications of Deductive Databases, \nKluwer Publications, MA, 1994. \n [ROSE98] Rosenthal, A., \"Multi-Tier Architecture,\" Private Communication, \nBedford, MA, August 1998. \n [RUME86] Rumelhart, D.E., HINTON, G.E. and Williams, R.J., \u201cLearning \nInternal representations by error pr opagation.\u201d In D.E. Rumelhart and J.L. \nMacClelland, editors, \u201cParallel Distri buted Processing.\u201d Cambridge, Ma: MIT \nPress 1986 \n [SIGM96] Proceedings of the ACM SIGMOD Workshop on Data Mining, \nMontreal, Canada, May 1996. \n [SIGM98] Proceedings of t he 1998 ACM SIGMOD Conf erence, Seattle, WA, \nJune 1998. \n [SIMO95] Simoudis, E. et al., \"Recon Data Mining S ystem,\" Technical Report, \nLockheed Martin Corporation, 1995. \n [SQL3] \"SQL3,\" American Na tional Standards Institute,  Draft, 1992 (a version \nalso presented by J. Melton at t he Department of Navy's DISWG NGCR \nmeeting, Salt Lake City, UT, November 1994). \n   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 257 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda [STAN98] Stanford Databas e Group Workshop, Jungalee Virtual Relational \nDatabase, September 1998 (a lso appeared in IEEE Data  Engineering Bulletin, \nJune 1998). \n [THUR87] Thuraisingham, B., \"Securit y Checking in Relational Database \nSystems Augmented by an Inference Engine ,\" Computers and Security, Vol. 6, \n1987. \n [THUR90a] Thuraisingham, B., \"Nonmon otonic Typed Multilevel Logic for \nMultilevel Secure Database System s,\" MITRE Report, June 1990 (also \npublished in the Proceedings of the 1992 Computer Security Foundations \nWorkshop, Franconia, NH, June 1991). \n [THUR90b] Thuraisingham, B., \"Recursion T heoretic Properties of the Inference \nProblem,\" MITRE Report, June 1990 (als o presented at the 1990 Computer \nSecurity Foundations Works hop, Franconia, NH, June 1990). \n [THUR90c] Thuraisingham, B., \"Novel Approaches to Handle the Inference \nProblem,\" Proceedings of the 1990 RADC Workshop in  Database Security, \nCastile, NY, June 1990. \n [THUR91] Thuraisingham, B., \"On the Use of Conceptual Structures to Handle \nthe Inference Problem,\" Proceedings of the 1991 IFIP Database Security \nConference, Shepherdstown, `WVA, November 1991. \n [THUR93] Thuraisingham, B.  et al., \"Design and Impl ementation of a Database \nInference Controller,\" Data and Knowled ge Engineering Journal , North Holland, \nVol. 8, December 1993. \n [THUR95] Thuraisingham, B. and Ford, W., \"Security Constraint Processing in a \nMultilevel Secure Distributed Da tabase Management System,\" IEEE \nTransactions on Knowledge and Da ta Engineering, Vol. 7, 1995. \n [THUR96a] Thuraisingham, B., \"Data War ehousing, Data Mining, and Security \n(Version I),\" Proceedings of the 10th IFIP Database Security Conference, \nComo, Italy, 1996.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 258 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n[THUR96b] Thuraisingham, B., \"Inter net Database Management,\" Database \nManagement, Auerbach Publications, NY, 1996. \n [THUR96c] Thuraisingham, B., \"Interacti ve Data Mining and the World Wide \nWeb,\" Proceedings of Compugraphics Conf erence, Paris, France, December \n1996. \n [THUR97] Thuraisingham, B., \" Data  Management System s Evolution and \nInteroperation,\" CRC Press, FL, May 1997. \n [THUR98] Thuraisingham, B., \"Data Ware housing, Data Mining, and Security \n(Version 2),\" Keynote Address at Sec ond Pacific Asia Conference on Data \nMining, Melbourne, Australia, April 1998. \n [THUR99] Thuraisingham, B. , \u201cData Mining: Technologies, Techniques, Tools \nand Trends.\u201d CRC Press, 1999 \n \n[TKDE93] Special Issue on Data Mini ng, IEEE Transactions on Knowledge and \nData Engineering, December 1993. \n [TKDE96] Special Issue on Data Mini ng, IEEE Transactions on Knowledge and \nData Engineering, December 1996. \n [TRUE89] Trueblood, R. and Potter, W., \"H yper-Semantic Data Modeling,\" Data \nand Knowledge Engineering Journal, Vol. 4, North Holland, 1989. \n [TSUR98] Tsur, D. et al., \"Query Flocks:  A Generalization of  Association Rule \nMining,\" Proceedings of the 1998 ACM S IGMOD Conference, Seattle, WA, \nJune 1998. \n [TSIC82] Tsichritzis, D.  and Lochovsky, F., \"Data Models,\" Prentice Hall, NJ, \n1982.   Bibliograf\u00eda \nT\u00e9cnicas de An\u00e1lisis de Datos  P\u00e1gina 259 de 266 \n \u00a9 Jos\u00e9 M. Molina / Jes\u00fas Garc\u00eda  \n[ULLM88] Ullman, J. D., \"Principle s of Database and Knowledge Base \nManagement Systems,\" Volumes I and 11, Computer Science Press, MD 1988. \n [UTG88] Utgoff, P.E., \u201cAn Incremental ID 3.\u201d In Proc. Fifth Int. Conf. Machine \nLearning, pages 107-120,  San Mateo, CA, 1988 \n [VIS95] Proceedings of t he 1995 Workshop on Visualiz ation and Databases, \nAtlanta, GA, October 1 997 (Ed: G. Grinstein) \n [VIS97] Proceedings of the 1997 Workshop on Visualiz ation and Data Mining, \nPhoenix, AZ, October 1997 (Ed: G. Grinstein). \n [VLDB98] Proceedings of t he Very Large Database Conf erence, New York City, \nNY, August 1998. \n [WIED92] Wiederhold, G., \"Mediators in t he Architecture of Future Information \nSystems,\" IEEE Comp uter, March 1992. \n \n[WOEL86] Woelk, D. et al., \"An Obje ct-Oriented Approach to Multimedia \nDatabases,\" Proceedings of the ACM SIGMOD Conference, Washington DC, \nJune 1986. \n [XML98] Extended Markup Language, Docu ment published by the World Wide \nWeb Consortium, Cambridge, MA, February 1998.  \n ", "language": "PDF", "image": "PDF", "pagetype": "PDF", "links": "PDF"}